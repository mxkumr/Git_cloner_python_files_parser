repo_url,file_name,keyword_count,keyword_percentage,identifier_count,identifier_percentage,literal_count,literal_percentage,constant_count,constant_percentage,comment_count,comment_percentage,non_english_count,non_english_percentage,function_count,class_count,variable_count,docstring_count,keywords,identifiers,module_attrs,function_names,class_names,variables,docstrings,literals,constants,comments,non_english
https://github.com/d2l-ai/d2l-zh,setup.py,0,0.0,3,18.75,12,75.0,1,6.25,0,0.0,0,0.0,0,0,1,0,,"d2l, requirements, setuptools","d2l.__version__, setuptools.find_packages, setuptools.setup",,,requirements,,">=3.5, D2L Developers, Dive into Deep Learning, MIT-0, d2l, d2l.devs@gmail.com, https://d2l.ai, jupyter==1.0.0, matplotlib==3.5.1, numpy==1.21.5, pandas==1.2.4, requests==2.25.1",True,,
https://github.com/d2l-ai/d2l-zh,submit-job.py,0,0.0,76,36.71,112,54.11,15,7.25,4,1.93,0,0.0,3,0,34,0,,"__name__, add, add_argument, argparse, args, batch, boto3, botocore, client, cloudwatch, command, datetime, describeJobsResponse, describe_jobs, dict, endTime, event, get, get_log_events, int, jobDefinition, jobId, jobName, jobQueue, jobType, job_type, job_type_info, keys, kwargs, lastTimestamp, len, list, logEvents, logGroupName, logStreamName, main, name, nextToken, nowInMillis, open, original_repo, os, parameters, parse_args, parser, print, printLogs, profile, random, re, region, remote, running, safe_to_use_script, save_path, saved_output, session, set, sn, source_ref, spin, spinner, startTime, startswith, status, status_set, str, submitJobResponse, submit_job, sys, time, timeout, timestamp, wait, work_dir, write","argparse.ArgumentDefaultsHelpFormatter, argparse.ArgumentParser, boto3.Session, botocore.compat.total_seconds, botocore.config.Config, datetime.datetime, datetime.now, datetime.utcfromtimestamp, datetime.utcnow, os.environ, random.randint, re.sub, sys.exit, sys.stdout.flush, time.sleep","main, nowInMillis, printLogs",,"args, batch, cloudwatch, config, describeJobsResponse, endTime, event, jobDefinition, jobId, jobName, jobQueue, jobType, job_type, job_type_info, kwargs, lastTimestamp, logEvents, logGroupName, logStreamName, nextToken, parameters, parser, running, safe_to_use_script, session, sn, spin, spinner, startTime, status, status_set, submitJobResponse, timestamp, wait",,", 
, Job [%s - %s] is %-9s... %s, Job [{}, {}] is RUNNING., "", -, --command, --job-type, --name, --original-repo, --profile, --region, --remote, --safe-to-use-script, --save-path, --saved-output, --source-ref, --timeout, --wait, --work-dir, -push, -release, ., .000, /, /aws/batch/job, =, Batch_JobID=, COMMAND, D2L-CI-CPU, D2L-CI-GPU, Default region when creating new connections, FAILED, False, GITHUB_ENV, Job [{} - {}] {}, None, ORIGINAL_REPO, Output [{}]:
 {}, REMOTE, RUNNING, SAFE_TO_USE_SCRIPT, SAVED_OUTPUT, SAVE_PATH, SOURCE_REF, SUCCEEDED, Submitted job [{} - {}] to the job queue [{}], True, WORK_DIR, Z, [^A-Za-z0-9_\-], [{}] {}, \, __main__, a, attemptDurationSeconds, batch, batch/temp/{}, batch_jobid, block wait until the job completes. Non-zero exit code if job fails., ci-cpu, ci-cpu-push, ci-cpu-release, ci-gpu, ci-gpu-mxnet, ci-gpu-paddle, ci-gpu-tf, ci-gpu-torch, command to run, container, d2l-ci, d2l-ci-cpu-builder-push:7, d2l-ci-cpu-builder-release:1, d2l-ci-cpu-builder:2, d2l-ci-zh-gpu-mxnet:1, d2l-ci-zh-gpu-paddle:1, d2l-ci-zh-gpu-tf:1, d2l-ci-zh-gpu-torch:1, d2l-zh, events, git repo address. https://github.com/d2l-ai/d2l-zh, git rev-parse HEAD | tee stdout.log, https://github.com/d2l-ai/d2l-zh, job timeout in seconds, jobId, job_definition, job_queue, jobs, logGroupName, logStreamName, logs, master, message, name of the job, name of the repo, nextForwardToken, nextToken, output to be saved, relative to working directory. it can be either a single file or a directory, profile name of aws account., ref in d2l-zh main github. e.g. master, refs/pull/500/head, s3 path where files are saved., startFromHead, startTime, status, store_true, timeout, timestamp, type of job to submit., us-west-2, whether the script changes from the actor is safe. We assume it is safe if the actor has write permission to our repo, working directory inside the repo. e.g. scripts/preprocess, |","0, 1, 10, 1000, 1000.0, 128, 1970, 20, 23, 5, 7200, 80, False, None, True","# Create push job types for GPUs with same definitions, # Enforce AWS Batch jobName rules, # Export Batch_JobID to Github Actions Environment Variable, # wrap command with double quotation mark, so that batch can treat it as a single command",
https://github.com/d2l-ai/d2l-zh,mxnet.py,0,0.0,851,48.38,368,20.92,58,3.3,175,9.95,307,17.45,141,33,390,145,,"Accumulator, Activation, AddNorm, AdditiveAttention, Animator, ArrayDataset, AttentionDecoder, B, BERTEncoder, BERTModel, BananasDataset, BatchNorm, Benchmark, Block, Compose, Conv2D, DATA_HUB, DATA_URL, DataLoader, Dataset, Decoder, Dense, DotProductAttention, Dropout, Embedding, Encoder, EncoderBlock, EncoderDecoder, FashionMNIST, GRU, GlobalAvgPool2D, IPython, K, L2Loss, LayerNorm, MaskLM, MaskedSoftmaxCELoss, MultiHeadAttention, NextSentencePred, Normal, NotImplementedError, P, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, Rectangle, Residual, Resize, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Sequential, SoftmaxCELoss, SoftmaxCrossEntropyLoss, T, Timer, ToTensor, TokenEmbedding, Trainer, TransformerEncoder, USE_MXNET, USE_PYTORCH, USE_TENSORFLOW, VOCSegDataset, VOC_CLASSES, VOC_COLORMAP, Vocab, W_k, W_o, W_q, W_v, X, X_shard, X_shards, X_valid_len, Xavier, Xs, Y, Y_hat, Y_valid_len, Ys, _WikiTextDataset, __call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, __name__, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _read_wiki, _replace_mlm_tokens, _token_freqs, abs, acc, accuracy, add, add_patch, addnorm1, addnorm2, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, annotate, append, arange, areas1, areas2, argmax, args, argsort, array, as_in_context, as_in_ctx, asnumpy, assign_anchor_to_bbox, assigned_bb, astype, attach_grad, attention, attention_weight_seq, attention_weights, avg, ax, axes, backward, base_dir, batch, batch_class_labels, batch_dot, batch_idx, batch_mask, batch_offset, batch_size, batchify, bb_idx, bbox, bbox_mask, bbox_offset, bbox_to_rect, bboxes, begin_state, below_min_idx, bleu, blk, blks, bn1, bn2, bos, box_area, box_center_to_corner, box_corner_to_center, box_idx, box_iou, box_j, boxes, boxes1, boxes2, boxes_per_pixel, build_array_nmt, c_anc, c_assigned_bb, cache_dir, candidate_pred_positions, candidates, center, center_h, center_w, centers, char, cla, class_id, class_labels, clear_output, cls_prob, cls_probs, cmap, cmp, col_discard, collect_params, collections, color, colorbar, colormap, colormap2label, colors, combined, concat, concatenate, conf, config_axes, content, contexts, contexts_negatives, contour, conv1, conv2, conv3, copyfile, corpus, corr2d, cos, cosh, count_corpus, counter, counts, cpu, crop_size, csv_data, csv_fname, ctx, cumsum, cur_len, cx, cy, d2l, data, data_arrays, data_dir, data_iter, data_iter_fn, dataset, dec_X, dec_input, dec_state, decoder, default_values, dense, dense1, dense2, description, detach, device, devices, dict, display, dot, download, download_all, download_extract, draw, dropout, dtype, elem, elems, embed_size, embedding, embedding_name, enc_X, enc_outputs, enc_valid_len, encoded_X, encoder, enumerate, epoch, eps, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_accuracy_gpus, evaluate_loss, examples, exp, expand_dims, extend, extract_text, extractall, eye, f_grad, feature, feature_dim, features, ffn, ffn_num_hiddens, ffn_num_outputs, fig, figsize, file_name, filename, filter, first_block, fixed_crop, flatten, float, float32, fmt, fmts, fname, folder, folder_name, forward, forward_fn, fp, freq, full, gca, generator, genfromtxt, get, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_input, get_negatives, get_params, get_tokens_and_segments, get_xaxis, get_yaxis, gpu, grad, grad_clipping, grid, ground_truth, has_one_axis, hasattr, hashlib, height, hexdigest, hidden, hist, hyperparams, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, imgs, imread, imshow, in_height, in_width, index, indices, indices_true, inds, init_state, initial_indices, initial_indices_per_batch, initialize, inputs, insert, int, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, iou_threshold, is_next, is_train, isinstance, items, iterrows, jaccard, keep, keys, kwargs, label, label_count, label_seq, label_set, label_subs, label_tokens, labels, legend, len, len_label, len_pred, lines, linreg, linspace, list, ln, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, log, loss, lower, lr, ls, masked_X, masked_softmax, masked_token, masks, math, matmul, matplotlib, matplotlib_inline, matrices, matrix, max, max_idx, max_ious, max_len, max_num_mlm_preds, max_tokens, max_window_size, maximum, mean, meshgrid, metric, min, min_freq, minimum, mlm, mlm_Y_hat, mlm_Y_shard, mlm_Y_shards, mlm_input_tokens, mlm_l, mlm_ls, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mlm_weights_X_shard, mlm_weights_X_shards, mlp, mnist_test, mnist_train, multibox_detection, multibox_prior, multibox_target, mxnet, n_valid_per_label, name, ncols, ndim, neg, negative, negatives, next_sentence, nms, nms_threshold, nn_Module, no_space, non_keep, nonzero, norm, normal, normalize_image, nrows, nsp, nsp_Y_hat, nsp_data_from_paragraph, nsp_l, nsp_labels, nsp_ls, nsp_y_shard, nsp_y_shards, num_anchors, num_batches, num_channels, num_classes, num_cols, num_epochs, num_examples, num_gpus, num_gt_boxes, num_heads, num_hiddens, num_layers, num_matches, num_mlm_preds, num_noise_words, num_pred_positions, num_preds, num_ratios, num_residuals, num_rows, num_sizes, num_steps, num_subseqs, num_tokens, num_workers, numpy, obj, offset, offset_boxes, offset_h, offset_inverse, offset_pred, offset_preds, offset_w, offset_wh, offset_xy, one_hot, ones, ones_like, open, os, out, out_grid, output, output_concat, output_seq, outputs, padding_token, pandas, paragraph, paragraphs, param, params, parts, patch, patches, pcm, plot, plt, population, pos, pos_embedding, pos_encoding, pos_threshold, power, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_X_shard, pred_positions_X_shards, pred_positions_and_labels, pred_seq, pred_shard, pred_shards, pred_tokens, predict, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, predicted_bb, predicted_bbox, preds, prefix, premise, premises, preprocess_nmt, prev_char, print, property, queries, rand, randn, random, random_crop, range, ratio_tensor, ratios, raw_text, rcParams, re, read, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, readlines, record, rect, reduce_mean, reduce_sum, relu, remove, reorg_test, reorg_train_valid, repeat, replace, requests, reserved_tokens, reset, reshape, resize, resnet18, resnet_block, results, review, rgb_mean, rgb_std, rnn, rnn_layer, round, row, row_axes, row_discard, row_matrices, rows, rstrip, s1, s2, sampling_weights, save_attention_weights, scale, score, scores, segment_embedding, segments, segments_X_shard, segments_X_shards, self, sentence, sentences, seq_data_iter_random, seq_data_iter_sequential, sequence, sequence_mask, set, set_axes, set_figsize, set_hatch, set_index, set_matplotlib_formats, set_title, set_xlabel, set_xlim, set_xscale, set_ylabel, set_ylim, set_yscale, sgd, sha1, sha1_hash, shape, shift_x, shift_y, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, shutil, sin, sinh, size, size_tensor, sizes, softmax, sorted, source, speed, split, split_and_load, split_batch, split_batch_multi_inputs, split_f, sqrt, squared_loss, squeeze, src_array, src_sentence, src_tokens, src_valid_len, src_vocab, stack, start, state, states, std, step, steps, steps_h, steps_w, stop, str, strides, strip, subplots, subsample, subsampled, sum, super, swapaxes, synthetic_data, sys, tanh, tarfile, target, target_dir, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, theta, tik, tile, time, timer, times, titles, to_tokens, token, token_embedding, token_freqs, token_ids, token_to_idx, tokenize, tokenize_nmt, tokens, tokens_X_shard, tokens_X_shards, tokens_a, tokens_b, tr_name, train_2d, train_acc, train_acc_sum, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_data, train_epoch_ch3, train_epoch_ch8, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_seq2seq, train_set, train_tokens, trainer, trainer_fn, transforms, transpose, transpose_output, transpose_qkv, true, trues, truncate_pad, try_all_gpus, try_gpu, tuple, txt_fname, union_areas, unique, unk, unknown_idx, update, updater, url, use_1x1conv, use_bias, use_random_iter, use_svg_display, utils, val_iter, valid_len, valid_lens, valid_lens_x_shard, valid_lens_x_shards, valid_ratio, values, vecs, vision, voc_colormap2label, voc_dir, voc_label_indices, voc_rand_crop, vocab, vocab_size, w, w_v, waitall, weights, width, window_size, write, x1, x2, xlabel, xlim, xlist, xscale, xy, xytext, y1, y2, y_hat, y_shard, y_shards, ylabel, ylim, ylist, yscale, zeros, zip, zipfile","IPython.display, collections.Counter, collections.defaultdict, hashlib.sha1, math.exp, math.floor, math.pow, math.sqrt, matplotlib.pyplot, matplotlib_inline.backend_inline, mxnet.autograd, mxnet.context, mxnet.gluon, mxnet.gluon.data.vision.transforms, mxnet.gluon.nn, mxnet.gluon.rnn, mxnet.image, mxnet.init, mxnet.np, mxnet.npx, os.listdir, os.makedirs, os.path.dirname, os.path.exists, os.path.join, os.path.splitext, pd.read_csv, random.choice, random.choices, random.randint, random.random, random.shuffle, random.uniform, re.sub, requests.get, shutil.copy, sys.modules, sys.platform.startswith, tarfile.open, time.time, zipfile.ZipFile","__call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _read_wiki, _replace_mlm_tokens, accuracy, add, annotate, assign_anchor_to_bbox, attention_weights, avg, batchify, bbox_to_rect, begin_state, bleu, box_center_to_corner, box_corner_to_center, box_iou, build_array_nmt, copyfile, corr2d, count_corpus, cumsum, data, download, download_all, download_extract, draw, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_accuracy_gpus, evaluate_loss, extract_text, filter, forward, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_negatives, get_tokens_and_segments, grad_clipping, has_one_axis, init_state, keep, linreg, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, masked_softmax, multibox_detection, multibox_prior, multibox_target, nms, no_space, normalize_image, offset_boxes, offset_inverse, plot, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, preprocess_nmt, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, reorg_test, reorg_train_valid, reset, resnet18, resnet_block, seq_data_iter_random, seq_data_iter_sequential, set_axes, set_figsize, sgd, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, split_batch, split_batch_multi_inputs, squared_loss, start, stop, subsample, sum, synthetic_data, to_tokens, token_freqs, tokenize, tokenize_nmt, train_2d, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_epoch_ch3, train_epoch_ch8, train_seq2seq, transpose_output, transpose_qkv, truncate_pad, try_all_gpus, try_gpu, unk, use_svg_display, voc_colormap2label, voc_label_indices, voc_rand_crop","Accumulator, AddNorm, AdditiveAttention, Animator, AttentionDecoder, BERTEncoder, BERTModel, BananasDataset, Benchmark, Decoder, DotProductAttention, Encoder, EncoderBlock, EncoderDecoder, MaskLM, MaskedSoftmaxCELoss, MultiHeadAttention, NextSentencePred, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, Residual, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Timer, TokenEmbedding, TransformerEncoder, VOCSegDataset, Vocab, _WikiTextDataset","B, DATA_HUB, DATA_URL, USE_MXNET, USE_PYTORCH, USE_TENSORFLOW, VOC_CLASSES, VOC_COLORMAP, X, X_shard, X_shards, X_valid_len, Xs, Y, Y_hat, Y_valid_len, Ys, abs, acc, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, arange, areas1, areas2, argmax, array, assigned_bb, astype, attention_weight_seq, ax, axes, b, base_dir, batch, batch_class_labels, batch_idx, batch_mask, batch_offset, batch_size, bb_idx, bbox, bbox_mask, bbox_offset, below_min_idx, blk, bos, box_area, box_idx, box_j, boxes, boxes_per_pixel, c_anc, c_assigned_bb, candidate_pred_positions, center, center_h, center_w, centers, char, class_id, class_labels, cls_prob, cmp, col_discard, color, colormap, colormap2label, colors, combined, concat, conf, contexts, contexts_negatives, corpus, cos, cosh, counter, counts, csv_data, csv_fname, cur_len, cx, cy, d, d2l, data, data_arrays, data_dir, data_iter, dataset, dec_X, dec_input, dec_state, device, devices, elem, elems, enc_X, enc_outputs, enc_valid_len, encoded_X, epoch, examples, exp, eye, feature, features, fig, figsize, file_name, float32, fmt, fname, folder_name, fp, freq, generator, get_input, h, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, in_height, in_width, index, indices, indices_true, inds, initial_indices, initial_indices_per_batch, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, is_next, jaccard, keep, keys, l, label, label_count, label_set, label_subs, label_tokens, labels, legend, len_label, len_pred, lines, linspace, log, loss, ls, masked_X, masked_token, masks, matmul, matrix, max_idx, max_ious, max_len, max_num_mlm_preds, meshgrid, metric, mlm_Y_hat, mlm_Y_shard, mlm_input_tokens, mlm_l, mlm_ls, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mlm_weights_X_shard, mnist_test, mnist_train, n, n_valid_per_label, neg, negative, negatives, net, next_sentence, nn_Module, non_keep, norm, normal, nsp_Y_hat, nsp_data_from_paragraph, nsp_l, nsp_labels, nsp_ls, nsp_y_shard, num_anchors, num_batches, num_classes, num_cols, num_gt_boxes, num_matches, num_mlm_preds, num_pred_positions, num_ratios, num_rows, num_sizes, num_subseqs, num_tokens, num_workers, numpy, obj, offset, offset_h, offset_pred, offset_w, offset_wh, offset_xy, ones, out, out_grid, output, output_concat, output_seq, outputs, paragraph, paragraphs, param, params, parts, patch, patches, pcm, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_X_shard, pred_positions_and_labels, pred_shard, pred_shards, pred_tokens, predict, predicted_bb, predicted_bbox, preds, premise, premises, queries, r, rand, randn, ratio_tensor, raw_text, rect, reduce_mean, reduce_sum, reserved_tokens, reshape, results, review, row, row_axes, row_discard, row_matrices, rows, s, s1, s2, sampling_weights, score, scores, segments, segments_X_shard, sentence, sentences, sequence, sha1, sha1_hash, shape, shift_x, shift_y, sin, sinh, size, size_tensor, source, speed, src_array, src_tokens, src_valid_len, src_vocab, stack, state, steps_h, steps_w, subsampled, tanh, target, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, timer, titles, to, token, token_ids, tokens, tokens_X_shard, tokens_a, tokens_b, train_acc, train_acc_sum, train_data, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_set, train_tokens, trainer, trans, transpose, true, trues, txt_fname, union_areas, unique, updater, url, val_iter, valid_len, valid_lens, valid_lens_x_shard, values, vecs, voc_dir, vocab, w, weights, window_size, x, x1, x2, y, y1, y2, y_hat, y_shard, y_shards, zeros","BERT模型

Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

Defined in :numref:`subsec_bert_input_rep`, BERT编码器

Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, GloVe嵌入, Transformer编码器

Defined in :numref:`sec_transformer`, Transformer编码器块

Defined in :numref:`sec_transformer`, 一个用于加载VOC数据集的自定义数据集

Defined in :numref:`sec_semantic_segmentation`, 一个用于加载香蕉检测数据集的自定义数据集

Defined in :numref:`sec_object-detection-dataset`, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`, 下采样高频词

Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`, 使用多个GPU计算数据集上模型的精度

Defined in :numref:`sec_multi_gpu_concise`, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`, 启动计时器, 在n个变量上累加, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在非Windows的平台上，使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`, 均方损失

Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

Defined in :numref:`sec_transformer`, 多头注意力

Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

Defined in :numref:`sec_multi_gpu`, 将多输入'X'和'y'拆分到多个设备

Defined in :numref:`sec_natural-language-inference-attention`, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

Defined in :numref:`sec_anchor`, 显示矩阵热图

Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`, 构造一个Gluon数据迭代器

Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

Defined in :numref:`subsec_labeling-anchor-boxes`, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练

Defined in :numref:`sec_image_augmentation`, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`, 绘制图像列表

Defined in :numref:`sec_fashion_mnist`, 绘制数据点

Defined in :numref:`sec_calculus`, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

Defined in :numref:`sec_bert`, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`, 计算BLEU

Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

Defined in :numref:`sec_anchor`, 计算二维互相关运算

Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型一个迭代周期（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

Defined in :numref:`sec_calculus`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

Defined in :numref:`sec_kaggle_cifar10`, 读取所有VOC图像并标注

Defined in :numref:`sec_semantic_segmentation`, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu()]

Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`",", 	, 
,  ,  . ,  examples,  examples/sec on ,  sec,  sec/epoch,  tokens/sec on ,  training examples,  validation examples,  不存在于 ,  词元/秒 , #1f77b4, #ff7f0e, ,, , , , test acc , , train acc , , x1: , , x2: , ,.!?, -, ->, -o, ., .., ..., .1f, .3f, .4f, .gz, .jpg, .png, .tar, .zip, /, 01ada507287d82875905620988597833ad4e0903, 090b5e7e70c295757f55df93cb0a180b9691891a, 0b8703943ccdb6eb788e6f091b8946e82231bc4d, 0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d, 2068874e4b9a9f0fb07ebe0ad2b29754449ccacd, 319d85e578af0cdc590547f26231e4e31cdf1e42, 3c914d17d80b1459be871a5039ac23e752a53cbe, 4e443f8a2eca6b1dac8a6c57641b67dd40621a49, 585e9cc93e70b39160e7921475f9bcd7d31219ce, 5de26c8fce5ccdea9f91267273464dc968d20d72, 76e5be1548fd8222e5074cf0faae75edff8cf93f, 94646ad1522d915e7b0f9296181140edcf86a4f5, 9fcde07509c7e87ec61c640c1b2753d9041758e4, : , <bos>, <cls>, <eos>, <mask>, <pad>, <sep>, <unk>, BERT模型

    Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

    Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

    Defined in :numref:`subsec_bert_input_rep`, BERT编码器

    Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, Done, GloVe嵌入, ImageSets, JPEGImages, Reds, SNLI, Segmentation, SegmentationClass, Transformer编码器

    Defined in :numref:`sec_transformer`, Transformer编码器块

    Defined in :numref:`sec_transformer`, VOC2012, VOCdevkit, VOCtrainval_11-May-2012.tar, [^A-Za-z]+, \(, \), \s{2,}, __len__, aclImdb, adam, aeroplane, airfoil, airfoil_self_noise.dat, ankle boot, b, b5116e234e9eb9076672cfeabf5469f3eec904fa, background, bag, banana-detection, banana-detection.zip, bananas_train, bananas_val, bicycle, bird, boat, bottle, bus, c, c1816da3821ae9f43899be655002f6c723e91b88, car, cat, cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a, center, chair, char, cifar10_tiny, coat, contradiction, cow, data, diningtable, discard, dog, dog_tiny, dress, entailment, epoch, epoch , f, fa19780a7b011d9b009e8bff8e99922a8ee2eb90, fba480ffa8aa7e0febbb511d181409f899b9baa5, figure.figsize, float32, fra-eng, fra-eng.zip, fra.txt, g, g-., glove.42B.300d.zip, glove.42b.300d, glove.6B.100d.zip, glove.6B.50d.zip, glove.6b.100d, glove.6b.50d, horse, hotdog, hotdog.zip, http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, http://d2l-data.s3-accelerate.amazonaws.com/, https://nlp.stanford.edu/projects/snli/snli_1.0.zip, https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip, images, img_name, int32, k, kaggle_cifar10_tiny.zip, kaggle_dog_tiny.zip, kaggle_house_pred_test.csv, kaggle_house_pred_train.csv, kaggle_house_test, kaggle_house_train, label.csv, learning_rate, linear, loss, loss , loss: , m, m--, motorbike, ndim, neg, negative, neutral, perplexity, person, pos, pos_embedding, positive, potted plant, ptb, ptb.train.txt, ptb.zip, pullover, r, r:, rb, read , relu, sandal, sgd, sheep, shirt, sneaker, snli_1.0_test.txt, snli_1.0_train.txt, sofa, svg, t-shirt, tanh, test, test acc, time traveller, time_machine, timemachine.txt, train, train acc, train loss, train.txt, train_valid, train_valid_test, traveller, trouser, tv/monitor, unknown, utf-8, val.txt, valid, vec.txt, voc2012, w, wb, wiki.en, wiki.en.zip, wiki.train.tokens, wikitext-2, win, word, x1, x2,  ,  , 一个用于加载VOC数据集的自定义数据集

    Defined in :numref:`sec_semantic_segmentation`, 一个用于加载香蕉检测数据集的自定义数据集

    Defined in :numref:`sec_object-detection-dataset`, 下载, 下载DATA_HUB中的所有文件

    Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

    Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

    Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

    Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

    Defined in :numref:`sec_kaggle_house`, 下采样高频词

    Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

    Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

    Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

    Defined in :numref:`sec_bbox`, 位置编码

    Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用GPU计算模型在数据集上的精度

    Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

    Defined in :numref:`sec_calculus`, 使用多个GPU计算数据集上模型的精度

    Defined in :numref:`sec_multi_gpu_concise`, 使用真实边界框标记锚框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

    Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

    Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

    Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

    Defined in :numref:`sec_object-detection-dataset`, 只有zip/tar文件可以被解压缩, 启动计时器, 困惑度 , 在n个变量上累加, 在prefix后面生成新字符

    Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在非Windows的平台上，使用4个进程来读取数据

    Defined in :numref:`sec_fashion_mnist`, 在预测期间整理测试集，以方便读取

    Defined in :numref:`sec_kaggle_cifar10`, 均方损失

    Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

    Defined in :numref:`sec_transformer`, 多头注意力

    Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

    Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

    Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

    Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

    Defined in :numref:`sec_multi_gpu`, 将多输入'X'和'y'拆分到多个设备

    Defined in :numref:`sec_natural-language-inference-attention`, 将文件复制到目标目录

    Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

    Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

    Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

    Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

    Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

    Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

    Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

    Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

    Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

    Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

    Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

    Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

    Defined in :numref:`sec_anchor`, 显示矩阵热图

    Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

    Defined in :numref:`sec_semantic_segmentation`, 构造一个Gluon数据迭代器

    Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 正在从, 残差连接后进行层规范化

    Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

    Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

    Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

    Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

    Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用多GPU进行小批量训练

    Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练

    Defined in :numref:`sec_image_augmentation`, 用定制的训练机优化2D目标函数

    Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

    Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

    Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

    Defined in :numref:`sec_machine_translation`, 绘制图像列表

    Defined in :numref:`sec_fashion_mnist`, 绘制数据点

    Defined in :numref:`sec_calculus`, 统计词元的频率

    Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

    Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

    Defined in :numref:`sec_bert`, 裁剪梯度

    Defined in :numref:`sec_rnn_scratch`, 计算BLEU

    Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

    Defined in :numref:`sec_anchor`, 计算二维互相关运算

    Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

    Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

    Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

    Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型一个迭代周期（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 训练模型（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

    Defined in :numref:`sec_calculus`, 设置matplotlib的轴

    Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

    Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

    Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

    Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

    Defined in :numref:`sec_kaggle_cifar10`, 读取所有VOC图像并标注

    Defined in :numref:`sec_semantic_segmentation`, 读取香蕉检测数据集中的图像和标签

    Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

    Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

    Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu()]

    Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

    Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

    Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

    Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

    Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

    Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

    Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

    Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

    Defined in :numref:`sec_attention-scoring-functions`, 错误：未知词元类型：, 随机裁剪特征和标签图像

    Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

    Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

    Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`","0, 0.0, 0.0001, 0.009999999, 0.01, 0.1, 0.15, 0.22, 0.224, 0.225, 0.229, 0.3, 0.35, 0.406, 0.456, 0.485, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1, 1.0, 1.5, 10, 1000, 10000, 1000000.0, 1048576, 128, 1500, 192, 1e-06, 1e-08, 2, 2.5, 20, 200, 255, 256, 28, 3, 3.0, 3.5, 4, 5, 5.5, 50, 500, 512, 6, 600, 64, 9, False, None, True","#    d2lbook build lib, # (boxes1的数量,boxes2的数量,2), # 0和1分别标记片段A和B, # 10%的时间：保持词不变, # 10%的时间：用随机词替换该词, # 4个维度：储存训练损失，训练准确度，实例数，特点数, # 80%的时间：将词替换为“<mask>”词元, # Don't edit it directly, # Exclude, # Readthetrainingset., # The below part is generated automatically through:, # areas1：(boxes1的数量,),, # areas2：(boxes2的数量,), # boxes1,boxes2,areas1,areas2的形状:, # boxes1：(boxes1的数量,4),, # boxes2：(boxes2的数量,4),, # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量), # inter_upperlefts,inter_lowerrights,inters的形状:, # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens), # label的形状：(batch_size,num_steps), # num_hiddens/num_heads), # paragraphs是三重列表的嵌套, # pos_threshold是一个用于非背景预测的阈值, # pred的形状：(batch_size,num_steps,vocab_size), # queries的形状：(batch_size，查询的个数，1，num_hidden), # scores的形状：(batch_size，查询的个数，“键-值”对的个数), # self.w_v仅有一个输出，因此从形状中移除最后那个维度。, # valid_lens不包括'<pad>'的计数, # valid_len的形状：(batch_size,), # values的形状：(batch_size，“键－值”对的个数，值的维度), # weights的形状：(batch_size,num_steps,1), # 一旦序列结束词元被预测，输出序列的生成就完成了, # 上下文窗口中间i, # 下面是与“d2l.train_epoch_ch3”的主要不同, # 为了将锚点移动到像素的中心，需要设置偏移量。, # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元, # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax), # 从上下文词中排除中心词, # 从随机偏移量开始划分序列, # 使用'flatten=False'只转换最后一个轴，以便其他轴的形状保持不变, # 使用小写字母替换大写字母, # 使用广播的方式进行求和, # 使用空格替换不间断空格, # 保存注意力权重（稍后讨论）, # 假设batch_size=2，num_pred_positions=3, # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数), # 创建一个足够长的P, # 初始化模型, # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5, # 因为位置编码值在-1和1之间，, # 因为已经调用了mean函数, # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, # 因此嵌入值乘以嵌入维度的平方根进行缩放，, # 在x轴上缩放步长, # 在y轴上缩放步长, # 在单词和标点符号之间插入空格, # 在第一次迭代或使用随机抽样时初始化state, # 在维度扩展后，, # 填充词元的预测将通过乘以0权重在损失中过滤掉, # 处理矩形输入, # 如果X有一个轴，输出True, # 它的输出形状是(时间步数*批量大小,词表大小), # 并行运行, # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入, # 所以将所有文本行展平到一个列表中, # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次, # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测, # 找到所有的non_keep索引，并将类设置为背景, # 按出现频率排序, # 损失的总和,样本数量, # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,, # 未知词元的索引为0, # 查询设备列表, # 正确预测数、预测总数, # 正确预测的数量，预测的总数量, # 每个中心点都将有“boxes_per_pixel”个锚框，, # 添加批量轴, # 然后再与位置编码相加。, # 生成“boxes_per_pixel”个高和宽，, # 生成锚框的所有中心点, # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引, # 缓存k个随机采样结果, # 要形成“中心词-上下文词”对，每个句子至少需要有2个词, # 训练损失之和,词元数量, # 训练损失之和，训练准确率之和，样本数, # 训练数据集中样本最少的类别中的样本数, # 训练模型, # 跳过文件头行(列名), # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens), # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,, # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，, # 那么batch_idx是np.array（[0,0,0,1,1,1]）, # 除以2来获得半高和半宽, # 预测num_preds步, # 预热期, # 验证集中每个类别的样本数, #################   WARNING   ################, #1f77b4'), #ff7f0e'), BERT的下一句预测任务

Defined in :numref:`subsec_mlm`""""""
def __init__(self, **kwargs):
super(NextSentencePred, self).__init__(**kwargs)
self.output = nn.Dense(2)

def forward(self, X):
# X的形状：(batchsize，num_hiddens)
return self.output(X)

class BERTModel(nn.Block):
BERT模型, BERT编码器

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000, **kwargs):
super(BERTEncoder, self).__init__(**kwargs)
self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
self.segment_embedding = nn.Embedding(2, num_hiddens)
self.blks = nn.Sequential()
for _ in range(num_layers):
self.blks.add(d2l.EncoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, True))
# 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
self.pos_embedding = self.params.get('pos_embedding',
shape=(1, max_len, num_hiddens))

def forward(self, tokens, segments, valid_lens):
# 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
X = self.token_embedding(tokens) + self.segment_embedding(segments)
X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]
for blk in self.blks:
X = blk(X, valid_lens)
return X

class MaskLM(nn.Block):
BERT的掩蔽语言模型任务, Defined in :numref:`sec_bbox`
# 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
# ((左上x,左上y),宽,高)
return d2l.plt.Rectangle(
xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
fill=False, edgecolor=color, linewidth=2)

def multibox_prior(data, sizes, ratios):
生成以每个像素为中心具有不同形状的锚框, Defined in :numref:`sec_bert-dataset`
file_name = os.path.join(data_dir, 'wiki.train.tokens')
with open(file_name, 'r') as f:
lines = f.readlines()
# 大写字母转换为小写字母
paragraphs = [line.strip().lower().split(' . ')
for line in lines if len(line.split(' . ')) >= 2]
random.shuffle(paragraphs)
return paragraphs

def _get_next_sentence(sentence, next_sentence, paragraphs):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-dataset`
nsp_data_from_paragraph = []
for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence(
paragraph[i], paragraph[i + 1], paragraphs)
# 考虑1个'<cls>'词元和2个'<sep>'词元
if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph.append((tokens, segments, is_next))
return nsp_data_from_paragraph

def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`
mlm_ls, nsp_ls, ls = [], [], []
for (tokens_X_shard, segments_X_shard, valid_lens_x_shard,
pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard,
nsp_y_shard) in zip(
tokens_X_shards, segments_X_shards, valid_lens_x_shards,
pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards,
nsp_y_shards):
# 前向传播
_, mlm_Y_hat, nsp_Y_hat = net(
tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1),
pred_positions_X_shard)
# 计算遮蔽语言模型损失
mlm_l = loss(
mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y_shard.reshape(-1),
mlm_weights_X_shard.reshape((-1, 1)))
mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8)
# 计算下一句子预测任务的损失
nsp_l = loss(nsp_Y_hat, nsp_y_shard)
nsp_l = nsp_l.mean()
mlm_ls.append(mlm_l)
nsp_ls.append(nsp_l)
ls.append(mlm_l + nsp_l)
npx.waitall()
return mlm_ls, nsp_ls, ls

d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
'01ada507287d82875905620988597833ad4e0903')

def read_imdb(data_dir, is_train):
读取IMDb评论数据集文本序列和标签, Defined in :numref:`sec_hybridize`
self.description = description

def __enter__(self):
self.timer = d2l.Timer()
return self

def __exit__(self, *args):
print(f'{self.description}: {self.timer.stop():.4f} sec')

def split_batch(X, y, devices):
将X和y拆分到多个设备上, Defined in :numref:`sec_minibatches`
# 初始化模型
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(init.Normal(sigma=0.01))
trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams)
loss = gluon.loss.L2Loss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(X.shape[0])
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')

class Benchmark:
用于测量运行时间, Defined in :numref:`sec_minibatches`
data = np.genfromtxt(d2l.download('airfoil'),
dtype=np.float32, delimiter='\t')
data = (data - data.mean(axis=0)) / data.std(axis=0)
data_iter = d2l.load_array(
(data[:n, :-1], data[:n, -1]), batch_size, is_train=True)
return data_iter, data.shape[1]-1

def train_ch11(trainer_fn, states, hyperparams, data_iter,
feature_dim, num_epochs=2):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.params = get_params(vocab_size, num_hiddens, device)
self.init_state, self.forward_fn = init_state, forward_fn

def __call__(self, X, state):
X = npx.one_hot(X.T, self.vocab_size)
return self.forward_fn(X, state, self.params)

def begin_state(self, batch_size, ctx):
return self.init_state(batch_size, self.num_hiddens, ctx)

def predict_ch8(prefix, num_preds, net, vocab, device):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
# 增量地绘制多条线
if legend is None:
legend = []
d2l.use_svg_display()
self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
if nrows * ncols == 1:
self.axes = [self.axes, ]
# 使用lambda函数捕获参数
self.config_axes = lambda: d2l.set_axes(
self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self.X, self.Y, self.fmts = None, None, fmts

def add(self, x, y):
# 向图表中添加多个数据点
if not hasattr(y, ""__len__""):
y = [y]
n = len(y)
if not hasattr(x, ""__len__""):
x = [x] * n
if not self.X:
self.X = [[] for _ in range(n)]
if not self.Y:
self.Y = [[] for _ in range(n)]
for i, (a, b) in enumerate(zip(x, y)):
if a is not None and b is not None:
self.X[i].append(a)
self.Y[i].append(b)
self.axes[0].cla()
for x, y, fmt in zip(self.X, self.Y, self.fmts):
self.axes[0].plot(x, y, fmt)
self.config_axes()
display.display(self.fig)
display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
训练模型（定义见第3章）, Defined in :numref:`sec_synonyms`
self.idx_to_token, self.idx_to_vec = self._load_embedding(
embedding_name)
self.unknown_idx = 0
self.token_to_idx = {token: idx for idx, token in
enumerate(self.idx_to_token)}

def _load_embedding(self, embedding_name):
idx_to_token, idx_to_vec = ['<unk>'], []
data_dir = d2l.download_extract(embedding_name)
# GloVe网站：https://nlp.stanford.edu/projects/glove/
# fastText网站：https://fasttext.cc/
with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
for line in f:
elems = line.rstrip().split(' ')
token, elems = elems[0], [float(elem) for elem in elems[1:]]
# 跳过标题信息，例如fastText中的首行
if len(elems) > 1:
idx_to_token.append(token)
idx_to_vec.append(elems)
idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
return idx_to_token, d2l.tensor(idx_to_vec)

def __getitem__(self, tokens):
indices = [self.token_to_idx.get(token, self.unknown_idx)
for token in tokens]
vecs = self.idx_to_vec[d2l.tensor(indices)]
return vecs

def __len__(self):
return len(self.idx_to_token)

def get_tokens_and_segments(tokens_a, tokens_b=None):
获取输入序列的词元及其片段索引, Defined in :numref:`subsec_prepare_mlm_data`
candidate_pred_positions = []
# tokens是一个字符串列表
for i, token in enumerate(tokens):
# 在遮蔽语言模型任务中不会预测特殊词元
if token in ['<cls>', '<sep>']:
continue
candidate_pred_positions.append(i)
# 遮蔽语言模型任务中预测15%的随机词元
num_mlm_preds = max(1, round(len(tokens) * 0.15))
mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0])
pred_positions = [v[0] for v in pred_positions_and_labels]
mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]

def _pad_bert_inputs(examples, max_len, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`subsec_prepare_mlm_data`
def __init__(self, paragraphs, max_len):
# 输入paragraphs[i]是代表段落的句子字符串列表；
# 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表
paragraphs = [d2l.tokenize(
paragraph, token='word') for paragraph in paragraphs]
sentences = [sentence for paragraph in paragraphs
for sentence in paragraph]
self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
'<pad>', '<mask>', '<cls>', '<sep>'])
# 获取下一句子预测任务的数据
examples = []
for paragraph in paragraphs:
examples.extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self.vocab, max_len))
# 获取遮蔽语言模型任务的数据
examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
+ (segments, is_next))
for tokens, segments, is_next in examples]
# 填充输入
(self.all_token_ids, self.all_segments, self.valid_lens,
self.all_pred_positions, self.all_mlm_weights,
self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
examples, max_len, self.vocab)

def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx], self.all_pred_positions[idx],
self.all_mlm_weights[idx], self.all_mlm_labels[idx],
self.nsp_labels[idx])

def __len__(self):
return len(self.all_token_ids)

def load_data_wiki(batch_size, max_len):
加载WikiText-2数据集, Transformer编码器块

Defined in :numref:`sec_transformer`""""""
def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias=False, **kwargs):
super(EncoderBlock, self).__init__(**kwargs)
self.attention = d2l.MultiHeadAttention(
num_hiddens, num_heads, dropout, use_bias)
self.addnorm1 = AddNorm(dropout)
self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self.addnorm2 = AddNorm(dropout)

def forward(self, X, valid_lens):
Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
return self.addnorm2(Y, self.ffn(Y))

class TransformerEncoder(d2l.Encoder):
Transformer编码器, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`""""""
sentences = read_ptb()
vocab = d2l.Vocab(sentences, min_freq=10)
subsampled, counter = subsample(sentences, vocab)
corpus = [vocab[line] for line in subsampled]
all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size)
all_negatives = get_negatives(
all_contexts, vocab, counter, num_noise_words)
dataset = gluon.data.ArrayDataset(
all_centers, all_contexts, all_negatives)
data_iter = gluon.data.DataLoader(
dataset, batch_size, shuffle=True,batchify_fn=batchify,
num_workers=d2l.get_dataloader_workers())
return data_iter, vocab

d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')

d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')

d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')

class TokenEmbedding:
GloVe嵌入, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_data = read_snli(data_dir, True)
test_data = read_snli(data_dir, False)
train_set = SNLIDataset(train_data, num_steps)
test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers)
test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,
num_workers=num_workers)
return train_iter, test_iter, train_set.vocab

def split_batch_multi_inputs(X, y, devices):
将多输入'X'和'y'拆分到多个设备, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 下采样高频词

Defined in :numref:`sec_word2vec_data`""""""
# 排除未知词元'<unk>'
sentences = [[token for token in line if vocab[token] != vocab.unk]
for line in sentences]
counter = d2l.count_corpus(sentences)
num_tokens = sum(counter.values())

# 如果在下采样期间保留词元，则返回True
def keep(token):
return(random.uniform(0, 1) <
math.sqrt(1e-4 / counter[token] * num_tokens))

return ([[token for token in line if keep(token)] for line in sentences],
counter)

def get_centers_and_contexts(corpus, max_window_size):
返回跳元模型中的中心词和上下文词, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`""""""
if not device:  # 查询第一个参数所在的第一个设备
device = list(net.collect_params().values())[0].list_ctx()[0]
metric = d2l.Accumulator(2)  # 正确预测的数量，总预测的数量
for X, y in data_iter:
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
metric.add(d2l.accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
batch_size, anchors = labels.shape[0], anchors.squeeze(0)
batch_offset, batch_mask, batch_class_labels = [], [], []
device, num_anchors = anchors.ctx, anchors.shape[0]
for i in range(batch_size):
label = labels[i, :, :]
anchors_bbox_map = assign_anchor_to_bbox(
label[:, 1:], anchors, device)
bbox_mask = np.tile((np.expand_dims((anchors_bbox_map >= 0),
axis=-1)), (1, 4)).astype('int32')
# 将类标签和分配的边界框坐标初始化为零
class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)
assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,
ctx=device)
# 使用真实边界框来标记锚框的类别。
# 如果一个锚框没有被分配，标记其为背景（值为零）
indices_true = np.nonzero(anchors_bbox_map >= 0)[0]
bb_idx = anchors_bbox_map[indices_true]
class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1
assigned_bb[indices_true] = label[bb_idx, 1:]
# 偏移量转换
offset = offset_boxes(anchors, assigned_bb) * bbox_mask
batch_offset.append(offset.reshape(-1))
batch_mask.append(bbox_mask.reshape(-1))
batch_class_labels.append(class_labels)
bbox_offset = d2l.stack(batch_offset)
bbox_mask = d2l.stack(batch_mask)
class_labels = d2l.stack(batch_class_labels)
return (bbox_offset, bbox_mask, class_labels)

def offset_inverse(anchors, offset_preds):
根据带有预测偏移量的锚框来预测边界框, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`""""""
voc_dir = d2l.download_extract('voc2012', os.path.join(
'VOCdevkit', 'VOC2012'))
num_workers = d2l.get_dataloader_workers()
train_iter = gluon.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size,
shuffle=True, last_batch='discard', num_workers=num_workers)
test_iter = gluon.data.DataLoader(
VOCSegDataset(False, crop_size, voc_dir), batch_size,
last_batch='discard', num_workers=num_workers)
return train_iter, test_iter

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

def read_csv_labels(fname):
读取fname来给标签字典返回一个文件名, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`""""""
train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),
batch_size, shuffle=True)
val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),
batch_size)
return train_iter, val_iter

d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

def read_voc_images(voc_dir, is_train=True):
读取所有VOC图像并标注, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 在n个变量上累加
def __init__(self, n):
Defined in :numref:`sec_softmax_scratch`, 在非Windows的平台上，使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 0 if sys.platform.startswith('win') else 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`""""""
for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile(os.path.join(data_dir, 'test', test_file),
os.path.join(data_dir, 'train_valid_test', 'test',
'unknown'))

d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')

d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')

def read_ptb():
将PTB数据集加载到文本行的列表中, 基于位置的前馈网络

Defined in :numref:`sec_transformer`""""""
def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):
super(PositionWiseFFN, self).__init__(**kwargs)
self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,
activation='relu')
self.dense2 = nn.Dense(ffn_num_outputs, flatten=False)

def forward(self, X):
return self.dense2(self.dense1(X))

class AddNorm(nn.Block):
残差连接后进行层规范化, 多头注意力

Defined in :numref:`sec_multihead-attention`""""""
def __init__(self, num_hiddens, num_heads, dropout, use_bias=False,
**kwargs):
super(MultiHeadAttention, self).__init__(**kwargs)
self.num_heads = num_heads
self.attention = d2l.DotProductAttention(dropout)
self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)

def forward(self, queries, keys, values, valid_lens):
# queries，keys，values的形状:
# (batch_size，查询或者“键－值”对的个数，num_hiddens)
# valid_lens　的形状:
# (batch_size，)或(batch_size，查询的个数)
# 经过变换后，输出的queries，keys，values　的形状:
# (batch_size*num_heads，查询或者“键－值”对的个数，
# num_hiddens/num_heads)
queries = transpose_qkv(self.W_q(queries), self.num_heads)
keys = transpose_qkv(self.W_k(keys), self.num_heads)
values = transpose_qkv(self.W_v(values), self.num_heads)

if valid_lens is not None:
# 在轴0，将第一项（标量或者矢量）复制num_heads次，
# 然后如此复制第二项，然后诸如此类。
valid_lens = valid_lens.repeat(self.num_heads, axis=0)

# output的形状:(batch_size*num_heads，查询的个数，
# num_hiddens/num_heads)
output = self.attention(queries, keys, values, valid_lens)

# output_concat的形状:(batch_size，查询的个数，num_hiddens)
output_concat = transpose_output(output, self.num_heads)
return self.W_o(output_concat)

def transpose_qkv(X, num_heads):
为了多注意力头的并行计算而变换形状, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
B = scores.argsort()[::-1]
keep = []  # 保留预测边界框的指标
while B.size > 0:
i = B[0]
keep.append(i)
if B.size == 1: break
iou = box_iou(boxes[i, :].reshape(-1, 4),
boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
inds = np.nonzero(iou <= iou_threshold)[0]
B = B[inds + 1]
return np.array(keep, dtype=np.int32, ctx=boxes.ctx)

def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
pos_threshold=0.009999999):
使用非极大值抑制来预测边界框, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def extract_text(s):
# 删除我们不会使用的信息
s = re.sub('\\(', '', s)
s = re.sub('\\)', '', s)
# 用一个空格替换两个或多个连续的空格
s = re.sub('\\s{2,}', ' ', s)
return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
if is_train else 'snli_1.0_test.txt')
with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]]
premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
hypotheses = [extract_text(row[2]) for row in rows if row[0] \
in label_set]
labels = [label_set[row[0]] for row in rows if row[0] in label_set]
return premises, hypotheses, labels

class SNLIDataset(gluon.data.Dataset):
用于加载SNLI数据集的自定义数据集, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`""""""
os.makedirs(target_dir, exist_ok=True)
shutil.copy(filename, target_dir)

def reorg_train_valid(data_dir, labels, valid_ratio):
将验证集从原始的训练集中拆分出来, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`""""""
num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
# 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU
jaccard = box_iou(anchors, ground_truth)
# 对于每个锚框，分配的真实边界框的张量
anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)
# 根据阈值，决定是否分配真实边界框
max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)
anc_i = np.nonzero(max_ious >= iou_threshold)[0]
box_j = indices[max_ious >= iou_threshold]
anchors_bbox_map[anc_i] = box_j
col_discard = np.full((num_anchors,), -1)
row_discard = np.full((num_gt_boxes,), -1)
for _ in range(num_gt_boxes):
max_idx = np.argmax(jaccard)
box_idx = (max_idx % num_gt_boxes).astype('int32')
anc_idx = (max_idx / num_gt_boxes).astype('int32')
anchors_bbox_map[anc_idx] = box_idx
jaccard[:, box_idx] = col_discard
jaccard[anc_idx, :] = row_discard
return anchors_bbox_map

def offset_boxes(anchors, assigned_bb, eps=1e-6):
对锚框偏移量的转换, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
for param in params:
param[:] = param - lr * param.grad / batch_size

def load_array(data_arrays, batch_size, is_train=True):
构造一个Gluon数据迭代器, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示所有边界框

Defined in :numref:`sec_anchor`""""""
def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj

labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))

def box_iou(boxes1, boxes2):
计算两个锚框或边界框列表中成对的交并比, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = np.zeros(256 ** 3)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 根据n个采样权重在{1,...,n}中随机抽取
def __init__(self, sampling_weights):
Defined in :numref:`sec_word2vec_data`, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqEncoder, self).__init__(**kwargs)
# 嵌入层
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)

def forward(self, X, *args):
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
# 在循环神经网络模型中，第一个轴对应于时间步
X = X.swapaxes(0, 1)
state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)
output, state = self.rnn(X, state)
# output的形状:(num_steps,batch_size,num_hiddens)
# state的形状:(num_layers,batch_size,num_hiddens)
return output, state

class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):
带遮蔽的softmax交叉熵损失函数, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`""""""
X_shards, y_shards = split_f(features, labels, devices)
with autograd.record():
pred_shards = [net(X_shard) for X_shard in X_shards]
ls = [loss(pred_shard, y_shard) for pred_shard, y_shard
in zip(pred_shards, y_shards)]
for l in ls:
l.backward()
# True标志允许使用过时的梯度，这很有用（例如，在微调BERT中）
trainer.step(labels.shape[0], ignore_stale_grad=True)
train_loss_sum = sum([float(l.sum()) for l in ls])
train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)
for pred_shard, y_shard in zip(pred_shards, y_shards))
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus(), split_f=d2l.split_batch):
用多GPU进行模型训练, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`""""""
# s1和s2是稍后将使用的内部状态变量
x1, x2, s1, s2 = -5, -2, 0, 0
results = [(x1, x2)]
for i in range(steps):
if f_grad:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
else:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
results.append((x1, x2))
print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')
return results

def show_trace_2d(f, results):
显示优化过程中2D变量的轨迹, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`""""""
def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.Sequential()
for i in range(num_residuals):
if i == 0 and not first_block:
blk.add(d2l.Residual(
num_channels, use_1x1conv=True, strides=2))
else:
blk.add(d2l.Residual(num_channels))
return blk

net = nn.Sequential()
# 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
nn.BatchNorm(), nn.Activation('relu'))
net.add(resnet_block(64, 2, first_block=True),
resnet_block(128, 2),
resnet_block(256, 2),
resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
return net

def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):
使用多个GPU计算数据集上模型的精度, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def forward(self, X, state):
raise NotImplementedError

class EncoderDecoder(nn.Block):
编码器-解码器架构的基类, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`""""""
def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)

# queries的形状：(batch_size，查询的个数，d)
# keys的形状：(batch_size，“键－值”对的个数，d)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
# valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
def forward(self, queries, keys, values, valid_lens=None):
d = queries.shape[-1]
# 设置transpose_b=True为了交换keys的最后两个维度
scores = npx.batch_dot(queries, keys, transpose_b=True) / math.sqrt(d)
self.attention_weights = masked_softmax(scores, valid_lens)
return npx.batch_dot(self.dropout(self.attention_weights), values)

class AttentionDecoder(d2l.Decoder):
带有注意力机制解码器的基本接口, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
if isinstance(net, gluon.Block):
params = [p.data() for p in net.collect_params().values()]
else:
params = net.params
norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))
if norm > theta:
for param in params:
param.grad[:] *= theta / norm

def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
训练模型一个迭代周期（定义见第8章）, 计算BLEU

Defined in :numref:`sec_seq2seq_training`""""""
pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
len_pred, len_label = len(pred_tokens), len(label_tokens)
score = math.exp(min(0, 1 - len_label / len_pred))
for n in range(1, k + 1):
num_matches, label_subs = 0, collections.defaultdict(int)
for i in range(len_label - n + 1):
label_subs[' '.join(label_tokens[i: i + n])] += 1
for i in range(len_pred - n + 1):
if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
num_matches += 1
label_subs[' '.join(pred_tokens[i: i + n])] -= 1
score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
return score

def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
cmap='Reds'):
显示矩阵热图, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def evaluate_accuracy(net, data_iter):
计算在指定数据集上模型的精度, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`""""""
net.initialize(init.Xavier(), force_reinit=True, ctx=device)
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr})
loss = MaskedSoftmaxCELoss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[10, num_epochs])
for epoch in range(num_epochs):
timer = d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失求和，词元数量
for batch in data_iter:
X, X_valid_len, Y, Y_valid_len = [
x.as_in_ctx(device) for x in batch]
bos = np.array([tgt_vocab['<bos>']] * Y.shape[0],
ctx=device).reshape(-1, 1)
dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # 强制教学
with autograd.record():
Y_hat, _ = net(X, dec_input, X_valid_len)
l = loss(Y_hat, Y, Y_valid_len)
l.backward()
d2l.grad_clipping(net, 1)
num_tokens = Y_valid_len.sum()
trainer.step(num_tokens)
metric.add(l.sum(), num_tokens)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, (metric[0] / metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
f'tokens/sec on {str(device)}')

def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights=False):
序列到序列模型的预测, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
# 训练损失总和、训练准确度总和、样本数
metric = Accumulator(3)
if isinstance(updater, gluon.Trainer):
updater = updater.step
for X, y in train_iter:
# 计算梯度并更新参数
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
updater(X.shape[0])
metric.add(float(l.sum()), accuracy(y_hat, y), y.size)
# 返回训练损失和训练精度
return metric[0] / metric[2], metric[1] / metric[2]

class Animator:
在动画中绘制数据, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
loss = gluon.loss.SoftmaxCrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
# 初始化
if isinstance(net, gluon.Block):
net.initialize(ctx=device, force_reinit=True,
init=init.Normal(0.01))
trainer = gluon.Trainer(net.collect_params(),
'sgd', {'learning_rate': lr})
updater = lambda batch_size: trainer.step(batch_size)
else:
updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(
net, train_iter, loss, updater, device, use_random_iter)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, [ppl])
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(nn.Block):
循环神经网络模型, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`""""""
data_dir = d2l.download_extract('banana-detection')
csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
else 'bananas_val', 'label.csv')
csv_data = pd.read_csv(csv_fname)
csv_data = csv_data.set_index('img_name')
images, targets = [], []
for img_name, target in csv_data.iterrows():
images.append(image.imread(
os.path.join(data_dir, 'bananas_train' if is_train else
'bananas_val', 'images', f'{img_name}')))
# 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
# 其中所有图像都具有相同的香蕉类（索引为0）
targets.append(list(target))
return images, np.expand_dims(np.array(targets), 1) / 256

class BananasDataset(gluon.data.Dataset):
一个用于加载香蕉检测数据集的自定义数据集, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu()]

Defined in :numref:`sec_use_gpu`""""""
devices = [npx.gpu(i) for i in range(npx.num_gpus())]
return devices if devices else [npx.cpu()]

def corr2d(X, K):
计算二维互相关运算, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`""""""
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
train_data = read_imdb(data_dir, True)
test_data = read_imdb(data_dir, False)
train_tokens = d2l.tokenize(train_data[0], token='word')
test_tokens = d2l.tokenize(test_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5)
train_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
test_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])
train_iter = d2l.load_array((train_features, train_data[1]), batch_size)
test_iter = d2l.load_array((test_features, test_data[1]), batch_size,
is_train=False)
return train_iter, test_iter, vocab

def predict_sentiment(net, vocab, sequence):
预测文本序列的情感, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(nn.Block):
编码器-解码器架构的基本编码器接口, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`""""""
# 索引为1、2、...（索引0是词表中排除的未知标记）
sampling_weights = [counter[vocab.to_tokens(i)]**0.75
for i in range(1, len(vocab))]
all_negatives, generator = [], RandomGenerator(sampling_weights)
for contexts in all_contexts:
negatives = []
while len(negatives) < len(contexts) * K:
neg = generator.draw()
# 噪声词不能是上下文词
if neg not in contexts:
negatives.append(neg)
all_negatives.append(negatives)
return all_negatives

def batchify(data):
返回带有负采样的跳元模型的小批量样本, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`""""""
X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
X = X.transpose(0, 2, 1, 3)
return X.reshape(X.shape[0], X.shape[1], -1)

class PositionalEncoding(nn.Block):
位置编码, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`""""""
# X:3D张量，valid_lens:1D或2D张量
if valid_lens is None:
return npx.softmax(X)
else:
shape = X.shape
if valid_lens.ndim == 1:
valid_lens = valid_lens.repeat(shape[1])
else:
valid_lens = valid_lens.reshape(-1)
# 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,
value=-1e6, axis=1)
return npx.softmax(X).reshape(shape)

class AdditiveAttention(nn.Block):
加性注意力, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
feature, rect = image.random_crop(feature, (width, height))
label = image.fixed_crop(label, *rect)
return feature, label

class VOCSegDataset(gluon.data.Dataset):
一个用于加载VOC数据集的自定义数据集, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失","(boxes1的数量,boxes2的数量,2), 0和1分别标记片段A和B, 10%的时间：保持词不变, 10%的时间：用随机词替换该词, 4个维度：储存训练损失，训练准确度，实例数，特点数, 80%的时间：将词替换为“<mask>”词元, BERT模型, BERT的下一句预测任务, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`""""""
def __init__(self, **kwargs):
super(NextSentencePred, self).__init__(**kwargs)
self.output = nn.Dense(2)

def forward(self, X):
# X的形状：(batchsize，num_hiddens)
return self.output(X)

class BERTModel(nn.Block):
BERT模型, BERT的掩蔽语言模型任务, BERT编码器, BERT编码器

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
num_layers, dropout, max_len=1000, **kwargs):
super(BERTEncoder, self).__init__(**kwargs)
self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
self.segment_embedding = nn.Embedding(2, num_hiddens)
self.blks = nn.Sequential()
for _ in range(num_layers):
self.blks.add(d2l.EncoderBlock(
num_hiddens, ffn_num_hiddens, num_heads, dropout, True))
# 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
self.pos_embedding = self.params.get('pos_embedding',
shape=(1, max_len, num_hiddens))

def forward(self, tokens, segments, valid_lens):
# 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
X = self.token_embedding(tokens) + self.segment_embedding(segments)
X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]
for blk in self.blks:
X = blk(X, valid_lens)
return X

class MaskLM(nn.Block):
BERT的掩蔽语言模型任务, Defined in :numref:`sec_bbox`
# 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
# ((左上x,左上y),宽,高)
return d2l.plt.Rectangle(
xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
fill=False, edgecolor=color, linewidth=2)

def multibox_prior(data, sizes, ratios):
生成以每个像素为中心具有不同形状的锚框, Defined in :numref:`sec_bert-dataset`
file_name = os.path.join(data_dir, 'wiki.train.tokens')
with open(file_name, 'r') as f:
lines = f.readlines()
# 大写字母转换为小写字母
paragraphs = [line.strip().lower().split(' . ')
for line in lines if len(line.split(' . ')) >= 2]
random.shuffle(paragraphs)
return paragraphs

def _get_next_sentence(sentence, next_sentence, paragraphs):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-dataset`
nsp_data_from_paragraph = []
for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence(
paragraph[i], paragraph[i + 1], paragraphs)
# 考虑1个'<cls>'词元和2个'<sep>'词元
if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph.append((tokens, segments, is_next))
return nsp_data_from_paragraph

def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`
mlm_ls, nsp_ls, ls = [], [], []
for (tokens_X_shard, segments_X_shard, valid_lens_x_shard,
pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard,
nsp_y_shard) in zip(
tokens_X_shards, segments_X_shards, valid_lens_x_shards,
pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards,
nsp_y_shards):
# 前向传播
_, mlm_Y_hat, nsp_Y_hat = net(
tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1),
pred_positions_X_shard)
# 计算遮蔽语言模型损失
mlm_l = loss(
mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y_shard.reshape(-1),
mlm_weights_X_shard.reshape((-1, 1)))
mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8)
# 计算下一句子预测任务的损失
nsp_l = loss(nsp_Y_hat, nsp_y_shard)
nsp_l = nsp_l.mean()
mlm_ls.append(mlm_l)
nsp_ls.append(nsp_l)
ls.append(mlm_l + nsp_l)
npx.waitall()
return mlm_ls, nsp_ls, ls

d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
'01ada507287d82875905620988597833ad4e0903')

def read_imdb(data_dir, is_train):
读取IMDb评论数据集文本序列和标签, Defined in :numref:`sec_hybridize`
self.description = description

def __enter__(self):
self.timer = d2l.Timer()
return self

def __exit__(self, *args):
print(f'{self.description}: {self.timer.stop():.4f} sec')

def split_batch(X, y, devices):
将X和y拆分到多个设备上, Defined in :numref:`sec_minibatches`
# 初始化模型
net = nn.Sequential()
net.add(nn.Dense(1))
net.initialize(init.Normal(sigma=0.01))
trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams)
loss = gluon.loss.L2Loss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
with autograd.record():
l = loss(net(X), y)
l.backward()
trainer.step(X.shape[0])
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')

class Benchmark:
用于测量运行时间, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.params = get_params(vocab_size, num_hiddens, device)
self.init_state, self.forward_fn = init_state, forward_fn

def __call__(self, X, state):
X = npx.one_hot(X.T, self.vocab_size)
return self.forward_fn(X, state, self.params)

def begin_state(self, batch_size, ctx):
return self.init_state(batch_size, self.num_hiddens, ctx)

def predict_ch8(prefix, num_preds, net, vocab, device):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
# 增量地绘制多条线
if legend is None:
legend = []
d2l.use_svg_display()
self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
if nrows * ncols == 1:
self.axes = [self.axes, ]
# 使用lambda函数捕获参数
self.config_axes = lambda: d2l.set_axes(
self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self.X, self.Y, self.fmts = None, None, fmts

def add(self, x, y):
# 向图表中添加多个数据点
if not hasattr(y, ""__len__""):
y = [y]
n = len(y)
if not hasattr(x, ""__len__""):
x = [x] * n
if not self.X:
self.X = [[] for _ in range(n)]
if not self.Y:
self.Y = [[] for _ in range(n)]
for i, (a, b) in enumerate(zip(x, y)):
if a is not None and b is not None:
self.X[i].append(a)
self.Y[i].append(b)
self.axes[0].cla()
for x, y, fmt in zip(self.X, self.Y, self.fmts):
self.axes[0].plot(x, y, fmt)
self.config_axes()
display.display(self.fig)
display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
训练模型（定义见第3章）, Defined in :numref:`sec_synonyms`
self.idx_to_token, self.idx_to_vec = self._load_embedding(
embedding_name)
self.unknown_idx = 0
self.token_to_idx = {token: idx for idx, token in
enumerate(self.idx_to_token)}

def _load_embedding(self, embedding_name):
idx_to_token, idx_to_vec = ['<unk>'], []
data_dir = d2l.download_extract(embedding_name)
# GloVe网站：https://nlp.stanford.edu/projects/glove/
# fastText网站：https://fasttext.cc/
with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
for line in f:
elems = line.rstrip().split(' ')
token, elems = elems[0], [float(elem) for elem in elems[1:]]
# 跳过标题信息，例如fastText中的首行
if len(elems) > 1:
idx_to_token.append(token)
idx_to_vec.append(elems)
idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
return idx_to_token, d2l.tensor(idx_to_vec)

def __getitem__(self, tokens):
indices = [self.token_to_idx.get(token, self.unknown_idx)
for token in tokens]
vecs = self.idx_to_vec[d2l.tensor(indices)]
return vecs

def __len__(self):
return len(self.idx_to_token)

def get_tokens_and_segments(tokens_a, tokens_b=None):
获取输入序列的词元及其片段索引, Defined in :numref:`subsec_prepare_mlm_data`
candidate_pred_positions = []
# tokens是一个字符串列表
for i, token in enumerate(tokens):
# 在遮蔽语言模型任务中不会预测特殊词元
if token in ['<cls>', '<sep>']:
continue
candidate_pred_positions.append(i)
# 遮蔽语言模型任务中预测15%的随机词元
num_mlm_preds = max(1, round(len(tokens) * 0.15))
mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0])
pred_positions = [v[0] for v in pred_positions_and_labels]
mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]

def _pad_bert_inputs(examples, max_len, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`subsec_prepare_mlm_data`
def __init__(self, paragraphs, max_len):
# 输入paragraphs[i]是代表段落的句子字符串列表；
# 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表
paragraphs = [d2l.tokenize(
paragraph, token='word') for paragraph in paragraphs]
sentences = [sentence for paragraph in paragraphs
for sentence in paragraph]
self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
'<pad>', '<mask>', '<cls>', '<sep>'])
# 获取下一句子预测任务的数据
examples = []
for paragraph in paragraphs:
examples.extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self.vocab, max_len))
# 获取遮蔽语言模型任务的数据
examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
+ (segments, is_next))
for tokens, segments, is_next in examples]
# 填充输入
(self.all_token_ids, self.all_segments, self.valid_lens,
self.all_pred_positions, self.all_mlm_weights,
self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
examples, max_len, self.vocab)

def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx], self.all_pred_positions[idx],
self.all_mlm_weights[idx], self.all_mlm_labels[idx],
self.nsp_labels[idx])

def __len__(self):
return len(self.all_token_ids)

def load_data_wiki(batch_size, max_len):
加载WikiText-2数据集, GloVe嵌入, Transformer编码器, Transformer编码器块, Transformer编码器块

Defined in :numref:`sec_transformer`""""""
def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,
use_bias=False, **kwargs):
super(EncoderBlock, self).__init__(**kwargs)
self.attention = d2l.MultiHeadAttention(
num_hiddens, num_heads, dropout, use_bias)
self.addnorm1 = AddNorm(dropout)
self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)
self.addnorm2 = AddNorm(dropout)

def forward(self, X, valid_lens):
Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
return self.addnorm2(Y, self.ffn(Y))

class TransformerEncoder(d2l.Encoder):
Transformer编码器, areas1：(boxes1的数量,),, areas2：(boxes2的数量,), boxes1,boxes2,areas1,areas2的形状:, boxes1：(boxes1的数量,4),, boxes2：(boxes2的数量,4),, inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量), inter_upperlefts,inter_lowerrights,inters的形状:, key的形状：(batch_size，1，“键－值”对的个数，num_hiddens), label的形状：(batch_size,num_steps), paragraphs是三重列表的嵌套, pos_threshold是一个用于非背景预测的阈值, pred的形状：(batch_size,num_steps,vocab_size), queries的形状：(batch_size，查询的个数，1，num_hidden), scores的形状：(batch_size，查询的个数，“键-值”对的个数), self.w_v仅有一个输出，因此从形状中移除最后那个维度。, valid_lens不包括'<pad>'的计数, valid_len的形状：(batch_size,), values的形状：(batch_size，“键－值”对的个数，值的维度), weights的形状：(batch_size,num_steps,1), 一个用于加载VOC数据集的自定义数据集, 一个用于加载香蕉检测数据集的自定义数据集, 一旦序列结束词元被预测，输出序列的生成就完成了, 上下文窗口中间i, 下载, 下载DATA_HUB中的所有文件, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载Fashion-MNIST数据集，然后将其加载到内存中, 下载PTB数据集，然后将其加载到内存中, 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`""""""
sentences = read_ptb()
vocab = d2l.Vocab(sentences, min_freq=10)
subsampled, counter = subsample(sentences, vocab)
corpus = [vocab[line] for line in subsampled]
all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size)
all_negatives = get_negatives(
all_contexts, vocab, counter, num_noise_words)
dataset = gluon.data.ArrayDataset(
all_centers, all_contexts, all_negatives)
data_iter = gluon.data.DataLoader(
dataset, batch_size, shuffle=True,batchify_fn=batchify,
num_workers=d2l.get_dataloader_workers())
return data_iter, vocab

d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')

d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')

d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')

class TokenEmbedding:
GloVe嵌入, 下载SNLI数据集并返回数据迭代器和词表, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_data = read_snli(data_dir, True)
test_data = read_snli(data_dir, False)
train_set = SNLIDataset(train_data, num_steps)
test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,
num_workers=num_workers)
test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,
num_workers=num_workers)
return train_iter, test_iter, train_set.vocab

def split_batch_multi_inputs(X, y, devices):
将多输入'X'和'y'拆分到多个设备, 下载一个DATA_HUB中的文件，返回本地文件名, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 下载并解压zip/tar文件, 下采样高频词, 下采样高频词

Defined in :numref:`sec_word2vec_data`""""""
# 排除未知词元'<unk>'
sentences = [[token for token in line if vocab[token] != vocab.unk]
for line in sentences]
counter = d2l.count_corpus(sentences)
num_tokens = sum(counter.values())

# 如果在下采样期间保留词元，则返回True
def keep(token):
return(random.uniform(0, 1) <
math.sqrt(1e-4 / counter[token] * num_tokens))

return ([[token for token in line if keep(token)] for line in sentences],
counter)

def get_centers_and_contexts(corpus, max_window_size):
返回跳元模型中的中心词和上下文词, 下面是与“d2l.train_epoch_ch3”的主要不同, 不存在于, 中随机抽取, 为了多注意力头的并行计算而变换形状, 为了将锚点移动到像素的中心，需要设置偏移量。, 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元, 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax), 从上下文词中排除中心词, 从随机偏移量开始划分序列, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）, 从（左上，右下）转换到（中间，宽度，高度）, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 位置编码, 使用'flatten=False'只转换最后一个轴，以便其他轴的形状保持不变, 使用GPU计算模型在数据集上的精度, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`""""""
if not device:  # 查询第一个参数所在的第一个设备
device = list(net.collect_params().values())[0].list_ctx()[0]
metric = d2l.Accumulator(2)  # 正确预测的数量，总预测的数量
for X, y in data_iter:
X, y = X.as_in_ctx(device), y.as_in_ctx(device)
metric.add(d2l.accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 使用svg格式在Jupyter中显示绘图, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用多个GPU计算数据集上模型的精度, 使用小写字母替换大写字母, 使用广播的方式进行求和, 使用真实边界框标记锚框, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
batch_size, anchors = labels.shape[0], anchors.squeeze(0)
batch_offset, batch_mask, batch_class_labels = [], [], []
device, num_anchors = anchors.ctx, anchors.shape[0]
for i in range(batch_size):
label = labels[i, :, :]
anchors_bbox_map = assign_anchor_to_bbox(
label[:, 1:], anchors, device)
bbox_mask = np.tile((np.expand_dims((anchors_bbox_map >= 0),
axis=-1)), (1, 4)).astype('int32')
# 将类标签和分配的边界框坐标初始化为零
class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)
assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,
ctx=device)
# 使用真实边界框来标记锚框的类别。
# 如果一个锚框没有被分配，标记其为背景（值为零）
indices_true = np.nonzero(anchors_bbox_map >= 0)[0]
bb_idx = anchors_bbox_map[indices_true]
class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1
assigned_bb[indices_true] = label[bb_idx, 1:]
# 偏移量转换
offset = offset_boxes(anchors, assigned_bb) * bbox_mask
batch_offset.append(offset.reshape(-1))
batch_mask.append(bbox_mask.reshape(-1))
batch_class_labels.append(class_labels)
bbox_offset = d2l.stack(batch_offset)
bbox_mask = d2l.stack(batch_mask)
class_labels = d2l.stack(batch_class_labels)
return (bbox_offset, bbox_mask, class_labels)

def offset_inverse(anchors, offset_preds):
根据带有预测偏移量的锚框来预测边界框, 使用空格替换不间断空格, 使用随机抽样生成一个小批量子序列, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 使用非极大值抑制来预测边界框, 使用顺序分区生成一个小批量子序列, 保存注意力权重（稍后讨论）, 假设batch_size=2，num_pred_positions=3, 停止计时器并将时间记录在列表中, 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数), 创建一个足够长的P, 初始化模型, 加性注意力, 加载VOC语义分割数据集, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`""""""
voc_dir = d2l.download_extract('voc2012', os.path.join(
'VOCdevkit', 'VOC2012'))
num_workers = d2l.get_dataloader_workers()
train_iter = gluon.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size,
shuffle=True, last_batch='discard', num_workers=num_workers)
test_iter = gluon.data.DataLoader(
VOCSegDataset(False, crop_size, voc_dir), batch_size,
last_batch='discard', num_workers=num_workers)
return train_iter, test_iter

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

def read_csv_labels(fname):
读取fname来给标签字典返回一个文件名, 加载WikiText-2数据集, 加载序列数据的迭代器, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 加载香蕉检测数据集, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`""""""
train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),
batch_size, shuffle=True)
val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),
batch_size)
return train_iter, val_iter

d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

def read_voc_images(voc_dir, is_train=True):
读取所有VOC图像并标注, 只有zip/tar文件可以被解压缩, 启动计时器, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5, 因为位置编码值在-1和1之间，, 因为已经调用了mean函数, 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, 因此嵌入值乘以嵌入维度的平方根进行缩放，, 困惑度, 在n个变量上累加, 在n个变量上累加
def __init__(self, n):
Defined in :numref:`sec_softmax_scratch`, 在prefix后面生成新字符, 在x轴上缩放步长, 在y轴上缩放步长, 在动画中绘制数据, 在单词和标点符号之间插入空格, 在第一次迭代或使用随机抽样时初始化state, 在第六章定义, 在维度扩展后，, 在非Windows的平台上，使用4个进程来读取数据, 在非Windows的平台上，使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 0 if sys.platform.startswith('win') else 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 在预测期间整理测试集，以方便读取, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`""""""
for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile(os.path.join(data_dir, 'test', test_file),
os.path.join(data_dir, 'train_valid_test', 'test',
'unknown'))

d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')

d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')

def read_ptb():
将PTB数据集加载到文本行的列表中, 均方损失, 基于位置的前馈网络, 基于位置的前馈网络

Defined in :numref:`sec_transformer`""""""
def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs):
super(PositionWiseFFN, self).__init__(**kwargs)
self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,
activation='relu')
self.dense2 = nn.Dense(ffn_num_outputs, flatten=False)

def forward(self, X):
return self.dense2(self.dense1(X))

class AddNorm(nn.Block):
残差连接后进行层规范化, 填充词元的预测将通过乘以0权重在损失中过滤掉, 处理矩形输入, 多头注意力, 多头注意力

Defined in :numref:`sec_multihead-attention`""""""
def __init__(self, num_hiddens, num_heads, dropout, use_bias=False,
**kwargs):
super(MultiHeadAttention, self).__init__(**kwargs)
self.num_heads = num_heads
self.attention = d2l.DotProductAttention(dropout)
self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)
self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)

def forward(self, queries, keys, values, valid_lens):
# queries，keys，values的形状:
# (batch_size，查询或者“键－值”对的个数，num_hiddens)
# valid_lens　的形状:
# (batch_size，)或(batch_size，查询的个数)
# 经过变换后，输出的queries，keys，values　的形状:
# (batch_size*num_heads，查询或者“键－值”对的个数，
# num_hiddens/num_heads)
queries = transpose_qkv(self.W_q(queries), self.num_heads)
keys = transpose_qkv(self.W_k(keys), self.num_heads)
values = transpose_qkv(self.W_v(values), self.num_heads)

if valid_lens is not None:
# 在轴0，将第一项（标量或者矢量）复制num_heads次，
# 然后如此复制第二项，然后诸如此类。
valid_lens = valid_lens.repeat(self.num_heads, axis=0)

# output的形状:(batch_size*num_heads，查询的个数，
# num_hiddens/num_heads)
output = self.attention(queries, keys, values, valid_lens)

# output_concat的形状:(batch_size，查询的个数，num_hiddens)
output_concat = transpose_output(output, self.num_heads)
return self.W_o(output_concat)

def transpose_qkv(X, num_heads):
为了多注意力头的并行计算而变换形状, 如果X有一个轴，输出True, 如果存在，则返回gpu, 它的输出形状是(时间步数*批量大小,词表大小), 对锚框偏移量的转换, 对预测边界框的置信度进行排序, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
B = scores.argsort()[::-1]
keep = []  # 保留预测边界框的指标
while B.size > 0:
i = B[0]
keep.append(i)
if B.size == 1: break
iou = box_iou(boxes[i, :].reshape(-1, 4),
boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
inds = np.nonzero(iou <= iou_threshold)[0]
B = B[inds + 1]
return np.array(keep, dtype=np.int32, ctx=boxes.ctx)

def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
pos_threshold=0.009999999):
使用非极大值抑制来预测边界框, 将PTB数据集加载到文本行的列表中, 将SNLI数据集解析为前提、假设和标签, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def extract_text(s):
# 删除我们不会使用的信息
s = re.sub('\\(', '', s)
s = re.sub('\\)', '', s)
# 用一个空格替换两个或多个连续的空格
s = re.sub('\\s{2,}', ' ', s)
return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
if is_train else 'snli_1.0_test.txt')
with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]]
premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
hypotheses = [extract_text(row[2]) for row in rows if row[0] \
in label_set]
labels = [label_set[row[0]] for row in rows if row[0] in label_set]
return premises, hypotheses, labels

class SNLIDataset(gluon.data.Dataset):
用于加载SNLI数据集的自定义数据集, 将VOC标签中的RGB值映射到它们的类别索引, 将X和y拆分到多个设备上, 将多输入'X'和'y'拆分到多个设备, 将文件复制到目标目录, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`""""""
os.makedirs(target_dir, exist_ok=True)
shutil.copy(filename, target_dir)

def reorg_train_valid(data_dir, labels, valid_ratio):
将验证集从原始的训练集中拆分出来, 将文本行拆分为单词或字符词元, 将时间机器数据集加载到文本行的列表中, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 将最接近的真实边界框分配给锚框, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`""""""
num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
# 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU
jaccard = box_iou(anchors, ground_truth)
# 对于每个锚框，分配的真实边界框的张量
anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)
# 根据阈值，决定是否分配真实边界框
max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)
anc_i = np.nonzero(max_ious >= iou_threshold)[0]
box_j = indices[max_ious >= iou_threshold]
anchors_bbox_map[anc_i] = box_j
col_discard = np.full((num_anchors,), -1)
row_discard = np.full((num_gt_boxes,), -1)
for _ in range(num_gt_boxes):
max_idx = np.argmax(jaccard)
box_idx = (max_idx % num_gt_boxes).astype('int32')
anc_idx = (max_idx / num_gt_boxes).astype('int32')
anchors_bbox_map[anc_idx] = box_idx
jaccard[:, box_idx] = col_discard
jaccard[anc_idx, :] = row_discard
return anchors_bbox_map

def offset_boxes(anchors, assigned_bb, eps=1e-6):
对锚框偏移量的转换, 将机器翻译的文本序列转换成小批量, 将验证集从原始的训练集中拆分出来, 小批量随机梯度下降, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
for param in params:
param[:] = param - lr * param.grad / batch_size

def load_array(data_arrays, batch_size, is_train=True):
构造一个Gluon数据迭代器, 带有注意力机制解码器的基本接口, 带遮蔽的softmax交叉熵损失函数, 并行运行, 序列到序列模型的预测, 循环神经网络模型, 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入, 截断或填充文本序列, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 所以将所有文本行展平到一个列表中, 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次, 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测, 找到所有的non_keep索引，并将类设置为背景, 按出现频率排序, 损失的总和,样本数量, 文本词表, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示优化过程中2D变量的轨迹, 显示所有边界框, 显示所有边界框

Defined in :numref:`sec_anchor`""""""
def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj

labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))

def box_iou(boxes1, boxes2):
计算两个锚框或边界框列表中成对的交并比, 显示矩阵热图, 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,, 未知词元的索引为0, 构建从RGB到VOC类别索引的映射, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = np.zeros(256 ** 3)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 构造一个Gluon数据迭代器, 查询设备列表, 根据n个采样权重在, 根据n个采样权重在{1,...,n}中随机抽取
def __init__(self, sampling_weights):
Defined in :numref:`sec_word2vec_data`, 根据带有预测偏移量的锚框来预测边界框, 正在从, 正确预测数、预测总数, 正确预测的数量，预测的总数量, 残差连接后进行层规范化, 每个中心点都将有“boxes_per_pixel”个锚框，, 添加批量轴, 然后再与位置编码相加。, 生成y=Xw+b+噪声, 生成“boxes_per_pixel”个高和宽，, 生成以每个像素为中心具有不同形状的锚框, 生成锚框的所有中心点, 用GPU训练模型, 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引, 用于加载SNLI数据集的自定义数据集, 用于序列到序列学习的循环神经网络编码器, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqEncoder, self).__init__(**kwargs)
# 嵌入层
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)

def forward(self, X, *args):
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
# 在循环神经网络模型中，第一个轴对应于时间步
X = X.swapaxes(0, 1)
state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)
output, state = self.rnn(X, state)
# output的形状:(num_steps,batch_size,num_hiddens)
# state的形状:(num_layers,batch_size,num_hiddens)
return output, state

class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):
带遮蔽的softmax交叉熵损失函数, 用于测量运行时间, 用多GPU进行小批量训练, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`""""""
X_shards, y_shards = split_f(features, labels, devices)
with autograd.record():
pred_shards = [net(X_shard) for X_shard in X_shards]
ls = [loss(pred_shard, y_shard) for pred_shard, y_shard
in zip(pred_shards, y_shards)]
for l in ls:
l.backward()
# True标志允许使用过时的梯度，这很有用（例如，在微调BERT中）
trainer.step(labels.shape[0], ignore_stale_grad=True)
train_loss_sum = sum([float(l.sum()) for l in ls])
train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)
for pred_shard, y_shard in zip(pred_shards, y_shards))
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus(), split_f=d2l.split_batch):
用多GPU进行模型训练, 用多GPU进行模型训练, 用定制的训练机优化2D目标函数, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`""""""
# s1和s2是稍后将使用的内部状态变量
x1, x2, s1, s2 = -5, -2, 0, 0
results = [(x1, x2)]
for i in range(steps):
if f_grad:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
else:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
results.append((x1, x2))
print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')
return results

def show_trace_2d(f, results):
显示优化过程中2D变量的轨迹, 稍加修改的ResNet-18模型, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`""""""
def resnet_block(num_channels, num_residuals, first_block=False):
blk = nn.Sequential()
for i in range(num_residuals):
if i == 0 and not first_block:
blk.add(d2l.Residual(
num_channels, use_1x1conv=True, strides=2))
else:
blk.add(d2l.Residual(num_channels))
return blk

net = nn.Sequential()
# 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),
nn.BatchNorm(), nn.Activation('relu'))
net.add(resnet_block(64, 2, first_block=True),
resnet_block(128, 2),
resnet_block(256, 2),
resnet_block(512, 2))
net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))
return net

def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):
使用多个GPU计算数据集上模型的精度, 线性回归模型, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 绘制列表长度对的直方图, 绘制图像列表, 绘制数据点, 统计词元的频率, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 缓存k个随机采样结果, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def forward(self, X, state):
raise NotImplementedError

class EncoderDecoder(nn.Block):
编码器-解码器架构的基类, 编码器-解码器架构的基类, 缩放点积注意力, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`""""""
def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)

# queries的形状：(batch_size，查询的个数，d)
# keys的形状：(batch_size，“键－值”对的个数，d)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
# valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
def forward(self, queries, keys, values, valid_lens=None):
d = queries.shape[-1]
# 设置transpose_b=True为了交换keys的最后两个维度
scores = npx.batch_dot(queries, keys, transpose_b=True) / math.sqrt(d)
self.attention_weights = masked_softmax(scores, valid_lens)
return npx.batch_dot(self.dropout(self.attention_weights), values)

class AttentionDecoder(d2l.Decoder):
带有注意力机制解码器的基本接口, 获取输入序列的词元及其片段索引, 裁剪梯度, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
if isinstance(net, gluon.Block):
params = [p.data() for p in net.collect_params().values()]
else:
params = net.params
norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))
if norm > theta:
for param in params:
param.grad[:] *= theta / norm

def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
训练模型一个迭代周期（定义见第8章）, 要形成“中心词-上下文词”对，每个句子至少需要有2个词, 计算BLEU, 计算BLEU

Defined in :numref:`sec_seq2seq_training`""""""
pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
len_pred, len_label = len(pred_tokens), len(label_tokens)
score = math.exp(min(0, 1 - len_label / len_pred))
for n in range(1, k + 1):
num_matches, label_subs = 0, collections.defaultdict(int)
for i in range(len_label - n + 1):
label_subs[' '.join(label_tokens[i: i + n])] += 1
for i in range(len_pred - n + 1):
if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
num_matches += 1
label_subs[' '.join(pred_tokens[i: i + n])] -= 1
score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
return score

def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
cmap='Reds'):
显示矩阵热图, 计算两个锚框或边界框列表中成对的交并比, 计算二维互相关运算, 计算在指定数据集上模型的精度, 计算预测正确的数量, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def evaluate_accuracy(net, data_iter):
计算在指定数据集上模型的精度, 训练序列到序列模型, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`""""""
net.initialize(init.Xavier(), force_reinit=True, ctx=device)
trainer = gluon.Trainer(net.collect_params(), 'adam',
{'learning_rate': lr})
loss = MaskedSoftmaxCELoss()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[10, num_epochs])
for epoch in range(num_epochs):
timer = d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失求和，词元数量
for batch in data_iter:
X, X_valid_len, Y, Y_valid_len = [
x.as_in_ctx(device) for x in batch]
bos = np.array([tgt_vocab['<bos>']] * Y.shape[0],
ctx=device).reshape(-1, 1)
dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # 强制教学
with autograd.record():
Y_hat, _ = net(X, dec_input, X_valid_len)
l = loss(Y_hat, Y, Y_valid_len)
l.backward()
d2l.grad_clipping(net, 1)
num_tokens = Y_valid_len.sum()
trainer.step(num_tokens)
metric.add(l.sum(), num_tokens)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, (metric[0] / metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
f'tokens/sec on {str(device)}')

def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights=False):
序列到序列模型的预测, 训练损失之和,词元数量, 训练损失之和，训练准确率之和，样本数, 训练数据集中样本最少的类别中的样本数, 训练模型, 训练模型一个迭代周期（定义见第3章）, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
# 训练损失总和、训练准确度总和、样本数
metric = Accumulator(3)
if isinstance(updater, gluon.Trainer):
updater = updater.step
for X, y in train_iter:
# 计算梯度并更新参数
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
updater(X.shape[0])
metric.add(float(l.sum()), accuracy(y_hat, y), y.size)
# 返回训练损失和训练精度
return metric[0] / metric[2], metric[1] / metric[2]

class Animator:
在动画中绘制数据, 训练模型一个迭代周期（定义见第8章）, 训练模型（定义见第3章）, 训练模型（定义见第8章）, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
loss = gluon.loss.SoftmaxCrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
# 初始化
if isinstance(net, gluon.Block):
net.initialize(ctx=device, force_reinit=True,
init=init.Normal(0.01))
trainer = gluon.Trainer(net.collect_params(),
'sgd', {'learning_rate': lr})
updater = lambda batch_size: trainer.step(batch_size)
else:
updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(
net, train_iter, loss, updater, device, use_random_iter)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, [ppl])
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(nn.Block):
循环神经网络模型, 记录多次运行时间, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的图表大小, 设置matplotlib的轴, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 评估给定数据集上模型的损失, 词元/秒, 词元化“英语－法语”数据数据集, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 读取IMDb评论数据集文本序列和标签, 读取fname来给标签字典返回一个文件名, 读取所有VOC图像并标注, 读取香蕉检测数据集中的图像和标签, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`""""""
data_dir = d2l.download_extract('banana-detection')
csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
else 'bananas_val', 'label.csv')
csv_data = pd.read_csv(csv_fname)
csv_data = csv_data.set_index('img_name')
images, targets = [], []
for img_name, target in csv_data.iterrows():
images.append(image.imread(
os.path.join(data_dir, 'bananas_train' if is_train else
'bananas_val', 'images', f'{img_name}')))
# 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
# 其中所有图像都具有相同的香蕉类（索引为0）
targets.append(list(target))
return images, np.expand_dims(np.array(targets), 1) / 256

class BananasDataset(gluon.data.Dataset):
一个用于加载香蕉检测数据集的自定义数据集, 跳过文件头行(列名), 载入“英语－法语”数据集, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens), 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,, 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，, 返回Fashion-MNIST数据集的文本标签, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回带有负采样的跳元模型的小批量样本, 返回平均时间, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu, 返回所有可用的GPU，如果没有GPU，则返回[cpu()]

Defined in :numref:`sec_use_gpu`""""""
devices = [npx.gpu(i) for i in range(npx.num_gpus())]
return devices if devices else [npx.cpu()]

def corr2d(X, K):
计算二维互相关运算, 返回数据迭代器和IMDb评论数据集的词表, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`""""""
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
train_data = read_imdb(data_dir, True)
test_data = read_imdb(data_dir, False)
train_tokens = d2l.tokenize(train_data[0], token='word')
test_tokens = d2l.tokenize(test_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5)
train_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
test_features = np.array([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])
train_iter = d2l.load_array((train_features, train_data[1]), batch_size)
test_iter = d2l.load_array((test_features, test_data[1]), batch_size,
is_train=False)
return train_iter, test_iter, vocab

def predict_sentiment(net, vocab, sequence):
预测文本序列的情感, 返回时光机器数据集的词元索引列表和词表, 返回时光机器数据集的迭代器和词表, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回时间总和, 返回累计时间, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(nn.Block):
编码器-解码器架构的基本编码器接口, 返回负采样中的噪声词, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`""""""
# 索引为1、2、...（索引0是词表中排除的未知标记）
sampling_weights = [counter[vocab.to_tokens(i)]**0.75
for i in range(1, len(vocab))]
all_negatives, generator = [], RandomGenerator(sampling_weights)
for contexts in all_contexts:
negatives = []
while len(negatives) < len(contexts) * K:
neg = generator.draw()
# 噪声词不能是上下文词
if neg not in contexts:
negatives.append(neg)
all_negatives.append(negatives)
return all_negatives

def batchify(data):
返回带有负采样的跳元模型的小批量样本, 返回跳元模型中的中心词和上下文词, 逆转transpose_qkv函数的操作, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`""""""
X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
X = X.transpose(0, 2, 1, 3)
return X.reshape(X.shape[0], X.shape[1], -1)

class PositionalEncoding(nn.Block):
位置编码, 通过在最后一个轴上掩蔽元素来执行softmax操作, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`""""""
# X:3D张量，valid_lens:1D或2D张量
if valid_lens is None:
return npx.softmax(X)
else:
shape = X.shape
if valid_lens.ndim == 1:
valid_lens = valid_lens.repeat(shape[1])
else:
valid_lens = valid_lens.reshape(-1)
# 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,
value=-1e6, axis=1)
return npx.softmax(X).reshape(shape)

class AdditiveAttention(nn.Block):
加性注意力, 那么batch_idx是np.array（[0,0,0,1,1,1]）, 错误：未知词元类型：, 除以2来获得半高和半宽, 随机裁剪特征和标签图像, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
feature, rect = image.random_crop(feature, (width, height))
label = image.fixed_crop(label, *rect)
return feature, label

class VOCSegDataset(gluon.data.Dataset):
一个用于加载VOC数据集的自定义数据集, 预处理“英语－法语”数据集, 预测num_preds步, 预测前提和假设之间的逻辑关系, 预测文本序列的情感, 预测标签（定义见第3章）, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失, 预热期, 验证集中每个类别的样本数, ，否则返回cpu"
https://github.com/d2l-ai/d2l-zh,paddle.py,0,0.0,831,47.11,374,21.2,60,3.4,182,10.32,317,17.97,141,34,381,143,,"Accumulator, AdaptiveAvgPool2D, AddNorm, AdditiveAttention, Animator, AttentionDecoder, B, BERTEncoder, BERTModel, BananasDataset, BatchNorm2D, Benchmark, Compose, Conv2D, CrossEntropyLoss, DATA_HUB, DATA_URL, Decoder, DotProductAttention, Dropout, Embedding, Encoder, EncoderBlock, EncoderDecoder, Flatten, GRU, IPython, K, LSTM, Layer, LayerNorm, Linear, MSELoss, MaskLM, MaskedSoftmaxCELoss, MultiHeadAttention, NextSentencePred, NotImplementedError, P, PIL, PTBDataset, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, ReLU, Rectangle, Residual, Resize, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Sequential, T, Tanh, Timer, ToTensor, TokenEmbedding, TransformerEncoder, VOCSegDataset, VOC_CLASSES, VOC_COLORMAP, Vocab, W_k, W_o, W_q, W_v, X, X_valid_len, XavierUniform, Xs, Xtype, Y, Y_hat, Y_valid_len, Ys, _WikiTextDataset, __call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, __name__, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _read_wiki, _replace_mlm_tokens, _token_freqs, abs, acc, accuracy, add, add_patch, add_sublayer, addnorm1, addnorm2, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, annotate, append, apply, arange, areas1, areas2, argmax, args, assign_anchor_to_bbox, assigned_bb, astype, attention, attention_weight_seq, attention_weights, avg, ax, axes, backward, base_dir, batch, batch_class_labels, batch_idx, batch_mask, batch_offset, batch_size, batchify, bb_idx, bbox, bbox_mask, bbox_offset, bbox_to_rect, bboxes, begin_state, below_min_idx, bias, bleu, blk, blks, bn1, bn2, bos, box_area, box_center_to_corner, box_corner_to_center, box_idx, box_iou, box_j, boxes, boxes1, boxes2, boxes_per_pixel, build_array_nmt, c_anc, c_assigned_bb, cache_dir, candidate_pred_positions, candidates, center, center_h, center_w, centers, char, cla, class_id, class_labels, clear_grad, clear_output, cls_prob, cls_probs, cmap, cmp, col_discard, collections, color, colorbar, colormap, colormap2label, colors, combined, concat, conf, config_axes, content, context, contexts, contexts_negatives, contour, conv1, conv2, conv3, copyfile, corpus, corr2d, cosh, count_corpus, counter, counts, crop_size, csv_data, csv_fname, cumsum, cur_len, cx, cy, d2l, data, data_arrays, data_dir, data_iter, data_iter_fn, dataset, dec_X, dec_input, dec_state, decoder, default_values, dense1, dense2, description, detach, devices, dim, display, download, download_all, download_extract, draw, dropout, dtype, elem, elems, embed_size, embedding, embedding_name, enc_X, enc_outputs, enc_valid_len, encoded_X, encoder, enumerate, epoch, eps, eval, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_loss, examples, exp, ext, extend, extract_text, extractall, eye, f_grad, feature, feature_dim, features, ffn, ffn_num_hiddens, ffn_num_input, ffn_num_outputs, fig, figsize, file_name, filename, filter, first_block, flatten, float, fmt, fmts, fname, folder, folder_name, forward, forward_fn, fp, freq, functional, gca, generator, get, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_input, get_negatives, get_params, get_tokens_and_segments, get_xaxis, get_yaxis, grad, grad_clipping, grid, ground_truth, has_one_axis, hasattr, hashlib, height, hexdigest, hid_in_features, hidden, hidden_size, hist, hyperparams, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, imgs, imshow, in_channels, in_height, in_width, index, indices, indices_true, inds, init_state, init_weights, initial_indices, initial_indices_per_batch, initializer, input_channels, inputs, insert, int, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, iou_threshold, is_next, is_train, isinstance, item, items, iter, iterrows, jaccard, keep, key_size, keys, kwargs, label, label_count, label_seq, label_set, label_subs, label_tokens, labels, legend, len, len_label, len_pred, linear, lines, linreg, linspace, list, ln, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, log, loss, lower, lr, mask, masked_X, masked_softmax, masked_token, masks, math, matmul, matplotlib, matplotlib_inline, matrices, matrix, max_idx, max_ious, max_len, max_num_mlm_preds, max_tokens, max_window_size, maxlen, mean, meshgrid, metric, min, min_freq, mlm, mlm_Y, mlm_Y_hat, mlm_in_features, mlm_input_tokens, mlm_l, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mlm_weights_X, mlp, mnist_test, mnist_train, multibox_detection, multibox_prior, multibox_target, n_valid_per_label, name, ncols, ndim, neg, negative, negatives, net, next, next_sentence, nms, nms_threshold, nn_Module, no_space, non_keep, norm, norm_shape, normal, normalize_image, normalized_shape, nrows, nsp, nsp_Y_hat, nsp_data_from_paragraph, nsp_in_features, nsp_l, nsp_labels, nsp_y, num_anchors, num_batches, num_channels, num_classes, num_cols, num_directions, num_epochs, num_examples, num_gt_boxes, num_heads, num_hiddens, num_inputs, num_layers, num_matches, num_mlm_preds, num_noise_words, num_pred_positions, num_preds, num_ratios, num_residuals, num_rows, num_sizes, num_steps, num_subseqs, num_tokens, num_workers, numel, numpy, obj, offset, offset_boxes, offset_h, offset_inverse, offset_pred, offset_preds, offset_w, offset_wh, offset_xy, one_hot, open, os, out, out_channels, out_grid, output, output_concat, output_seq, outputs, padding_token, paddle, paddlescatter, pandas, paragraph, paragraphs, param, parameters, params, parts, patch, patches, pcm, place, plot, plt, population, pos, pos_embedding, pos_encoding, pos_threshold, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_X, pred_positions_and_labels, pred_seq, pred_tokens, predict, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, predicted_bb, predicted_bbox, preds, prefix, premise, premises, preprocess_nmt, prev_char, print, property, queries, query_size, random, range, ratio_tensor, ratios, raw_text, rcParams, re, read, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, readlines, rect, reduce_mean, reduce_sum, reduction, relu, remove, reorg_test, reorg_train_valid, replace, requests, reserved_tokens, reset, reshape, resize, resnet18, resnet_block, results, review, rnn, rnn_layer, round, row, row_axes, row_discard, row_matrices, rows, rstrip, s1, s2, sampling_weights, save_attention_weights, scale, score, scores, segment_embedding, segments, segments_X, self, sentence, sentences, seq_data_iter_random, seq_data_iter_sequential, sequence, sequence_mask, set_axes, set_figsize, set_hatch, set_index, set_matplotlib_formats, set_title, set_value, set_xlabel, set_xlim, set_xscale, set_ylabel, set_ylim, set_yscale, sgd, sha1, sha1_hash, shape, shift_x, shift_y, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, shutil, sinh, size, size_tensor, sizes, softmax, sorted, source, speed, split, split_batch, squared_loss, squeeze, src_array, src_sentence, src_tokens, src_valid_len, src_vocab, stack, start, state, states, std, step, steps, steps_h, steps_w, stop, stop_gradient, str, strides, strip, subplots, subsample, subsampled, sum, super, synthetic_data, sys, t, tarfile, target, target_dir, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, theta, tik, time, timer, times, titles, to, to_tokens, token, token_embedding, token_freqs, token_ids, token_to_idx, tokenize, tokenize_nmt, tokens, tokens_X, tokens_a, tokens_b, train, train_2d, train_acc, train_acc_sum, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_data, train_epoch_ch3, train_epoch_ch8, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_seq2seq, train_set, train_tokens, trainer, trainer_fn, transform, transpose, transpose_output, transpose_qkv, true, trues, truncate_pad, try_all_gpus, try_gpu, tuple, txt_fname, type, union_areas, unique, uniques, unk, unknown_idx, unsqueeze, unweighted_loss, update, updater, url, use_1x1conv, use_bias, use_random_iter, use_svg_display, val_iter, valid_len, valid_lens, valid_lens_x, valid_ratio, value, value_size, values, vecs, voc_colormap2label, voc_dir, voc_label_indices, voc_rand_crop, vocab, vocab_size, w_v, warnings, weight, weight_hh_attr, weight_ih_attr, weighted_loss, weights, width, window_size, write, x, x1, x2, xlabel, xlim, xlist, xscale, xy, xytext, y1, y2, y_hat, ylabel, ylim, ylist, yscale, zeros, zip, zipfile","IPython.display, PIL.Image, collections.Counter, collections.defaultdict, hashlib.sha1, math.exp, math.floor, math.pow, math.sqrt, matplotlib.pyplot, matplotlib_inline.backend_inline, np.array, np.float32, np.genfromtxt, os.listdir, os.makedirs, os.path.dirname, os.path.exists, os.path.join, os.path.splitext, paddle.CPUPlace, paddle.CUDAPlace, paddle.DataParallel, paddle.ParamAttr, paddle.abs, paddle.arange, paddle.argmax, paddle.argsort, paddle.bmm, paddle.cast, paddle.concat, paddle.cos, paddle.cosh, paddle.create_parameter, paddle.device.cuda.device_count, paddle.disable_signal_handler, paddle.exp, paddle.eye, paddle.float32, paddle.full, paddle.int32, paddle.int64, paddle.io.DataLoader, paddle.io.Dataset, paddle.io.TensorDataset, paddle.is_tensor, paddle.linspace, paddle.log, paddle.matmul, paddle.max, paddle.maximum, paddle.meshgrid, paddle.minimum, paddle.nn, paddle.nn.Layer, paddle.nn.functional, paddle.nn.initializer.Assign, paddle.nn.initializer.Normal, paddle.no_grad, paddle.nonzero, paddle.normal, paddle.ones, paddle.ones_like, paddle.optimizer.Adam, paddle.optimizer.Optimizer, paddle.optimizer.SGD, paddle.pow, paddle.rand, paddle.randn, paddle.repeat_interleave, paddle.reshape, paddle.set_device, paddle.sin, paddle.sinh, paddle.sqrt, paddle.stack, paddle.sum, paddle.tanh, paddle.tile, paddle.to_tensor, paddle.unsqueeze, paddle.vision.datasets.FashionMNIST, paddle.vision.image.image_load, paddle.vision.set_image_backend, paddle.vision.transforms, paddle.vision.transforms.Normalize, paddle.vision.transforms.RandomCrop, paddle.vision.transforms.crop, paddle.zeros, paddlevision.image_load, pd.read_csv, random.choice, random.choices, random.randint, random.random, random.shuffle, random.uniform, re.sub, requests.get, shutil.copy, sys.modules, tarfile.open, time.time, warnings.filterwarnings, zipfile.ZipFile","__call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _read_wiki, _replace_mlm_tokens, accuracy, add, annotate, assign_anchor_to_bbox, attention_weights, avg, batchify, bbox_to_rect, begin_state, bleu, box_center_to_corner, box_corner_to_center, box_iou, build_array_nmt, copyfile, corr2d, count_corpus, cumsum, data, download, download_all, download_extract, draw, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_loss, extract_text, filter, forward, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_negatives, get_tokens_and_segments, grad_clipping, has_one_axis, init_state, init_weights, keep, linreg, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, masked_softmax, multibox_detection, multibox_prior, multibox_target, nms, no_space, normalize_image, offset_boxes, offset_inverse, plot, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, preprocess_nmt, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, reorg_test, reorg_train_valid, reset, resnet18, resnet_block, seq_data_iter_random, seq_data_iter_sequential, sequence_mask, set_axes, set_figsize, sgd, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, split_batch, squared_loss, start, stop, subsample, sum, synthetic_data, to_tokens, token_freqs, tokenize, tokenize_nmt, train_2d, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_epoch_ch3, train_epoch_ch8, train_seq2seq, transpose_output, transpose_qkv, truncate_pad, try_all_gpus, try_gpu, unk, use_svg_display, voc_colormap2label, voc_label_indices, voc_rand_crop","Accumulator, AddNorm, AdditiveAttention, Animator, AttentionDecoder, BERTEncoder, BERTModel, BananasDataset, Benchmark, Decoder, DotProductAttention, Encoder, EncoderBlock, EncoderDecoder, MaskLM, MaskedSoftmaxCELoss, MultiHeadAttention, NextSentencePred, PTBDataset, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, Residual, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Timer, TokenEmbedding, TransformerEncoder, VOCSegDataset, Vocab, _WikiTextDataset","B, DATA_HUB, DATA_URL, VOC_CLASSES, VOC_COLORMAP, X, X_valid_len, Xs, Xtype, Y, Y_hat, Y_valid_len, Ys, abs, acc, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, arange, areas1, areas2, argmax, array, assigned_bb, astype, attention_weight_seq, ax, axes, b, base_dir, batch, batch_class_labels, batch_idx, batch_mask, batch_offset, batch_size, bb_idx, bbox, bbox_mask, bbox_offset, below_min_idx, blk, bos, box_area, box_idx, box_j, boxes, boxes_per_pixel, c_anc, c_assigned_bb, candidate_pred_positions, center, center_h, center_w, centers, char, class_id, class_labels, cls_prob, cmp, col_discard, color, colormap, colormap2label, colors, combined, concat, conf, context, contexts, contexts_negatives, corpus, cos, cosh, counter, counts, csv_data, csv_fname, cur_len, cx, cy, d, d2l, data, data_arrays, data_dir, data_iter, dataset, dec_X, dec_input, dec_state, device, devices, elem, elems, enc_X, enc_outputs, enc_valid_len, encoded_X, epoch, examples, exp, ext, eye, feature, features, fig, figsize, file_name, float32, fmt, fname, folder_name, fp, freq, generator, get_input, h, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, in_height, in_width, index, indices, indices_true, inds, initial_indices, initial_indices_per_batch, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, is_next, jaccard, keep, keys, l, label, label_count, label_set, label_subs, label_tokens, labels, legend, len_label, len_pred, lines, linspace, log, loss, mask, masked_X, masked_token, masks, matmul, matrix, max_idx, max_ious, max_len, max_num_mlm_preds, maxlen, meshgrid, metric, mlm_Y_hat, mlm_input_tokens, mlm_l, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mnist_test, mnist_train, n, n_valid_per_label, neg, negative, negatives, net, next_sentence, nn_Module, non_keep, norm, normal, nsp_Y_hat, nsp_data_from_paragraph, nsp_l, nsp_labels, num_anchors, num_batches, num_classes, num_cols, num_gt_boxes, num_matches, num_mlm_preds, num_pred_positions, num_ratios, num_rows, num_sizes, num_subseqs, num_tokens, num_workers, numpy, obj, offset, offset_h, offset_pred, offset_w, offset_wh, offset_xy, ones, optimizer, out, out_grid, output, output_concat, output_seq, outputs, paragraph, paragraphs, param, params, parts, patch, patches, pcm, place, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_and_labels, pred_tokens, predict, predicted_bb, predicted_bbox, preds, premise, premises, queries, r, rand, randn, ratio_tensor, raw_text, rect, reduce_mean, reduce_sum, reserved_tokens, reshape, results, review, row, row_axes, row_discard, row_matrices, rows, s, s1, s2, sampling_weights, score, scores, segments, sentence, sentences, sequence, sha1, sha1_hash, shape, shift_x, shift_y, sin, sinh, size, size_tensor, source, speed, src_array, src_tokens, src_valid_len, src_vocab, stack, state, steps_h, steps_w, subsampled, tanh, target, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, timer, titles, to, token, token_ids, tokens, tokens_a, tokens_b, train_acc, train_acc_sum, train_data, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_set, train_tokens, trans, transpose, true, trues, txt_fname, union_areas, uniques, unweighted_loss, updater, url, val_iter, valid_len, valid_lens, values, vecs, voc_dir, vocab, w, weight_hh_attr, weight_ih_attr, weighted_loss, weights, window_size, x, x1, x2, y, y1, y2, y_hat, zeros","BERT模型

Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

Defined in :numref:`subsec_bert_input_rep`, BERT编码器

Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_image_augmentation`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_multihead-attention`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_semantic_segmentation`, Defined in :numref:`sec_semantic_segmentation`

Defined in :numref:`sec_semantic_segmentation`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, GloVe嵌入, transformer编码器

Defined in :numref:`sec_transformer`, transformer编码器块

Defined in :numref:`sec_transformer`, 一个用于加载香蕉检测数据集的自定义数据集

Defined in :numref:`sec_object-detection-dataset`, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`, 下采样高频词

Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`, 启动计时器, 在n个变量上累加, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

Defined in :numref:`sec_seq2seq_decoder`, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`, 均方损失

Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

Defined in :numref:`sec_transformer`, 如果存在，则返回gpu(i)，否则返回cpu()。

Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

Defined in :numref:`sec_multi_gpu`, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

Defined in :numref:`sec_anchor`, 显示矩阵热图

Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`, 构造一个Paddle数据迭代器

Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

Defined in :numref:`subsec_labeling-anchor-boxes`, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`, 绘制图像列表

Defined in :numref:`sec_fashion_mnist`, 绘制数据点

Defined in :numref:`sec_calculus`, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

Defined in :numref:`sec_bert`, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`, 计算BLEU

Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

Defined in :numref:`sec_anchor`, 计算二维互相关运算

Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 训练网络一个迭代周期（定义见第8章)

Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

Defined in :numref:`sec_calculus`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失。

Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

Defined in :numref:`sec_kaggle_cifar10`, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。

Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`",", 	, 
,  ,  . ,  examples,  examples/sec on ,  sec,  sec/epoch,  tokens/sec on ,  training examples,  validation examples,  不存在于 ,  词元/秒 , #1f77b4, #ff7f0e, ,, , , , test acc , , train acc , , x1: , , x2: , ,.!?, -, ->, -o, ., .., ..., .1f, .3f, .4f, .gz, .jpg, .png, .tar, .zip, /, 01ada507287d82875905620988597833ad4e0903, 090b5e7e70c295757f55df93cb0a180b9691891a, 0b8703943ccdb6eb788e6f091b8946e82231bc4d, 0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d, 2068874e4b9a9f0fb07ebe0ad2b29754449ccacd, 319d85e578af0cdc590547f26231e4e31cdf1e42, 3c914d17d80b1459be871a5039ac23e752a53cbe, 4e443f8a2eca6b1dac8a6c57641b67dd40621a49, 585e9cc93e70b39160e7921475f9bcd7d31219ce, 5de26c8fce5ccdea9f91267273464dc968d20d72, 76e5be1548fd8222e5074cf0faae75edff8cf93f, 94646ad1522d915e7b0f9296181140edcf86a4f5, 9fcde07509c7e87ec61c640c1b2753d9041758e4, : , <bos>, <cls>, <eos>, <mask>, <pad>, <sep>, <unk>, BERT模型

    Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

    Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

    Defined in :numref:`subsec_bert_input_rep`, BERT编码器

    Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_image_augmentation`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_multihead-attention`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_semantic_segmentation`, Defined in :numref:`sec_semantic_segmentation`

    Defined in :numref:`sec_semantic_segmentation`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, Done, GloVe嵌入, ImageSets, JPEGImages, Reds, SNLI, Segmentation, SegmentationClass, VOC2012, VOCdevkit, VOCtrainval_11-May-2012.tar, [^A-Za-z]+, \(, \), \s{2,}, __len__, aclImdb, aeroplane, airfoil, airfoil_self_noise.dat, ankle boot, b, b5116e234e9eb9076672cfeabf5469f3eec904fa, background, bag, banana-detection, banana-detection.zip, bananas_train, bananas_val, bicycle, bird, boat, bottle, bus, c, c1816da3821ae9f43899be655002f6c723e91b88, car, cat, cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a, center, chair, char, cifar10_tiny, coat, contradiction, cow, cv2, data, diningtable, dog, dog_tiny, dress, entailment, epoch, epoch , f, fa19780a7b011d9b009e8bff8e99922a8ee2eb90, fba480ffa8aa7e0febbb511d181409f899b9baa5, fc, figure.figsize, float32, fra-eng, fra-eng.zip, fra.txt, g, g-., global_avg_pool, glove.42B.300d.zip, glove.42b.300d, glove.6B.100d.zip, glove.6B.50d.zip, glove.6b.100d, glove.6b.50d, gpu:{}, horse, hotdog, hotdog.zip, http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, http://d2l-data.s3-accelerate.amazonaws.com/, https://nlp.stanford.edu/projects/snli/snli_1.0.zip, https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip, ignore, images, img_name, int32, int64, k, kaggle_cifar10_tiny.zip, kaggle_dog_tiny.zip, kaggle_house_pred_test.csv, kaggle_house_pred_train.csv, kaggle_house_test, kaggle_house_train, label.csv, linear, loss, loss , loss: , m, m--, motorbike, ndim, neg, negative, neutral, none, perplexity, person, pos, positive, potted plant, ptb, ptb.train.txt, ptb.zip, pullover, r, r:, rb, read , resnet_block1, resnet_block2, resnet_block3, resnet_block4, sandal, sheep, shirt, sneaker, snli_1.0_test.txt, snli_1.0_train.txt, sofa, svg, t-shirt, test, test acc, time traveller, time_machine, timemachine.txt, train, train acc, train loss, train.txt, train_valid, train_valid_test, training on, transformer编码器

    Defined in :numref:`sec_transformer`, transformer编码器块

    Defined in :numref:`sec_transformer`, traveller, trouser, tv/monitor, unknown, utf-8, val.txt, valid, vec.txt, voc2012, w, wb, wiki.en, wiki.en.zip, wiki.train.tokens, wikitext-2, word, x1, x2,  ,  , 一个用于加载VOC数据集的自定义数据集
    Defined in :numref:`sec_semantic_segmentation`, 一个用于加载香蕉检测数据集的自定义数据集

    Defined in :numref:`sec_object-detection-dataset`, 下载, 下载DATA_HUB中的所有文件

    Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

    Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

    Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

    Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

    Defined in :numref:`sec_kaggle_house`, 下采样高频词

    Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

    Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

    Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

    Defined in :numref:`sec_bbox`, 位置编码

    Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用4个进程来读取数据

    Defined in :numref:`sec_fashion_mnist`, 使用GPU计算模型在数据集上的精度

    Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

    Defined in :numref:`sec_calculus`, 使用真实边界框标记锚框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

    Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

    Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

    Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

    Defined in :numref:`sec_object-detection-dataset`, 只有zip/tar文件可以被解压缩, 启动计时器, 困惑度 , 在n个变量上累加, 在prefix后面生成新字符

    Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

    Defined in :numref:`sec_seq2seq_decoder`, 在预测期间整理测试集，以方便读取

    Defined in :numref:`sec_kaggle_cifar10`, 均方损失

    Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

    Defined in :numref:`sec_transformer`, 如果存在，则返回gpu(i)，否则返回cpu()。

    Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

    Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

    Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

    Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

    Defined in :numref:`sec_multi_gpu`, 将文件复制到目标目录

    Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

    Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

    Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

    Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

    Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

    Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

    Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

    Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

    Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

    Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

    Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

    Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

    Defined in :numref:`sec_anchor`, 显示矩阵热图

    Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

    Defined in :numref:`sec_semantic_segmentation`, 构造一个Paddle数据迭代器

    Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 正在从, 残差连接后进行层规范化

    Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

    Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

    Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

    Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

    Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用多GPU进行小批量训练
    飞桨不支持在notebook上进行多GPU训练
    Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练
    Defined in :numref:`sec_image_augmentation`, 用定制的训练机优化2D目标函数

    Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

    Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

    Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

    Defined in :numref:`sec_machine_translation`, 绘制图像列表

    Defined in :numref:`sec_fashion_mnist`, 绘制数据点

    Defined in :numref:`sec_calculus`, 统计词元的频率

    Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

    Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

    Defined in :numref:`sec_bert`, 裁剪梯度

    Defined in :numref:`sec_rnn_scratch`, 计算BLEU

    Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

    Defined in :numref:`sec_anchor`, 计算二维互相关运算

    Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

    Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

    Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

    Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 训练网络一个迭代周期（定义见第8章)

    Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

    Defined in :numref:`sec_calculus`, 设置matplotlib的轴

    Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失。

    Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

    Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

    Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

    Defined in :numref:`sec_kaggle_cifar10`, 读取所有VOC图像并标注
    Defined in :numref:`sec_semantic_segmentation`, 读取香蕉检测数据集中的图像和标签

    Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

    Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

    Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。

    Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

    Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

    Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

    Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

    Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

    Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

    Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

    Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

    Defined in :numref:`sec_attention-scoring-functions`, 错误：未知词元类型：, 随机裁剪特征和标签图像

    Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

    Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

    Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`","0, 0.0, 0.0001, 0.009999999, 0.01, 0.1, 0.15, 0.22, 0.224, 0.225, 0.229, 0.3, 0.35, 0.406, 0.456, 0.485, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1, 1.0, 1.5, 10, 1000, 10000, 10000.0, 1000000.0, 1048576, 128, 1500, 192, 1e-06, 1e-08, 2, 2.5, 20, 200, 255, 256, 28, 3, 3.0, 3.5, 4, 5, 5.5, 50, 500, 512, 6, 600, 64, 768, 9, False, None, True","#    d2lbook build lib, # ((左上x,左上y),宽,高), # BERT微调所需的, # Don't edit it directly, # GloVe网站：https://nlp.stanford.edu/projects/glove/, # PIL图片, # PaddlePaddle的GRU层output的形状:(batch_size,time_steps,num_directions * num_hiddens),, # The below part is generated automatically through:, # X的形状：(batchsize,num_hiddens), # fastText网站：https://fasttext.cc/, # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens), # label的形状：(batch_size,num_steps), # num_hiddens/num_heads), # pred的形状：(batch_size,num_steps,vocab_size), # queries的形状：(batch_size，查询的个数，1，num_hidden), # scores的形状：(batch_size，查询的个数，“键-值”对的个数), # self.w_v仅有一个输出，因此从形状中移除最后那个维度。, # state[0]的形状:(num_layers,batch_size,num_hiddens), # tokens是一个字符串列表, # valid_len的形状：(batch_size,), # values的形状：(batch_size，“键－值”对的个数，值的维度), # 一旦序列结束词元被预测，输出序列的生成就完成了, # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1, # 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU, # 使用PaddlePaddle内置的优化器和损失函数, # 使用lambda函数捕获参数, # 使用定制的优化器和损失函数, # 使用广播方式进行求和, # 使用真实边界框来标记锚框的类别。, # 保存注意力权重（稍后讨论）, # 保留预测边界框的指标, # 偏移量转换, # 其中所有图像都具有相同的香蕉类（索引为0）, # 减去1，是因为我们需要考虑标签, # 创建一个足够长的P, # 初始化, # 初始化模型, # 删除我们不会使用的信息, # 前向传播, # 向图表中添加多个数据点, # 命中缓存, # 噪声词不能是上下文词, # 因为位置编码值在-1和1之间，, # 因此嵌入值乘以嵌入维度的平方根进行缩放，, # 图片张量, # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数, # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）, # 在循环神经网络模型中，第一个轴对应于时间步, # 在维度扩展后，, # 在这里，initial_indices包含子序列的随机起始索引, # 在遮蔽语言模型任务中不会预测特殊词元, # 在随机抽样的迭代过程中，, # 在预测时将net设置为评估模式, # 填充, # 填充输入, # 增量地绘制多条线, # 大写字母转换为小写字母, # 如果X有一个轴，输出True, # 如果一个锚框没有被分配，我们标记其为背景（值为零）, # 如果在下采样期间保留词元，则返回True, # 如果未提及状态，则默认为0, # 对于每个锚框，分配的真实边界框的张量, # 将模型设置为训练模式, # 将类标签和分配的边界框坐标初始化为零, # 将词元列表展平成一个列表, # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：, # 嵌入层, # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入, # 截断, # 排除未知词元'<unk>', # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,, # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻, # 根据阈值，决定是否分配真实边界框, # 正确预测的数量，总预测的数量, # 添加批量轴, # 然后再与位置编码相加。, # 用一个空格替换两个或多个连续的空格, # 索引为1、2、...（索引0是词表中排除的未知标记）, # 考虑1个'<cls>'词元和2个'<sep>'词元, # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表, # 获取下一句子预测任务的数据, # 获取遮蔽语言模型任务的数据, # 计算下一句子预测任务的损失, # 计算梯度并更新参数, # 计算遮蔽语言模型损失, # 训练和预测, # 训练损失总和、训练准确度总和、样本数, # 训练模型, # 设置为评估模式, # 跳过标题信息，例如fastText中的首行, # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens), # 输入paragraphs[i]是代表段落的句子字符串列表；, # 输出'X'的形状：(batch_size,num_steps,embed_size), # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,, # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，, # 返回从pos位置开始的长度为num_steps的序列, # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y）, # 这里的tokens是1D列表或2D列表, # 遮蔽语言模型任务中预测15%的随机词元, # 长度为num_steps的子序列的起始索引, # 需设定time_major=True,指定input的第一个维度为time_steps, #################   WARNING   ################, #1f77b4'), #ff7f0e'), BERT模型

Defined in :numref:`subsec_nsp`""""""
def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
ffn_num_hiddens, num_heads, num_layers, dropout,
max_len=1000, key_size=768, query_size=768, value_size=768,
hid_in_features=768, mlm_in_features=768,
nsp_in_features=768):
super(BERTModel, self).__init__()
self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
dropout, max_len=max_len, key_size=key_size,
query_size=query_size, value_size=value_size)
self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
nn.Tanh())
self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
self.nsp = NextSentencePred(nsp_in_features)

def forward(self, tokens, segments, valid_lens=None,
pred_positions=None):
encoded_X = self.encoder(tokens, segments, valid_lens)
if pred_positions is not None:
mlm_Y_hat = self.mlm(encoded_X, pred_positions)
else:
mlm_Y_hat = None
# 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引
nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
return encoded_X, mlm_Y_hat, nsp_Y_hat

d2l.DATA_HUB['wikitext-2'] = (
'https://s3.amazonaws.com/research.metamind.io/wikitext/'
'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')

def _read_wiki(data_dir):
Defined in :numref:`sec_bert-dataset`, BERT的掩蔽语言模型任务

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
super(MaskLM, self).__init__(**kwargs)
self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
nn.ReLU(),
nn.LayerNorm(num_hiddens),
nn.Linear(num_hiddens, vocab_size))

def forward(self, X, pred_positions):
num_pred_positions = pred_positions.shape[1]
pred_positions = pred_positions.reshape([-1])
batch_size = X.shape[0]
batch_idx = paddle.arange(0, batch_size)
# 假设batch_size=2，num_pred_positions=3
# 那么batch_idx是np.array（[0,0,0,1,1]）
batch_idx = paddle.repeat_interleave(batch_idx, num_pred_positions)
masked_X = X[batch_idx, pred_positions]
masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
mlm_Y_hat = self.mlp(masked_X)
return mlm_Y_hat

class NextSentencePred(nn.Layer):
BERT的下一句预测任务, Defined in :numref:`sec_bert-dataset`
# 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元
mlm_input_tokens = [token for token in tokens]
pred_positions_and_labels = []
# 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测
random.shuffle(candidate_pred_positions)
for mlm_pred_position in candidate_pred_positions:
if len(pred_positions_and_labels) >= num_mlm_preds:
break
masked_token = None
# 80%的时间：将词替换为“<mask>”词元
if random.random() < 0.8:
masked_token = '<mask>'
else:
# 10%的时间：保持词不变
if random.random() < 0.5:
masked_token = tokens[mlm_pred_position]
# 10%的时间：用随机词替换该词
else:
masked_token = random.choice(vocab.idx_to_token)
mlm_input_tokens[mlm_pred_position] = masked_token
pred_positions_and_labels.append(
(mlm_pred_position, tokens[mlm_pred_position]))
return mlm_input_tokens, pred_positions_and_labels

def _get_mlm_data_from_tokens(tokens, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`sec_bert-dataset`
if random.random() < 0.5:
is_next = True
else:
# paragraphs是三重列表的嵌套
next_sentence = random.choice(random.choice(paragraphs))
is_next = False
return sentence, next_sentence, is_next

def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_hybridize`
self.description = description

def __enter__(self):
self.timer = d2l.Timer()
return self

def __exit__(self, *args):
print(f'{self.description}: {self.timer.stop():.4f} sec')

def split_batch(X, y, devices):
将X和y拆分到多个设备上, Defined in :numref:`sec_language_model`
if use_random_iter:
self.data_iter_fn = d2l.seq_data_iter_random
else:
self.data_iter_fn = d2l.seq_data_iter_sequential
self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
self.batch_size, self.num_steps = batch_size, num_steps

def __iter__(self):
return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps,
use_random_iter=False, max_tokens=10000):
返回时光机器数据集的迭代器和词表, Defined in :numref:`sec_minibatches`
# 初始化模型
net = nn.Sequential(nn.Linear(5, 1))
def init_weights(m):
if type(m) == nn.Linear:
paddle.nn.initializer.Normal(m.weight, std=0.01)

net.apply(init_weights)

optimizer = trainer_fn(parameters=net.parameters(), **hyperparams)
loss = nn.MSELoss(reduction='none')
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
optimizer.clear_grad()
out = net(X)
y = y.reshape(out.shape)
l = loss(out, y)
l.mean().backward()
optimizer.step()
n += X.shape[0]
if n % 200 == 0:
timer.stop()
# MSELoss计算平方误差时不带系数1/2
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss) / 2,))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')

class Benchmark:
用于测量运行时间, Defined in :numref:`sec_minibatches`
data = np.genfromtxt(d2l.download('airfoil'),
dtype=np.float32, delimiter='\t')
data = d2l.tensor((data - data.mean(axis=0)) / data.std(axis=0))
data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),
batch_size, is_train=True)
return data_iter, data.shape[1]-1

def train_ch11(trainer_fn, states, hyperparams, data_iter,
feature_dim, num_epochs=2):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_multihead-attention`
def __init__(self, key_size, query_size, value_size, num_hiddens,
num_heads, dropout, bias=False, **kwargs):
super(MultiHeadAttention, self).__init__(**kwargs)
self.num_heads = num_heads
self.attention = d2l.DotProductAttention(dropout)
self.W_q = nn.Linear(query_size, num_hiddens, bias_attr=bias)
self.W_k = nn.Linear(key_size, num_hiddens, bias_attr=bias)
self.W_v = nn.Linear(value_size, num_hiddens, bias_attr=bias)
self.W_o = nn.Linear(num_hiddens, num_hiddens, bias_attr=bias)

def forward(self, queries, keys, values, valid_lens):
# queries，keys，values的形状:
# (batch_size，查询或者“键－值”对的个数，num_hiddens)
# valid_lens　的形状:
# (batch_size，)或(batch_size，查询的个数)
# 经过变换后，输出的queries，keys，values　的形状:
# (batch_size*num_heads，查询或者“键－值”对的个数，
# num_hiddens/num_heads)
queries = transpose_qkv(self.W_q(queries), self.num_heads)
keys = transpose_qkv(self.W_k(keys), self.num_heads)
values = transpose_qkv(self.W_v(values), self.num_heads)
if valid_lens is not None:
# 在轴0，将第一项（标量或者矢量）复制num_heads次，
# 然后如此复制第二项，然后诸如此类。
valid_lens = paddle.repeat_interleave(
valid_lens, repeats=self.num_heads, axis=0)

# output的形状:(batch_size*num_heads，查询的个数，
# num_hiddens/num_heads)
output = self.attention(queries, keys, values, valid_lens)

# output_concat的形状:(batch_size，查询的个数，num_hiddens)
output_concat = transpose_output(output, self.num_heads)
return self.W_o(output_concat)

def transpose_qkv(X, num_heads):
为了多注意力头的并行计算而变换形状, Defined in :numref:`sec_semantic_segmentation`

Defined in :numref:`sec_semantic_segmentation`""""""
读取所有VOC图像并标注, Defined in :numref:`sec_softmax_scratch`
self.data = [0.0] * n

def add(self, *args):
self.data = [a + float(b) for a, b in zip(self.data, args)]

def reset(self):
self.data = [0.0] * len(self.data)

def __getitem__(self, idx):
return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
训练模型一个迭代周期（定义见第3章）, Defined in :numref:`sec_text_preprocessing`
if tokens is None:
tokens = []
if reserved_tokens is None:
reserved_tokens = []
# 按出现频率排序
counter = count_corpus(tokens)
self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
reverse=True)
# 未知词元的索引为0
self.idx_to_token = ['<unk>'] + reserved_tokens
self.token_to_idx = {token: idx
for idx, token in enumerate(self.idx_to_token)}
for token, freq in self._token_freqs:
if freq < min_freq:
break
if token not in self.token_to_idx:
self.idx_to_token.append(token)
self.token_to_idx[token] = len(self.idx_to_token) - 1

def __len__(self):
return len(self.idx_to_token)

def __getitem__(self, tokens):
if not isinstance(tokens, (list, tuple)):
return self.token_to_idx.get(tokens, self.unk)
return [self.__getitem__(token) for token in tokens]

def to_tokens(self, indices):
if not isinstance(indices, (list, tuple)):
return self.idx_to_token[indices]
return [self.idx_to_token[index] for index in indices]

@property
def unk(self):  # 未知词元的索引为0
return 0

@property
def token_freqs(self):
return self._token_freqs

def count_corpus(tokens):
统计词元的频率, Defined in :numref:`sec_word2vec_data`
# Exclude
self.population = list(range(1, len(sampling_weights) + 1))
self.sampling_weights = sampling_weights
self.candidates = []
self.i = 0

def draw(self):
if self.i == len(self.candidates):
# 缓存k个随机采样结果
self.candidates = random.choices(
self.population, self.sampling_weights, k=10000)
self.i = 0
self.i += 1
return self.candidates[self.i - 1]

generator = RandomGenerator([2, 3, 4])
[generator.draw() for _ in range(10)]

def get_negatives(all_contexts, vocab, counter, K):
返回负采样中的噪声词, Defined in :numref:`subsec_prepare_mlm_data`
max_num_mlm_preds = round(max_len * 0.15)
all_token_ids, all_segments, valid_lens,  = [], [], []
all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
nsp_labels = []
for (token_ids, pred_positions, mlm_pred_label_ids, segments,
is_next) in examples:
all_token_ids.append(paddle.to_tensor(token_ids + [vocab['<pad>']] * (
max_len - len(token_ids)), dtype=paddle.int64))
all_segments.append(paddle.to_tensor(segments + [0] * (
max_len - len(segments)), dtype=paddle.int64))
# valid_lens不包括'<pad>'的计数
valid_lens.append(paddle.to_tensor(len(token_ids), dtype=paddle.float32))
all_pred_positions.append(paddle.to_tensor(pred_positions + [0] * (
max_num_mlm_preds - len(pred_positions)), dtype=paddle.int64))
# 填充词元的预测将通过乘以0权重在损失中过滤掉
all_mlm_weights.append(
paddle.to_tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (
max_num_mlm_preds - len(pred_positions)),
dtype=paddle.float32))
all_mlm_labels.append(paddle.to_tensor(mlm_pred_label_ids + [0] * (
max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=paddle.int64))
nsp_labels.append(paddle.to_tensor(is_next, dtype=paddle.int64))
return (all_token_ids, all_segments, valid_lens, all_pred_positions,
all_mlm_weights, all_mlm_labels, nsp_labels)

class _WikiTextDataset(paddle.io.Dataset):
Defined in :numref:`subsec_prepare_mlm_data`, GloVe嵌入
def __init__(self, embedding_name):
Defined in :numref:`sec_synonyms`, transformer编码器块

Defined in :numref:`sec_transformer`""""""
def __init__(self, key_size, query_size, value_size, num_hiddens,
norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
dropout, use_bias=False, **kwargs):
super(EncoderBlock, self).__init__(**kwargs)
self.attention = d2l.MultiHeadAttention(
key_size, query_size, value_size, num_hiddens, num_heads, dropout,
use_bias)
self.addnorm1 = AddNorm(norm_shape, dropout)
self.ffn = PositionWiseFFN(
ffn_num_input, ffn_num_hiddens, num_hiddens)
self.addnorm2 = AddNorm(norm_shape, dropout)

def forward(self, X, valid_lens):
Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
return self.addnorm2(Y, self.ffn(Y))

class TransformerEncoder(d2l.Encoder):
transformer编码器, 一个用于加载VOC数据集的自定义数据集
Defined in :numref:`sec_semantic_segmentation`""""""

def __init__(self, is_train, crop_size, voc_dir):
self.transform = paddle.vision.transforms.Normalize(
mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
self.crop_size = crop_size
features, labels = read_voc_images(voc_dir, is_train=is_train)
self.features = [self.normalize_image(feature)
for feature in self.filter(features)]
self.labels = self.filter(labels)
self.colormap2label = voc_colormap2label()
print('read ' + str(len(self.features)) + ' examples')

def normalize_image(self, img):
return self.transform(img.astype(""float32"") / 255)

def filter(self, imgs):
return [img for img in imgs if (
img.shape[1] >= self.crop_size[0] and
img.shape[2] >= self.crop_size[1])]

def __getitem__(self, idx):
feature = paddle.to_tensor(self.features[idx],dtype='float32')
label = paddle.to_tensor(self.labels[idx],dtype='float32')
feature, label = voc_rand_crop(feature,label,
*self.crop_size)
return (feature, voc_label_indices(label, self.colormap2label))

def __len__(self):
return len(self.features)

def load_data_voc(batch_size, crop_size):
加载VOC语义分割数据集, 一个用于加载香蕉检测数据集的自定义数据集

Defined in :numref:`sec_object-detection-dataset`""""""
def __init__(self, is_train):
self.features, self.labels = read_data_bananas(is_train)
print('read ' + str(len(self.features)) + (f' training examples' if
is_train else f' validation examples'))

def __getitem__(self, idx):
return (paddle.to_tensor(self.features[idx], dtype='float32').transpose([2, 0, 1]), self.labels[idx])

def __len__(self):
return len(self.features)

def load_data_bananas(batch_size):
加载香蕉检测数据集, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`""""""
fname = download(name)
base_dir = os.path.dirname(fname)
data_dir, ext = os.path.splitext(fname)
if ext == '.zip':
fp = zipfile.ZipFile(fname, 'r')
elif ext in ('.tar', '.gz'):
fp = tarfile.open(fname, 'r')
else:
assert False, '只有zip/tar文件可以被解压缩'
fp.extractall(base_dir)
return os.path.join(base_dir, folder) if folder else data_dir

def download_all():
下载DATA_HUB中的所有文件, 从零开始实现的循环神经网络模型
def __init__(self, vocab_size, num_hiddens,
get_params, init_state, forward_fn):
Defined in :numref:`sec_rnn_scratch`, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`""""""
cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
x1 = cx - 0.5 * w
y1 = cy - 0.5 * h
x2 = cx + 0.5 * w
y2 = cy + 0.5 * h
boxes = d2l.stack((x1, y1, x2, y2), axis=-1)
return boxes

def bbox_to_rect(bbox, color):
Defined in :numref:`sec_bbox`, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用非极大值抑制来预测边界框

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
batch_size = cls_probs.shape[0]
anchors = anchors.squeeze(0)
num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
out = []
for i in range(batch_size):
cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape([-1, 4])
conf = paddle.max(cls_prob[1:], 0)
class_id = paddle.argmax(cls_prob[1:], 0)
predicted_bb = offset_inverse(anchors, offset_pred)
keep = nms(predicted_bb, conf, nms_threshold)

# 找到所有的non_keep索引，并将类设置为背景
all_idx = paddle.arange(num_anchors, dtype='int64')
combined = paddle.concat((keep, all_idx))
uniques, counts = combined.unique(return_counts=True)
non_keep = uniques[counts == 1]
all_id_sorted = paddle.concat([keep, non_keep])
class_id[non_keep.numpy()] = -1
class_id = class_id[all_id_sorted]
conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
# pos_threshold是一个用于非背景预测的阈值
below_min_idx = (conf < pos_threshold)
class_id[below_min_idx.numpy()] = -1
conf[below_min_idx.numpy()] = 1 - conf[below_min_idx.numpy()]
pred_info = paddle.concat((paddle.to_tensor(class_id, dtype='float32').unsqueeze(1),
paddle.to_tensor(conf, dtype='float32').unsqueeze(1),
predicted_bb), axis=1)
out.append(pred_info)
return paddle.stack(out)

d2l.DATA_HUB['banana-detection'] = (
d2l.DATA_URL + 'banana-detection.zip',
'5de26c8fce5ccdea9f91267273464dc968d20d72')

def read_data_bananas(is_train=True):
读取香蕉检测数据集中的图像和标签, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始划分序列
offset = random.randint(0, num_steps)
num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
Xs = d2l.tensor(corpus[offset: offset + num_tokens])
Ys = d2l.tensor(corpus[offset + 1: offset + 1 + num_tokens])
Xs, Ys = Xs.reshape((batch_size, -1)), Ys.reshape((batch_size, -1))
num_batches = Xs.shape[1] // num_steps
for i in range(0, num_steps * num_batches, num_steps):
X = Xs[:, i: i + num_steps]
Y = Ys[:, i: i + num_steps]
yield X, Y

class SeqDataLoader:
加载序列数据的迭代器, 加载WikiText-2数据集

Defined in :numref:`subsec_prepare_mlm_data`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
paragraphs = _read_wiki(data_dir)
train_set = _WikiTextDataset(paragraphs, max_len)
train_iter = paddle.io.DataLoader(dataset=train_set, batch_size=batch_size, return_list=True,
shuffle=True, num_workers=num_workers)
return train_iter, train_set.vocab

def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
segments_X, valid_lens_x,
pred_positions_X, mlm_weights_X,
mlm_Y, nsp_y):
Defined in :numref:`sec_bert-pretraining`, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`""""""
state = net.begin_state(batch_size=1)
outputs = [vocab[prefix[0]]]
get_input = lambda: d2l.reshape(d2l.tensor(outputs[-1], place=device), (1, 1))
for y in prefix[1:]:  # 预热期
_, state = net(get_input(), state)
outputs.append(vocab[y])
for _ in range(num_preds):  # 预测num_preds步
y, state = net(get_input(), state)
outputs.append(int(paddle.reshape(paddle.argmax(y,axis=1),shape=[1])))
return ''.join([vocab.idx_to_token[i] for i in outputs])

def grad_clipping(net, theta):
裁剪梯度, 在动画中绘制数据
def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
figsize=(3.5, 2.5)):
Defined in :numref:`sec_softmax_scratch`, 在序列中屏蔽不相关的项

Defined in :numref:`sec_seq2seq_decoder`""""""
maxlen = X.shape[1]
mask = paddle.arange((maxlen), dtype=paddle.float32)[None, :] < valid_len[:, None]
Xtype = X.dtype
X = X.astype(paddle.float32)
X[~mask] = float(value)
return X.astype(Xtype)

class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
带遮蔽的softmax交叉熵损失函数, 基于位置的前馈网络

Defined in :numref:`sec_transformer`""""""
def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
**kwargs):
super(PositionWiseFFN, self).__init__(**kwargs)
self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
self.relu = nn.ReLU()
self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

def forward(self, X):
return self.dense2(self.relu(self.dense1(X)))

class AddNorm(nn.Layer):
残差连接后进行层规范化, 如果存在，则返回gpu(i)，否则返回cpu()。

Defined in :numref:`sec_use_gpu`""""""
if paddle.device.cuda.device_count() >= i + 1:
return paddle.CUDAPlace(i)
return paddle.CPUPlace()

def try_all_gpus():
返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。, 对锚框偏移量的转换

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
c_anc = d2l.box_corner_to_center(anchors)
c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
offset = d2l.concat([offset_xy, offset_wh], axis=1)
return offset

def multibox_target(anchors, labels):
使用真实边界框标记锚框, 将PTB数据集加载到文本行的列表中

Defined in :numref:`sec_word2vec_data`""""""
data_dir = d2l.download_extract('ptb')
# Readthetrainingset.
with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
raw_text = f.read()
return [line.split() for line in raw_text.split('\n')]

def subsample(sentences, vocab):
下采样高频词, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`""""""
if token == 'word':
return [line.split() for line in lines]
elif token == 'char':
return [list(line) for line in lines]
else:
print('错误：未知词元类型：' + token)

class Vocab:
文本词表, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`""""""
lines = [vocab[l] for l in lines]
lines = [l + [vocab['<eos>']] for l in lines]
array = d2l.tensor([truncate_pad(
l, num_steps, vocab['<pad>']) for l in lines])
valid_len = d2l.reduce_sum(
d2l.astype(array != vocab['<pad>'], d2l.int32), 1)
return array, valid_len

def load_data_nmt(batch_size, num_steps, num_examples=600):
返回翻译数据集的迭代器和词表, 将验证集从原始的训练集中拆分出来

Defined in :numref:`sec_kaggle_cifar10`""""""
# 训练数据集中样本最少的类别中的样本数
n = collections.Counter(labels.values()).most_common()[-1][1]
# 验证集中每个类别的样本数
n_valid_per_label = max(1, math.floor(n * valid_ratio))
label_count = {}
for train_file in os.listdir(os.path.join(data_dir, 'train')):
label = labels[train_file.split('.')[0]]
fname = os.path.join(data_dir, 'train', train_file)
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train_valid', label))
if label not in label_count or label_count[label] < n_valid_per_label:
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'valid', label))
label_count[label] = label_count.get(label, 0) + 1
else:
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train', label))
return n_valid_per_label

def reorg_test(data_dir):
在预测期间整理测试集，以方便读取, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
with paddle.no_grad():
for i, param in enumerate(params):
param -= lr * params[i].grad / batch_size
params[i].set_value(param)
params[i].clear_gradient()

def load_array(data_arrays, batch_size, is_train=True):
构造一个Paddle数据迭代器, 循环神经网络模型

Defined in :numref:`sec_rnn-concise`""""""
def __init__(self, rnn_layer, vocab_size, **kwargs):
super(RNNModel, self).__init__(**kwargs)
self.rnn = rnn_layer
self.vocab_size = vocab_size
self.num_hiddens = self.rnn.hidden_size
# 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
if self.rnn.num_directions==1:
self.num_directions = 1
self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
else:
self.num_directions = 2
self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

def forward(self, inputs, state):
X = F.one_hot(inputs.T, self.vocab_size)
Y, state = self.rnn(X, state)
# 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
# 它的输出形状是(时间步数*批量大小,词表大小)。
output = self.linear(Y.reshape((-1, Y.shape[-1])))
return output, state

def begin_state(self, batch_size=1):
if not isinstance(self.rnn, nn.LSTM):
# nn.GRU以张量作为隐状态
return  paddle.zeros(shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens])
else:
# nn.LSTM以元组作为隐状态
return (paddle.zeros(
shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens]),
paddle.zeros(
shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens]))

d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
'94646ad1522d915e7b0f9296181140edcf86a4f5')

def read_data_nmt():
载入“英语－法语”数据集, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = paddle.zeros([256 ** 3], dtype=paddle.int64)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 根据带有预测偏移量的锚框来预测边界框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
anc = d2l.box_corner_to_center(anchors)
pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)
predicted_bbox = d2l.box_center_to_corner(pred_bbox)
return predicted_bbox

def nms(boxes, scores, iou_threshold):
对预测边界框的置信度进行排序, 生成以每个像素为中心具有不同形状的锚框

Defined in :numref:`sec_anchor`""""""
in_height, in_width = data.shape[-2:]
place, num_sizes, num_ratios = data.place, len(sizes), len(ratios)
boxes_per_pixel = (num_sizes + num_ratios - 1)
size_tensor = paddle.to_tensor(sizes, place=place)
ratio_tensor = paddle.to_tensor(ratios, place=place)

# 为了将锚点移动到像素的中心，需要设置偏移量。
# 因为一个像素的的高为1且宽为1，我们选择偏移我们的中心0.5
offset_h, offset_w = 0.5, 0.5
steps_h = 1.0 / in_height  # 在y轴上缩放步长
steps_w = 1.0 / in_width  # 在x轴上缩放步长

# 生成锚框的所有中心点
center_h = (paddle.arange(in_height) + offset_h) * steps_h
center_w = (paddle.arange(in_width) + offset_w) * steps_w
shift_y, shift_x = paddle.meshgrid(center_h, center_w)
shift_y, shift_x = shift_y.reshape([-1]), shift_x.reshape([-1])

# 生成“boxes_per_pixel”个高和宽，
# 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)
w = paddle.concat((size_tensor * paddle.sqrt(ratio_tensor[0]),
sizes[0] * paddle.sqrt(ratio_tensor[1:])))\
* in_height / in_width  # 处理矩形输入
h = paddle.concat((size_tensor / paddle.sqrt(ratio_tensor[0]),
sizes[0] / paddle.sqrt(ratio_tensor[1:])))
# 除以2来获得半高和半宽
anchor_manipulations = paddle.tile(paddle.stack((-w, -h, w, h)).T,
(in_height * in_width, 1)) / 2

# 每个中心点都将有“boxes_per_pixel”个锚框，
# 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次
out_grid = paddle.stack([shift_x, shift_y, shift_x, shift_y], axis=1)
out_grid = paddle.tile(out_grid, repeat_times=[boxes_per_pixel]).reshape((-1, out_grid.shape[1]))
output = out_grid + anchor_manipulations
return output.unsqueeze(0)

def show_bboxes(axes, bboxes, labels=None, colors=None):
显示所有边界框, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`""""""
def init_weights(m):
if type(m) == nn.Linear or type(m) == nn.Conv2D:
nn.initializer.XavierUniform(m.weight)
net.apply(init_weights)
print('training on', device)
net.to(device)
optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters())
loss = nn.CrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=['train loss', 'train acc', 'test acc'])
timer, num_batches = d2l.Timer(), len(train_iter)
for epoch in range(num_epochs):
# 训练损失之和，训练准确率之和，样本数
metric = d2l.Accumulator(3)
net.train()
for i, (X, y) in enumerate(train_iter):
timer.start()
optimizer.clear_grad()
X, y = paddle.to_tensor(X, place=device), paddle.to_tensor(y, place=device)
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
optimizer.step()
with paddle.no_grad():
metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
timer.stop()
train_l = metric[0] / metric[2]
train_acc = metric[1] / metric[2]
if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
animator.add(epoch + (i + 1) / num_batches,
(train_l, train_acc, None))
test_acc = evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(device)}')

class Residual(nn.Layer):
def __init__(self, input_channels, num_channels, use_1x1conv=False,
strides=1):
super(Residual, self).__init__()
self.conv1 = nn.Conv2D(input_channels, num_channels, kernel_size=3,
padding=1, stride=strides)
self.conv2 = nn.Conv2D(num_channels, num_channels, kernel_size=3,
padding=1)
if use_1x1conv:
self.conv3 = nn.Conv2D(input_channels, num_channels,
kernel_size=1, stride=strides)
else:
self.conv3 = None
self.bn1 = nn.BatchNorm2D(num_channels)
self.bn2 = nn.BatchNorm2D(num_channels)
self.relu = nn.ReLU()

def forward(self, X):
Y = F.relu(self.bn1(self.conv1(X)))
Y = self.bn2(self.conv2(Y))
if self.conv3:
X = self.conv3(X)
Y += X
return F.relu(Y)

d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
'090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():
将时间机器数据集加载到文本行的列表中, 用于加载SNLI数据集的自定义数据集

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def __init__(self, dataset, num_steps, vocab=None):
self.num_steps = num_steps
all_premise_tokens = d2l.tokenize(dataset[0])
all_hypothesis_tokens = d2l.tokenize(dataset[1])
if vocab is None:
self.vocab = d2l.Vocab(all_premise_tokens + \
all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])
else:
self.vocab = vocab
self.premises = self._pad(all_premise_tokens)
self.hypotheses = self._pad(all_hypothesis_tokens)
self.labels = paddle.to_tensor(dataset[2])
print('read ' + str(len(self.premises)) + ' examples')

def _pad(self, lines):
return paddle.to_tensor([d2l.truncate_pad(
self.vocab[line], self.num_steps, self.vocab['<pad>'])
for line in lines])

def __getitem__(self, idx):
return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]

def __len__(self):
return len(self.premises)

def load_data_snli(batch_size, num_steps=50):
下载SNLI数据集并返回数据迭代器和词表, 用多GPU进行小批量训练
飞桨不支持在notebook上进行多GPU训练
Defined in :numref:`sec_image_augmentation`""""""
if isinstance(X, list):
# 微调BERT中所需（稍后讨论）
X = [paddle.to_tensor(x, place=devices[0]) for x in X]
else:
X = paddle.to_tensor(X, place=devices[0])
y = paddle.to_tensor(y, place=devices[0])
net.train()
trainer.clear_grad()
pred = net(X)
l = loss(pred, y)
l.sum().backward()
trainer.step()
train_loss_sum = l.sum()
train_acc_sum = d2l.accuracy(pred, y)
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus()):
Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练
Defined in :numref:`sec_image_augmentation`""""""
timer, num_batches = d2l.Timer(), len(train_iter)
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
legend=['train loss', 'train acc', 'test acc'])
net = paddle.DataParallel(net)
for epoch in range(num_epochs):
# 4个维度：储存训练损失，训练准确度，实例数，特点数
metric = d2l.Accumulator(4)
for i, (features, labels) in enumerate(train_iter):
timer.start()
l, acc = train_batch_ch13(
net, features, labels, loss, trainer, devices)
metric.add(l, acc, labels.shape[0], labels.numel())
timer.stop()
if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
animator.add(epoch + (i + 1) / num_batches,
(metric[0] / metric[2], metric[1] / metric[3],
None))
test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {metric[0] / metric[2]:.3f}, train acc '
f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
f'{str(devices)}')

d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',
'fba480ffa8aa7e0febbb511d181409f899b9baa5')

def box_corner_to_center(boxes):
从（左上，右下）转换到（中间，宽度，高度）, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`""""""
# s1和s2是稍后将使用的内部状态变量
x1, x2, s1, s2 = -5, -2, 0, 0
results = [(x1, x2)]
for i in range(steps):
if f_grad:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
else:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
results.append((x1, x2))
print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')
return results

def show_trace_2d(f, results):
显示优化过程中2D变量的轨迹, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`""""""
def resnet_block(in_channels, out_channels, num_residuals,
first_block=False):
blk = []
for i in range(num_residuals):
if i == 0 and not first_block:
blk.append(d2l.Residual(in_channels, out_channels,
use_1x1conv=True, strides=2))
else:
blk.append(d2l.Residual(out_channels, out_channels))
return nn.Sequential(*blk)

# 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
net = nn.Sequential(
nn.Conv2D(in_channels, 64, kernel_size=3, stride=1, padding=1),
nn.BatchNorm2D(64),
nn.ReLU())
net.add_sublayer(""resnet_block1"", resnet_block(
64, 64, 2, first_block=True))
net.add_sublayer(""resnet_block2"", resnet_block(64, 128, 2))
net.add_sublayer(""resnet_block3"", resnet_block(128, 256, 2))
net.add_sublayer(""resnet_block4"", resnet_block(256, 512, 2))
net.add_sublayer(""global_avg_pool"", nn.AdaptiveAvgPool2D((1, 1)))
net.add_sublayer(""fc"", nn.Sequential(nn.Flatten(),
nn.Linear(512, num_classes)))
return net

def train_batch_ch13(net, X, y, loss, trainer, devices):
Defined in :numref:`sec_image_augmentation`, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`""""""
d2l.set_figsize()
_, _, patches = d2l.plt.hist(
[[len(l) for l in xlist], [len(l) for l in ylist]])
d2l.plt.xlabel(xlabel)
d2l.plt.ylabel(ylabel)
for patch in patches[1].patches:
patch.set_hatch('/')
d2l.plt.legend(legend)

def truncate_pad(line, num_steps, padding_token):
截断或填充文本序列, 编码器-解码器架构的基本编码器接口
def __init__(self, **kwargs):
super(Encoder, self).__init__(**kwargs)

def forward(self, X, *args):
raise NotImplementedError

class Decoder(nn.Layer):
编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, encoder, decoder, **kwargs):
super(EncoderDecoder, self).__init__(**kwargs)
self.encoder = encoder
self.decoder = decoder

def forward(self, enc_X, dec_X, *args):
enc_outputs = self.encoder(enc_X, *args)
dec_state = self.decoder.init_state(enc_outputs, *args)
return self.decoder(dec_X, dec_state)

class Seq2SeqEncoder(d2l.Encoder):
用于序列到序列学习的循环神经网络编码器, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`""""""
def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)

# queries的形状：(batch_size，查询的个数，d)
# keys的形状：(batch_size，“键－值”对的个数，d)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
# valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
def forward(self, queries, keys, values, valid_lens=None):
d = queries.shape[-1]
# 设置transpose_b=True为了交换keys的最后两个维度
scores = paddle.bmm(queries, keys.transpose((0,2,1))) / math.sqrt(d)
self.attention_weights = masked_softmax(scores, valid_lens)
return paddle.bmm(self.dropout(self.attention_weights), values)

class AttentionDecoder(d2l.Decoder):
带有注意力机制解码器的基本接口, 获取输入序列的词元及其片段索引

Defined in :numref:`sec_bert`""""""
tokens = ['<cls>'] + tokens_a + ['<sep>']
# 0和1分别标记片段A和B
segments = [0] * (len(tokens_a) + 2)
if tokens_b is not None:
tokens += tokens_b + ['<sep>']
segments += [1] * (len(tokens_b) + 1)
return tokens, segments

class BERTEncoder(nn.Layer):
BERT编码器, 计算BLEU

Defined in :numref:`sec_seq2seq_training`""""""
pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
len_pred, len_label = len(pred_tokens), len(label_tokens)
score = math.exp(min(0, 1 - len_label / len_pred))
for n in range(1, k + 1):
num_matches, label_subs = 0, collections.defaultdict(int)
for i in range(len_label - n + 1):
label_subs[' '.join(label_tokens[i: i + n])] += 1
for i in range(len_pred - n + 1):
if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
num_matches += 1
label_subs[' '.join(pred_tokens[i: i + n])] -= 1
score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
return score

def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
cmap='Reds'):
显示矩阵热图, 计算两个锚框或边界框列表中成对的交并比

Defined in :numref:`sec_anchor`""""""
box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
(boxes[:, 3] - boxes[:, 1]))
# boxes1,boxes2,areas1,areas2的形状:
# boxes1：(boxes1的数量,4),
# boxes2：(boxes2的数量,4),
# areas1：(boxes1的数量,),
# areas2：(boxes2的数量,)
areas1 = box_area(boxes1)
areas2 = box_area(boxes2)
# inter_upperlefts,inter_lowerrights,inters的形状:
# (boxes1的数量,boxes2的数量,2)
inter_upperlefts = paddle.maximum(boxes1[:, None, :2], boxes2[:, :2])
inter_lowerrights = paddle.minimum(boxes1[:, None, 2:], boxes2[:, 2:])
inters = (inter_lowerrights - inter_upperlefts).clip(min=0)
# inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)
inter_areas = inters[:, :, 0] * inters[:, :, 1]
union_areas = areas1[:, None] + areas2 - inter_areas
return inter_areas / union_areas

def assign_anchor_to_bbox(ground_truth, anchors, place, iou_threshold=0.5):
将最接近的真实边界框分配给锚框, 计算二维互相关运算

Defined in :numref:`sec_conv_layer`""""""
h, w = K.shape
Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
for i in range(Y.shape[0]):
for j in range(Y.shape[1]):
Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
return Y

def evaluate_accuracy_gpu(net, data_iter, device=None):
使用GPU计算模型在数据集上的精度, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`""""""
if isinstance(net, paddle.nn.Layer):
net.eval()  # 将模型设置为评估模式
metric = Accumulator(2)  # 正确预测数、预测总数
with paddle.no_grad():
for X, y in data_iter:
metric.add(accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

class Accumulator:
在n个变量上累加, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def accuracy(y_hat, y):
计算预测正确的数量, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`""""""
optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())
loss = MaskedSoftmaxCELoss()
net.train()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[10, num_epochs])
for epoch in range(num_epochs):
timer = d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
for batch in data_iter:
optimizer.clear_grad()
X, X_valid_len, Y, Y_valid_len = [paddle.to_tensor(x, place=device) for x in batch]
bos = paddle.to_tensor([tgt_vocab['<bos>']] * Y.shape[0]).reshape([-1, 1])
dec_input = paddle.concat([bos, Y[:, :-1]], 1)  # 强制教学
Y_hat, _ = net(X, dec_input, X_valid_len.squeeze())
l = loss(Y_hat, Y, Y_valid_len.squeeze())
l.backward()	# 损失函数的标量进行“反向传播”
d2l.grad_clipping(net, 1)
num_tokens = Y_valid_len.sum()
optimizer.step()
with paddle.no_grad():
metric.add(l.sum(), num_tokens)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, (metric[0] / metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
f'tokens/sec on {str(device)}')

def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights=False):
序列到序列模型的预测, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
test_acc = evaluate_accuracy(net, test_iter)
animator.add(epoch + 1, train_metrics + (test_acc,))
train_loss, train_acc = train_metrics
assert train_loss < 0.5, train_loss
assert train_acc <= 1 and train_acc > 0.7, train_acc
assert test_acc <= 1 and test_acc > 0.7, test_acc

def predict_ch3(net, test_iter, n=6):
预测标签（定义见第3章）, 训练网络一个迭代周期（定义见第8章)

Defined in :numref:`sec_rnn_scratch`""""""
state, timer = None, d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失之和,词元数量
for X, Y in train_iter:
if state is None or use_random_iter:
# 在第一次迭代或使用随机抽样时初始化state
state = net.begin_state(batch_size=X.shape[0])
else:
if isinstance(net, nn.Layer) and not isinstance(state, tuple):
# state对于nn.GRU是个张量
state.stop_gradient=True
else:
# state对于nn.LSTM或对于我们从零开始实现的模型是个张量
for s in state:
s.stop_gradient=True
y = paddle.reshape(Y.T,shape=[-1])
X = paddle.to_tensor(X, place=device)
y = paddle.to_tensor(y, place=device)
y_hat, state = net(X, state)
l = loss(y_hat, y).mean()
if isinstance(updater, paddle.optimizer.Optimizer):
updater.clear_grad()
l.backward()
grad_clipping(net, 1)
updater.step()
else:
l.backward()
grad_clipping(net, 1)
# 因为已经调用了mean函数
updater(batch_size=1)

metric.add(l * d2l.size(y), d2l.size(y))
return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()

def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):
训练模型（定义见第8章）, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 评估给定数据集上模型的损失。

Defined in :numref:`sec_model_selection`""""""
metric = d2l.Accumulator(2)  # 损失的总和, 样本数量
for X, y in data_iter:
out = net(X)
y = y.reshape(out.shape)
l = loss(out, y)
metric.add(l.sum(), l.numel())
return metric[0] / metric[1]

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('..', 'data')):
下载一个DATA_HUB中的文件，返回本地文件名, 读取IMDb评论数据集文本序列和标签

Defined in :numref:`sec_sentiment`""""""
data, labels = [], []
for label in ('pos', 'neg'):
folder_name = os.path.join(data_dir, 'train' if is_train else 'test',
label)
for file in os.listdir(folder_name):
with open(os.path.join(folder_name, file), 'rb') as f:
review = f.read().decode('utf-8').replace('\n', '')
data.append(review)
labels.append(1 if label == 'pos' else 0)
return data, labels

def load_data_imdb(batch_size, num_steps=500):
返回数据迭代器和IMDb评论数据集的词表, 读取fname来给标签字典返回一个文件名

Defined in :numref:`sec_kaggle_cifar10`""""""
with open(fname, 'r') as f:
# 跳过文件头行(列名)
lines = f.readlines()[1:]
tokens = [l.rstrip().split(',') for l in lines]
return dict(((name, label) for name, label in tokens))

def copyfile(filename, target_dir):
将文件复制到目标目录, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回带有负采样的跳元模型的小批量样本

Defined in :numref:`sec_word2vec_data`""""""
max_len = max(len(c) + len(n) for _, c, n in data)
centers, contexts_negatives, masks, labels = [], [], [], []
for center, context, negative in data:
cur_len = len(context) + len(negative)
centers += [center]
contexts_negatives += \
[context + negative + [0] * (max_len - cur_len)]
masks += [[1] * cur_len + [0] * (max_len - cur_len)]
labels += [[1] * len(context) + [0] * (max_len - len(context))]
return (d2l.reshape(d2l.tensor(centers), (-1, 1)), d2l.tensor(
contexts_negatives), d2l.tensor(masks), d2l.tensor(labels))

def load_data_ptb(batch_size, max_window_size, num_noise_words):
下载PTB数据集，然后将其加载到内存中, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`""""""
lines = read_time_machine()
tokens = tokenize(lines, 'char')
vocab = Vocab(tokens)
# 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
# 所以将所有文本行展平到一个列表中
corpus = [vocab[token] for line in tokens for token in line]
if max_tokens > 0:
corpus = corpus[:max_tokens]
return corpus, vocab

def seq_data_iter_random(corpus, batch_size, num_steps):
使用随机抽样生成一个小批量子序列, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回跳元模型中的中心词和上下文词

Defined in :numref:`sec_word2vec_data`""""""
centers, contexts = [], []
for line in corpus:
# 要形成“中心词-上下文词”对，每个句子至少需要有2个词
if len(line) < 2:
continue
centers += line
for i in range(len(line)):  # 上下文窗口中间i
window_size = random.randint(1, max_window_size)
indices = list(range(max(0, i - window_size),
min(len(line), i + 1 + window_size)))
# 从上下文词中排除中心词
indices.remove(i)
contexts.append([line[idx] for idx in indices])
return centers, contexts

class RandomGenerator:
根据n个采样权重在{1,...,n}中随机抽取, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`""""""
X = X.reshape((-1, num_heads, X.shape[1], X.shape[2]))
X = X.transpose((0, 2, 1, 3))
return X.reshape((X.shape[0], X.shape[1], -1))

class PositionalEncoding(nn.Layer):
位置编码, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`""""""
# X:3D张量，valid_lens:1D或2D张量
if valid_lens is None:
return nn.functional.softmax(X, axis=-1)
else:
shape = X.shape
if valid_lens.dim() == 1:
valid_lens = paddle.repeat_interleave(valid_lens, shape[1])
else:
valid_lens = valid_lens.reshape((-1,))
# 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
X = d2l.sequence_mask(X.reshape((-1, shape[-1])), valid_lens,
value=-1e6)
return nn.functional.softmax(X.reshape(shape), axis=-1)

class AdditiveAttention(nn.Layer):
加性注意力, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
rect = paddle.vision.transforms.RandomCrop((height, width))._get_param(
img=feature, output_size=(height, width))
feature = paddle.vision.transforms.crop(feature, *rect)
label = paddle.vision.transforms.crop(label, *rect)
return feature, label

class VOCSegDataset(paddle.io.Dataset):
Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
def no_space(char, prev_char):
return char in set(',.!?') and prev_char != ' '

# 使用空格替换不间断空格
# 使用小写字母替换大写字母
text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
# 在单词和标点符号之间插入空格
out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char
for i, char in enumerate(text)]
return ''.join(out)

def tokenize_nmt(text, num_examples=None):
词元化“英语－法语”数据数据集, 预测文本序列的情感

Defined in :numref:`sec_sentiment_rnn`""""""
sequence = paddle.to_tensor(vocab[sequence.split()], place=d2l.try_gpu())
label = paddle.argmax(net(sequence.reshape((1, -1))), axis=1)
return 'positive' if label == 1 else 'negative'

d2l.DATA_HUB['SNLI'] = (
'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
'9fcde07509c7e87ec61c640c1b2753d9041758e4')

def read_snli(data_dir, is_train):
将SNLI数据集解析为前提、假设和标签","((左上x,左上y),宽,高), BERT微调所需的, BERT模型, BERT模型

Defined in :numref:`subsec_nsp`""""""
def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
ffn_num_hiddens, num_heads, num_layers, dropout,
max_len=1000, key_size=768, query_size=768, value_size=768,
hid_in_features=768, mlm_in_features=768,
nsp_in_features=768):
super(BERTModel, self).__init__()
self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
dropout, max_len=max_len, key_size=key_size,
query_size=query_size, value_size=value_size)
self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
nn.Tanh())
self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
self.nsp = NextSentencePred(nsp_in_features)

def forward(self, tokens, segments, valid_lens=None,
pred_positions=None):
encoded_X = self.encoder(tokens, segments, valid_lens)
if pred_positions is not None:
mlm_Y_hat = self.mlm(encoded_X, pred_positions)
else:
mlm_Y_hat = None
# 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引
nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
return encoded_X, mlm_Y_hat, nsp_Y_hat

d2l.DATA_HUB['wikitext-2'] = (
'https://s3.amazonaws.com/research.metamind.io/wikitext/'
'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')

def _read_wiki(data_dir):
Defined in :numref:`sec_bert-dataset`, BERT的下一句预测任务, BERT的掩蔽语言模型任务, BERT的掩蔽语言模型任务

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
super(MaskLM, self).__init__(**kwargs)
self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
nn.ReLU(),
nn.LayerNorm(num_hiddens),
nn.Linear(num_hiddens, vocab_size))

def forward(self, X, pred_positions):
num_pred_positions = pred_positions.shape[1]
pred_positions = pred_positions.reshape([-1])
batch_size = X.shape[0]
batch_idx = paddle.arange(0, batch_size)
# 假设batch_size=2，num_pred_positions=3
# 那么batch_idx是np.array（[0,0,0,1,1]）
batch_idx = paddle.repeat_interleave(batch_idx, num_pred_positions)
masked_X = X[batch_idx, pred_positions]
masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
mlm_Y_hat = self.mlp(masked_X)
return mlm_Y_hat

class NextSentencePred(nn.Layer):
BERT的下一句预测任务, BERT编码器, Defined in :numref:`sec_bert-dataset`
# 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元
mlm_input_tokens = [token for token in tokens]
pred_positions_and_labels = []
# 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测
random.shuffle(candidate_pred_positions)
for mlm_pred_position in candidate_pred_positions:
if len(pred_positions_and_labels) >= num_mlm_preds:
break
masked_token = None
# 80%的时间：将词替换为“<mask>”词元
if random.random() < 0.8:
masked_token = '<mask>'
else:
# 10%的时间：保持词不变
if random.random() < 0.5:
masked_token = tokens[mlm_pred_position]
# 10%的时间：用随机词替换该词
else:
masked_token = random.choice(vocab.idx_to_token)
mlm_input_tokens[mlm_pred_position] = masked_token
pred_positions_and_labels.append(
(mlm_pred_position, tokens[mlm_pred_position]))
return mlm_input_tokens, pred_positions_and_labels

def _get_mlm_data_from_tokens(tokens, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`sec_bert-dataset`
if random.random() < 0.5:
is_next = True
else:
# paragraphs是三重列表的嵌套
next_sentence = random.choice(random.choice(paragraphs))
is_next = False
return sentence, next_sentence, is_next

def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_hybridize`
self.description = description

def __enter__(self):
self.timer = d2l.Timer()
return self

def __exit__(self, *args):
print(f'{self.description}: {self.timer.stop():.4f} sec')

def split_batch(X, y, devices):
将X和y拆分到多个设备上, Defined in :numref:`sec_language_model`
if use_random_iter:
self.data_iter_fn = d2l.seq_data_iter_random
else:
self.data_iter_fn = d2l.seq_data_iter_sequential
self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
self.batch_size, self.num_steps = batch_size, num_steps

def __iter__(self):
return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps,
use_random_iter=False, max_tokens=10000):
返回时光机器数据集的迭代器和词表, Defined in :numref:`sec_minibatches`
# 初始化模型
net = nn.Sequential(nn.Linear(5, 1))
def init_weights(m):
if type(m) == nn.Linear:
paddle.nn.initializer.Normal(m.weight, std=0.01)

net.apply(init_weights)

optimizer = trainer_fn(parameters=net.parameters(), **hyperparams)
loss = nn.MSELoss(reduction='none')
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
optimizer.clear_grad()
out = net(X)
y = y.reshape(out.shape)
l = loss(out, y)
l.mean().backward()
optimizer.step()
n += X.shape[0]
if n % 200 == 0:
timer.stop()
# MSELoss计算平方误差时不带系数1/2
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss) / 2,))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')

class Benchmark:
用于测量运行时间, Defined in :numref:`sec_multihead-attention`
def __init__(self, key_size, query_size, value_size, num_hiddens,
num_heads, dropout, bias=False, **kwargs):
super(MultiHeadAttention, self).__init__(**kwargs)
self.num_heads = num_heads
self.attention = d2l.DotProductAttention(dropout)
self.W_q = nn.Linear(query_size, num_hiddens, bias_attr=bias)
self.W_k = nn.Linear(key_size, num_hiddens, bias_attr=bias)
self.W_v = nn.Linear(value_size, num_hiddens, bias_attr=bias)
self.W_o = nn.Linear(num_hiddens, num_hiddens, bias_attr=bias)

def forward(self, queries, keys, values, valid_lens):
# queries，keys，values的形状:
# (batch_size，查询或者“键－值”对的个数，num_hiddens)
# valid_lens　的形状:
# (batch_size，)或(batch_size，查询的个数)
# 经过变换后，输出的queries，keys，values　的形状:
# (batch_size*num_heads，查询或者“键－值”对的个数，
# num_hiddens/num_heads)
queries = transpose_qkv(self.W_q(queries), self.num_heads)
keys = transpose_qkv(self.W_k(keys), self.num_heads)
values = transpose_qkv(self.W_v(values), self.num_heads)
if valid_lens is not None:
# 在轴0，将第一项（标量或者矢量）复制num_heads次，
# 然后如此复制第二项，然后诸如此类。
valid_lens = paddle.repeat_interleave(
valid_lens, repeats=self.num_heads, axis=0)

# output的形状:(batch_size*num_heads，查询的个数，
# num_hiddens/num_heads)
output = self.attention(queries, keys, values, valid_lens)

# output_concat的形状:(batch_size，查询的个数，num_hiddens)
output_concat = transpose_output(output, self.num_heads)
return self.W_o(output_concat)

def transpose_qkv(X, num_heads):
为了多注意力头的并行计算而变换形状, Defined in :numref:`sec_semantic_segmentation`

Defined in :numref:`sec_semantic_segmentation`""""""
读取所有VOC图像并标注, Defined in :numref:`sec_softmax_scratch`
self.data = [0.0] * n

def add(self, *args):
self.data = [a + float(b) for a, b in zip(self.data, args)]

def reset(self):
self.data = [0.0] * len(self.data)

def __getitem__(self, idx):
return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
训练模型一个迭代周期（定义见第3章）, Defined in :numref:`sec_text_preprocessing`
if tokens is None:
tokens = []
if reserved_tokens is None:
reserved_tokens = []
# 按出现频率排序
counter = count_corpus(tokens)
self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
reverse=True)
# 未知词元的索引为0
self.idx_to_token = ['<unk>'] + reserved_tokens
self.token_to_idx = {token: idx
for idx, token in enumerate(self.idx_to_token)}
for token, freq in self._token_freqs:
if freq < min_freq:
break
if token not in self.token_to_idx:
self.idx_to_token.append(token)
self.token_to_idx[token] = len(self.idx_to_token) - 1

def __len__(self):
return len(self.idx_to_token)

def __getitem__(self, tokens):
if not isinstance(tokens, (list, tuple)):
return self.token_to_idx.get(tokens, self.unk)
return [self.__getitem__(token) for token in tokens]

def to_tokens(self, indices):
if not isinstance(indices, (list, tuple)):
return self.idx_to_token[indices]
return [self.idx_to_token[index] for index in indices]

@property
def unk(self):  # 未知词元的索引为0
return 0

@property
def token_freqs(self):
return self._token_freqs

def count_corpus(tokens):
统计词元的频率, Defined in :numref:`sec_word2vec_data`
# Exclude
self.population = list(range(1, len(sampling_weights) + 1))
self.sampling_weights = sampling_weights
self.candidates = []
self.i = 0

def draw(self):
if self.i == len(self.candidates):
# 缓存k个随机采样结果
self.candidates = random.choices(
self.population, self.sampling_weights, k=10000)
self.i = 0
self.i += 1
return self.candidates[self.i - 1]

generator = RandomGenerator([2, 3, 4])
[generator.draw() for _ in range(10)]

def get_negatives(all_contexts, vocab, counter, K):
返回负采样中的噪声词, Defined in :numref:`subsec_prepare_mlm_data`
max_num_mlm_preds = round(max_len * 0.15)
all_token_ids, all_segments, valid_lens,  = [], [], []
all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
nsp_labels = []
for (token_ids, pred_positions, mlm_pred_label_ids, segments,
is_next) in examples:
all_token_ids.append(paddle.to_tensor(token_ids + [vocab['<pad>']] * (
max_len - len(token_ids)), dtype=paddle.int64))
all_segments.append(paddle.to_tensor(segments + [0] * (
max_len - len(segments)), dtype=paddle.int64))
# valid_lens不包括'<pad>'的计数
valid_lens.append(paddle.to_tensor(len(token_ids), dtype=paddle.float32))
all_pred_positions.append(paddle.to_tensor(pred_positions + [0] * (
max_num_mlm_preds - len(pred_positions)), dtype=paddle.int64))
# 填充词元的预测将通过乘以0权重在损失中过滤掉
all_mlm_weights.append(
paddle.to_tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (
max_num_mlm_preds - len(pred_positions)),
dtype=paddle.float32))
all_mlm_labels.append(paddle.to_tensor(mlm_pred_label_ids + [0] * (
max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=paddle.int64))
nsp_labels.append(paddle.to_tensor(is_next, dtype=paddle.int64))
return (all_token_ids, all_segments, valid_lens, all_pred_positions,
all_mlm_weights, all_mlm_labels, nsp_labels)

class _WikiTextDataset(paddle.io.Dataset):
Defined in :numref:`subsec_prepare_mlm_data`, GloVe嵌入, GloVe嵌入
def __init__(self, embedding_name):
Defined in :numref:`sec_synonyms`, GloVe网站：https://nlp.stanford.edu/projects/glove/, PIL图片, PaddlePaddle的GRU层output的形状:(batch_size,time_steps,num_directions * num_hiddens),, X的形状：(batchsize,num_hiddens), ]。, fastText网站：https://fasttext.cc/, key的形状：(batch_size，1，“键－值”对的个数，num_hiddens), label的形状：(batch_size,num_steps), pred的形状：(batch_size,num_steps,vocab_size), queries的形状：(batch_size，查询的个数，1，num_hidden), scores的形状：(batch_size，查询的个数，“键-值”对的个数), self.w_v仅有一个输出，因此从形状中移除最后那个维度。, state[0]的形状:(num_layers,batch_size,num_hiddens), tokens是一个字符串列表, transformer编码器, transformer编码器块, transformer编码器块

Defined in :numref:`sec_transformer`""""""
def __init__(self, key_size, query_size, value_size, num_hiddens,
norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
dropout, use_bias=False, **kwargs):
super(EncoderBlock, self).__init__(**kwargs)
self.attention = d2l.MultiHeadAttention(
key_size, query_size, value_size, num_hiddens, num_heads, dropout,
use_bias)
self.addnorm1 = AddNorm(norm_shape, dropout)
self.ffn = PositionWiseFFN(
ffn_num_input, ffn_num_hiddens, num_hiddens)
self.addnorm2 = AddNorm(norm_shape, dropout)

def forward(self, X, valid_lens):
Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
return self.addnorm2(Y, self.ffn(Y))

class TransformerEncoder(d2l.Encoder):
transformer编码器, valid_len的形状：(batch_size,), values的形状：(batch_size，“键－值”对的个数，值的维度), 。, 一个用于加载VOC数据集的自定义数据集, 一个用于加载VOC数据集的自定义数据集
Defined in :numref:`sec_semantic_segmentation`""""""

def __init__(self, is_train, crop_size, voc_dir):
self.transform = paddle.vision.transforms.Normalize(
mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
self.crop_size = crop_size
features, labels = read_voc_images(voc_dir, is_train=is_train)
self.features = [self.normalize_image(feature)
for feature in self.filter(features)]
self.labels = self.filter(labels)
self.colormap2label = voc_colormap2label()
print('read ' + str(len(self.features)) + ' examples')

def normalize_image(self, img):
return self.transform(img.astype(""float32"") / 255)

def filter(self, imgs):
return [img for img in imgs if (
img.shape[1] >= self.crop_size[0] and
img.shape[2] >= self.crop_size[1])]

def __getitem__(self, idx):
feature = paddle.to_tensor(self.features[idx],dtype='float32')
label = paddle.to_tensor(self.labels[idx],dtype='float32')
feature, label = voc_rand_crop(feature,label,
*self.crop_size)
return (feature, voc_label_indices(label, self.colormap2label))

def __len__(self):
return len(self.features)

def load_data_voc(batch_size, crop_size):
加载VOC语义分割数据集, 一个用于加载香蕉检测数据集的自定义数据集, 一个用于加载香蕉检测数据集的自定义数据集

Defined in :numref:`sec_object-detection-dataset`""""""
def __init__(self, is_train):
self.features, self.labels = read_data_bananas(is_train)
print('read ' + str(len(self.features)) + (f' training examples' if
is_train else f' validation examples'))

def __getitem__(self, idx):
return (paddle.to_tensor(self.features[idx], dtype='float32').transpose([2, 0, 1]), self.labels[idx])

def __len__(self):
return len(self.features)

def load_data_bananas(batch_size):
加载香蕉检测数据集, 一旦序列结束词元被预测，输出序列的生成就完成了, 下载, 下载DATA_HUB中的所有文件, 下载Fashion-MNIST数据集，然后将其加载到内存中, 下载PTB数据集，然后将其加载到内存中, 下载SNLI数据集并返回数据迭代器和词表, 下载一个DATA_HUB中的文件，返回本地文件名, 下载并解压zip/tar文件, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`""""""
fname = download(name)
base_dir = os.path.dirname(fname)
data_dir, ext = os.path.splitext(fname)
if ext == '.zip':
fp = zipfile.ZipFile(fname, 'r')
elif ext in ('.tar', '.gz'):
fp = tarfile.open(fname, 'r')
else:
assert False, '只有zip/tar文件可以被解压缩'
fp.extractall(base_dir)
return os.path.join(base_dir, folder) if folder else data_dir

def download_all():
下载DATA_HUB中的所有文件, 下采样高频词, 不存在于, 中随机抽取, 为了多注意力头的并行计算而变换形状, 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1, 从零开始实现的循环神经网络模型, 从零开始实现的循环神经网络模型
def __init__(self, vocab_size, num_hiddens,
get_params, init_state, forward_fn):
Defined in :numref:`sec_rnn_scratch`, 从（中间，宽度，高度）转换到（左上，右下）, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`""""""
cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
x1 = cx - 0.5 * w
y1 = cy - 0.5 * h
x2 = cx + 0.5 * w
y2 = cy + 0.5 * h
boxes = d2l.stack((x1, y1, x2, y2), axis=-1)
return boxes

def bbox_to_rect(bbox, color):
Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）, 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU, 位置编码, 使用4个进程来读取数据, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 使用GPU计算模型在数据集上的精度, 使用PaddlePaddle内置的优化器和损失函数, 使用lambda函数捕获参数, 使用svg格式在Jupyter中显示绘图, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用定制的优化器和损失函数, 使用广播方式进行求和, 使用真实边界框来标记锚框的类别。, 使用真实边界框标记锚框, 使用随机抽样生成一个小批量子序列, 使用非极大值抑制来预测边界框, 使用非极大值抑制来预测边界框

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
batch_size = cls_probs.shape[0]
anchors = anchors.squeeze(0)
num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
out = []
for i in range(batch_size):
cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape([-1, 4])
conf = paddle.max(cls_prob[1:], 0)
class_id = paddle.argmax(cls_prob[1:], 0)
predicted_bb = offset_inverse(anchors, offset_pred)
keep = nms(predicted_bb, conf, nms_threshold)

# 找到所有的non_keep索引，并将类设置为背景
all_idx = paddle.arange(num_anchors, dtype='int64')
combined = paddle.concat((keep, all_idx))
uniques, counts = combined.unique(return_counts=True)
non_keep = uniques[counts == 1]
all_id_sorted = paddle.concat([keep, non_keep])
class_id[non_keep.numpy()] = -1
class_id = class_id[all_id_sorted]
conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
# pos_threshold是一个用于非背景预测的阈值
below_min_idx = (conf < pos_threshold)
class_id[below_min_idx.numpy()] = -1
conf[below_min_idx.numpy()] = 1 - conf[below_min_idx.numpy()]
pred_info = paddle.concat((paddle.to_tensor(class_id, dtype='float32').unsqueeze(1),
paddle.to_tensor(conf, dtype='float32').unsqueeze(1),
predicted_bb), axis=1)
out.append(pred_info)
return paddle.stack(out)

d2l.DATA_HUB['banana-detection'] = (
d2l.DATA_URL + 'banana-detection.zip',
'5de26c8fce5ccdea9f91267273464dc968d20d72')

def read_data_bananas(is_train=True):
读取香蕉检测数据集中的图像和标签, 使用顺序分区生成一个小批量子序列, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始划分序列
offset = random.randint(0, num_steps)
num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
Xs = d2l.tensor(corpus[offset: offset + num_tokens])
Ys = d2l.tensor(corpus[offset + 1: offset + 1 + num_tokens])
Xs, Ys = Xs.reshape((batch_size, -1)), Ys.reshape((batch_size, -1))
num_batches = Xs.shape[1] // num_steps
for i in range(0, num_steps * num_batches, num_steps):
X = Xs[:, i: i + num_steps]
Y = Ys[:, i: i + num_steps]
yield X, Y

class SeqDataLoader:
加载序列数据的迭代器, 保存注意力权重（稍后讨论）, 保留预测边界框的指标, 偏移量转换, 停止计时器并将时间记录在列表中, 其中所有图像都具有相同的香蕉类（索引为0）, 减去1，是因为我们需要考虑标签, 创建一个足够长的P, 初始化, 初始化模型, 删除我们不会使用的信息, 前向传播, 加性注意力, 加载VOC语义分割数据集, 加载WikiText-2数据集, 加载WikiText-2数据集

Defined in :numref:`subsec_prepare_mlm_data`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')
paragraphs = _read_wiki(data_dir)
train_set = _WikiTextDataset(paragraphs, max_len)
train_iter = paddle.io.DataLoader(dataset=train_set, batch_size=batch_size, return_list=True,
shuffle=True, num_workers=num_workers)
return train_iter, train_set.vocab

def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,
segments_X, valid_lens_x,
pred_positions_X, mlm_weights_X,
mlm_Y, nsp_y):
Defined in :numref:`sec_bert-pretraining`, 加载序列数据的迭代器, 加载香蕉检测数据集, 只有zip/tar文件可以被解压缩, 向图表中添加多个数据点, 启动计时器, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 命中缓存, 噪声词不能是上下文词, 因为位置编码值在-1和1之间，, 因此嵌入值乘以嵌入维度的平方根进行缩放，, 困惑度, 图片张量, 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数, 在n个变量上累加, 在prefix后面生成新字符, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`""""""
state = net.begin_state(batch_size=1)
outputs = [vocab[prefix[0]]]
get_input = lambda: d2l.reshape(d2l.tensor(outputs[-1], place=device), (1, 1))
for y in prefix[1:]:  # 预热期
_, state = net(get_input(), state)
outputs.append(vocab[y])
for _ in range(num_preds):  # 预测num_preds步
y, state = net(get_input(), state)
outputs.append(int(paddle.reshape(paddle.argmax(y,axis=1),shape=[1])))
return ''.join([vocab.idx_to_token[i] for i in outputs])

def grad_clipping(net, theta):
裁剪梯度, 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）, 在动画中绘制数据, 在动画中绘制数据
def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
figsize=(3.5, 2.5)):
Defined in :numref:`sec_softmax_scratch`, 在序列中屏蔽不相关的项, 在序列中屏蔽不相关的项

Defined in :numref:`sec_seq2seq_decoder`""""""
maxlen = X.shape[1]
mask = paddle.arange((maxlen), dtype=paddle.float32)[None, :] < valid_len[:, None]
Xtype = X.dtype
X = X.astype(paddle.float32)
X[~mask] = float(value)
return X.astype(Xtype)

class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
带遮蔽的softmax交叉熵损失函数, 在循环神经网络模型中，第一个轴对应于时间步, 在第六章定义, 在维度扩展后，, 在这里，initial_indices包含子序列的随机起始索引, 在遮蔽语言模型任务中不会预测特殊词元, 在随机抽样的迭代过程中，, 在预测时将net设置为评估模式, 在预测期间整理测试集，以方便读取, 均方损失, 基于位置的前馈网络, 基于位置的前馈网络

Defined in :numref:`sec_transformer`""""""
def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
**kwargs):
super(PositionWiseFFN, self).__init__(**kwargs)
self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
self.relu = nn.ReLU()
self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

def forward(self, X):
return self.dense2(self.relu(self.dense1(X)))

class AddNorm(nn.Layer):
残差连接后进行层规范化, 填充, 填充输入, 增量地绘制多条线, 大写字母转换为小写字母, 如果X有一个轴，输出True, 如果一个锚框没有被分配，我们标记其为背景（值为零）, 如果在下采样期间保留词元，则返回True, 如果存在，则返回gpu, 如果存在，则返回gpu(i)，否则返回cpu()。

Defined in :numref:`sec_use_gpu`""""""
if paddle.device.cuda.device_count() >= i + 1:
return paddle.CUDAPlace(i)
return paddle.CPUPlace()

def try_all_gpus():
返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。, 如果未提及状态，则默认为0, 对于每个锚框，分配的真实边界框的张量, 对锚框偏移量的转换, 对锚框偏移量的转换

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
c_anc = d2l.box_corner_to_center(anchors)
c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
offset = d2l.concat([offset_xy, offset_wh], axis=1)
return offset

def multibox_target(anchors, labels):
使用真实边界框标记锚框, 对预测边界框的置信度进行排序, 将PTB数据集加载到文本行的列表中, 将PTB数据集加载到文本行的列表中

Defined in :numref:`sec_word2vec_data`""""""
data_dir = d2l.download_extract('ptb')
# Readthetrainingset.
with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
raw_text = f.read()
return [line.split() for line in raw_text.split('\n')]

def subsample(sentences, vocab):
下采样高频词, 将SNLI数据集解析为前提、假设和标签, 将VOC标签中的RGB值映射到它们的类别索引, 将X和y拆分到多个设备上, 将文件复制到目标目录, 将文本行拆分为单词或字符词元, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`""""""
if token == 'word':
return [line.split() for line in lines]
elif token == 'char':
return [list(line) for line in lines]
else:
print('错误：未知词元类型：' + token)

class Vocab:
文本词表, 将时间机器数据集加载到文本行的列表中, 将最接近的真实边界框分配给锚框, 将机器翻译的文本序列转换成小批量, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`""""""
lines = [vocab[l] for l in lines]
lines = [l + [vocab['<eos>']] for l in lines]
array = d2l.tensor([truncate_pad(
l, num_steps, vocab['<pad>']) for l in lines])
valid_len = d2l.reduce_sum(
d2l.astype(array != vocab['<pad>'], d2l.int32), 1)
return array, valid_len

def load_data_nmt(batch_size, num_steps, num_examples=600):
返回翻译数据集的迭代器和词表, 将模型设置为训练模式, 将类标签和分配的边界框坐标初始化为零, 将词元列表展平成一个列表, 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：, 将验证集从原始的训练集中拆分出来, 将验证集从原始的训练集中拆分出来

Defined in :numref:`sec_kaggle_cifar10`""""""
# 训练数据集中样本最少的类别中的样本数
n = collections.Counter(labels.values()).most_common()[-1][1]
# 验证集中每个类别的样本数
n_valid_per_label = max(1, math.floor(n * valid_ratio))
label_count = {}
for train_file in os.listdir(os.path.join(data_dir, 'train')):
label = labels[train_file.split('.')[0]]
fname = os.path.join(data_dir, 'train', train_file)
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train_valid', label))
if label not in label_count or label_count[label] < n_valid_per_label:
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'valid', label))
label_count[label] = label_count.get(label, 0) + 1
else:
copyfile(fname, os.path.join(data_dir, 'train_valid_test',
'train', label))
return n_valid_per_label

def reorg_test(data_dir):
在预测期间整理测试集，以方便读取, 小批量随机梯度下降, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
with paddle.no_grad():
for i, param in enumerate(params):
param -= lr * params[i].grad / batch_size
params[i].set_value(param)
params[i].clear_gradient()

def load_array(data_arrays, batch_size, is_train=True):
构造一个Paddle数据迭代器, 嵌入层, 带有注意力机制解码器的基本接口, 带遮蔽的softmax交叉熵损失函数, 序列到序列模型的预测, 循环神经网络模型, 循环神经网络模型

Defined in :numref:`sec_rnn-concise`""""""
def __init__(self, rnn_layer, vocab_size, **kwargs):
super(RNNModel, self).__init__(**kwargs)
self.rnn = rnn_layer
self.vocab_size = vocab_size
self.num_hiddens = self.rnn.hidden_size
# 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
if self.rnn.num_directions==1:
self.num_directions = 1
self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
else:
self.num_directions = 2
self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

def forward(self, inputs, state):
X = F.one_hot(inputs.T, self.vocab_size)
Y, state = self.rnn(X, state)
# 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
# 它的输出形状是(时间步数*批量大小,词表大小)。
output = self.linear(Y.reshape((-1, Y.shape[-1])))
return output, state

def begin_state(self, batch_size=1):
if not isinstance(self.rnn, nn.LSTM):
# nn.GRU以张量作为隐状态
return  paddle.zeros(shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens])
else:
# nn.LSTM以元组作为隐状态
return (paddle.zeros(
shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens]),
paddle.zeros(
shape=[self.num_directions * self.rnn.num_layers,
batch_size, self.num_hiddens]))

d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
'94646ad1522d915e7b0f9296181140edcf86a4f5')

def read_data_nmt():
载入“英语－法语”数据集, 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入, 截断, 截断或填充文本序列, 排除未知词元'<unk>', 文本词表, 显示优化过程中2D变量的轨迹, 显示所有边界框, 显示矩阵热图, 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,, 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻, 构建从RGB到VOC类别索引的映射, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = paddle.zeros([256 ** 3], dtype=paddle.int64)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 构造一个Paddle数据迭代器, 根据n个采样权重在, 根据带有预测偏移量的锚框来预测边界框, 根据带有预测偏移量的锚框来预测边界框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
anc = d2l.box_corner_to_center(anchors)
pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]
pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]
pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)
predicted_bbox = d2l.box_center_to_corner(pred_bbox)
return predicted_bbox

def nms(boxes, scores, iou_threshold):
对预测边界框的置信度进行排序, 根据阈值，决定是否分配真实边界框, 正在从, 正确预测的数量，总预测的数量, 残差连接后进行层规范化, 添加批量轴, 然后再与位置编码相加。, 生成y=Xw+b+噪声, 生成以每个像素为中心具有不同形状的锚框, 生成以每个像素为中心具有不同形状的锚框

Defined in :numref:`sec_anchor`""""""
in_height, in_width = data.shape[-2:]
place, num_sizes, num_ratios = data.place, len(sizes), len(ratios)
boxes_per_pixel = (num_sizes + num_ratios - 1)
size_tensor = paddle.to_tensor(sizes, place=place)
ratio_tensor = paddle.to_tensor(ratios, place=place)

# 为了将锚点移动到像素的中心，需要设置偏移量。
# 因为一个像素的的高为1且宽为1，我们选择偏移我们的中心0.5
offset_h, offset_w = 0.5, 0.5
steps_h = 1.0 / in_height  # 在y轴上缩放步长
steps_w = 1.0 / in_width  # 在x轴上缩放步长

# 生成锚框的所有中心点
center_h = (paddle.arange(in_height) + offset_h) * steps_h
center_w = (paddle.arange(in_width) + offset_w) * steps_w
shift_y, shift_x = paddle.meshgrid(center_h, center_w)
shift_y, shift_x = shift_y.reshape([-1]), shift_x.reshape([-1])

# 生成“boxes_per_pixel”个高和宽，
# 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)
w = paddle.concat((size_tensor * paddle.sqrt(ratio_tensor[0]),
sizes[0] * paddle.sqrt(ratio_tensor[1:])))\
* in_height / in_width  # 处理矩形输入
h = paddle.concat((size_tensor / paddle.sqrt(ratio_tensor[0]),
sizes[0] / paddle.sqrt(ratio_tensor[1:])))
# 除以2来获得半高和半宽
anchor_manipulations = paddle.tile(paddle.stack((-w, -h, w, h)).T,
(in_height * in_width, 1)) / 2

# 每个中心点都将有“boxes_per_pixel”个锚框，
# 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次
out_grid = paddle.stack([shift_x, shift_y, shift_x, shift_y], axis=1)
out_grid = paddle.tile(out_grid, repeat_times=[boxes_per_pixel]).reshape((-1, out_grid.shape[1]))
output = out_grid + anchor_manipulations
return output.unsqueeze(0)

def show_bboxes(axes, bboxes, labels=None, colors=None):
显示所有边界框, 用GPU训练模型, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`""""""
def init_weights(m):
if type(m) == nn.Linear or type(m) == nn.Conv2D:
nn.initializer.XavierUniform(m.weight)
net.apply(init_weights)
print('training on', device)
net.to(device)
optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters())
loss = nn.CrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
legend=['train loss', 'train acc', 'test acc'])
timer, num_batches = d2l.Timer(), len(train_iter)
for epoch in range(num_epochs):
# 训练损失之和，训练准确率之和，样本数
metric = d2l.Accumulator(3)
net.train()
for i, (X, y) in enumerate(train_iter):
timer.start()
optimizer.clear_grad()
X, y = paddle.to_tensor(X, place=device), paddle.to_tensor(y, place=device)
y_hat = net(X)
l = loss(y_hat, y)
l.backward()
optimizer.step()
with paddle.no_grad():
metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
timer.stop()
train_l = metric[0] / metric[2]
train_acc = metric[1] / metric[2]
if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
animator.add(epoch + (i + 1) / num_batches,
(train_l, train_acc, None))
test_acc = evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
f'test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
f'on {str(device)}')

class Residual(nn.Layer):
def __init__(self, input_channels, num_channels, use_1x1conv=False,
strides=1):
super(Residual, self).__init__()
self.conv1 = nn.Conv2D(input_channels, num_channels, kernel_size=3,
padding=1, stride=strides)
self.conv2 = nn.Conv2D(num_channels, num_channels, kernel_size=3,
padding=1)
if use_1x1conv:
self.conv3 = nn.Conv2D(input_channels, num_channels,
kernel_size=1, stride=strides)
else:
self.conv3 = None
self.bn1 = nn.BatchNorm2D(num_channels)
self.bn2 = nn.BatchNorm2D(num_channels)
self.relu = nn.ReLU()

def forward(self, X):
Y = F.relu(self.bn1(self.conv1(X)))
Y = self.bn2(self.conv2(Y))
if self.conv3:
X = self.conv3(X)
Y += X
return F.relu(Y)

d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
'090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():
将时间机器数据集加载到文本行的列表中, 用一个空格替换两个或多个连续的空格, 用于加载SNLI数据集的自定义数据集, 用于加载SNLI数据集的自定义数据集

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def __init__(self, dataset, num_steps, vocab=None):
self.num_steps = num_steps
all_premise_tokens = d2l.tokenize(dataset[0])
all_hypothesis_tokens = d2l.tokenize(dataset[1])
if vocab is None:
self.vocab = d2l.Vocab(all_premise_tokens + \
all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])
else:
self.vocab = vocab
self.premises = self._pad(all_premise_tokens)
self.hypotheses = self._pad(all_hypothesis_tokens)
self.labels = paddle.to_tensor(dataset[2])
print('read ' + str(len(self.premises)) + ' examples')

def _pad(self, lines):
return paddle.to_tensor([d2l.truncate_pad(
self.vocab[line], self.num_steps, self.vocab['<pad>'])
for line in lines])

def __getitem__(self, idx):
return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]

def __len__(self):
return len(self.premises)

def load_data_snli(batch_size, num_steps=50):
下载SNLI数据集并返回数据迭代器和词表, 用于序列到序列学习的循环神经网络编码器, 用于测量运行时间, 用多GPU进行小批量训练, 用多GPU进行小批量训练
飞桨不支持在notebook上进行多GPU训练
Defined in :numref:`sec_image_augmentation`""""""
if isinstance(X, list):
# 微调BERT中所需（稍后讨论）
X = [paddle.to_tensor(x, place=devices[0]) for x in X]
else:
X = paddle.to_tensor(X, place=devices[0])
y = paddle.to_tensor(y, place=devices[0])
net.train()
trainer.clear_grad()
pred = net(X)
l = loss(pred, y)
l.sum().backward()
trainer.step()
train_loss_sum = l.sum()
train_acc_sum = d2l.accuracy(pred, y)
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus()):
Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练, 用多GPU进行模型训练
Defined in :numref:`sec_image_augmentation`""""""
timer, num_batches = d2l.Timer(), len(train_iter)
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
legend=['train loss', 'train acc', 'test acc'])
net = paddle.DataParallel(net)
for epoch in range(num_epochs):
# 4个维度：储存训练损失，训练准确度，实例数，特点数
metric = d2l.Accumulator(4)
for i, (features, labels) in enumerate(train_iter):
timer.start()
l, acc = train_batch_ch13(
net, features, labels, loss, trainer, devices)
metric.add(l, acc, labels.shape[0], labels.numel())
timer.stop()
if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
animator.add(epoch + (i + 1) / num_batches,
(metric[0] / metric[2], metric[1] / metric[3],
None))
test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
animator.add(epoch + 1, (None, None, test_acc))
print(f'loss {metric[0] / metric[2]:.3f}, train acc '
f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
f'{str(devices)}')

d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',
'fba480ffa8aa7e0febbb511d181409f899b9baa5')

def box_corner_to_center(boxes):
从（左上，右下）转换到（中间，宽度，高度）, 用定制的训练机优化2D目标函数, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`""""""
# s1和s2是稍后将使用的内部状态变量
x1, x2, s1, s2 = -5, -2, 0, 0
results = [(x1, x2)]
for i in range(steps):
if f_grad:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
else:
x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
results.append((x1, x2))
print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')
return results

def show_trace_2d(f, results):
显示优化过程中2D变量的轨迹, 稍加修改的ResNet-18模型, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`""""""
def resnet_block(in_channels, out_channels, num_residuals,
first_block=False):
blk = []
for i in range(num_residuals):
if i == 0 and not first_block:
blk.append(d2l.Residual(in_channels, out_channels,
use_1x1conv=True, strides=2))
else:
blk.append(d2l.Residual(out_channels, out_channels))
return nn.Sequential(*blk)

# 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
net = nn.Sequential(
nn.Conv2D(in_channels, 64, kernel_size=3, stride=1, padding=1),
nn.BatchNorm2D(64),
nn.ReLU())
net.add_sublayer(""resnet_block1"", resnet_block(
64, 64, 2, first_block=True))
net.add_sublayer(""resnet_block2"", resnet_block(64, 128, 2))
net.add_sublayer(""resnet_block3"", resnet_block(128, 256, 2))
net.add_sublayer(""resnet_block4"", resnet_block(256, 512, 2))
net.add_sublayer(""global_avg_pool"", nn.AdaptiveAvgPool2D((1, 1)))
net.add_sublayer(""fc"", nn.Sequential(nn.Flatten(),
nn.Linear(512, num_classes)))
return net

def train_batch_ch13(net, X, y, loss, trainer, devices):
Defined in :numref:`sec_image_augmentation`, 索引为1、2、...（索引0是词表中排除的未知标记）, 线性回归模型, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 绘制列表长度对的直方图, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`""""""
d2l.set_figsize()
_, _, patches = d2l.plt.hist(
[[len(l) for l in xlist], [len(l) for l in ylist]])
d2l.plt.xlabel(xlabel)
d2l.plt.ylabel(ylabel)
for patch in patches[1].patches:
patch.set_hatch('/')
d2l.plt.legend(legend)

def truncate_pad(line, num_steps, padding_token):
截断或填充文本序列, 绘制图像列表, 绘制数据点, 统计词元的频率, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本编码器接口
def __init__(self, **kwargs):
super(Encoder, self).__init__(**kwargs)

def forward(self, X, *args):
raise NotImplementedError

class Decoder(nn.Layer):
编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基类, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, encoder, decoder, **kwargs):
super(EncoderDecoder, self).__init__(**kwargs)
self.encoder = encoder
self.decoder = decoder

def forward(self, enc_X, dec_X, *args):
enc_outputs = self.encoder(enc_X, *args)
dec_state = self.decoder.init_state(enc_outputs, *args)
return self.decoder(dec_X, dec_state)

class Seq2SeqEncoder(d2l.Encoder):
用于序列到序列学习的循环神经网络编码器, 缩放点积注意力, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`""""""
def __init__(self, dropout, **kwargs):
super(DotProductAttention, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)

# queries的形状：(batch_size，查询的个数，d)
# keys的形状：(batch_size，“键－值”对的个数，d)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
# valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
def forward(self, queries, keys, values, valid_lens=None):
d = queries.shape[-1]
# 设置transpose_b=True为了交换keys的最后两个维度
scores = paddle.bmm(queries, keys.transpose((0,2,1))) / math.sqrt(d)
self.attention_weights = masked_softmax(scores, valid_lens)
return paddle.bmm(self.dropout(self.attention_weights), values)

class AttentionDecoder(d2l.Decoder):
带有注意力机制解码器的基本接口, 考虑1个'<cls>'词元和2个'<sep>'词元, 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表, 获取下一句子预测任务的数据, 获取输入序列的词元及其片段索引, 获取输入序列的词元及其片段索引

Defined in :numref:`sec_bert`""""""
tokens = ['<cls>'] + tokens_a + ['<sep>']
# 0和1分别标记片段A和B
segments = [0] * (len(tokens_a) + 2)
if tokens_b is not None:
tokens += tokens_b + ['<sep>']
segments += [1] * (len(tokens_b) + 1)
return tokens, segments

class BERTEncoder(nn.Layer):
BERT编码器, 获取遮蔽语言模型任务的数据, 裁剪梯度, 计算BLEU, 计算BLEU

Defined in :numref:`sec_seq2seq_training`""""""
pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
len_pred, len_label = len(pred_tokens), len(label_tokens)
score = math.exp(min(0, 1 - len_label / len_pred))
for n in range(1, k + 1):
num_matches, label_subs = 0, collections.defaultdict(int)
for i in range(len_label - n + 1):
label_subs[' '.join(label_tokens[i: i + n])] += 1
for i in range(len_pred - n + 1):
if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
num_matches += 1
label_subs[' '.join(pred_tokens[i: i + n])] -= 1
score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
return score

def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
cmap='Reds'):
显示矩阵热图, 计算下一句子预测任务的损失, 计算两个锚框或边界框列表中成对的交并比, 计算两个锚框或边界框列表中成对的交并比

Defined in :numref:`sec_anchor`""""""
box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
(boxes[:, 3] - boxes[:, 1]))
# boxes1,boxes2,areas1,areas2的形状:
# boxes1：(boxes1的数量,4),
# boxes2：(boxes2的数量,4),
# areas1：(boxes1的数量,),
# areas2：(boxes2的数量,)
areas1 = box_area(boxes1)
areas2 = box_area(boxes2)
# inter_upperlefts,inter_lowerrights,inters的形状:
# (boxes1的数量,boxes2的数量,2)
inter_upperlefts = paddle.maximum(boxes1[:, None, :2], boxes2[:, :2])
inter_lowerrights = paddle.minimum(boxes1[:, None, 2:], boxes2[:, 2:])
inters = (inter_lowerrights - inter_upperlefts).clip(min=0)
# inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)
inter_areas = inters[:, :, 0] * inters[:, :, 1]
union_areas = areas1[:, None] + areas2 - inter_areas
return inter_areas / union_areas

def assign_anchor_to_bbox(ground_truth, anchors, place, iou_threshold=0.5):
将最接近的真实边界框分配给锚框, 计算二维互相关运算, 计算二维互相关运算

Defined in :numref:`sec_conv_layer`""""""
h, w = K.shape
Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
for i in range(Y.shape[0]):
for j in range(Y.shape[1]):
Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))
return Y

def evaluate_accuracy_gpu(net, data_iter, device=None):
使用GPU计算模型在数据集上的精度, 计算在指定数据集上模型的精度, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`""""""
if isinstance(net, paddle.nn.Layer):
net.eval()  # 将模型设置为评估模式
metric = Accumulator(2)  # 正确预测数、预测总数
with paddle.no_grad():
for X, y in data_iter:
metric.add(accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

class Accumulator:
在n个变量上累加, 计算梯度并更新参数, 计算遮蔽语言模型损失, 计算预测正确的数量, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def accuracy(y_hat, y):
计算预测正确的数量, 训练和预测, 训练序列到序列模型, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`""""""
optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())
loss = MaskedSoftmaxCELoss()
net.train()
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[10, num_epochs])
for epoch in range(num_epochs):
timer = d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
for batch in data_iter:
optimizer.clear_grad()
X, X_valid_len, Y, Y_valid_len = [paddle.to_tensor(x, place=device) for x in batch]
bos = paddle.to_tensor([tgt_vocab['<bos>']] * Y.shape[0]).reshape([-1, 1])
dec_input = paddle.concat([bos, Y[:, :-1]], 1)  # 强制教学
Y_hat, _ = net(X, dec_input, X_valid_len.squeeze())
l = loss(Y_hat, Y, Y_valid_len.squeeze())
l.backward()	# 损失函数的标量进行“反向传播”
d2l.grad_clipping(net, 1)
num_tokens = Y_valid_len.sum()
optimizer.step()
with paddle.no_grad():
metric.add(l.sum(), num_tokens)
if (epoch + 1) % 10 == 0:
animator.add(epoch + 1, (metric[0] / metric[1],))
print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
f'tokens/sec on {str(device)}')

def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
device, save_attention_weights=False):
序列到序列模型的预测, 训练损失总和、训练准确度总和、样本数, 训练模型, 训练模型一个迭代周期（定义见第3章）, 训练模型（定义见第3章）, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
test_acc = evaluate_accuracy(net, test_iter)
animator.add(epoch + 1, train_metrics + (test_acc,))
train_loss, train_acc = train_metrics
assert train_loss < 0.5, train_loss
assert train_acc <= 1 and train_acc > 0.7, train_acc
assert test_acc <= 1 and test_acc > 0.7, test_acc

def predict_ch3(net, test_iter, n=6):
预测标签（定义见第3章）, 训练模型（定义见第8章）, 训练网络一个迭代周期（定义见第8章, 训练网络一个迭代周期（定义见第8章)

Defined in :numref:`sec_rnn_scratch`""""""
state, timer = None, d2l.Timer()
metric = d2l.Accumulator(2)  # 训练损失之和,词元数量
for X, Y in train_iter:
if state is None or use_random_iter:
# 在第一次迭代或使用随机抽样时初始化state
state = net.begin_state(batch_size=X.shape[0])
else:
if isinstance(net, nn.Layer) and not isinstance(state, tuple):
# state对于nn.GRU是个张量
state.stop_gradient=True
else:
# state对于nn.LSTM或对于我们从零开始实现的模型是个张量
for s in state:
s.stop_gradient=True
y = paddle.reshape(Y.T,shape=[-1])
X = paddle.to_tensor(X, place=device)
y = paddle.to_tensor(y, place=device)
y_hat, state = net(X, state)
l = loss(y_hat, y).mean()
if isinstance(updater, paddle.optimizer.Optimizer):
updater.clear_grad()
l.backward()
grad_clipping(net, 1)
updater.step()
else:
l.backward()
grad_clipping(net, 1)
# 因为已经调用了mean函数
updater(batch_size=1)

metric.add(l * d2l.size(y), d2l.size(y))
return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()

def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):
训练模型（定义见第8章）, 记录多次运行时间, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的图表大小, 设置matplotlib的轴, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 设置为评估模式, 评估给定数据集上模型的损失。, 评估给定数据集上模型的损失。

Defined in :numref:`sec_model_selection`""""""
metric = d2l.Accumulator(2)  # 损失的总和, 样本数量
for X, y in data_iter:
out = net(X)
y = y.reshape(out.shape)
l = loss(out, y)
metric.add(l.sum(), l.numel())
return metric[0] / metric[1]

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('..', 'data')):
下载一个DATA_HUB中的文件，返回本地文件名, 词元/秒, 词元化“英语－法语”数据数据集, 读取IMDb评论数据集文本序列和标签, 读取IMDb评论数据集文本序列和标签

Defined in :numref:`sec_sentiment`""""""
data, labels = [], []
for label in ('pos', 'neg'):
folder_name = os.path.join(data_dir, 'train' if is_train else 'test',
label)
for file in os.listdir(folder_name):
with open(os.path.join(folder_name, file), 'rb') as f:
review = f.read().decode('utf-8').replace('\n', '')
data.append(review)
labels.append(1 if label == 'pos' else 0)
return data, labels

def load_data_imdb(batch_size, num_steps=500):
返回数据迭代器和IMDb评论数据集的词表, 读取fname来给标签字典返回一个文件名, 读取fname来给标签字典返回一个文件名

Defined in :numref:`sec_kaggle_cifar10`""""""
with open(fname, 'r') as f:
# 跳过文件头行(列名)
lines = f.readlines()[1:]
tokens = [l.rstrip().split(',') for l in lines]
return dict(((name, label) for name, label in tokens))

def copyfile(filename, target_dir):
将文件复制到目标目录, 读取所有VOC图像并标注, 读取香蕉检测数据集中的图像和标签, 跳过标题信息，例如fastText中的首行, 载入“英语－法语”数据集, 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens), 输入paragraphs[i]是代表段落的句子字符串列表；, 输出'X'的形状：(batch_size,num_steps,embed_size), 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,, 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，, 返回Fashion-MNIST数据集的文本标签, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回从pos位置开始的长度为num_steps的序列, 返回带有负采样的跳元模型的小批量样本, 返回带有负采样的跳元模型的小批量样本

Defined in :numref:`sec_word2vec_data`""""""
max_len = max(len(c) + len(n) for _, c, n in data)
centers, contexts_negatives, masks, labels = [], [], [], []
for center, context, negative in data:
cur_len = len(context) + len(negative)
centers += [center]
contexts_negatives += \
[context + negative + [0] * (max_len - cur_len)]
masks += [[1] * cur_len + [0] * (max_len - cur_len)]
labels += [[1] * len(context) + [0] * (max_len - len(context))]
return (d2l.reshape(d2l.tensor(centers), (-1, 1)), d2l.tensor(
contexts_negatives), d2l.tensor(masks), d2l.tensor(labels))

def load_data_ptb(batch_size, max_window_size, num_noise_words):
下载PTB数据集，然后将其加载到内存中, 返回平均时间, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu, 返回数据迭代器和IMDb评论数据集的词表, 返回时光机器数据集的词元索引列表和词表, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`""""""
lines = read_time_machine()
tokens = tokenize(lines, 'char')
vocab = Vocab(tokens)
# 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
# 所以将所有文本行展平到一个列表中
corpus = [vocab[token] for line in tokens for token in line]
if max_tokens > 0:
corpus = corpus[:max_tokens]
return corpus, vocab

def seq_data_iter_random(corpus, batch_size, num_steps):
使用随机抽样生成一个小批量子序列, 返回时光机器数据集的迭代器和词表, 返回时间总和, 返回累计时间, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表, 返回负采样中的噪声词, 返回跳元模型中的中心词和上下文词, 返回跳元模型中的中心词和上下文词

Defined in :numref:`sec_word2vec_data`""""""
centers, contexts = [], []
for line in corpus:
# 要形成“中心词-上下文词”对，每个句子至少需要有2个词
if len(line) < 2:
continue
centers += line
for i in range(len(line)):  # 上下文窗口中间i
window_size = random.randint(1, max_window_size)
indices = list(range(max(0, i - window_size),
min(len(line), i + 1 + window_size)))
# 从上下文词中排除中心词
indices.remove(i)
contexts.append([line[idx] for idx in indices])
return centers, contexts

class RandomGenerator:
根据n个采样权重在{1,...,n}中随机抽取, 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y）, 这里的tokens是1D列表或2D列表, 逆转transpose_qkv函数的操作, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`""""""
X = X.reshape((-1, num_heads, X.shape[1], X.shape[2]))
X = X.transpose((0, 2, 1, 3))
return X.reshape((X.shape[0], X.shape[1], -1))

class PositionalEncoding(nn.Layer):
位置编码, 通过在最后一个轴上掩蔽元素来执行softmax操作, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`""""""
# X:3D张量，valid_lens:1D或2D张量
if valid_lens is None:
return nn.functional.softmax(X, axis=-1)
else:
shape = X.shape
if valid_lens.dim() == 1:
valid_lens = paddle.repeat_interleave(valid_lens, shape[1])
else:
valid_lens = valid_lens.reshape((-1,))
# 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
X = d2l.sequence_mask(X.reshape((-1, shape[-1])), valid_lens,
value=-1e6)
return nn.functional.softmax(X.reshape(shape), axis=-1)

class AdditiveAttention(nn.Layer):
加性注意力, 遮蔽语言模型任务中预测15%的随机词元, 错误：未知词元类型：, 长度为num_steps的子序列的起始索引, 随机裁剪特征和标签图像, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
rect = paddle.vision.transforms.RandomCrop((height, width))._get_param(
img=feature, output_size=(height, width))
feature = paddle.vision.transforms.crop(feature, *rect)
label = paddle.vision.transforms.crop(label, *rect)
return feature, label

class VOCSegDataset(paddle.io.Dataset):
Defined in :numref:`sec_semantic_segmentation`, 需设定time_major=True,指定input的第一个维度为time_steps, 预处理“英语－法语”数据集, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
def no_space(char, prev_char):
return char in set(',.!?') and prev_char != ' '

# 使用空格替换不间断空格
# 使用小写字母替换大写字母
text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
# 在单词和标点符号之间插入空格
out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char
for i, char in enumerate(text)]
return ''.join(out)

def tokenize_nmt(text, num_examples=None):
词元化“英语－法语”数据数据集, 预测前提和假设之间的逻辑关系, 预测文本序列的情感, 预测文本序列的情感

Defined in :numref:`sec_sentiment_rnn`""""""
sequence = paddle.to_tensor(vocab[sequence.split()], place=d2l.try_gpu())
label = paddle.argmax(net(sequence.reshape((1, -1))), axis=1)
return 'positive' if label == 1 else 'negative'

d2l.DATA_HUB['SNLI'] = (
'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',
'9fcde07509c7e87ec61c640c1b2753d9041758e4')

def read_snli(data_dir, is_train):
将SNLI数据集解析为前提、假设和标签, 预测标签（定义见第3章）, 飞桨不支持在notebook上进行多GPU训练, ，否则返回cpu"
https://github.com/d2l-ai/d2l-zh,tensorflow.py,0,0.0,494,48.77,201,19.84,37,3.65,98,9.67,183,18.07,86,25,203,92,,"Accumulator, AddNorm, AdditiveAttention, Animator, AttentionDecoder, Benchmark, DATA_HUB, DATA_URL, Decoder, DotProductAttention, Encoder, EncoderBlock, EncoderDecoder, IPython, K, MaskedSoftmaxCELoss, MultiHeadAttention, NotImplementedError, P, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, Rectangle, Residual, Seq2SeqEncoder, SeqDataLoader, Timer, TrainCallback, TransformerEncoder, Updater, Vocab, W_k, W_o, W_q, W_v, X, X_valid_len, Xs, Y, Y_hat, Y_valid_len, Ys, __call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, __name__, _device_name, _token_freqs, abs, accuracy, add, addnorm1, addnorm2, animator, annotate, append, apply_gradients, arange, argmax, args, assign_sub, astype, attention, attention_weight_seq, attention_weights, avg, ax, axes, base_dir, batch, batch_size, bbox, bbox_to_rect, begin_state, bias, bleu, blk, blks, bn1, bn2, bos, box_center_to_corner, box_corner_to_center, boxes, build_array_nmt, cache_dir, call, cell, char, cla, clear_output, cmap, cmp, collections, color, colorbar, compile, config_axes, content, contour, conv1, conv2, conv3, corpus, corr2d, cosh, count_corpus, counter, cumsum, cx, cy, d2l, data, data_arrays, data_dir, data_iter, data_iter_fn, dataset, db, dec_X, dec_input, dec_state, decoder, dense, dense1, dense2, description, device_name, dict, display, download, download_all, download_extract, dropout, dtype, dw, embed_size, embedding, enc_X, enc_outputs, enc_valid_len, encoder, enumerate, epoch, evaluate, evaluate_accuracy, evaluate_loss, ext, extractall, eye, f_grad, feature_dim, features, ffn, ffn_num_hiddens, ffn_num_outputs, fig, figsize, fit, flatten, float, fmt, fmts, fname, folder, forward_fn, fp, freq, gca, get, get_data_ch11, get_fashion_mnist_labels, get_initial_state, get_input, get_params, get_xaxis, get_yaxis, grad, grad_clipping, gradient, gradients, grads, grid, has_one_axis, hasattr, hashlib, hexdigest, hist, hyperparams, idx, idx_to_token, img, imgs, imshow, index, indices, init_state, initial_indices, initial_indices_per_batch, inputs, int, int32, is_train, isinstance, items, iter, key_size, keys, kwargs, l_sum, label, label_one_hot, label_seq, label_subs, label_tokens, labels, legend, len, len_label, len_pred, lines, linreg, linspace, list, ln, load_array, load_corpus_time_machine, load_data_fashion_mnist, load_data_nmt, load_data_time_machine, log, logs, loss, lower, lr, mask, masked_softmax, math, matmul, matplotlib, matplotlib_inline, matrices, matrix, max_len, max_tokens, maxlen, mean, meshgrid, metric, metrics, min, min_freq, mnist_test, mnist_train, name, ncols, ndim, net, net_fn, new_grad, next, nn_Module, no_space, norm_shape, normalized_shape, nrows, num_batches, num_channels, num_cols, num_epochs, num_examples, num_gpus, num_heads, num_hiddens, num_layers, num_matches, num_preds, num_rows, num_steps, num_subseqs, num_tokens, numpy, offset, on_epoch_begin, on_epoch_end, open, os, out, output, output_concat, output_seq, outputs, padding_token, pandas, param, params, parts, patch, patches, pcm, plot, plt, pos, pos_encoding, ppl, pred, pred_seq, pred_tokens, predict, predict_ch3, predict_ch8, predict_seq2seq, preds, prefix, preprocess_nmt, prev_char, print, process, property, queries, query_size, random, range, rcParams, re, read, read_data_nmt, read_time_machine, readlines, reduce_sum, relu, replace, requests, reserved_tokens, reset, reshape, resize, resize_fn, results, rnn, rnn_layer, row_axes, row_matrices, s1, s2, save_attention_weights, scale, scope, score, scores, self, seq_data_iter_random, seq_data_iter_sequential, sequence_mask, set_axes, set_figsize, set_hatch, set_matplotlib_formats, set_title, set_xlabel, set_xlim, set_xscale, set_ylabel, set_ylim, set_yscale, sgd, sha1, sha1_hash, shape, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, shuffle, shutil, sinh, size, sorted, source, speed, split, squared_loss, src_array, src_sentence, src_tokens, src_valid_len, src_vocab, stack, start, state, states, std, steps, stop, str, strategy, strides, subplots, sum, super, synthetic_data, sys, tape, tarfile, target, tensor, tensorflow, test_acc, test_iter, text_labels, tgt_array, tgt_valid_len, tgt_vocab, theta, tik, time, timer, times, titles, to_tokens, token, token_freqs, token_to_idx, tokenize, tokenize_nmt, tokens, train_2d, train_acc, train_ch11, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_epoch_ch3, train_epoch_ch8, train_iter, train_loss, train_metrics, train_seq2seq, trainable_variables, trainer, trainer_fn, transpose_output, transpose_qkv, true, trues, truncate_pad, try_all_gpus, try_gpu, tuple, unk, unweighted_loss, update, updater, url, use_1x1conv, use_random_iter, use_svg_display, valid_len, valid_lens, value, value_size, values, vocab, vocab_size, w, w_v, weighted_loss, weights, write, x, x1, x2, xlabel, xlim, xlist, xscale, xy, xytext, y1, y2, y_hat, ylabel, ylim, ylist, yscale, zeros, zip, zipfile","IPython.display, collections.Counter, collections.defaultdict, hashlib.sha1, math.exp, math.pow, matplotlib.pyplot, matplotlib_inline.backend_inline, np.arange, np.array, np.cos, np.float32, np.genfromtxt, np.power, np.sin, np.zeros, os.makedirs, os.path.dirname, os.path.exists, os.path.join, os.path.splitext, random.randint, random.shuffle, re.sub, requests.get, sys.modules, tarfile.open, tf.GradientTape, tf.IndexedSlices, tf.Variable, tf.abs, tf.argmax, tf.cast, tf.concat, tf.config.experimental.list_physical_devices, tf.constant, tf.convert_to_tensor, tf.cos, tf.cosh, tf.data.Dataset.from_tensor_slices, tf.data.experimental.cardinality, tf.device, tf.distribute.OneDeviceStrategy, tf.exp, tf.expand_dims, tf.eye, tf.float32, tf.greater, tf.image.resize_with_pad, tf.int32, tf.keras.Model, tf.keras.Sequential, tf.keras.activations.relu, tf.keras.callbacks.Callback, tf.keras.datasets.fashion_mnist.load_data, tf.keras.layers.BatchNormalization, tf.keras.layers.Conv2D, tf.keras.layers.Dense, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.layers.GRUCell, tf.keras.layers.Layer, tf.keras.layers.LayerNormalization, tf.keras.layers.RNN, tf.keras.layers.ReLU, tf.keras.layers.StackedRNNCells, tf.keras.losses.CategoricalCrossentropy, tf.keras.losses.Loss, tf.keras.losses.MeanSquaredError, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.optimizers.Adam, tf.keras.optimizers.Optimizer, tf.keras.optimizers.SGD, tf.linspace, tf.math.log, tf.math.reduce_mean, tf.math.sqrt, tf.matmul, tf.meshgrid, tf.nn.softmax, tf.nn.tanh, tf.one_hot, tf.ones, tf.ones_like, tf.random.normal, tf.random.uniform, tf.random_normal_initializer, tf.range, tf.reduce_mean, tf.reduce_sum, tf.repeat, tf.reshape, tf.sin, tf.sinh, tf.size, tf.squeeze, tf.stack, tf.tanh, tf.transpose, tf.where, tf.zeros, time.time, zipfile.ZipFile","__call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, accuracy, add, annotate, attention_weights, avg, bbox_to_rect, begin_state, bleu, box_center_to_corner, box_corner_to_center, build_array_nmt, call, corr2d, count_corpus, cumsum, data, download, download_all, download_extract, evaluate_accuracy, evaluate_loss, get_data_ch11, get_fashion_mnist_labels, grad_clipping, has_one_axis, init_state, linreg, load_array, load_corpus_time_machine, load_data_fashion_mnist, load_data_nmt, load_data_time_machine, masked_softmax, no_space, on_epoch_begin, on_epoch_end, plot, predict_ch3, predict_ch8, predict_seq2seq, preprocess_nmt, read_data_nmt, read_time_machine, reset, seq_data_iter_random, seq_data_iter_sequential, sequence_mask, set_axes, set_figsize, sgd, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, squared_loss, start, stop, sum, synthetic_data, to_tokens, token_freqs, tokenize, tokenize_nmt, train_2d, train_ch11, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_epoch_ch3, train_epoch_ch8, train_seq2seq, transpose_output, transpose_qkv, truncate_pad, try_all_gpus, try_gpu, unk, use_svg_display","Accumulator, AddNorm, AdditiveAttention, Animator, AttentionDecoder, Benchmark, Decoder, DotProductAttention, Encoder, EncoderBlock, EncoderDecoder, MaskedSoftmaxCELoss, MultiHeadAttention, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, Residual, Seq2SeqEncoder, SeqDataLoader, Timer, TrainCallback, TransformerEncoder, Updater, Vocab","DATA_HUB, DATA_URL, X, X_valid_len, Xs, Y, Y_hat, Y_valid_len, Ys, abs, animator, arange, argmax, array, astype, attention_weight_seq, ax, axes, b, base_dir, batch, batch_size, blk, bos, boxes, callback, char, cmp, concat, corpus, cos, cosh, counter, cx, cy, d, d2l, data, data_arrays, data_dir, data_iter, dataset, db, dec_X, dec_input, dec_state, device, device_name, devices, dw, enc_X, enc_outputs, enc_valid_len, epoch, exp, ext, eye, features, figsize, float32, fmt, fname, fp, freq, get_input, grad, gradients, grads, h, idx, img, index, initial_indices, initial_indices_per_batch, int32, keys, l, l_sum, label_one_hot, label_subs, label_tokens, legend, len_label, len_pred, lines, linspace, log, loss, mask, matmul, matrix, maxlen, meshgrid, metric, metrics, mnist_test, mnist_train, n, net, new_grad, nn_Module, norm, normal, num_batches, num_cols, num_examples, num_gpus, num_matches, num_rows, num_subseqs, num_tokens, numpy, offset, ones, optimizer, out, output, output_concat, output_seq, outputs, p, param, params, parts, patch, patches, pcm, ppl, pred, pred_tokens, predict, preds, process, q, queries, r, rand, reduce_mean, reduce_sum, reserved_tokens, reshape, resize_fn, results, row_axes, row_matrices, s1, s2, score, scores, sha1, sha1_hash, shape, sin, sinh, size, source, speed, src_array, src_tokens, src_valid_len, src_vocab, stack, state, strategy, tanh, tape, target, tensor, test_acc, text, text_labels, tgt_array, tgt_valid_len, tgt_vocab, theta, timer, titles, token, tokens, train_acc, train_loss, train_metrics, transpose, true, trues, unweighted_loss, updater, url, valid_len, valid_lens, values, vocab, w, weighted_loss, weights, x, x1, x2, y, y1, y2, y_hat, zeros","Additiveattention.

Defined in :numref:`sec_attention-scoring-functions`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn-concise`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`subsec_linear_model`, Scaleddotproductattention.

Defined in :numref:`subsec_additive-attention`, Transformer编码器

Defined in :numref:`sec_transformer`, Transformer编码器块

Defined in :numref:`sec_transformer`, 一个以可视化的训练进展的回调

Defined in :numref:`sec_lenet`, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加载序列数据的迭代器, 启动计时器, 在n个变量上累加, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

Defined in :numref:`sec_seq2seq_decoder`, 均方损失

Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

Defined in :numref:`sec_transformer`, 多头注意力

Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

Defined in :numref:`sec_use_gpu`, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`, 显示矩阵热图

Defined in :numref:`sec_attention-cues`, 构造一个TensorFlow数据迭代器

Defined in :numref:`sec_linear_concise`, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

Defined in :numref:`sec_linear_scratch`, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`, 用小批量随机梯度下降法更新参数

Defined in :numref:`sec_softmax_scratch`, 线性回归模型

Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`, 绘制图像列表

Defined in :numref:`sec_fashion_mnist`, 绘制数据点

Defined in :numref:`sec_calculus`, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`, 计算BLEU

Defined in :numref:`sec_seq2seq_training`, 计算二维互相关运算, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型一个迭代周期（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

Defined in :numref:`sec_calculus`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`",", 	, 
,  ,  examples/sec on ,  sec,  sec/epoch,  tokens/sec on ,  不存在于 ,  词元/秒 , #1f77b4, #ff7f0e, , , , test acc , , train acc , , x1: , , x2: , ,.!?, -, ->, -o, .., ..., .1f, .3f, .4f, .gz, .tar, .zip, /, /CPU:0, /GPU:, 090b5e7e70c295757f55df93cb0a180b9691891a, 585e9cc93e70b39160e7921475f9bcd7d31219ce, 76e5be1548fd8222e5074cf0faae75edff8cf93f, 94646ad1522d915e7b0f9296181140edcf86a4f5, : , <bos>, <eos>, <pad>, <unk>, Additiveattention.

    Defined in :numref:`sec_attention-scoring-functions`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn-concise`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`subsec_linear_model`, Done, GPU, Reds, Scaleddotproductattention.

    Defined in :numref:`subsec_additive-attention`, Transformer编码器

    Defined in :numref:`sec_transformer`, Transformer编码器块

    Defined in :numref:`sec_transformer`, [^A-Za-z]+, __len__, accuracy, airfoil, airfoil_self_noise.dat, ankle boot, bag, char, coat, data, dress, epoch, epoch , f, fa19780a7b011d9b009e8bff8e99922a8ee2eb90, figure.figsize, fra-eng, fra-eng.zip, fra.txt, g-., http://d2l-data.s3-accelerate.amazonaws.com/, int32, kaggle_house_pred_test.csv, kaggle_house_pred_train.csv, kaggle_house_test, kaggle_house_train, linear, loss, loss , loss: , m--, ndim, none, perplexity, pullover, r, r:, rb, same, sandal, shirt, sneaker, svg, t-shirt, test acc, time traveller, time_machine, timemachine.txt, train, train acc, train loss, traveller, trouser, utf-8, wb, word, x1, x2,  ,  , 一个以可视化的训练进展的回调

    Defined in :numref:`sec_lenet`, 下载, 下载DATA_HUB中的所有文件

    Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

    Defined in :numref:`sec_fashion_mnist`, 下载一个DATA_HUB中的文件，返回本地文件名

    Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

    Defined in :numref:`sec_kaggle_house`, 为了多注意力头的并行计算而变换形状

    Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

    Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

    Defined in :numref:`sec_bbox`, 位置编码

    Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用svg格式在Jupyter中显示绘图

    Defined in :numref:`sec_calculus`, 使用随机抽样生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 使用顺序分区生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加载序列数据的迭代器, 只有zip/tar文件可以被解压缩, 启动计时器, 困惑度 , 在n个变量上累加, 在prefix后面生成新字符

    Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

    Defined in :numref:`sec_seq2seq_decoder`, 均方损失

    Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

    Defined in :numref:`sec_transformer`, 多头注意力

    Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`, 将文本行拆分为单词或字符词元

    Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

    Defined in :numref:`sec_text_preprocessing`, 将机器翻译的文本序列转换成小批量

    Defined in :numref:`subsec_mt_data_loading`, 小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

    Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

    Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

    Defined in :numref:`sec_seq2seq_training`, 截断或填充文本序列

    Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

    Defined in :numref:`subsec_gd-learningrate`, 显示矩阵热图

    Defined in :numref:`sec_attention-cues`, 构造一个TensorFlow数据迭代器

    Defined in :numref:`sec_linear_concise`, 正在从, 残差连接后进行层规范化

    Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

    Defined in :numref:`sec_linear_scratch`, 用GPU训练模型(在第六章定义)

    Defined in :numref:`sec_lenet`, 用于序列到序列学习的循环神经网络编码器

    Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用定制的训练机优化2D目标函数

    Defined in :numref:`subsec_gd-learningrate`, 用小批量随机梯度下降法更新参数

    Defined in :numref:`sec_softmax_scratch`, 线性回归模型

    Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

    Defined in :numref:`sec_machine_translation`, 绘制图像列表

    Defined in :numref:`sec_fashion_mnist`, 绘制数据点

    Defined in :numref:`sec_calculus`, 统计词元的频率

    Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`, 裁剪梯度

    Defined in :numref:`sec_rnn_scratch`, 计算BLEU

    Defined in :numref:`sec_seq2seq_training`, 计算二维互相关运算, 计算在指定数据集上模型的精度

    Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

    Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

    Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型一个迭代周期（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 训练模型（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

    Defined in :numref:`sec_calculus`, 设置matplotlib的轴

    Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

    Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

    Defined in :numref:`sec_machine_translation`, 载入“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

    Defined in :numref:`sec_fashion_mnist`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

    Defined in :numref:`sec_use_gpu`, 返回时光机器数据集的词元索引列表和词表

    Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

    Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

    Defined in :numref:`subsec_mt_data_loading`, 逆转transpose_qkv函数的操作

    Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

    Defined in :numref:`sec_attention-scoring-functions`, 错误：未知词元类型：, 预处理“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 预测标签（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`","0, 0.0, 0.01, 0.1, 0.22, 0.3, 0.35, 0.5, 0.6, 0.7, 0.9, 1, 1.0, 1.5, 10, 1000, 10000, 1000000.0, 1048576, 1500, 2, 2.5, 20, 200, 255, 28, 3, 3.0, 3.5, 5, 5.5, 50, 6, 600, False, None, True","#    d2lbook build lib, # (batch_size*num_heads，查询或者“键－值”对的个数，, # (batch_size，)或(batch_size，查询的个数), # (batch_size，查询或者“键－值”对的个数，num_hiddens), # Don't edit it directly, # Keras内置的损失接受的是（标签，预测），这不同于用户在本书中的实现。, # Keras的loss默认返回一个批量的平均损失, # Keras默认返回一个批量中的平均损失, # MeanSquaredError计算平方误差时不带系数1/2, # The below part is generated automatically through:, # X:3D张量，valid_lens:1D或2D张量, # keys的形状：(batch_size，“键－值”对的个数，d), # num_hiddens/num_heads), # output_concat的形状:(batch_size，查询的个数，num_hiddens), # output的形状:(batch_size*num_heads，查询的个数，, # queries的形状：(batch_size，查询的个数，d), # queries，keys，values的形状:, # rnn返回两个以上的值, # s1和s2是稍后将使用的内部状态变量, # valid_lens　的形状:, # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数), # values的形状：(batch_size，“键－值”对的个数，值的维度), # 从随机偏移量开始划分序列, # 使用lambda函数捕获参数, # 使用小写字母替换大写字母, # 使用空格替换不间断空格, # 初始化模型, # 向图表中添加多个数据点, # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, # 在单词和标点符号之间插入空格, # 在第一次迭代或使用随机抽样时初始化state, # 在轴0，将第一项（标量或者矢量）复制num_heads次，, # 增量地绘制多条线, # 如果X有一个轴，输出True, # 强制教学, # 所以将所有文本行展平到一个列表中, # 按出现频率排序, # 损失的总和,样本数量, # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0, # 未知词元的索引为0, # 本书的实现接受（预测，标签），例如我们上面实现的“交叉熵”, # 然后如此复制第二项，然后诸如此类。, # 经过变换后，输出的queries，keys，values　的形状:, # 计算梯度并更新参数, # 训练损失之和,词元数量, # 训练损失总和、训练准确度总和、样本数, # 训练损失总和，词元数量, # 返回训练损失和训练精度, # 预测num_preds步, # 预热期, #################   WARNING   ################, Additiveattention.

Defined in :numref:`sec_attention-scoring-functions`""""""
def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
super().__init__(**kwargs)
self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=False)
self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=False)
self.w_v = tf.keras.layers.Dense(1, use_bias=False)
self.dropout = tf.keras.layers.Dropout(dropout)

def call(self, queries, keys, values, valid_lens, **kwargs):
queries, keys = self.W_q(queries), self.W_k(keys)
# 在维度扩展后，
# queries的形状：(batch_size，查询的个数，1，num_hidden)
# key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
# 使用广播方式进行求和
features = tf.expand_dims(queries, axis=2) + tf.expand_dims(
keys, axis=1)
features = tf.nn.tanh(features)
# self.w_v仅有一个输出，因此从形状中移除最后那个维度。
# scores的形状：(batch_size，查询的个数，“键-值”对的个数)
scores = tf.squeeze(self.w_v(features), axis=-1)
self.attention_weights = masked_softmax(scores, valid_lens)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
return tf.matmul(self.dropout(
self.attention_weights, **kwargs), values)

class DotProductAttention(tf.keras.layers.Layer):
Scaleddotproductattention., Defined in :numref:`sec_minibatches`
# 初始化模型
w = tf.Variable(tf.random.normal(shape=(feature_dim, 1),
mean=0, stddev=0.01),trainable=True)
b = tf.Variable(tf.zeros(1), trainable=True)

# 训练模型
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()

for _ in range(num_epochs):
for X, y in data_iter:
with tf.GradientTape() as g:
l = tf.math.reduce_mean(loss(net(X), y))

dw, db = g.gradient(l, [w, b])
trainer_fn([w, b], [dw, db], states, hyperparams)
n += X.shape[0]
if n % 200 == 0:
timer.stop()
p = n/X.shape[0]
q = p/tf.data.experimental.cardinality(data_iter).numpy()
r = (d2l.evaluate_loss(net, data_iter, loss),)
animator.add(q, r)
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
return timer.cumsum(), animator.Y[0]

def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.init_state, self.forward_fn = init_state, forward_fn
self.trainable_variables = get_params(vocab_size, num_hiddens)

def __call__(self, X, state):
X = tf.one_hot(tf.transpose(X), self.vocab_size)
X = tf.cast(X, tf.float32)
return self.forward_fn(X, state, self.trainable_variables)

def begin_state(self, batch_size, *args, **kwargs):
return self.init_state(batch_size, self.num_hiddens)

def predict_ch8(prefix, num_preds, net, vocab):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
self.data = [0.0] * n

def add(self, *args):
self.data = [a + float(b) for a, b in zip(self.data, args)]

def reset(self):
self.data = [0.0] * len(self.data)

def __getitem__(self, idx):
return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
训练模型一个迭代周期（定义见第3章）, Transformer编码器

Defined in :numref:`sec_transformer`""""""
def __init__(self, vocab_size, key_size, query_size, value_size,
num_hiddens, norm_shape, ffn_num_hiddens, num_heads,
num_layers, dropout, bias=False, **kwargs):
super().__init__(**kwargs)
self.num_hiddens = num_hiddens
self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)
self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
self.blks = [EncoderBlock(
key_size, query_size, value_size, num_hiddens, norm_shape,
ffn_num_hiddens, num_heads, dropout, bias) for _ in range(
num_layers)]

def call(self, X, valid_lens, **kwargs):
# 因为位置编码值在-1和1之间，
# 因此嵌入值乘以嵌入维度的平方根进行缩放，
# 然后再与位置编码相加。
X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(
tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)
self.attention_weights = [None] * len(self.blks)
for i, blk in enumerate(self.blks):
X = blk(X, valid_lens, **kwargs)
self.attention_weights[
i] = blk.attention.attention.attention_weights
return X

def annotate(text, xy, xytext):
d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
arrowprops=dict(arrowstyle='->'))

def train_2d(trainer, steps=20, f_grad=None):
用定制的训练机优化2D目标函数, 一个以可视化的训练进展的回调

Defined in :numref:`sec_lenet`""""""
def __init__(self, net, train_iter, test_iter, num_epochs, device_name):
self.timer = d2l.Timer()
self.animator = d2l.Animator(
xlabel='epoch', xlim=[1, num_epochs], legend=[
'train loss', 'train acc', 'test acc'])
self.net = net
self.train_iter = train_iter
self.test_iter = test_iter
self.num_epochs = num_epochs
self.device_name = device_name

def on_epoch_begin(self, epoch, logs=None):
self.timer.start()

def on_epoch_end(self, epoch, logs):
self.timer.stop()
test_acc = self.net.evaluate(
self.test_iter, verbose=0, return_dict=True)['accuracy']
metrics = (logs['loss'], logs['accuracy'], test_acc)
self.animator.add(epoch + 1, metrics)
if epoch == self.num_epochs - 1:
batch_size = next(iter(self.train_iter))[0].shape[0]
num_examples = batch_size * tf.data.experimental.cardinality(
self.train_iter).numpy()
print(f'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, '
f'test acc {metrics[2]:.3f}')
print(f'{num_examples / self.timer.avg():.1f} examples/sec on '
f'{str(self.device_name)}')

def train_ch6(net_fn, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`""""""
mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()
# 将所有数字除以255，使所有像素值介于0和1之间，在最后添加一个批处理维度，
# 并将标签转换为int32。
process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,
tf.cast(y, dtype='int32'))
resize_fn = lambda X, y: (
tf.image.resize_with_pad(X, resize, resize) if resize else X, y)
return (
tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(
batch_size).shuffle(len(mnist_train[0])).map(resize_fn),
tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(
batch_size).map(resize_fn))

def accuracy(y_hat, y):
计算预测正确的数量, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`""""""
# 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
# 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
# num_hiddens/num_heads)
X = tf.reshape(X, shape=(X.shape[0], X.shape[1], num_heads, -1))

# 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
# num_hiddens/num_heads)
X = tf.transpose(X, perm=(0, 2, 1, 3))

# 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
# num_hiddens/num_heads)
return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))


def transpose_output(X, num_heads):
逆转transpose_qkv函数的操作, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`""""""
def __init__(self, num_hiddens, dropout, max_len=1000):
super().__init__()
self.dropout = tf.keras.layers.Dropout(dropout)
# 创建一个足够长的P
self.P = np.zeros((1, max_len, num_hiddens))
X = np.arange(max_len, dtype=np.float32).reshape(
-1,1)/np.power(10000, np.arange(
0, num_hiddens, 2, dtype=np.float32) / num_hiddens)
self.P[:, :, 0::2] = np.sin(X)
self.P[:, :, 1::2] = np.cos(X)

def call(self, X, **kwargs):
X = X + self.P[:, :X.shape[1], :]
return self.dropout(X, **kwargs)

class PositionWiseFFN(tf.keras.layers.Layer):
基于位置的前馈网络, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 在动画中绘制数据
def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
figsize=(3.5, 2.5)):
Defined in :numref:`sec_softmax_scratch`, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
for param, grad in zip(params, grads):
param.assign_sub(lr*grad/batch_size)

def load_array(data_arrays, batch_size, is_train=True):
构造一个TensorFlow数据迭代器, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`""""""
def __init__(self, **kwargs):
super(AttentionDecoder, self).__init__(**kwargs)

@property
def attention_weights(self):
raise NotImplementedError

class MultiHeadAttention(tf.keras.layers.Layer):
多头注意力, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`""""""
def __init__(self, valid_len):
super().__init__(reduction='none')
self.valid_len = valid_len

# pred的形状：(batch_size,num_steps,vocab_size)
# label的形状：(batch_size,num_steps)
# valid_len的形状：(batch_size,)
def call(self, label, pred):
weights = tf.ones_like(label, dtype=tf.float32)
weights = sequence_mask(weights, self.valid_len)
label_one_hot = tf.one_hot(label, depth=pred.shape[-1])
unweighted_loss = tf.keras.losses.CategoricalCrossentropy(
from_logits=True, reduction='none')(label_one_hot, pred)
weighted_loss = tf.reduce_mean((unweighted_loss*weights), axis=1)
return weighted_loss

def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
训练序列到序列模型, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`""""""
src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
src_vocab['<eos>']]
enc_valid_len = tf.constant([len(src_tokens)])
src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
# 添加批量轴
enc_X = tf.expand_dims(src_tokens, axis=0)
enc_outputs = net.encoder(enc_X, enc_valid_len, training=False)
dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
# 添加批量轴
dec_X = tf.expand_dims(tf.constant([tgt_vocab['<bos>']]), axis=0)
output_seq, attention_weight_seq = [], []
for _ in range(num_steps):
Y, dec_state = net.decoder(dec_X, dec_state, training=False)
# 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
dec_X = tf.argmax(Y, axis=2)
pred = tf.squeeze(dec_X, axis=0)
# 保存注意力权重
if save_attention_weights:
attention_weight_seq.append(net.decoder.attention_weights)
# 一旦序列结束词元被预测，输出序列的生成就完成了
if pred == tgt_vocab['<eos>']:
break
output_seq.append(pred.numpy())
return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq,
shape = -1).numpy().tolist())), attention_weight_seq

def bleu(pred_seq, label_seq, k):
计算BLEU, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`""""""
d2l.set_figsize()
d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
d2l.arange(-3.0, 1.0, 0.1))
d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
d2l.plt.xlabel('x1')
d2l.plt.ylabel('x2')

d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')

def get_data_ch11(batch_size=10, n=1500):
Defined in :numref:`sec_minibatches`, 显示矩阵热图

Defined in :numref:`sec_attention-cues`""""""
d2l.use_svg_display()
num_rows, num_cols = matrices.shape[0], matrices.shape[1]
fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
sharex=True, sharey=True, squeeze=False)
for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)
if i == num_rows - 1:
ax.set_xlabel(xlabel)
if j == 0:
ax.set_ylabel(ylabel)
if titles:
ax.set_title(titles[j])
fig.colorbar(pcm, ax=axes, shrink=0.6);

def masked_softmax(X, valid_lens):
通过在最后一个轴上掩蔽元素来执行softmax操作, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`""""""
def __init__(self, normalized_shape, dropout, **kwargs):
super().__init__(**kwargs)
self.dropout = tf.keras.layers.Dropout(dropout)
self.ln = tf.keras.layers.LayerNormalization(normalized_shape)

def call(self, X, Y, **kwargs):
return self.ln(self.dropout(Y, **kwargs) + X)

class EncoderBlock(tf.keras.layers.Layer):
Transformer编码器块, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):
super().__init__(*kwargs)
# 嵌入层
self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)
self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(
[tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
for _ in range(num_layers)]), return_sequences=True,
return_state=True)

def call(self, X, *args, **kwargs):
# 输入'X'的形状：(batch_size,num_steps)
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
output = self.rnn(X, **kwargs)
state = output[1:]
return output[0], state

def sequence_mask(X, valid_len, value=0):
在序列中屏蔽不相关的项, 用于测量运行时间
def __init__(self, description='Done'):
Defined in :numref:`sec_hybridize`, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def call(self, X, state, **kwargs):
raise NotImplementedError

class EncoderDecoder(tf.keras.Model):
编码器-解码器架构的基类, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
theta = tf.constant(theta, dtype=tf.float32)
new_grad = []
for grad in grads:
if isinstance(grad, tf.IndexedSlices):
new_grad.append(tf.convert_to_tensor(grad))
else:
new_grad.append(grad)
norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()
for grad in new_grad))
norm = tf.cast(norm, tf.float32)
if tf.greater(norm, theta):
for i, grad in enumerate(new_grad):
new_grad[i] = grad * theta / norm
else:
new_grad = new_grad
return new_grad

def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):
训练模型一个迭代周期（定义见第8章）, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`""""""
metric = Accumulator(2)  # 正确预测数、预测总数
for X, y in data_iter:
metric.add(accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

class Accumulator:
在n个变量上累加, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
test_acc = evaluate_accuracy(net, test_iter)
animator.add(epoch + 1, train_metrics + (test_acc,))
train_loss, train_acc = train_metrics
assert train_loss < 0.5, train_loss
assert train_acc <= 1 and train_acc > 0.7, train_acc
assert test_acc <= 1 and test_acc > 0.7, test_acc

class Updater():
用小批量随机梯度下降法更新参数, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
with strategy.scope():
loss = tf.keras.losses.SparseCategoricalCrossentropy(
from_logits=True)
updater = tf.keras.optimizers.SGD(lr)
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,
use_random_iter)
if (epoch + 1) % 10 == 0:
print(predict('time traveller'))
animator.add(epoch + 1, [ppl])
device = d2l.try_gpu()._device_name
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(tf.keras.layers.Layer):
Defined in :numref:`sec_rnn-concise`, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`""""""
num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))
devices = [tf.device(f'/GPU:{i}') for i in range(num_gpus)]
return devices if devices else [tf.device('/CPU:0')]

def corr2d(X, K):
计算二维互相关运算, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(tf.keras.layers.Layer):
编码器-解码器架构的基本编码器接口, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失","(batch_size*num_heads，查询或者“键－值”对的个数，, (batch_size，)或(batch_size，查询的个数), (batch_size，查询或者“键－值”对的个数，num_hiddens), Additiveattention.

Defined in :numref:`sec_attention-scoring-functions`""""""
def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
super().__init__(**kwargs)
self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=False)
self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=False)
self.w_v = tf.keras.layers.Dense(1, use_bias=False)
self.dropout = tf.keras.layers.Dropout(dropout)

def call(self, queries, keys, values, valid_lens, **kwargs):
queries, keys = self.W_q(queries), self.W_k(keys)
# 在维度扩展后，
# queries的形状：(batch_size，查询的个数，1，num_hidden)
# key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
# 使用广播方式进行求和
features = tf.expand_dims(queries, axis=2) + tf.expand_dims(
keys, axis=1)
features = tf.nn.tanh(features)
# self.w_v仅有一个输出，因此从形状中移除最后那个维度。
# scores的形状：(batch_size，查询的个数，“键-值”对的个数)
scores = tf.squeeze(self.w_v(features), axis=-1)
self.attention_weights = masked_softmax(scores, valid_lens)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
return tf.matmul(self.dropout(
self.attention_weights, **kwargs), values)

class DotProductAttention(tf.keras.layers.Layer):
Scaleddotproductattention., Defined in :numref:`sec_minibatches`
# 初始化模型
w = tf.Variable(tf.random.normal(shape=(feature_dim, 1),
mean=0, stddev=0.01),trainable=True)
b = tf.Variable(tf.zeros(1), trainable=True)

# 训练模型
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()

for _ in range(num_epochs):
for X, y in data_iter:
with tf.GradientTape() as g:
l = tf.math.reduce_mean(loss(net(X), y))

dw, db = g.gradient(l, [w, b])
trainer_fn([w, b], [dw, db], states, hyperparams)
n += X.shape[0]
if n % 200 == 0:
timer.stop()
p = n/X.shape[0]
q = p/tf.data.experimental.cardinality(data_iter).numpy()
r = (d2l.evaluate_loss(net, data_iter, loss),)
animator.add(q, r)
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
return timer.cumsum(), animator.Y[0]

def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.init_state, self.forward_fn = init_state, forward_fn
self.trainable_variables = get_params(vocab_size, num_hiddens)

def __call__(self, X, state):
X = tf.one_hot(tf.transpose(X), self.vocab_size)
X = tf.cast(X, tf.float32)
return self.forward_fn(X, state, self.trainable_variables)

def begin_state(self, batch_size, *args, **kwargs):
return self.init_state(batch_size, self.num_hiddens)

def predict_ch8(prefix, num_preds, net, vocab):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
self.data = [0.0] * n

def add(self, *args):
self.data = [a + float(b) for a, b in zip(self.data, args)]

def reset(self):
self.data = [0.0] * len(self.data)

def __getitem__(self, idx):
return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
训练模型一个迭代周期（定义见第3章）, Keras内置的损失接受的是（标签，预测），这不同于用户在本书中的实现。, Keras的loss默认返回一个批量的平均损失, Keras默认返回一个批量中的平均损失, MeanSquaredError计算平方误差时不带系数1/2, Transformer编码器, Transformer编码器

Defined in :numref:`sec_transformer`""""""
def __init__(self, vocab_size, key_size, query_size, value_size,
num_hiddens, norm_shape, ffn_num_hiddens, num_heads,
num_layers, dropout, bias=False, **kwargs):
super().__init__(**kwargs)
self.num_hiddens = num_hiddens
self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)
self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
self.blks = [EncoderBlock(
key_size, query_size, value_size, num_hiddens, norm_shape,
ffn_num_hiddens, num_heads, dropout, bias) for _ in range(
num_layers)]

def call(self, X, valid_lens, **kwargs):
# 因为位置编码值在-1和1之间，
# 因此嵌入值乘以嵌入维度的平方根进行缩放，
# 然后再与位置编码相加。
X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(
tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)
self.attention_weights = [None] * len(self.blks)
for i, blk in enumerate(self.blks):
X = blk(X, valid_lens, **kwargs)
self.attention_weights[
i] = blk.attention.attention.attention_weights
return X

def annotate(text, xy, xytext):
d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
arrowprops=dict(arrowstyle='->'))

def train_2d(trainer, steps=20, f_grad=None):
用定制的训练机优化2D目标函数, Transformer编码器块, X:3D张量，valid_lens:1D或2D张量, keys的形状：(batch_size，“键－值”对的个数，d), output_concat的形状:(batch_size，查询的个数，num_hiddens), output的形状:(batch_size*num_heads，查询的个数，, queries的形状：(batch_size，查询的个数，d), queries，keys，values的形状:, rnn返回两个以上的值, s1和s2是稍后将使用的内部状态变量, valid_lens　的形状:, valid_lens的形状:(batch_size，)或者(batch_size，查询的个数), values的形状：(batch_size，“键－值”对的个数，值的维度), 一个以可视化的训练进展的回调, 一个以可视化的训练进展的回调

Defined in :numref:`sec_lenet`""""""
def __init__(self, net, train_iter, test_iter, num_epochs, device_name):
self.timer = d2l.Timer()
self.animator = d2l.Animator(
xlabel='epoch', xlim=[1, num_epochs], legend=[
'train loss', 'train acc', 'test acc'])
self.net = net
self.train_iter = train_iter
self.test_iter = test_iter
self.num_epochs = num_epochs
self.device_name = device_name

def on_epoch_begin(self, epoch, logs=None):
self.timer.start()

def on_epoch_end(self, epoch, logs):
self.timer.stop()
test_acc = self.net.evaluate(
self.test_iter, verbose=0, return_dict=True)['accuracy']
metrics = (logs['loss'], logs['accuracy'], test_acc)
self.animator.add(epoch + 1, metrics)
if epoch == self.num_epochs - 1:
batch_size = next(iter(self.train_iter))[0].shape[0]
num_examples = batch_size * tf.data.experimental.cardinality(
self.train_iter).numpy()
print(f'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, '
f'test acc {metrics[2]:.3f}')
print(f'{num_examples / self.timer.avg():.1f} examples/sec on '
f'{str(self.device_name)}')

def train_ch6(net_fn, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 下载, 下载DATA_HUB中的所有文件, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载Fashion-MNIST数据集，然后将其加载到内存中, 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`""""""
mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()
# 将所有数字除以255，使所有像素值介于0和1之间，在最后添加一个批处理维度，
# 并将标签转换为int32。
process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,
tf.cast(y, dtype='int32'))
resize_fn = lambda X, y: (
tf.image.resize_with_pad(X, resize, resize) if resize else X, y)
return (
tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(
batch_size).shuffle(len(mnist_train[0])).map(resize_fn),
tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(
batch_size).map(resize_fn))

def accuracy(y_hat, y):
计算预测正确的数量, 下载一个DATA_HUB中的文件，返回本地文件名, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 下载并解压zip/tar文件, 不存在于, 为了多注意力头的并行计算而变换形状, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`""""""
# 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
# 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
# num_hiddens/num_heads)
X = tf.reshape(X, shape=(X.shape[0], X.shape[1], num_heads, -1))

# 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
# num_hiddens/num_heads)
X = tf.transpose(X, perm=(0, 2, 1, 3))

# 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
# num_hiddens/num_heads)
return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))


def transpose_output(X, num_heads):
逆转transpose_qkv函数的操作, 从随机偏移量开始划分序列, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）, 从（左上，右下）转换到（中间，宽度，高度）, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 位置编码, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`""""""
def __init__(self, num_hiddens, dropout, max_len=1000):
super().__init__()
self.dropout = tf.keras.layers.Dropout(dropout)
# 创建一个足够长的P
self.P = np.zeros((1, max_len, num_hiddens))
X = np.arange(max_len, dtype=np.float32).reshape(
-1,1)/np.power(10000, np.arange(
0, num_hiddens, 2, dtype=np.float32) / num_hiddens)
self.P[:, :, 0::2] = np.sin(X)
self.P[:, :, 1::2] = np.cos(X)

def call(self, X, **kwargs):
X = X + self.P[:, :X.shape[1], :]
return self.dropout(X, **kwargs)

class PositionWiseFFN(tf.keras.layers.Layer):
基于位置的前馈网络, 使用lambda函数捕获参数, 使用svg格式在Jupyter中显示绘图, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用小写字母替换大写字母, 使用空格替换不间断空格, 使用随机抽样生成一个小批量子序列, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 使用顺序分区生成一个小批量子序列, 停止计时器并将时间记录在列表中, 初始化模型, 加载序列数据的迭代器, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 只有zip/tar文件可以被解压缩, 向图表中添加多个数据点, 启动计时器, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, 困惑度, 在n个变量上累加, 在prefix后面生成新字符, 在动画中绘制数据, 在动画中绘制数据
def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
figsize=(3.5, 2.5)):
Defined in :numref:`sec_softmax_scratch`, 在单词和标点符号之间插入空格, 在序列中屏蔽不相关的项, 在第一次迭代或使用随机抽样时初始化state, 在第六章定义, 在轴0，将第一项（标量或者矢量）复制num_heads次，, 均方损失, 基于位置的前馈网络, 增量地绘制多条线, 多头注意力, 如果X有一个轴，输出True, 如果存在，则返回gpu, 将文本行拆分为单词或字符词元, 将时间机器数据集加载到文本行的列表中, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 将机器翻译的文本序列转换成小批量, 小批量随机梯度下降, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
for param, grad in zip(params, grads):
param.assign_sub(lr*grad/batch_size)

def load_array(data_arrays, batch_size, is_train=True):
构造一个TensorFlow数据迭代器, 带有注意力机制解码器的基本接口, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`""""""
def __init__(self, **kwargs):
super(AttentionDecoder, self).__init__(**kwargs)

@property
def attention_weights(self):
raise NotImplementedError

class MultiHeadAttention(tf.keras.layers.Layer):
多头注意力, 带遮蔽的softmax交叉熵损失函数, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`""""""
def __init__(self, valid_len):
super().__init__(reduction='none')
self.valid_len = valid_len

# pred的形状：(batch_size,num_steps,vocab_size)
# label的形状：(batch_size,num_steps)
# valid_len的形状：(batch_size,)
def call(self, label, pred):
weights = tf.ones_like(label, dtype=tf.float32)
weights = sequence_mask(weights, self.valid_len)
label_one_hot = tf.one_hot(label, depth=pred.shape[-1])
unweighted_loss = tf.keras.losses.CategoricalCrossentropy(
from_logits=True, reduction='none')(label_one_hot, pred)
weighted_loss = tf.reduce_mean((unweighted_loss*weights), axis=1)
return weighted_loss

def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
训练序列到序列模型, 序列到序列模型的预测, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`""""""
src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
src_vocab['<eos>']]
enc_valid_len = tf.constant([len(src_tokens)])
src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
# 添加批量轴
enc_X = tf.expand_dims(src_tokens, axis=0)
enc_outputs = net.encoder(enc_X, enc_valid_len, training=False)
dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
# 添加批量轴
dec_X = tf.expand_dims(tf.constant([tgt_vocab['<bos>']]), axis=0)
output_seq, attention_weight_seq = [], []
for _ in range(num_steps):
Y, dec_state = net.decoder(dec_X, dec_state, training=False)
# 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
dec_X = tf.argmax(Y, axis=2)
pred = tf.squeeze(dec_X, axis=0)
# 保存注意力权重
if save_attention_weights:
attention_weight_seq.append(net.decoder.attention_weights)
# 一旦序列结束词元被预测，输出序列的生成就完成了
if pred == tgt_vocab['<eos>']:
break
output_seq.append(pred.numpy())
return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq,
shape = -1).numpy().tolist())), attention_weight_seq

def bleu(pred_seq, label_seq, k):
计算BLEU, 强制教学, 截断或填充文本序列, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 所以将所有文本行展平到一个列表中, 按出现频率排序, 损失的总和,样本数量, 文本词表, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示优化过程中2D变量的轨迹, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`""""""
d2l.set_figsize()
d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
d2l.arange(-3.0, 1.0, 0.1))
d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
d2l.plt.xlabel('x1')
d2l.plt.ylabel('x2')

d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')

def get_data_ch11(batch_size=10, n=1500):
Defined in :numref:`sec_minibatches`, 显示矩阵热图, 显示矩阵热图

Defined in :numref:`sec_attention-cues`""""""
d2l.use_svg_display()
num_rows, num_cols = matrices.shape[0], matrices.shape[1]
fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
sharex=True, sharey=True, squeeze=False)
for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)
if i == num_rows - 1:
ax.set_xlabel(xlabel)
if j == 0:
ax.set_ylabel(ylabel)
if titles:
ax.set_title(titles[j])
fig.colorbar(pcm, ax=axes, shrink=0.6);

def masked_softmax(X, valid_lens):
通过在最后一个轴上掩蔽元素来执行softmax操作, 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0, 未知词元的索引为0, 本书的实现接受（预测，标签），例如我们上面实现的“交叉熵”, 构造一个TensorFlow数据迭代器, 正在从, 残差连接后进行层规范化, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`""""""
def __init__(self, normalized_shape, dropout, **kwargs):
super().__init__(**kwargs)
self.dropout = tf.keras.layers.Dropout(dropout)
self.ln = tf.keras.layers.LayerNormalization(normalized_shape)

def call(self, X, Y, **kwargs):
return self.ln(self.dropout(Y, **kwargs) + X)

class EncoderBlock(tf.keras.layers.Layer):
Transformer编码器块, 然后如此复制第二项，然后诸如此类。, 生成y=Xw+b+噪声, 用GPU训练模型, 用于序列到序列学习的循环神经网络编码器, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):
super().__init__(*kwargs)
# 嵌入层
self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)
self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(
[tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
for _ in range(num_layers)]), return_sequences=True,
return_state=True)

def call(self, X, *args, **kwargs):
# 输入'X'的形状：(batch_size,num_steps)
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
output = self.rnn(X, **kwargs)
state = output[1:]
return output[0], state

def sequence_mask(X, valid_len, value=0):
在序列中屏蔽不相关的项, 用于测量运行时间, 用于测量运行时间
def __init__(self, description='Done'):
Defined in :numref:`sec_hybridize`, 用定制的训练机优化2D目标函数, 用小批量随机梯度下降法更新参数, 线性回归模型, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 经过变换后，输出的queries，keys，values　的形状:, 绘制列表长度对的直方图, 绘制图像列表, 绘制数据点, 统计词元的频率, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def call(self, X, state, **kwargs):
raise NotImplementedError

class EncoderDecoder(tf.keras.Model):
编码器-解码器架构的基类, 编码器-解码器架构的基类, 裁剪梯度, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
theta = tf.constant(theta, dtype=tf.float32)
new_grad = []
for grad in grads:
if isinstance(grad, tf.IndexedSlices):
new_grad.append(tf.convert_to_tensor(grad))
else:
new_grad.append(grad)
norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()
for grad in new_grad))
norm = tf.cast(norm, tf.float32)
if tf.greater(norm, theta):
for i, grad in enumerate(new_grad):
new_grad[i] = grad * theta / norm
else:
new_grad = new_grad
return new_grad

def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):
训练模型一个迭代周期（定义见第8章）, 计算BLEU, 计算二维互相关运算, 计算在指定数据集上模型的精度, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`""""""
metric = Accumulator(2)  # 正确预测数、预测总数
for X, y in data_iter:
metric.add(accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

class Accumulator:
在n个变量上累加, 计算梯度并更新参数, 计算预测正确的数量, 训练序列到序列模型, 训练损失之和,词元数量, 训练损失总和、训练准确度总和、样本数, 训练损失总和，词元数量, 训练模型一个迭代周期（定义见第3章）, 训练模型一个迭代周期（定义见第8章）, 训练模型（定义见第3章）, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
legend=['train loss', 'train acc', 'test acc'])
for epoch in range(num_epochs):
train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
test_acc = evaluate_accuracy(net, test_iter)
animator.add(epoch + 1, train_metrics + (test_acc,))
train_loss, train_acc = train_metrics
assert train_loss < 0.5, train_loss
assert train_acc <= 1 and train_acc > 0.7, train_acc
assert test_acc <= 1 and test_acc > 0.7, test_acc

class Updater():
用小批量随机梯度下降法更新参数, 训练模型（定义见第8章）, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
with strategy.scope():
loss = tf.keras.losses.SparseCategoricalCrossentropy(
from_logits=True)
updater = tf.keras.optimizers.SGD(lr)
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,
use_random_iter)
if (epoch + 1) % 10 == 0:
print(predict('time traveller'))
animator.add(epoch + 1, [ppl])
device = d2l.try_gpu()._device_name
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(tf.keras.layers.Layer):
Defined in :numref:`sec_rnn-concise`, 记录多次运行时间, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的图表大小, 设置matplotlib的轴, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 评估给定数据集上模型的损失, 词元/秒, 词元化“英语－法语”数据数据集, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 载入“英语－法语”数据集, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 返回Fashion-MNIST数据集的文本标签, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回平均时间, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`""""""
num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))
devices = [tf.device(f'/GPU:{i}') for i in range(num_gpus)]
return devices if devices else [tf.device('/CPU:0')]

def corr2d(X, K):
计算二维互相关运算, 返回时光机器数据集的词元索引列表和词表, 返回时光机器数据集的迭代器和词表, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回时间总和, 返回累计时间, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(tf.keras.layers.Layer):
编码器-解码器架构的基本编码器接口, 返回训练损失和训练精度, 逆转transpose_qkv函数的操作, 通过在最后一个轴上掩蔽元素来执行softmax操作, 错误：未知词元类型：, 预处理“英语－法语”数据集, 预测num_preds步, 预测标签（定义见第3章）, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失, 预热期, ，否则返回cpu"
https://github.com/d2l-ai/d2l-zh,torch.py,0,0.0,843,47.92,372,21.15,59,3.35,176,10.01,309,17.57,142,34,378,144,,"Accumulator, AdaptiveAvgPool2d, AddNorm, AdditiveAttention, Animator, AttentionDecoder, B, BERTEncoder, BERTModel, BananasDataset, BatchNorm2d, Benchmark, Compose, Conv2d, CrossEntropyLoss, DATA_HUB, DATA_URL, DataLoader, DataParallel, Decoder, DotProductAttention, Dropout, Embedding, Encoder, EncoderBlock, EncoderDecoder, Flatten, GRU, IPython, K, LSTM, LayerNorm, Linear, MSELoss, MaskLM, MaskedSoftmaxCELoss, Module, MultiHeadAttention, NextSentencePred, NotImplementedError, P, PIL, PTBDataset, Parameter, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, ReLU, Rectangle, Residual, Resize, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Sequential, T, Tanh, TensorDataset, Timer, ToTensor, TokenEmbedding, TransformerEncoder, VOCSegDataset, VOC_CLASSES, VOC_COLORMAP, Vocab, W_k, W_o, W_q, W_v, X, X_valid_len, Xs, Y, Y_hat, Y_valid_len, Ys, _WikiTextDataset, __call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, __name__, _flat_weights_names, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _parameters, _read_wiki, _replace_mlm_tokens, _token_freqs, abs, acc, accuracy, add, add_module, add_patch, addnorm1, addnorm2, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, annotate, append, apply, arange, areas1, areas2, argmax, args, assign_anchor_to_bbox, assigned_bb, astype, attention, attention_weight_seq, attention_weights, avg, ax, axes, backward, base_dir, batch, batch_class_labels, batch_idx, batch_mask, batch_offset, batch_size, batchify, bb_idx, bbox, bbox_mask, bbox_offset, bbox_to_rect, bboxes, begin_state, below_min_idx, bias, bidirectional, bleu, blk, blks, bn1, bn2, bos, box_area, box_center_to_corner, box_corner_to_center, box_idx, box_iou, box_j, boxes, boxes1, boxes2, boxes_per_pixel, build_array_nmt, c_anc, c_assigned_bb, cache_dir, candidate_pred_positions, candidates, center, center_h, center_w, centers, char, cla, class_id, class_labels, clear_output, cls_prob, cls_probs, cmap, cmp, col_discard, collections, color, colorbar, colormap, colormap2label, colors, combined, concat, conf, config_axes, content, context, contexts, contexts_negatives, contour, conv1, conv2, conv3, copyfile, corpus, corr2d, cosh, count_corpus, counter, counts, crop_size, csv_data, csv_fname, cumsum, cur_len, cx, cy, d2l, data, data_arrays, data_dir, data_iter, data_iter_fn, dataset, dec_X, dec_input, dec_state, decoder, default_values, dense1, dense2, description, detach, detach_, device, devices, dict, dim, display, download, download_all, download_extract, draw, dropout, dtype, elem, elems, embed_size, embedding, embedding_name, enc_X, enc_outputs, enc_valid_len, encoded_X, encoder, enumerate, epoch, eps, eval, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_loss, examples, exp, ext, extend, extract_text, extractall, eye, f_grad, feature, feature_dim, features, ffn, ffn_num_hiddens, ffn_num_input, ffn_num_outputs, fig, figsize, file_name, filename, filter, first_block, flatten, float, fmt, fmts, fname, folder, folder_name, forward, forward_fn, fp, freq, functional, gca, generator, get, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_input, get_negatives, get_params, get_tokens_and_segments, get_xaxis, get_yaxis, grad, grad_clipping, grid, ground_truth, has_one_axis, hasattr, hashlib, height, hexdigest, hid_in_features, hidden, hidden_size, hist, hyperparams, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, imgs, imshow, in_channels, in_height, in_width, index, indices, indices_true, inds, init, init_state, init_weights, initial_indices, initial_indices_per_batch, input_channels, inputs, insert, int, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, iou_threshold, is_next, is_train, isinstance, items, iter, iterrows, jaccard, keep, key_size, keys, kwargs, label, label_count, label_seq, label_set, label_subs, label_tokens, labels, legend, len, len_label, len_pred, linear, lines, linreg, linspace, list, ln, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, log, long, loss, lower, lr, mask, masked_X, masked_softmax, masked_token, masks, math, matmul, matplotlib, matplotlib_inline, matrices, matrix, max_idx, max_ious, max_len, max_num_mlm_preds, max_tokens, max_window_size, maxlen, mean, meshgrid, metric, min, min_freq, mlm, mlm_Y, mlm_Y_hat, mlm_in_features, mlm_input_tokens, mlm_l, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mlm_weights_X, mlp, mnist_test, mnist_train, mode, multibox_detection, multibox_prior, multibox_target, n_valid_per_label, name, ncols, ndim, neg, negative, negatives, net, next, next_sentence, nms, nms_threshold, nn_Module, no_space, non_keep, norm, norm_shape, normal, normalize_image, normalized_shape, nrows, nsp, nsp_Y_hat, nsp_data_from_paragraph, nsp_in_features, nsp_l, nsp_labels, nsp_y, num_anchors, num_batches, num_channels, num_classes, num_cols, num_directions, num_epochs, num_examples, num_gt_boxes, num_heads, num_hiddens, num_inputs, num_layers, num_matches, num_mlm_preds, num_noise_words, num_pred_positions, num_preds, num_ratios, num_residuals, num_rows, num_sizes, num_steps, num_subseqs, num_tokens, num_workers, numel, numpy, obj, offset, offset_boxes, offset_h, offset_inverse, offset_pred, offset_preds, offset_w, offset_wh, offset_xy, one_hot, open, optimizer, os, out, out_channels, out_grid, output, output_concat, output_seq, outputs, padding_token, pandas, paragraph, paragraphs, parallel, param, parameters, params, parts, patch, patches, pcm, permute, plot, plt, population, pos, pos_embedding, pos_encoding, pos_threshold, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_X, pred_positions_and_labels, pred_seq, pred_tokens, predict, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, predicted_bb, predicted_bbox, preds, prefix, premise, premises, preprocess_nmt, prev_char, print, property, queries, query_size, random, range, ratio_tensor, ratios, raw_text, rcParams, re, read, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, readlines, rect, reduce_mean, reduce_sum, reduction, relu, remove, reorg_test, reorg_train_valid, replace, requests, requires_grad, reserved_tokens, reset, reshape, resize, resnet18, resnet_block, results, review, rnn, rnn_layer, round, row, row_axes, row_discard, row_matrices, rows, rstrip, s1, s2, sampling_weights, save_attention_weights, scale, scatter, score, scores, segment_embedding, segments, segments_X, self, sentence, sentences, seq_data_iter_random, seq_data_iter_sequential, sequence, sequence_mask, set_axes, set_figsize, set_hatch, set_index, set_matplotlib_formats, set_title, set_xlabel, set_xlim, set_xscale, set_ylabel, set_ylim, set_yscale, sgd, sha1, sha1_hash, shape, shift_x, shift_y, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, shutil, sinh, size, size_tensor, sizes, softmax, sorted, source, speed, split, split_batch, squared_loss, squeeze, src_array, src_sentence, src_tokens, src_valid_len, src_vocab, stack, start, state, states, std, step, steps, steps_h, steps_w, stop, str, strides, strip, subplots, subsample, subsampled, sum, super, synthetic_data, sys, t, tarfile, target, target_dir, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, theta, tik, time, timer, times, titles, to, to_tokens, token, token_embedding, token_freqs, token_ids, token_to_idx, tokenize, tokenize_nmt, tokens, tokens_X, tokens_a, tokens_b, torch, torchvision, train, train_2d, train_acc, train_acc_sum, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_data, train_epoch_ch3, train_epoch_ch8, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_seq2seq, train_set, train_tokens, trainer, trainer_fn, transform, transpose, transpose_output, transpose_qkv, true, trues, truncate_pad, try_all_gpus, try_gpu, tuple, txt_fname, type, union_areas, unique, uniques, unk, unknown_idx, unsqueeze, unweighted_loss, update, updater, url, use_1x1conv, use_bias, use_random_iter, use_svg_display, val_iter, valid_len, valid_lens, valid_lens_x, valid_ratio, value, value_size, values, vecs, voc_colormap2label, voc_dir, voc_label_indices, voc_rand_crop, vocab, vocab_size, w, w_v, weight, weighted_loss, weights, width, window_size, write, x, x1, x2, xavier_init_weights, xavier_uniform_, xlabel, xlim, xlist, xscale, xy, xytext, y1, y2, y_hat, ylabel, ylim, ylist, yscale, zero_, zero_grad, zeros, zip, zipfile","IPython.display, PIL.Image, collections.Counter, collections.defaultdict, hashlib.sha1, math.exp, math.floor, math.pow, math.sqrt, matplotlib.pyplot, matplotlib_inline.backend_inline, np.array, np.float32, np.genfromtxt, os.listdir, os.makedirs, os.path.dirname, os.path.exists, os.path.join, os.path.splitext, pd.read_csv, random.choice, random.choices, random.randint, random.random, random.shuffle, random.uniform, re.sub, requests.get, shutil.copy, sys.modules, tarfile.open, time.time, torch.abs, torch.arange, torch.argmax, torch.argsort, torch.bmm, torch.cat, torch.cos, torch.cosh, torch.cuda.device_count, torch.device, torch.exp, torch.eye, torch.float32, torch.from_numpy, torch.full, torch.int32, torch.is_tensor, torch.linspace, torch.log, torch.long, torch.matmul, torch.max, torch.meshgrid, torch.min, torch.nn, torch.nn.Module, torch.nn.functional, torch.nn.init.normal_, torch.no_grad, torch.nonzero, torch.normal, torch.ones, torch.ones_like, torch.optim.Adam, torch.optim.Optimizer, torch.optim.SGD, torch.pow, torch.rand, torch.randn, torch.repeat_interleave, torch.sin, torch.sinh, torch.sqrt, torch.stack, torch.sum, torch.tanh, torch.tensor, torch.unsqueeze, torch.utils.data, torch.utils.data.DataLoader, torch.utils.data.Dataset, torch.zeros, torchvision.datasets.FashionMNIST, torchvision.io.image.ImageReadMode.RGB, torchvision.io.read_image, torchvision.transforms, torchvision.transforms.Normalize, torchvision.transforms.RandomCrop.get_params, torchvision.transforms.functional.crop, zipfile.ZipFile","__call__, __enter__, __exit__, __getitem__, __init__, __iter__, __len__, _get_batch_loss_bert, _get_mlm_data_from_tokens, _get_next_sentence, _get_nsp_data_from_paragraph, _load_embedding, _make_list, _pad, _pad_bert_inputs, _read_wiki, _replace_mlm_tokens, accuracy, add, annotate, assign_anchor_to_bbox, attention_weights, avg, batchify, bbox_to_rect, begin_state, bleu, box_center_to_corner, box_corner_to_center, box_iou, build_array_nmt, copyfile, corr2d, count_corpus, cumsum, data, download, download_all, download_extract, draw, evaluate_accuracy, evaluate_accuracy_gpu, evaluate_loss, extract_text, filter, forward, get_centers_and_contexts, get_data_ch11, get_dataloader_workers, get_fashion_mnist_labels, get_negatives, get_tokens_and_segments, grad_clipping, has_one_axis, init_state, init_weights, keep, linreg, load_array, load_corpus_time_machine, load_data_bananas, load_data_fashion_mnist, load_data_imdb, load_data_nmt, load_data_ptb, load_data_snli, load_data_time_machine, load_data_voc, load_data_wiki, masked_softmax, multibox_detection, multibox_prior, multibox_target, nms, no_space, normalize_image, offset_boxes, offset_inverse, plot, predict_ch3, predict_ch8, predict_sentiment, predict_seq2seq, predict_snli, preprocess_nmt, read_csv_labels, read_data_bananas, read_data_nmt, read_imdb, read_ptb, read_snli, read_time_machine, read_voc_images, reorg_test, reorg_train_valid, reset, resnet18, resnet_block, seq_data_iter_random, seq_data_iter_sequential, sequence_mask, set_axes, set_figsize, sgd, show_bboxes, show_heatmaps, show_images, show_list_len_pair_hist, show_trace_2d, split_batch, squared_loss, start, stop, subsample, sum, synthetic_data, to_tokens, token_freqs, tokenize, tokenize_nmt, train_2d, train_batch_ch13, train_ch11, train_ch13, train_ch3, train_ch6, train_ch8, train_concise_ch11, train_epoch_ch3, train_epoch_ch8, train_seq2seq, transpose_output, transpose_qkv, truncate_pad, try_all_gpus, try_gpu, unk, use_svg_display, voc_colormap2label, voc_label_indices, voc_rand_crop, xavier_init_weights","Accumulator, AddNorm, AdditiveAttention, Animator, AttentionDecoder, BERTEncoder, BERTModel, BananasDataset, Benchmark, Decoder, DotProductAttention, Encoder, EncoderBlock, EncoderDecoder, MaskLM, MaskedSoftmaxCELoss, MultiHeadAttention, NextSentencePred, PTBDataset, PositionWiseFFN, PositionalEncoding, RNNModel, RNNModelScratch, RandomGenerator, Residual, SNLIDataset, Seq2SeqEncoder, SeqDataLoader, Timer, TokenEmbedding, TransformerEncoder, VOCSegDataset, Vocab, _WikiTextDataset","B, DATA_HUB, DATA_URL, VOC_CLASSES, VOC_COLORMAP, X, X_valid_len, Xs, Y, Y_hat, Y_valid_len, Ys, abs, acc, all_centers, all_contexts, all_hypothesis_tokens, all_id_sorted, all_idx, all_mlm_labels, all_mlm_weights, all_negatives, all_pred_positions, all_premise_tokens, all_segments, all_token_ids, anc, anc_i, anc_idx, anchor_manipulations, anchors, anchors_bbox_map, animator, arange, areas1, areas2, argmax, array, assigned_bb, astype, attention_weight_seq, ax, axes, b, base_dir, batch, batch_class_labels, batch_idx, batch_mask, batch_offset, batch_size, bb_idx, bbox, bbox_mask, bbox_offset, below_min_idx, blk, bos, box_area, box_idx, box_j, boxes, boxes_per_pixel, c_anc, c_assigned_bb, candidate_pred_positions, center, center_h, center_w, centers, char, class_id, class_labels, cls_prob, cmp, col_discard, color, colormap, colormap2label, colors, combined, concat, conf, context, contexts, contexts_negatives, corpus, cos, cosh, counter, counts, csv_data, csv_fname, cur_len, cx, cy, d, d2l, data, data_arrays, data_dir, data_iter, dataset, dec_X, dec_input, dec_state, device, devices, elem, elems, enc_X, enc_outputs, enc_valid_len, encoded_X, epoch, examples, exp, ext, eye, feature, features, fig, figsize, file_name, float32, fmt, fname, folder_name, fp, freq, generator, get_input, h, hypotheses, hypothesis, i, idx, idx_to_token, idx_to_vec, images, img, img_name, in_height, in_width, index, indices, indices_true, inds, initial_indices, initial_indices_per_batch, int32, inter_areas, inter_lowerrights, inter_upperlefts, inters, iou, is_next, jaccard, keep, keys, l, label, label_count, label_set, label_subs, label_tokens, labels, legend, len_label, len_pred, lines, linspace, log, loss, mask, masked_X, masked_token, masks, matmul, matrix, max_idx, max_ious, max_len, max_num_mlm_preds, maxlen, meshgrid, metric, mlm_Y_hat, mlm_input_tokens, mlm_l, mlm_pred_label_ids, mlm_pred_labels, mlm_pred_position, mnist_test, mnist_train, mode, n, n_valid_per_label, neg, negative, negatives, net, next_sentence, nn_Module, non_keep, norm, normal, nsp_Y_hat, nsp_data_from_paragraph, nsp_l, nsp_labels, num_anchors, num_batches, num_classes, num_cols, num_gt_boxes, num_matches, num_mlm_preds, num_pred_positions, num_ratios, num_rows, num_sizes, num_subseqs, num_tokens, num_workers, numpy, obj, offset, offset_h, offset_pred, offset_w, offset_wh, offset_xy, ones, optimizer, out, out_grid, output, output_concat, output_seq, outputs, paragraph, paragraphs, param, params, parts, patch, patches, pcm, ppl, pred, pred_bbox, pred_bbox_wh, pred_bbox_xy, pred_info, pred_positions, pred_positions_and_labels, pred_tokens, predict, predicted_bb, predicted_bbox, preds, premise, premises, queries, r, rand, randn, ratio_tensor, raw_text, rect, reduce_mean, reduce_sum, reserved_tokens, reshape, results, review, row, row_axes, row_discard, row_matrices, rows, s, s1, s2, sampling_weights, score, scores, segments, sentence, sentences, sequence, sha1, sha1_hash, shape, shift_x, shift_y, sin, sinh, size, size_tensor, source, speed, src_array, src_tokens, src_valid_len, src_vocab, stack, state, steps_h, steps_w, subsampled, tanh, target, targets, tensor, test_acc, test_data, test_features, test_file, test_iter, test_set, test_tokens, text, text_color, text_labels, tgt_array, tgt_valid_len, tgt_vocab, timer, titles, to, token, token_ids, tokens, tokens_a, tokens_b, train_acc, train_acc_sum, train_data, train_features, train_file, train_iter, train_l, train_loss, train_loss_sum, train_metrics, train_set, train_tokens, trans, transpose, true, trues, txt_fname, union_areas, uniques, unweighted_loss, updater, url, val_iter, valid_len, valid_lens, values, vecs, voc_dir, vocab, w, weighted_loss, weights, window_size, x, x1, x2, y, y1, y2, y_hat, zeros","BERT模型

Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

Defined in :numref:`subsec_bert_input_rep`, BERT编码器

Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, GloVe嵌入, Transformer编码器

Defined in :numref:`sec_transformer`, Transformer编码器块

Defined in :numref:`sec_transformer`, 一个用于加载VOC数据集的自定义数据集

Defined in :numref:`sec_semantic_segmentation`, 一个用于加载香蕉检测数据集的自定义数据集

Defined in :numref:`sec_object-detection-dataset`, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

Defined in :numref:`sec_kaggle_house`, 下采样高频词

Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`, 启动计时器, 在n个变量上累加, 在prefix后面生成新字符

Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

Defined in :numref:`sec_seq2seq_decoder`, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`, 均方损失

Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

Defined in :numref:`sec_transformer`, 多头注意力

Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

Defined in :numref:`sec_multi_gpu`, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

Defined in :numref:`sec_anchor`, 显示矩阵热图

Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`, 构造一个PyTorch数据迭代器

Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

Defined in :numref:`subsec_labeling-anchor-boxes`, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练

Defined in :numref:`sec_image_augmentation`, 用定制的训练机优化2D目标函数

Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

Defined in :numref:`sec_machine_translation`, 绘制图像列表

Defined in :numref:`sec_fashion_mnist`, 绘制数据点

Defined in :numref:`sec_calculus`, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

Defined in :numref:`sec_bert`, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`, 计算BLEU

Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

Defined in :numref:`sec_anchor`, 计算二维互相关运算

Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第3章）

Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 训练网络一个迭代周期（定义见第8章）

Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

Defined in :numref:`sec_calculus`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

Defined in :numref:`sec_kaggle_cifar10`, 读取所有VOC图像并标注

Defined in :numref:`sec_semantic_segmentation`, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

Defined in :numref:`sec_attention-scoring-functions`, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`",", 	, 
,  ,  . ,  examples,  examples/sec on ,  sec,  sec/epoch,  tokens/sec on ,  training examples,  validation examples,  不存在于 ,  词元/秒 , #1f77b4, #ff7f0e, ,, , , , test acc , , train acc , , x1: , , x2: , ,.!?, -, ->, -o, ., .., ..., ../data, .1f, .3f, .4f, .gz, .jpg, .png, .tar, .zip, /, 01ada507287d82875905620988597833ad4e0903, 090b5e7e70c295757f55df93cb0a180b9691891a, 0b8703943ccdb6eb788e6f091b8946e82231bc4d, 0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d, 2068874e4b9a9f0fb07ebe0ad2b29754449ccacd, 319d85e578af0cdc590547f26231e4e31cdf1e42, 3c914d17d80b1459be871a5039ac23e752a53cbe, 4e443f8a2eca6b1dac8a6c57641b67dd40621a49, 585e9cc93e70b39160e7921475f9bcd7d31219ce, 5de26c8fce5ccdea9f91267273464dc968d20d72, 76e5be1548fd8222e5074cf0faae75edff8cf93f, 94646ad1522d915e7b0f9296181140edcf86a4f5, 9fcde07509c7e87ec61c640c1b2753d9041758e4, : , <bos>, <cls>, <eos>, <mask>, <pad>, <sep>, <unk>, BERT模型

    Defined in :numref:`subsec_nsp`, BERT的下一句预测任务

    Defined in :numref:`subsec_mlm`, BERT的掩蔽语言模型任务

    Defined in :numref:`subsec_bert_input_rep`, BERT编码器

    Defined in :numref:`subsec_bert_input_rep`, Defined in :numref:`sec_bbox`, Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`, Defined in :numref:`sec_hybridize`, Defined in :numref:`sec_language_model`, Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`, Defined in :numref:`sec_softmax_scratch`, Defined in :numref:`sec_synonyms`, Defined in :numref:`sec_text_preprocessing`, Defined in :numref:`sec_word2vec_data`, Defined in :numref:`subsec_linear_model`, Defined in :numref:`subsec_prepare_mlm_data`, Done, GloVe嵌入, ImageSets, JPEGImages, Reds, SNLI, Segmentation, SegmentationClass, Transformer编码器

    Defined in :numref:`sec_transformer`, Transformer编码器块

    Defined in :numref:`sec_transformer`, VOC2012, VOCdevkit, VOCtrainval_11-May-2012.tar, [^A-Za-z]+, \(, \), \s{2,}, __len__, aclImdb, aeroplane, airfoil, airfoil_self_noise.dat, ankle boot, b, b5116e234e9eb9076672cfeabf5469f3eec904fa, background, bag, banana-detection, banana-detection.zip, bananas_train, bananas_val, bicycle, bird, block, boat, bottle, bus, c, c1816da3821ae9f43899be655002f6c723e91b88, car, cat, cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a, center, chair, char, cifar10_tiny, coat, contradiction, cow, cpu, cuda:, data, diningtable, dog, dog_tiny, dress, entailment, epoch, epoch , f, fa19780a7b011d9b009e8bff8e99922a8ee2eb90, fba480ffa8aa7e0febbb511d181409f899b9baa5, fc, figure.figsize, fra-eng, fra-eng.zip, fra.txt, g, g-., global_avg_pool, glove.42B.300d.zip, glove.42b.300d, glove.6B.100d.zip, glove.6B.50d.zip, glove.6b.100d, glove.6b.50d, horse, hotdog, hotdog.zip, http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, http://d2l-data.s3-accelerate.amazonaws.com/, https://nlp.stanford.edu/projects/snli/snli_1.0.zip, https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip, ij, images, img_name, int32, k, kaggle_cifar10_tiny.zip, kaggle_dog_tiny.zip, kaggle_house_pred_test.csv, kaggle_house_pred_train.csv, kaggle_house_test, kaggle_house_train, label.csv, linear, loss, loss , loss: , m, m--, motorbike, ndim, neg, negative, neutral, none, perplexity, person, pos, positive, potted plant, ptb, ptb.train.txt, ptb.zip, pullover, r, r:, rb, read , resnet_block1, resnet_block2, resnet_block3, resnet_block4, sandal, sheep, shirt, sneaker, snli_1.0_test.txt, snli_1.0_train.txt, sofa, svg, t-shirt, test, test acc, time traveller, time_machine, timemachine.txt, train, train acc, train loss, train.txt, train_valid, train_valid_test, training on, traveller, trouser, tv/monitor, unknown, utf-8, val.txt, valid, vec.txt, voc2012, w, wb, weight, wiki.en, wiki.en.zip, wiki.train.tokens, wikitext-2, word, x1, x2,  ,  , 一个用于加载VOC数据集的自定义数据集

    Defined in :numref:`sec_semantic_segmentation`, 一个用于加载香蕉检测数据集的自定义数据集

    Defined in :numref:`sec_object-detection-dataset`, 下载, 下载DATA_HUB中的所有文件

    Defined in :numref:`sec_kaggle_house`, 下载Fashion-MNIST数据集，然后将其加载到内存中

    Defined in :numref:`sec_fashion_mnist`, 下载PTB数据集，然后将其加载到内存中

    Defined in :numref:`subsec_word2vec-minibatch-loading`, 下载SNLI数据集并返回数据迭代器和词表

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 下载一个DATA_HUB中的文件，返回本地文件名

    Defined in :numref:`sec_kaggle_house`, 下载并解压zip/tar文件

    Defined in :numref:`sec_kaggle_house`, 下采样高频词

    Defined in :numref:`sec_word2vec_data`, 为了多注意力头的并行计算而变换形状

    Defined in :numref:`sec_multihead-attention`, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）

    Defined in :numref:`sec_bbox`, 从（左上，右下）转换到（中间，宽度，高度）

    Defined in :numref:`sec_bbox`, 位置编码

    Defined in :numref:`sec_self-attention-and-positional-encoding`, 使用4个进程来读取数据

    Defined in :numref:`sec_fashion_mnist`, 使用GPU计算模型在数据集上的精度

    Defined in :numref:`sec_lenet`, 使用svg格式在Jupyter中显示绘图

    Defined in :numref:`sec_calculus`, 使用真实边界框标记锚框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 使用随机抽样生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 使用非极大值抑制来预测边界框

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 使用顺序分区生成一个小批量子序列

    Defined in :numref:`sec_language_model`, 停止计时器并将时间记录在列表中, 加性注意力

    Defined in :numref:`sec_attention-scoring-functions`, 加载VOC语义分割数据集

    Defined in :numref:`sec_semantic_segmentation`, 加载WikiText-2数据集

    Defined in :numref:`subsec_prepare_mlm_data`, 加载序列数据的迭代器, 加载香蕉检测数据集

    Defined in :numref:`sec_object-detection-dataset`, 只有zip/tar文件可以被解压缩, 启动计时器, 困惑度 , 在n个变量上累加, 在prefix后面生成新字符

    Defined in :numref:`sec_rnn_scratch`, 在动画中绘制数据, 在序列中屏蔽不相关的项

    Defined in :numref:`sec_seq2seq_decoder`, 在预测期间整理测试集，以方便读取

    Defined in :numref:`sec_kaggle_cifar10`, 均方损失

    Defined in :numref:`sec_linear_scratch`, 基于位置的前馈网络

    Defined in :numref:`sec_transformer`, 多头注意力

    Defined in :numref:`sec_multihead-attention`, 如果存在，则返回gpu(i)，否则返回cpu()

    Defined in :numref:`sec_use_gpu`, 对锚框偏移量的转换

    Defined in :numref:`subsec_labeling-anchor-boxes`, 对预测边界框的置信度进行排序

    Defined in :numref:`subsec_predicting-bounding-boxes-nms`, 将PTB数据集加载到文本行的列表中

    Defined in :numref:`sec_word2vec_data`, 将SNLI数据集解析为前提、假设和标签

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 将VOC标签中的RGB值映射到它们的类别索引

    Defined in :numref:`sec_semantic_segmentation`, 将X和y拆分到多个设备上

    Defined in :numref:`sec_multi_gpu`, 将文件复制到目标目录

    Defined in :numref:`sec_kaggle_cifar10`, 将文本行拆分为单词或字符词元

    Defined in :numref:`sec_text_preprocessing`, 将时间机器数据集加载到文本行的列表中

    Defined in :numref:`sec_text_preprocessing`, 将最接近的真实边界框分配给锚框

    Defined in :numref:`sec_anchor`, 将机器翻译的文本序列转换成小批量

    Defined in :numref:`subsec_mt_data_loading`, 将验证集从原始的训练集中拆分出来

    Defined in :numref:`sec_kaggle_cifar10`, 小批量随机梯度下降

    Defined in :numref:`sec_linear_scratch`, 带有注意力机制解码器的基本接口

    Defined in :numref:`sec_seq2seq_attention`, 带遮蔽的softmax交叉熵损失函数

    Defined in :numref:`sec_seq2seq_decoder`, 序列到序列模型的预测

    Defined in :numref:`sec_seq2seq_training`, 循环神经网络模型

    Defined in :numref:`sec_rnn-concise`, 截断或填充文本序列

    Defined in :numref:`sec_machine_translation`, 文本词表, 显示优化过程中2D变量的轨迹

    Defined in :numref:`subsec_gd-learningrate`, 显示所有边界框

    Defined in :numref:`sec_anchor`, 显示矩阵热图

    Defined in :numref:`sec_attention-cues`, 构建从RGB到VOC类别索引的映射

    Defined in :numref:`sec_semantic_segmentation`, 构造一个PyTorch数据迭代器

    Defined in :numref:`sec_linear_concise`, 根据n个采样权重在{1,...,n}中随机抽取, 根据带有预测偏移量的锚框来预测边界框

    Defined in :numref:`subsec_labeling-anchor-boxes`, 正在从, 残差连接后进行层规范化

    Defined in :numref:`sec_transformer`, 生成y=Xw+b+噪声

    Defined in :numref:`sec_linear_scratch`, 生成以每个像素为中心具有不同形状的锚框

    Defined in :numref:`sec_anchor`, 用GPU训练模型(在第六章定义)

    Defined in :numref:`sec_lenet`, 用于加载SNLI数据集的自定义数据集

    Defined in :numref:`sec_natural-language-inference-and-dataset`, 用于序列到序列学习的循环神经网络编码器

    Defined in :numref:`sec_seq2seq`, 用于测量运行时间, 用多GPU进行小批量训练

    Defined in :numref:`sec_image_augmentation`, 用多GPU进行模型训练

    Defined in :numref:`sec_image_augmentation`, 用定制的训练机优化2D目标函数

    Defined in :numref:`subsec_gd-learningrate`, 稍加修改的ResNet-18模型

    Defined in :numref:`sec_multi_gpu_concise`, 线性回归模型

    Defined in :numref:`sec_linear_scratch`, 绘制列表长度对的直方图

    Defined in :numref:`sec_machine_translation`, 绘制图像列表

    Defined in :numref:`sec_fashion_mnist`, 绘制数据点

    Defined in :numref:`sec_calculus`, 统计词元的频率

    Defined in :numref:`sec_text_preprocessing`, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口

    Defined in :numref:`sec_encoder-decoder`, 编码器-解码器架构的基类

    Defined in :numref:`sec_encoder-decoder`, 缩放点积注意力

    Defined in :numref:`subsec_additive-attention`, 获取输入序列的词元及其片段索引

    Defined in :numref:`sec_bert`, 裁剪梯度

    Defined in :numref:`sec_rnn_scratch`, 计算BLEU

    Defined in :numref:`sec_seq2seq_training`, 计算两个锚框或边界框列表中成对的交并比

    Defined in :numref:`sec_anchor`, 计算二维互相关运算

    Defined in :numref:`sec_conv_layer`, 计算在指定数据集上模型的精度

    Defined in :numref:`sec_softmax_scratch`, 计算预测正确的数量

    Defined in :numref:`sec_softmax_scratch`, 训练序列到序列模型

    Defined in :numref:`sec_seq2seq_decoder`, 训练模型一个迭代周期（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`, 训练模型（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 训练网络一个迭代周期（定义见第8章）

    Defined in :numref:`sec_rnn_scratch`, 记录多次运行时间, 设置matplotlib的图表大小

    Defined in :numref:`sec_calculus`, 设置matplotlib的轴

    Defined in :numref:`sec_calculus`, 评估给定数据集上模型的损失

    Defined in :numref:`sec_model_selection`, 词元化“英语－法语”数据数据集

    Defined in :numref:`sec_machine_translation`, 读取IMDb评论数据集文本序列和标签

    Defined in :numref:`sec_sentiment`, 读取fname来给标签字典返回一个文件名

    Defined in :numref:`sec_kaggle_cifar10`, 读取所有VOC图像并标注

    Defined in :numref:`sec_semantic_segmentation`, 读取香蕉检测数据集中的图像和标签

    Defined in :numref:`sec_object-detection-dataset`, 载入“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 返回Fashion-MNIST数据集的文本标签

    Defined in :numref:`sec_fashion_mnist`, 返回带有负采样的跳元模型的小批量样本

    Defined in :numref:`sec_word2vec_data`, 返回平均时间, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

    Defined in :numref:`sec_use_gpu`, 返回数据迭代器和IMDb评论数据集的词表

    Defined in :numref:`sec_sentiment`, 返回时光机器数据集的词元索引列表和词表

    Defined in :numref:`sec_text_preprocessing`, 返回时光机器数据集的迭代器和词表

    Defined in :numref:`sec_language_model`, 返回时间总和, 返回累计时间, 返回翻译数据集的迭代器和词表

    Defined in :numref:`subsec_mt_data_loading`, 返回负采样中的噪声词

    Defined in :numref:`sec_word2vec_data`, 返回跳元模型中的中心词和上下文词

    Defined in :numref:`sec_word2vec_data`, 逆转transpose_qkv函数的操作

    Defined in :numref:`sec_multihead-attention`, 通过在最后一个轴上掩蔽元素来执行softmax操作

    Defined in :numref:`sec_attention-scoring-functions`, 错误：未知词元类型：, 随机裁剪特征和标签图像

    Defined in :numref:`sec_semantic_segmentation`, 预处理“英语－法语”数据集

    Defined in :numref:`sec_machine_translation`, 预测前提和假设之间的逻辑关系

    Defined in :numref:`sec_natural-language-inference-attention`, 预测文本序列的情感

    Defined in :numref:`sec_sentiment_rnn`, 预测标签（定义见第3章）

    Defined in :numref:`sec_softmax_scratch`","0, 0.0, 0.0001, 0.009999999, 0.01, 0.1, 0.15, 0.22, 0.224, 0.225, 0.229, 0.3, 0.35, 0.406, 0.456, 0.485, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1, 1.0, 1.5, 10, 1000, 10000, 1000000.0, 1048576, 128, 1500, 192, 1e-06, 1e-08, 2, 2.5, 20, 200, 255, 256, 28, 3, 3.0, 3.5, 4, 5, 5.5, 50, 500, 512, 6, 600, 64, 768, 9, False, None, True","#    d2lbook build lib, # (batch_size*num_heads，查询或者“键－值”对的个数，, # (batch_size，)或(batch_size，查询的个数), # (batch_size，查询或者“键－值”对的个数，num_hiddens), # (boxes1的数量,boxes2的数量,2), # 0和1分别标记片段A和B, # 10%的时间：保持词不变, # 10%的时间：用随机词替换该词, # 4个维度：储存训练损失，训练准确度，实例数，特点数, # 80%的时间：将词替换为“<mask>”词元, # Alias defined in config.ini, # Don't edit it directly, # Exclude, # MSELoss计算平方误差时不带系数1/2, # PIL图片, # Readthetrainingset., # The below part is generated automatically through:, # X:3D张量，valid_lens:1D或2D张量, # areas1：(boxes1的数量,),, # areas2：(boxes2的数量,), # boxes1,boxes2,areas1,areas2的形状:, # boxes1：(boxes1的数量,4),, # boxes2：(boxes2的数量,4),, # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量), # inter_upperlefts,inter_lowerrights,inters的形状:, # keys的形状：(batch_size，“键－值”对的个数，d), # nn.GRU以张量作为隐状态, # nn.LSTM以元组作为隐状态, # num_hiddens/num_heads), # output_concat的形状:(batch_size，查询的个数，num_hiddens), # output的形状:(batch_size*num_heads，查询的个数，, # paragraphs是三重列表的嵌套, # pos_threshold是一个用于非背景预测的阈值, # queries的形状：(batch_size，查询的个数，d), # queries，keys，values的形状:, # s1和s2是稍后将使用的内部状态变量, # state对于nn.GRU是个张量, # state对于nn.LSTM或对于我们从零开始实现的模型是个张量, # valid_lens　的形状:, # valid_lens不包括'<pad>'的计数, # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数), # values的形状：(batch_size，“键－值”对的个数，值的维度), # 上下文窗口中间i, # 为了将锚点移动到像素的中心，需要设置偏移量。, # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元, # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax), # 从上下文词中排除中心词, # 从随机偏移量开始划分序列, # 使用小写字母替换大写字母, # 使用空格替换不间断空格, # 假设batch_size=2，num_pred_positions=3, # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数), # 初始化模型, # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5, # 因为已经调用了mean函数, # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, # 图片张量, # 在x轴上缩放步长, # 在y轴上缩放步长, # 在单词和标点符号之间插入空格, # 在第一次迭代或使用随机抽样时初始化state, # 在轴0，将第一项（标量或者矢量）复制num_heads次，, # 填充词元的预测将通过乘以0权重在损失中过滤掉, # 处理矩形输入, # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1, # 如果X有一个轴，输出True, # 它的输出形状是(时间步数*批量大小,词表大小)。, # 将模型设置为评估模式, # 强制教学, # 所以将所有文本行展平到一个列表中, # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次, # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测, # 找到所有的non_keep索引，并将类设置为背景, # 按出现频率排序, # 损失函数的标量进行“反向传播”, # 损失的总和,样本数量, # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0, # 未知词元的索引为0, # 正确预测数、预测总数, # 每个中心点都将有“boxes_per_pixel”个锚框，, # 然后如此复制第二项，然后诸如此类。, # 生成“boxes_per_pixel”个高和宽，, # 生成锚框的所有中心点, # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引, # 经过变换后，输出的queries，keys，values　的形状:, # 缓存k个随机采样结果, # 要形成“中心词-上下文词”对，每个句子至少需要有2个词, # 训练损失之和,词元数量, # 训练损失之和，训练准确率之和，样本数, # 训练损失总和，词元数量, # 训练数据集中样本最少的类别中的样本数, # 设置transpose_b=True为了交换keys的最后两个维度, # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层, # 跳过文件头行(列名), # 那么batch_idx是np.array（[0,0,0,1,1,1]）, # 除以2来获得半高和半宽, # 预测num_preds步, # 预热期, # 验证集中每个类别的样本数, #################   WARNING   ################, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`""""""
def __init__(self, num_inputs, **kwargs):
super(NextSentencePred, self).__init__(**kwargs)
self.output = nn.Linear(num_inputs, 2)

def forward(self, X):
# X的形状：(batchsize,num_hiddens)
return self.output(X)

class BERTModel(nn.Module):
BERT模型, BERT编码器

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
ffn_num_hiddens, num_heads, num_layers, dropout,
max_len=1000, key_size=768, query_size=768, value_size=768,
**kwargs):
super(BERTEncoder, self).__init__(**kwargs)
self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
self.segment_embedding = nn.Embedding(2, num_hiddens)
self.blks = nn.Sequential()
for i in range(num_layers):
self.blks.add_module(f""{i}"", d2l.EncoderBlock(
key_size, query_size, value_size, num_hiddens, norm_shape,
ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
# 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
num_hiddens))

def forward(self, tokens, segments, valid_lens):
# 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
X = self.token_embedding(tokens) + self.segment_embedding(segments)
X = X + self.pos_embedding.data[:, :X.shape[1], :]
for blk in self.blks:
X = blk(X, valid_lens)
return X

class MaskLM(nn.Module):
BERT的掩蔽语言模型任务, Defined in :numref:`sec_bbox`
# 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
# ((左上x,左上y),宽,高)
return d2l.plt.Rectangle(
xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
fill=False, edgecolor=color, linewidth=2)

def multibox_prior(data, sizes, ratios):
生成以每个像素为中心具有不同形状的锚框, Defined in :numref:`sec_bert-dataset`
file_name = os.path.join(data_dir, 'wiki.train.tokens')
with open(file_name, 'r') as f:
lines = f.readlines()
# 大写字母转换为小写字母
paragraphs = [line.strip().lower().split(' . ')
for line in lines if len(line.split(' . ')) >= 2]
random.shuffle(paragraphs)
return paragraphs

def _get_next_sentence(sentence, next_sentence, paragraphs):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-dataset`
nsp_data_from_paragraph = []
for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence(
paragraph[i], paragraph[i + 1], paragraphs)
# 考虑1个'<cls>'词元和2个'<sep>'词元
if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph.append((tokens, segments, is_next))
return nsp_data_from_paragraph

def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`
# 前向传播
_, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
valid_lens_x.reshape(-1),
pred_positions_X)
# 计算遮蔽语言模型损失
mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
mlm_weights_X.reshape(-1, 1)
mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
# 计算下一句子预测任务的损失
nsp_l = loss(nsp_Y_hat, nsp_y)
l = mlm_l + nsp_l
return mlm_l, nsp_l, l

d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
'01ada507287d82875905620988597833ad4e0903')

def read_imdb(data_dir, is_train):
读取IMDb评论数据集文本序列和标签, Defined in :numref:`sec_minibatches`
# 初始化模型
w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),
requires_grad=True)
b = torch.zeros((1), requires_grad=True)
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
# 训练模型
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
l = loss(net(X), y).mean()
l.backward()
trainer_fn([w, b], states, hyperparams)
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
return timer.cumsum(), animator.Y[0]

def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.params = get_params(vocab_size, num_hiddens, device)
self.init_state, self.forward_fn = init_state, forward_fn

def __call__(self, X, state):
X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
return self.forward_fn(X, state, self.params)

def begin_state(self, batch_size, device):
return self.init_state(batch_size, self.num_hiddens, device)

def predict_ch8(prefix, num_preds, net, vocab, device):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
# 增量地绘制多条线
if legend is None:
legend = []
d2l.use_svg_display()
self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
if nrows * ncols == 1:
self.axes = [self.axes, ]
# 使用lambda函数捕获参数
self.config_axes = lambda: d2l.set_axes(
self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self.X, self.Y, self.fmts = None, None, fmts

def add(self, x, y):
# 向图表中添加多个数据点
if not hasattr(y, ""__len__""):
y = [y]
n = len(y)
if not hasattr(x, ""__len__""):
x = [x] * n
if not self.X:
self.X = [[] for _ in range(n)]
if not self.Y:
self.Y = [[] for _ in range(n)]
for i, (a, b) in enumerate(zip(x, y)):
if a is not None and b is not None:
self.X[i].append(a)
self.Y[i].append(b)
self.axes[0].cla()
for x, y, fmt in zip(self.X, self.Y, self.fmts):
self.axes[0].plot(x, y, fmt)
self.config_axes()
display.display(self.fig)
display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
训练模型（定义见第3章）, Defined in :numref:`sec_synonyms`
self.idx_to_token, self.idx_to_vec = self._load_embedding(
embedding_name)
self.unknown_idx = 0
self.token_to_idx = {token: idx for idx, token in
enumerate(self.idx_to_token)}

def _load_embedding(self, embedding_name):
idx_to_token, idx_to_vec = ['<unk>'], []
data_dir = d2l.download_extract(embedding_name)
# GloVe网站：https://nlp.stanford.edu/projects/glove/
# fastText网站：https://fasttext.cc/
with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
for line in f:
elems = line.rstrip().split(' ')
token, elems = elems[0], [float(elem) for elem in elems[1:]]
# 跳过标题信息，例如fastText中的首行
if len(elems) > 1:
idx_to_token.append(token)
idx_to_vec.append(elems)
idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
return idx_to_token, d2l.tensor(idx_to_vec)

def __getitem__(self, tokens):
indices = [self.token_to_idx.get(token, self.unknown_idx)
for token in tokens]
vecs = self.idx_to_vec[d2l.tensor(indices)]
return vecs

def __len__(self):
return len(self.idx_to_token)

def get_tokens_and_segments(tokens_a, tokens_b=None):
获取输入序列的词元及其片段索引, Defined in :numref:`subsec_prepare_mlm_data`
candidate_pred_positions = []
# tokens是一个字符串列表
for i, token in enumerate(tokens):
# 在遮蔽语言模型任务中不会预测特殊词元
if token in ['<cls>', '<sep>']:
continue
candidate_pred_positions.append(i)
# 遮蔽语言模型任务中预测15%的随机词元
num_mlm_preds = max(1, round(len(tokens) * 0.15))
mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0])
pred_positions = [v[0] for v in pred_positions_and_labels]
mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]

def _pad_bert_inputs(examples, max_len, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`subsec_prepare_mlm_data`
def __init__(self, paragraphs, max_len):
# 输入paragraphs[i]是代表段落的句子字符串列表；
# 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表
paragraphs = [d2l.tokenize(
paragraph, token='word') for paragraph in paragraphs]
sentences = [sentence for paragraph in paragraphs
for sentence in paragraph]
self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
'<pad>', '<mask>', '<cls>', '<sep>'])
# 获取下一句子预测任务的数据
examples = []
for paragraph in paragraphs:
examples.extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self.vocab, max_len))
# 获取遮蔽语言模型任务的数据
examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
+ (segments, is_next))
for tokens, segments, is_next in examples]
# 填充输入
(self.all_token_ids, self.all_segments, self.valid_lens,
self.all_pred_positions, self.all_mlm_weights,
self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
examples, max_len, self.vocab)

def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx], self.all_pred_positions[idx],
self.all_mlm_weights[idx], self.all_mlm_labels[idx],
self.nsp_labels[idx])

def __len__(self):
return len(self.all_token_ids)

def load_data_wiki(batch_size, max_len):
加载WikiText-2数据集, Transformer编码器

Defined in :numref:`sec_transformer`""""""
def __init__(self, vocab_size, key_size, query_size, value_size,
num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
num_heads, num_layers, dropout, use_bias=False, **kwargs):
super(TransformerEncoder, self).__init__(**kwargs)
self.num_hiddens = num_hiddens
self.embedding = nn.Embedding(vocab_size, num_hiddens)
self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
self.blks = nn.Sequential()
for i in range(num_layers):
self.blks.add_module(""block""+str(i),
EncoderBlock(key_size, query_size, value_size, num_hiddens,
norm_shape, ffn_num_input, ffn_num_hiddens,
num_heads, dropout, use_bias))

def forward(self, X, valid_lens, *args):
# 因为位置编码值在-1和1之间，
# 因此嵌入值乘以嵌入维度的平方根进行缩放，
# 然后再与位置编码相加。
X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
self.attention_weights = [None] * len(self.blks)
for i, blk in enumerate(self.blks):
X = blk(X, valid_lens)
self.attention_weights[
i] = blk.attention.attention.attention_weights
return X

def annotate(text, xy, xytext):
d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
arrowprops=dict(arrowstyle='->'))

def train_2d(trainer, steps=20, f_grad=None):
用定制的训练机优化2D目标函数, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`""""""
num_workers = d2l.get_dataloader_workers()
sentences = read_ptb()
vocab = d2l.Vocab(sentences, min_freq=10)
subsampled, counter = subsample(sentences, vocab)
corpus = [vocab[line] for line in subsampled]
all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size)
all_negatives = get_negatives(
all_contexts, vocab, counter, num_noise_words)

class PTBDataset(torch.utils.data.Dataset):
def __init__(self, centers, contexts, negatives):
assert len(centers) == len(contexts) == len(negatives)
self.centers = centers
self.contexts = contexts
self.negatives = negatives

def __getitem__(self, index):
return (self.centers[index], self.contexts[index],
self.negatives[index])

def __len__(self):
return len(self.centers)

dataset = PTBDataset(all_centers, all_contexts, all_negatives)

data_iter = torch.utils.data.DataLoader(
dataset, batch_size, shuffle=True,
collate_fn=batchify, num_workers=num_workers)
return data_iter, vocab

d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')

d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')

d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')

class TokenEmbedding:
GloVe嵌入, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_data = read_snli(data_dir, True)
test_data = read_snli(data_dir, False)
train_set = SNLIDataset(train_data, num_steps)
test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
train_iter = torch.utils.data.DataLoader(train_set, batch_size,
shuffle=True,
num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(test_set, batch_size,
shuffle=False,
num_workers=num_workers)
return train_iter, test_iter, train_set.vocab

def predict_snli(net, vocab, premise, hypothesis):
预测前提和假设之间的逻辑关系, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 下采样高频词

Defined in :numref:`sec_word2vec_data`""""""
# 排除未知词元'<unk>'
sentences = [[token for token in line if vocab[token] != vocab.unk]
for line in sentences]
counter = d2l.count_corpus(sentences)
num_tokens = sum(counter.values())

# 如果在下采样期间保留词元，则返回True
def keep(token):
return(random.uniform(0, 1) <
math.sqrt(1e-4 / counter[token] * num_tokens))

return ([[token for token in line if keep(token)] for line in sentences],
counter)

def get_centers_and_contexts(corpus, max_window_size):
返回跳元模型中的中心词和上下文词, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`""""""
# 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
# 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
# num_hiddens/num_heads)
X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

# 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
# num_hiddens/num_heads)
X = X.permute(0, 2, 1, 3)

# 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
# num_hiddens/num_heads)
return X.reshape(-1, X.shape[2], X.shape[3])


def transpose_output(X, num_heads):
逆转transpose_qkv函数的操作, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`""""""
def __init__(self, num_hiddens, dropout, max_len=1000):
super(PositionalEncoding, self).__init__()
self.dropout = nn.Dropout(dropout)
# 创建一个足够长的P
self.P = d2l.zeros((1, max_len, num_hiddens))
X = d2l.arange(max_len, dtype=torch.float32).reshape(
-1, 1) / torch.pow(10000, torch.arange(
0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
self.P[:, :, 0::2] = torch.sin(X)
self.P[:, :, 1::2] = torch.cos(X)

def forward(self, X):
X = X + self.P[:, :X.shape[1], :].to(X.device)
return self.dropout(X)

class PositionWiseFFN(nn.Module):
基于位置的前馈网络, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`""""""
if isinstance(net, nn.Module):
net.eval()  # 设置为评估模式
if not device:
device = next(iter(net.parameters())).device
# 正确预测的数量，总预测的数量
metric = d2l.Accumulator(2)
with torch.no_grad():
for X, y in data_iter:
if isinstance(X, list):
# BERT微调所需的（之后将介绍）
X = [x.to(device) for x in X]
else:
X = X.to(device)
y = y.to(device)
metric.add(d2l.accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
batch_size, anchors = labels.shape[0], anchors.squeeze(0)
batch_offset, batch_mask, batch_class_labels = [], [], []
device, num_anchors = anchors.device, anchors.shape[0]
for i in range(batch_size):
label = labels[i, :, :]
anchors_bbox_map = assign_anchor_to_bbox(
label[:, 1:], anchors, device)
bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
1, 4)
# 将类标签和分配的边界框坐标初始化为零
class_labels = torch.zeros(num_anchors, dtype=torch.long,
device=device)
assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
device=device)
# 使用真实边界框来标记锚框的类别。
# 如果一个锚框没有被分配，标记其为背景（值为零）
indices_true = torch.nonzero(anchors_bbox_map >= 0)
bb_idx = anchors_bbox_map[indices_true]
class_labels[indices_true] = label[bb_idx, 0].long() + 1
assigned_bb[indices_true] = label[bb_idx, 1:]
# 偏移量转换
offset = offset_boxes(anchors, assigned_bb) * bbox_mask
batch_offset.append(offset.reshape(-1))
batch_mask.append(bbox_mask.reshape(-1))
batch_class_labels.append(class_labels)
bbox_offset = torch.stack(batch_offset)
bbox_mask = torch.stack(batch_mask)
class_labels = torch.stack(batch_class_labels)
return (bbox_offset, bbox_mask, class_labels)

def offset_inverse(anchors, offset_preds):
根据带有预测偏移量的锚框来预测边界框, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 加性注意力

Defined in :numref:`sec_attention-scoring-functions`""""""
def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
super(AdditiveAttention, self).__init__(**kwargs)
self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
self.w_v = nn.Linear(num_hiddens, 1, bias=False)
self.dropout = nn.Dropout(dropout)

def forward(self, queries, keys, values, valid_lens):
queries, keys = self.W_q(queries), self.W_k(keys)
# 在维度扩展后，
# queries的形状：(batch_size，查询的个数，1，num_hidden)
# key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
# 使用广播方式进行求和
features = queries.unsqueeze(2) + keys.unsqueeze(1)
features = torch.tanh(features)
# self.w_v仅有一个输出，因此从形状中移除最后那个维度。
# scores的形状：(batch_size，查询的个数，“键-值”对的个数)
scores = self.w_v(features).squeeze(-1)
self.attention_weights = masked_softmax(scores, valid_lens)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
return torch.bmm(self.dropout(self.attention_weights), values)

class DotProductAttention(nn.Module):
缩放点积注意力, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`""""""
voc_dir = d2l.download_extract('voc2012', os.path.join(
'VOCdevkit', 'VOC2012'))
num_workers = d2l.get_dataloader_workers()
train_iter = torch.utils.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size,
shuffle=True, drop_last=True, num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(
VOCSegDataset(False, crop_size, voc_dir), batch_size,
drop_last=True, num_workers=num_workers)
return train_iter, test_iter

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

def read_csv_labels(fname):
读取fname来给标签字典返回一个文件名, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`""""""
train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
batch_size, shuffle=True)
val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
batch_size)
return train_iter, val_iter

d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

def read_voc_images(voc_dir, is_train=True):
读取所有VOC图像并标注, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 在n个变量上累加
def __init__(self, n):
Defined in :numref:`sec_softmax_scratch`, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`""""""
for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile(os.path.join(data_dir, 'test', test_file),
os.path.join(data_dir, 'train_valid_test', 'test',
'unknown'))

d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')

d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')

def read_ptb():
将PTB数据集加载到文本行的列表中, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
B = torch.argsort(scores, dim=-1, descending=True)
keep = []  # 保留预测边界框的指标
while B.numel() > 0:
i = B[0]
keep.append(i)
if B.numel() == 1: break
iou = box_iou(boxes[i, :].reshape(-1, 4),
boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
B = B[inds + 1]
return d2l.tensor(keep, device=boxes.device)

def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
pos_threshold=0.009999999):
使用非极大值抑制来预测边界框, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def extract_text(s):
# 删除我们不会使用的信息
s = re.sub('\\(', '', s)
s = re.sub('\\)', '', s)
# 用一个空格替换两个或多个连续的空格
s = re.sub('\\s{2,}', ' ', s)
return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
if is_train else 'snli_1.0_test.txt')
with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]]
premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
hypotheses = [extract_text(row[2]) for row in rows if row[0] \
in label_set]
labels = [label_set[row[0]] for row in rows if row[0] in label_set]
return premises, hypotheses, labels

class SNLIDataset(torch.utils.data.Dataset):
用于加载SNLI数据集的自定义数据集, 将X和y拆分到多个设备上

Defined in :numref:`sec_multi_gpu`""""""
assert X.shape[0] == y.shape[0]
return (nn.parallel.scatter(X, devices),
nn.parallel.scatter(y, devices))

def resnet18(num_classes, in_channels=1):
稍加修改的ResNet-18模型, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`""""""
os.makedirs(target_dir, exist_ok=True)
shutil.copy(filename, target_dir)

def reorg_train_valid(data_dir, labels, valid_ratio):
将验证集从原始的训练集中拆分出来, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`""""""
num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
# 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU
jaccard = box_iou(anchors, ground_truth)
# 对于每个锚框，分配的真实边界框的张量
anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
device=device)
# 根据阈值，决定是否分配真实边界框
max_ious, indices = torch.max(jaccard, dim=1)
anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)
box_j = indices[max_ious >= iou_threshold]
anchors_bbox_map[anc_i] = box_j
col_discard = torch.full((num_anchors,), -1)
row_discard = torch.full((num_gt_boxes,), -1)
for _ in range(num_gt_boxes):
max_idx = torch.argmax(jaccard)
box_idx = (max_idx % num_gt_boxes).long()
anc_idx = (max_idx / num_gt_boxes).long()
anchors_bbox_map[anc_idx] = box_idx
jaccard[:, box_idx] = col_discard
jaccard[anc_idx, :] = row_discard
return anchors_bbox_map

def offset_boxes(anchors, assigned_bb, eps=1e-6):
对锚框偏移量的转换, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
with torch.no_grad():
for param in params:
param -= lr * param.grad / batch_size
param.grad.zero_()

def load_array(data_arrays, batch_size, is_train=True):
构造一个PyTorch数据迭代器, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`""""""
def __init__(self, **kwargs):
super(AttentionDecoder, self).__init__(**kwargs)

@property
def attention_weights(self):
raise NotImplementedError

class MultiHeadAttention(nn.Module):
多头注意力, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`""""""
# pred的形状：(batch_size,num_steps,vocab_size)
# label的形状：(batch_size,num_steps)
# valid_len的形状：(batch_size,)
def forward(self, pred, label, valid_len):
weights = torch.ones_like(label)
weights = sequence_mask(weights, valid_len)
self.reduction='none'
unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
pred.permute(0, 2, 1), label)
weighted_loss = (unweighted_loss * weights).mean(dim=1)
return weighted_loss

def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
训练序列到序列模型, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`""""""
# 在预测时将net设置为评估模式
net.eval()
src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
src_vocab['<eos>']]
enc_valid_len = torch.tensor([len(src_tokens)], device=device)
src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
# 添加批量轴
enc_X = torch.unsqueeze(
torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
enc_outputs = net.encoder(enc_X, enc_valid_len)
dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
# 添加批量轴
dec_X = torch.unsqueeze(torch.tensor(
[tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
output_seq, attention_weight_seq = [], []
for _ in range(num_steps):
Y, dec_state = net.decoder(dec_X, dec_state)
# 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
dec_X = Y.argmax(dim=2)
pred = dec_X.squeeze(dim=0).type(torch.int32).item()
# 保存注意力权重（稍后讨论）
if save_attention_weights:
attention_weight_seq.append(net.decoder.attention_weights)
# 一旦序列结束词元被预测，输出序列的生成就完成了
if pred == tgt_vocab['<eos>']:
break
output_seq.append(pred)
return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq

def bleu(pred_seq, label_seq, k):
计算BLEU, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`""""""
d2l.set_figsize()
d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
d2l.arange(-3.0, 1.0, 0.1), indexing='ij')
d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
d2l.plt.xlabel('x1')
d2l.plt.ylabel('x2')

d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')

def get_data_ch11(batch_size=10, n=1500):
Defined in :numref:`sec_minibatches`, 显示所有边界框

Defined in :numref:`sec_anchor`""""""
def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj

labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))

def box_iou(boxes1, boxes2):
计算两个锚框或边界框列表中成对的交并比, 显示矩阵热图

Defined in :numref:`sec_attention-cues`""""""
d2l.use_svg_display()
num_rows, num_cols = matrices.shape[0], matrices.shape[1]
fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
sharex=True, sharey=True, squeeze=False)
for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)
if i == num_rows - 1:
ax.set_xlabel(xlabel)
if j == 0:
ax.set_ylabel(ylabel)
if titles:
ax.set_title(titles[j])
fig.colorbar(pcm, ax=axes, shrink=0.6);

def masked_softmax(X, valid_lens):
通过在最后一个轴上掩蔽元素来执行softmax操作, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 根据n个采样权重在{1,...,n}中随机抽取
def __init__(self, sampling_weights):
Defined in :numref:`sec_word2vec_data`, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`""""""
def __init__(self, normalized_shape, dropout, **kwargs):
super(AddNorm, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)
self.ln = nn.LayerNorm(normalized_shape)

def forward(self, X, Y):
return self.ln(self.dropout(Y) + X)

class EncoderBlock(nn.Module):
Transformer编码器块, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqEncoder, self).__init__(**kwargs)
# 嵌入层
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
dropout=dropout)

def forward(self, X, *args):
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
# 在循环神经网络模型中，第一个轴对应于时间步
X = X.permute(1, 0, 2)
# 如果未提及状态，则默认为0
output, state = self.rnn(X)
# output的形状:(num_steps,batch_size,num_hiddens)
# state的形状:(num_layers,batch_size,num_hiddens)
return output, state

def sequence_mask(X, valid_len, value=0):
在序列中屏蔽不相关的项, 用于测量运行时间
def __init__(self, description='Done'):
Defined in :numref:`sec_hybridize`, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`""""""
if isinstance(X, list):
# 微调BERT中所需
X = [x.to(devices[0]) for x in X]
else:
X = X.to(devices[0])
y = y.to(devices[0])
net.train()
trainer.zero_grad()
pred = net(X)
l = loss(pred, y)
l.sum().backward()
trainer.step()
train_loss_sum = l.sum()
train_acc_sum = d2l.accuracy(pred, y)
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus()):
用多GPU进行模型训练, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def forward(self, X, state):
raise NotImplementedError

class EncoderDecoder(nn.Module):
编码器-解码器架构的基类, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
if isinstance(net, nn.Module):
params = [p for p in net.parameters() if p.requires_grad]
else:
params = net.params
norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
if norm > theta:
for param in params:
param.grad[:] *= theta / norm

def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
训练网络一个迭代周期（定义见第8章）, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def evaluate_accuracy(net, data_iter):
计算在指定数据集上模型的精度, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
# 将模型设置为训练模式
if isinstance(net, torch.nn.Module):
net.train()
# 训练损失总和、训练准确度总和、样本数
metric = Accumulator(3)
for X, y in train_iter:
# 计算梯度并更新参数
y_hat = net(X)
l = loss(y_hat, y)
if isinstance(updater, torch.optim.Optimizer):
# 使用PyTorch内置的优化器和损失函数
updater.zero_grad()
l.mean().backward()
updater.step()
else:
# 使用定制的优化器和损失函数
l.sum().backward()
updater(X.shape[0])
metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
# 返回训练损失和训练精度
return metric[0] / metric[2], metric[1] / metric[2]

class Animator:
在动画中绘制数据, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
loss = nn.CrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
# 初始化
if isinstance(net, nn.Module):
updater = torch.optim.SGD(net.parameters(), lr)
else:
updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(
net, train_iter, loss, updater, device, use_random_iter)
if (epoch + 1) % 10 == 0:
print(predict('time traveller'))
animator.add(epoch + 1, [ppl])
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(nn.Module):
循环神经网络模型, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`""""""
data_dir = d2l.download_extract('banana-detection')
csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
else 'bananas_val', 'label.csv')
csv_data = pd.read_csv(csv_fname)
csv_data = csv_data.set_index('img_name')
images, targets = [], []
for img_name, target in csv_data.iterrows():
images.append(torchvision.io.read_image(
os.path.join(data_dir, 'bananas_train' if is_train else
'bananas_val', 'images', f'{img_name}')))
# 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
# 其中所有图像都具有相同的香蕉类（索引为0）
targets.append(list(target))
return images, torch.tensor(targets).unsqueeze(1) / 256

class BananasDataset(torch.utils.data.Dataset):
一个用于加载香蕉检测数据集的自定义数据集, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`""""""
devices = [torch.device(f'cuda:{i}')
for i in range(torch.cuda.device_count())]
return devices if devices else [torch.device('cpu')]

def corr2d(X, K):
计算二维互相关运算, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`""""""
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
train_data = read_imdb(data_dir, True)
test_data = read_imdb(data_dir, False)
train_tokens = d2l.tokenize(train_data[0], token='word')
test_tokens = d2l.tokenize(test_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5)
train_features = torch.tensor([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
test_features = torch.tensor([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])
train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),
batch_size)
test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),
batch_size,
is_train=False)
return train_iter, test_iter, vocab

def predict_sentiment(net, vocab, sequence):
预测文本序列的情感, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(nn.Module):
编码器-解码器架构的基本编码器接口, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`""""""
# 索引为1、2、...（索引0是词表中排除的未知标记）
sampling_weights = [counter[vocab.to_tokens(i)]**0.75
for i in range(1, len(vocab))]
all_negatives, generator = [], RandomGenerator(sampling_weights)
for contexts in all_contexts:
negatives = []
while len(negatives) < len(contexts) * K:
neg = generator.draw()
# 噪声词不能是上下文词
if neg not in contexts:
negatives.append(neg)
all_negatives.append(negatives)
return all_negatives

def batchify(data):
返回带有负采样的跳元模型的小批量样本, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
rect = torchvision.transforms.RandomCrop.get_params(
feature, (height, width))
feature = torchvision.transforms.functional.crop(feature, *rect)
label = torchvision.transforms.functional.crop(label, *rect)
return feature, label

class VOCSegDataset(torch.utils.data.Dataset):
一个用于加载VOC数据集的自定义数据集, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失","(batch_size*num_heads，查询或者“键－值”对的个数，, (batch_size，)或(batch_size，查询的个数), (batch_size，查询或者“键－值”对的个数，num_hiddens), (boxes1的数量,boxes2的数量,2), 0和1分别标记片段A和B, 10%的时间：保持词不变, 10%的时间：用随机词替换该词, 4个维度：储存训练损失，训练准确度，实例数，特点数, 80%的时间：将词替换为“<mask>”词元, BERT模型, BERT的下一句预测任务, BERT的下一句预测任务

Defined in :numref:`subsec_mlm`""""""
def __init__(self, num_inputs, **kwargs):
super(NextSentencePred, self).__init__(**kwargs)
self.output = nn.Linear(num_inputs, 2)

def forward(self, X):
# X的形状：(batchsize,num_hiddens)
return self.output(X)

class BERTModel(nn.Module):
BERT模型, BERT的掩蔽语言模型任务, BERT编码器, BERT编码器

Defined in :numref:`subsec_bert_input_rep`""""""
def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
ffn_num_hiddens, num_heads, num_layers, dropout,
max_len=1000, key_size=768, query_size=768, value_size=768,
**kwargs):
super(BERTEncoder, self).__init__(**kwargs)
self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
self.segment_embedding = nn.Embedding(2, num_hiddens)
self.blks = nn.Sequential()
for i in range(num_layers):
self.blks.add_module(f""{i}"", d2l.EncoderBlock(
key_size, query_size, value_size, num_hiddens, norm_shape,
ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
# 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
num_hiddens))

def forward(self, tokens, segments, valid_lens):
# 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
X = self.token_embedding(tokens) + self.segment_embedding(segments)
X = X + self.pos_embedding.data[:, :X.shape[1], :]
for blk in self.blks:
X = blk(X, valid_lens)
return X

class MaskLM(nn.Module):
BERT的掩蔽语言模型任务, Defined in :numref:`sec_bbox`
# 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
# ((左上x,左上y),宽,高)
return d2l.plt.Rectangle(
xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
fill=False, edgecolor=color, linewidth=2)

def multibox_prior(data, sizes, ratios):
生成以每个像素为中心具有不同形状的锚框, Defined in :numref:`sec_bert-dataset`
file_name = os.path.join(data_dir, 'wiki.train.tokens')
with open(file_name, 'r') as f:
lines = f.readlines()
# 大写字母转换为小写字母
paragraphs = [line.strip().lower().split(' . ')
for line in lines if len(line.split(' . ')) >= 2]
random.shuffle(paragraphs)
return paragraphs

def _get_next_sentence(sentence, next_sentence, paragraphs):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-dataset`
nsp_data_from_paragraph = []
for i in range(len(paragraph) - 1):
tokens_a, tokens_b, is_next = _get_next_sentence(
paragraph[i], paragraph[i + 1], paragraphs)
# 考虑1个'<cls>'词元和2个'<sep>'词元
if len(tokens_a) + len(tokens_b) + 3 > max_len:
continue
tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
nsp_data_from_paragraph.append((tokens, segments, is_next))
return nsp_data_from_paragraph

def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
vocab):
Defined in :numref:`sec_bert-dataset`, Defined in :numref:`sec_bert-pretraining`
# 前向传播
_, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,
valid_lens_x.reshape(-1),
pred_positions_X)
# 计算遮蔽语言模型损失
mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\
mlm_weights_X.reshape(-1, 1)
mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)
# 计算下一句子预测任务的损失
nsp_l = loss(nsp_Y_hat, nsp_y)
l = mlm_l + nsp_l
return mlm_l, nsp_l, l

d2l.DATA_HUB['aclImdb'] = (
'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
'01ada507287d82875905620988597833ad4e0903')

def read_imdb(data_dir, is_train):
读取IMDb评论数据集文本序列和标签, Defined in :numref:`sec_minibatches`
# 初始化模型
w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),
requires_grad=True)
b = torch.zeros((1), requires_grad=True)
net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
# 训练模型
animator = d2l.Animator(xlabel='epoch', ylabel='loss',
xlim=[0, num_epochs], ylim=[0.22, 0.35])
n, timer = 0, d2l.Timer()
for _ in range(num_epochs):
for X, y in data_iter:
l = loss(net(X), y).mean()
l.backward()
trainer_fn([w, b], states, hyperparams)
n += X.shape[0]
if n % 200 == 0:
timer.stop()
animator.add(n/X.shape[0]/len(data_iter),
(d2l.evaluate_loss(net, data_iter, loss),))
timer.start()
print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
return timer.cumsum(), animator.Y[0]

def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):
Defined in :numref:`sec_minibatches`, Defined in :numref:`sec_rnn_scratch`
self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
self.params = get_params(vocab_size, num_hiddens, device)
self.init_state, self.forward_fn = init_state, forward_fn

def __call__(self, X, state):
X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
return self.forward_fn(X, state, self.params)

def begin_state(self, batch_size, device):
return self.init_state(batch_size, self.num_hiddens, device)

def predict_ch8(prefix, num_preds, net, vocab, device):
在prefix后面生成新字符, Defined in :numref:`sec_softmax_scratch`
# 增量地绘制多条线
if legend is None:
legend = []
d2l.use_svg_display()
self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
if nrows * ncols == 1:
self.axes = [self.axes, ]
# 使用lambda函数捕获参数
self.config_axes = lambda: d2l.set_axes(
self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
self.X, self.Y, self.fmts = None, None, fmts

def add(self, x, y):
# 向图表中添加多个数据点
if not hasattr(y, ""__len__""):
y = [y]
n = len(y)
if not hasattr(x, ""__len__""):
x = [x] * n
if not self.X:
self.X = [[] for _ in range(n)]
if not self.Y:
self.Y = [[] for _ in range(n)]
for i, (a, b) in enumerate(zip(x, y)):
if a is not None and b is not None:
self.X[i].append(a)
self.Y[i].append(b)
self.axes[0].cla()
for x, y, fmt in zip(self.X, self.Y, self.fmts):
self.axes[0].plot(x, y, fmt)
self.config_axes()
display.display(self.fig)
display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
训练模型（定义见第3章）, Defined in :numref:`sec_synonyms`
self.idx_to_token, self.idx_to_vec = self._load_embedding(
embedding_name)
self.unknown_idx = 0
self.token_to_idx = {token: idx for idx, token in
enumerate(self.idx_to_token)}

def _load_embedding(self, embedding_name):
idx_to_token, idx_to_vec = ['<unk>'], []
data_dir = d2l.download_extract(embedding_name)
# GloVe网站：https://nlp.stanford.edu/projects/glove/
# fastText网站：https://fasttext.cc/
with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:
for line in f:
elems = line.rstrip().split(' ')
token, elems = elems[0], [float(elem) for elem in elems[1:]]
# 跳过标题信息，例如fastText中的首行
if len(elems) > 1:
idx_to_token.append(token)
idx_to_vec.append(elems)
idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec
return idx_to_token, d2l.tensor(idx_to_vec)

def __getitem__(self, tokens):
indices = [self.token_to_idx.get(token, self.unknown_idx)
for token in tokens]
vecs = self.idx_to_vec[d2l.tensor(indices)]
return vecs

def __len__(self):
return len(self.idx_to_token)

def get_tokens_and_segments(tokens_a, tokens_b=None):
获取输入序列的词元及其片段索引, Defined in :numref:`subsec_prepare_mlm_data`
candidate_pred_positions = []
# tokens是一个字符串列表
for i, token in enumerate(tokens):
# 在遮蔽语言模型任务中不会预测特殊词元
if token in ['<cls>', '<sep>']:
continue
candidate_pred_positions.append(i)
# 遮蔽语言模型任务中预测15%的随机词元
num_mlm_preds = max(1, round(len(tokens) * 0.15))
mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
tokens, candidate_pred_positions, num_mlm_preds, vocab)
pred_positions_and_labels = sorted(pred_positions_and_labels,
key=lambda x: x[0])
pred_positions = [v[0] for v in pred_positions_and_labels]
mlm_pred_labels = [v[1] for v in pred_positions_and_labels]
return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]

def _pad_bert_inputs(examples, max_len, vocab):
Defined in :numref:`subsec_prepare_mlm_data`, Defined in :numref:`subsec_prepare_mlm_data`
def __init__(self, paragraphs, max_len):
# 输入paragraphs[i]是代表段落的句子字符串列表；
# 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表
paragraphs = [d2l.tokenize(
paragraph, token='word') for paragraph in paragraphs]
sentences = [sentence for paragraph in paragraphs
for sentence in paragraph]
self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
'<pad>', '<mask>', '<cls>', '<sep>'])
# 获取下一句子预测任务的数据
examples = []
for paragraph in paragraphs:
examples.extend(_get_nsp_data_from_paragraph(
paragraph, paragraphs, self.vocab, max_len))
# 获取遮蔽语言模型任务的数据
examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
+ (segments, is_next))
for tokens, segments, is_next in examples]
# 填充输入
(self.all_token_ids, self.all_segments, self.valid_lens,
self.all_pred_positions, self.all_mlm_weights,
self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
examples, max_len, self.vocab)

def __getitem__(self, idx):
return (self.all_token_ids[idx], self.all_segments[idx],
self.valid_lens[idx], self.all_pred_positions[idx],
self.all_mlm_weights[idx], self.all_mlm_labels[idx],
self.nsp_labels[idx])

def __len__(self):
return len(self.all_token_ids)

def load_data_wiki(batch_size, max_len):
加载WikiText-2数据集, GloVe嵌入, MSELoss计算平方误差时不带系数1/2, PIL图片, Transformer编码器, Transformer编码器

Defined in :numref:`sec_transformer`""""""
def __init__(self, vocab_size, key_size, query_size, value_size,
num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
num_heads, num_layers, dropout, use_bias=False, **kwargs):
super(TransformerEncoder, self).__init__(**kwargs)
self.num_hiddens = num_hiddens
self.embedding = nn.Embedding(vocab_size, num_hiddens)
self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
self.blks = nn.Sequential()
for i in range(num_layers):
self.blks.add_module(""block""+str(i),
EncoderBlock(key_size, query_size, value_size, num_hiddens,
norm_shape, ffn_num_input, ffn_num_hiddens,
num_heads, dropout, use_bias))

def forward(self, X, valid_lens, *args):
# 因为位置编码值在-1和1之间，
# 因此嵌入值乘以嵌入维度的平方根进行缩放，
# 然后再与位置编码相加。
X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
self.attention_weights = [None] * len(self.blks)
for i, blk in enumerate(self.blks):
X = blk(X, valid_lens)
self.attention_weights[
i] = blk.attention.attention.attention_weights
return X

def annotate(text, xy, xytext):
d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
arrowprops=dict(arrowstyle='->'))

def train_2d(trainer, steps=20, f_grad=None):
用定制的训练机优化2D目标函数, Transformer编码器块, X:3D张量，valid_lens:1D或2D张量, areas1：(boxes1的数量,),, areas2：(boxes2的数量,), boxes1,boxes2,areas1,areas2的形状:, boxes1：(boxes1的数量,4),, boxes2：(boxes2的数量,4),, inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量), inter_upperlefts,inter_lowerrights,inters的形状:, keys的形状：(batch_size，“键－值”对的个数，d), nn.GRU以张量作为隐状态, nn.LSTM以元组作为隐状态, output_concat的形状:(batch_size，查询的个数，num_hiddens), output的形状:(batch_size*num_heads，查询的个数，, paragraphs是三重列表的嵌套, pos_threshold是一个用于非背景预测的阈值, queries的形状：(batch_size，查询的个数，d), queries，keys，values的形状:, s1和s2是稍后将使用的内部状态变量, state对于nn.GRU是个张量, state对于nn.LSTM或对于我们从零开始实现的模型是个张量, valid_lens　的形状:, valid_lens不包括'<pad>'的计数, valid_lens的形状:(batch_size，)或者(batch_size，查询的个数), values的形状：(batch_size，“键－值”对的个数，值的维度), 一个用于加载VOC数据集的自定义数据集, 一个用于加载香蕉检测数据集的自定义数据集, 上下文窗口中间i, 下载, 下载DATA_HUB中的所有文件, 下载DATA_HUB中的所有文件

Defined in :numref:`sec_kaggle_house`""""""
for name in DATA_HUB:
download(name)

DATA_HUB['kaggle_house_train'] = (
DATA_URL + 'kaggle_house_pred_train.csv',
'585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
DATA_URL + 'kaggle_house_pred_test.csv',
'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
如果存在，则返回gpu(i)，否则返回cpu(), 下载Fashion-MNIST数据集，然后将其加载到内存中, 下载PTB数据集，然后将其加载到内存中, 下载PTB数据集，然后将其加载到内存中

Defined in :numref:`subsec_word2vec-minibatch-loading`""""""
num_workers = d2l.get_dataloader_workers()
sentences = read_ptb()
vocab = d2l.Vocab(sentences, min_freq=10)
subsampled, counter = subsample(sentences, vocab)
corpus = [vocab[line] for line in subsampled]
all_centers, all_contexts = get_centers_and_contexts(
corpus, max_window_size)
all_negatives = get_negatives(
all_contexts, vocab, counter, num_noise_words)

class PTBDataset(torch.utils.data.Dataset):
def __init__(self, centers, contexts, negatives):
assert len(centers) == len(contexts) == len(negatives)
self.centers = centers
self.contexts = contexts
self.negatives = negatives

def __getitem__(self, index):
return (self.centers[index], self.contexts[index],
self.negatives[index])

def __len__(self):
return len(self.centers)

dataset = PTBDataset(all_centers, all_contexts, all_negatives)

data_iter = torch.utils.data.DataLoader(
dataset, batch_size, shuffle=True,
collate_fn=batchify, num_workers=num_workers)
return data_iter, vocab

d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
'0b8703943ccdb6eb788e6f091b8946e82231bc4d')

d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
'b5116e234e9eb9076672cfeabf5469f3eec904fa')

d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
'c1816da3821ae9f43899be655002f6c723e91b88')

class TokenEmbedding:
GloVe嵌入, 下载SNLI数据集并返回数据迭代器和词表, 下载SNLI数据集并返回数据迭代器和词表

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
num_workers = d2l.get_dataloader_workers()
data_dir = d2l.download_extract('SNLI')
train_data = read_snli(data_dir, True)
test_data = read_snli(data_dir, False)
train_set = SNLIDataset(train_data, num_steps)
test_set = SNLIDataset(test_data, num_steps, train_set.vocab)
train_iter = torch.utils.data.DataLoader(train_set, batch_size,
shuffle=True,
num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(test_set, batch_size,
shuffle=False,
num_workers=num_workers)
return train_iter, test_iter, train_set.vocab

def predict_snli(net, vocab, premise, hypothesis):
预测前提和假设之间的逻辑关系, 下载一个DATA_HUB中的文件，返回本地文件名, 下载一个DATA_HUB中的文件，返回本地文件名

Defined in :numref:`sec_kaggle_house`""""""
assert name in DATA_HUB, f""{name} 不存在于 {DATA_HUB}""
url, sha1_hash = DATA_HUB[name]
os.makedirs(cache_dir, exist_ok=True)
fname = os.path.join(cache_dir, url.split('/')[-1])
if os.path.exists(fname):
sha1 = hashlib.sha1()
with open(fname, 'rb') as f:
while True:
data = f.read(1048576)
if not data:
break
sha1.update(data)
if sha1.hexdigest() == sha1_hash:
return fname  # 命中缓存
print(f'正在从{url}下载{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
f.write(r.content)
return fname

def download_extract(name, folder=None):
下载并解压zip/tar文件, 下载并解压zip/tar文件, 下采样高频词, 下采样高频词

Defined in :numref:`sec_word2vec_data`""""""
# 排除未知词元'<unk>'
sentences = [[token for token in line if vocab[token] != vocab.unk]
for line in sentences]
counter = d2l.count_corpus(sentences)
num_tokens = sum(counter.values())

# 如果在下采样期间保留词元，则返回True
def keep(token):
return(random.uniform(0, 1) <
math.sqrt(1e-4 / counter[token] * num_tokens))

return ([[token for token in line if keep(token)] for line in sentences],
counter)

def get_centers_and_contexts(corpus, max_window_size):
返回跳元模型中的中心词和上下文词, 不存在于, 中随机抽取, 为了多注意力头的并行计算而变换形状, 为了多注意力头的并行计算而变换形状

Defined in :numref:`sec_multihead-attention`""""""
# 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
# 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
# num_hiddens/num_heads)
X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

# 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
# num_hiddens/num_heads)
X = X.permute(0, 2, 1, 3)

# 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
# num_hiddens/num_heads)
return X.reshape(-1, X.shape[2], X.shape[3])


def transpose_output(X, num_heads):
逆转transpose_qkv函数的操作, 为了将锚点移动到像素的中心，需要设置偏移量。, 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元, 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax), 从上下文词中排除中心词, 从随机偏移量开始划分序列, 从零开始实现的循环神经网络模型, 从（中间，宽度，高度）转换到（左上，右下）, 从（左上，右下）转换到（中间，宽度，高度）, 从（左上，右下）转换到（中间，宽度，高度）

Defined in :numref:`sec_bbox`""""""
x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1
boxes = d2l.stack((cx, cy, w, h), axis=-1)
return boxes

def box_center_to_corner(boxes):
从（中间，宽度，高度）转换到（左上，右下）, 位置编码, 位置编码

Defined in :numref:`sec_self-attention-and-positional-encoding`""""""
def __init__(self, num_hiddens, dropout, max_len=1000):
super(PositionalEncoding, self).__init__()
self.dropout = nn.Dropout(dropout)
# 创建一个足够长的P
self.P = d2l.zeros((1, max_len, num_hiddens))
X = d2l.arange(max_len, dtype=torch.float32).reshape(
-1, 1) / torch.pow(10000, torch.arange(
0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
self.P[:, :, 0::2] = torch.sin(X)
self.P[:, :, 1::2] = torch.cos(X)

def forward(self, X):
X = X + self.P[:, :X.shape[1], :].to(X.device)
return self.dropout(X)

class PositionWiseFFN(nn.Module):
基于位置的前馈网络, 使用4个进程来读取数据, 使用4个进程来读取数据

Defined in :numref:`sec_fashion_mnist`""""""
return 4

def load_data_fashion_mnist(batch_size, resize=None):
下载Fashion-MNIST数据集，然后将其加载到内存中, 使用GPU计算模型在数据集上的精度, 使用GPU计算模型在数据集上的精度

Defined in :numref:`sec_lenet`""""""
if isinstance(net, nn.Module):
net.eval()  # 设置为评估模式
if not device:
device = next(iter(net.parameters())).device
# 正确预测的数量，总预测的数量
metric = d2l.Accumulator(2)
with torch.no_grad():
for X, y in data_iter:
if isinstance(X, list):
# BERT微调所需的（之后将介绍）
X = [x.to(device) for x in X]
else:
X = X.to(device)
y = y.to(device)
metric.add(d2l.accuracy(net(X), y), d2l.size(y))
return metric[0] / metric[1]

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
用GPU训练模型(在第六章定义), 使用svg格式在Jupyter中显示绘图, 使用svg格式在Jupyter中显示绘图

Defined in :numref:`sec_calculus`""""""
backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
设置matplotlib的图表大小, 使用小写字母替换大写字母, 使用真实边界框标记锚框, 使用真实边界框标记锚框

Defined in :numref:`subsec_labeling-anchor-boxes`""""""
batch_size, anchors = labels.shape[0], anchors.squeeze(0)
batch_offset, batch_mask, batch_class_labels = [], [], []
device, num_anchors = anchors.device, anchors.shape[0]
for i in range(batch_size):
label = labels[i, :, :]
anchors_bbox_map = assign_anchor_to_bbox(
label[:, 1:], anchors, device)
bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(
1, 4)
# 将类标签和分配的边界框坐标初始化为零
class_labels = torch.zeros(num_anchors, dtype=torch.long,
device=device)
assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,
device=device)
# 使用真实边界框来标记锚框的类别。
# 如果一个锚框没有被分配，标记其为背景（值为零）
indices_true = torch.nonzero(anchors_bbox_map >= 0)
bb_idx = anchors_bbox_map[indices_true]
class_labels[indices_true] = label[bb_idx, 0].long() + 1
assigned_bb[indices_true] = label[bb_idx, 1:]
# 偏移量转换
offset = offset_boxes(anchors, assigned_bb) * bbox_mask
batch_offset.append(offset.reshape(-1))
batch_mask.append(bbox_mask.reshape(-1))
batch_class_labels.append(class_labels)
bbox_offset = torch.stack(batch_offset)
bbox_mask = torch.stack(batch_mask)
class_labels = torch.stack(batch_class_labels)
return (bbox_offset, bbox_mask, class_labels)

def offset_inverse(anchors, offset_preds):
根据带有预测偏移量的锚框来预测边界框, 使用空格替换不间断空格, 使用随机抽样生成一个小批量子序列, 使用随机抽样生成一个小批量子序列

Defined in :numref:`sec_language_model`""""""
# 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
corpus = corpus[random.randint(0, num_steps - 1):]
# 减去1，是因为我们需要考虑标签
num_subseqs = (len(corpus) - 1) // num_steps
# 长度为num_steps的子序列的起始索引
initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
# 在随机抽样的迭代过程中，
# 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
random.shuffle(initial_indices)

def data(pos):
# 返回从pos位置开始的长度为num_steps的序列
return corpus[pos: pos + num_steps]

num_batches = num_subseqs // batch_size
for i in range(0, batch_size * num_batches, batch_size):
# 在这里，initial_indices包含子序列的随机起始索引
initial_indices_per_batch = initial_indices[i: i + batch_size]
X = [data(j) for j in initial_indices_per_batch]
Y = [data(j + 1) for j in initial_indices_per_batch]
yield d2l.tensor(X), d2l.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
使用顺序分区生成一个小批量子序列, 使用非极大值抑制来预测边界框, 使用顺序分区生成一个小批量子序列, 假设batch_size=2，num_pred_positions=3, 停止计时器并将时间记录在列表中, 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数), 初始化模型, 加性注意力, 加性注意力

Defined in :numref:`sec_attention-scoring-functions`""""""
def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
super(AdditiveAttention, self).__init__(**kwargs)
self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
self.w_v = nn.Linear(num_hiddens, 1, bias=False)
self.dropout = nn.Dropout(dropout)

def forward(self, queries, keys, values, valid_lens):
queries, keys = self.W_q(queries), self.W_k(keys)
# 在维度扩展后，
# queries的形状：(batch_size，查询的个数，1，num_hidden)
# key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
# 使用广播方式进行求和
features = queries.unsqueeze(2) + keys.unsqueeze(1)
features = torch.tanh(features)
# self.w_v仅有一个输出，因此从形状中移除最后那个维度。
# scores的形状：(batch_size，查询的个数，“键-值”对的个数)
scores = self.w_v(features).squeeze(-1)
self.attention_weights = masked_softmax(scores, valid_lens)
# values的形状：(batch_size，“键－值”对的个数，值的维度)
return torch.bmm(self.dropout(self.attention_weights), values)

class DotProductAttention(nn.Module):
缩放点积注意力, 加载VOC语义分割数据集, 加载VOC语义分割数据集

Defined in :numref:`sec_semantic_segmentation`""""""
voc_dir = d2l.download_extract('voc2012', os.path.join(
'VOCdevkit', 'VOC2012'))
num_workers = d2l.get_dataloader_workers()
train_iter = torch.utils.data.DataLoader(
VOCSegDataset(True, crop_size, voc_dir), batch_size,
shuffle=True, drop_last=True, num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(
VOCSegDataset(False, crop_size, voc_dir), batch_size,
drop_last=True, num_workers=num_workers)
return train_iter, test_iter

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

def read_csv_labels(fname):
读取fname来给标签字典返回一个文件名, 加载WikiText-2数据集, 加载序列数据的迭代器, 加载序列数据的迭代器
def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
Defined in :numref:`sec_language_model`, 加载香蕉检测数据集, 加载香蕉检测数据集

Defined in :numref:`sec_object-detection-dataset`""""""
train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
batch_size, shuffle=True)
val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
batch_size)
return train_iter, val_iter

d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

def read_voc_images(voc_dir, is_train=True):
读取所有VOC图像并标注, 只有zip/tar文件可以被解压缩, 启动计时器, 启动计时器
self.tik = time.time()

def stop(self):
停止计时器并将时间记录在列表中, 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5, 因为已经调用了mean函数, 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，, 困惑度, 图片张量, 在n个变量上累加, 在n个变量上累加
def __init__(self, n):
Defined in :numref:`sec_softmax_scratch`, 在prefix后面生成新字符, 在x轴上缩放步长, 在y轴上缩放步长, 在动画中绘制数据, 在单词和标点符号之间插入空格, 在序列中屏蔽不相关的项, 在第一次迭代或使用随机抽样时初始化state, 在第六章定义, 在轴0，将第一项（标量或者矢量）复制num_heads次，, 在预测期间整理测试集，以方便读取, 在预测期间整理测试集，以方便读取

Defined in :numref:`sec_kaggle_cifar10`""""""
for test_file in os.listdir(os.path.join(data_dir, 'test')):
copyfile(os.path.join(data_dir, 'test', test_file),
os.path.join(data_dir, 'train_valid_test', 'test',
'unknown'))

d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',
'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')

d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
'319d85e578af0cdc590547f26231e4e31cdf1e42')

def read_ptb():
将PTB数据集加载到文本行的列表中, 均方损失, 基于位置的前馈网络, 填充词元的预测将通过乘以0权重在损失中过滤掉, 处理矩形输入, 多头注意力, 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1, 如果X有一个轴，输出True, 如果存在，则返回gpu, 它的输出形状是(时间步数*批量大小,词表大小)。, 对锚框偏移量的转换, 对预测边界框的置信度进行排序, 对预测边界框的置信度进行排序

Defined in :numref:`subsec_predicting-bounding-boxes-nms`""""""
B = torch.argsort(scores, dim=-1, descending=True)
keep = []  # 保留预测边界框的指标
while B.numel() > 0:
i = B[0]
keep.append(i)
if B.numel() == 1: break
iou = box_iou(boxes[i, :].reshape(-1, 4),
boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
B = B[inds + 1]
return d2l.tensor(keep, device=boxes.device)

def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
pos_threshold=0.009999999):
使用非极大值抑制来预测边界框, 将PTB数据集加载到文本行的列表中, 将SNLI数据集解析为前提、假设和标签, 将SNLI数据集解析为前提、假设和标签

Defined in :numref:`sec_natural-language-inference-and-dataset`""""""
def extract_text(s):
# 删除我们不会使用的信息
s = re.sub('\\(', '', s)
s = re.sub('\\)', '', s)
# 用一个空格替换两个或多个连续的空格
s = re.sub('\\s{2,}', ' ', s)
return s.strip()
label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}
file_name = os.path.join(data_dir, 'snli_1.0_train.txt'
if is_train else 'snli_1.0_test.txt')
with open(file_name, 'r') as f:
rows = [row.split('\t') for row in f.readlines()[1:]]
premises = [extract_text(row[1]) for row in rows if row[0] in label_set]
hypotheses = [extract_text(row[2]) for row in rows if row[0] \
in label_set]
labels = [label_set[row[0]] for row in rows if row[0] in label_set]
return premises, hypotheses, labels

class SNLIDataset(torch.utils.data.Dataset):
用于加载SNLI数据集的自定义数据集, 将VOC标签中的RGB值映射到它们的类别索引, 将X和y拆分到多个设备上, 将X和y拆分到多个设备上

Defined in :numref:`sec_multi_gpu`""""""
assert X.shape[0] == y.shape[0]
return (nn.parallel.scatter(X, devices),
nn.parallel.scatter(y, devices))

def resnet18(num_classes, in_channels=1):
稍加修改的ResNet-18模型, 将文件复制到目标目录, 将文件复制到目标目录

Defined in :numref:`sec_kaggle_cifar10`""""""
os.makedirs(target_dir, exist_ok=True)
shutil.copy(filename, target_dir)

def reorg_train_valid(data_dir, labels, valid_ratio):
将验证集从原始的训练集中拆分出来, 将文本行拆分为单词或字符词元, 将时间机器数据集加载到文本行的列表中, 将时间机器数据集加载到文本行的列表中

Defined in :numref:`sec_text_preprocessing`""""""
with open(d2l.download('time_machine'), 'r') as f:
lines = f.readlines()
return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
将文本行拆分为单词或字符词元, 将最接近的真实边界框分配给锚框, 将最接近的真实边界框分配给锚框

Defined in :numref:`sec_anchor`""""""
num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
# 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU
jaccard = box_iou(anchors, ground_truth)
# 对于每个锚框，分配的真实边界框的张量
anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
device=device)
# 根据阈值，决定是否分配真实边界框
max_ious, indices = torch.max(jaccard, dim=1)
anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)
box_j = indices[max_ious >= iou_threshold]
anchors_bbox_map[anc_i] = box_j
col_discard = torch.full((num_anchors,), -1)
row_discard = torch.full((num_gt_boxes,), -1)
for _ in range(num_gt_boxes):
max_idx = torch.argmax(jaccard)
box_idx = (max_idx % num_gt_boxes).long()
anc_idx = (max_idx / num_gt_boxes).long()
anchors_bbox_map[anc_idx] = box_idx
jaccard[:, box_idx] = col_discard
jaccard[anc_idx, :] = row_discard
return anchors_bbox_map

def offset_boxes(anchors, assigned_bb, eps=1e-6):
对锚框偏移量的转换, 将机器翻译的文本序列转换成小批量, 将模型设置为评估模式, 将验证集从原始的训练集中拆分出来, 小批量随机梯度下降, 小批量随机梯度下降

Defined in :numref:`sec_linear_scratch`""""""
with torch.no_grad():
for param in params:
param -= lr * param.grad / batch_size
param.grad.zero_()

def load_array(data_arrays, batch_size, is_train=True):
构造一个PyTorch数据迭代器, 带有注意力机制解码器的基本接口, 带有注意力机制解码器的基本接口

Defined in :numref:`sec_seq2seq_attention`""""""
def __init__(self, **kwargs):
super(AttentionDecoder, self).__init__(**kwargs)

@property
def attention_weights(self):
raise NotImplementedError

class MultiHeadAttention(nn.Module):
多头注意力, 带遮蔽的softmax交叉熵损失函数, 带遮蔽的softmax交叉熵损失函数

Defined in :numref:`sec_seq2seq_decoder`""""""
# pred的形状：(batch_size,num_steps,vocab_size)
# label的形状：(batch_size,num_steps)
# valid_len的形状：(batch_size,)
def forward(self, pred, label, valid_len):
weights = torch.ones_like(label)
weights = sequence_mask(weights, valid_len)
self.reduction='none'
unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
pred.permute(0, 2, 1), label)
weighted_loss = (unweighted_loss * weights).mean(dim=1)
return weighted_loss

def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
训练序列到序列模型, 序列到序列模型的预测, 序列到序列模型的预测

Defined in :numref:`sec_seq2seq_training`""""""
# 在预测时将net设置为评估模式
net.eval()
src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
src_vocab['<eos>']]
enc_valid_len = torch.tensor([len(src_tokens)], device=device)
src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
# 添加批量轴
enc_X = torch.unsqueeze(
torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
enc_outputs = net.encoder(enc_X, enc_valid_len)
dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
# 添加批量轴
dec_X = torch.unsqueeze(torch.tensor(
[tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
output_seq, attention_weight_seq = [], []
for _ in range(num_steps):
Y, dec_state = net.decoder(dec_X, dec_state)
# 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
dec_X = Y.argmax(dim=2)
pred = dec_X.squeeze(dim=0).type(torch.int32).item()
# 保存注意力权重（稍后讨论）
if save_attention_weights:
attention_weight_seq.append(net.decoder.attention_weights)
# 一旦序列结束词元被预测，输出序列的生成就完成了
if pred == tgt_vocab['<eos>']:
break
output_seq.append(pred)
return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq

def bleu(pred_seq, label_seq, k):
计算BLEU, 强制教学, 循环神经网络模型, 截断或填充文本序列, 截断或填充文本序列

Defined in :numref:`sec_machine_translation`""""""
if len(line) > num_steps:
return line[:num_steps]  # 截断
return line + [padding_token] * (num_steps - len(line))  # 填充

def build_array_nmt(lines, vocab, num_steps):
将机器翻译的文本序列转换成小批量, 所以将所有文本行展平到一个列表中, 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次, 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测, 找到所有的non_keep索引，并将类设置为背景, 按出现频率排序, 损失函数的标量进行“反向传播”, 损失的总和,样本数量, 文本词表, 文本词表
def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
Defined in :numref:`sec_text_preprocessing`, 显示优化过程中2D变量的轨迹, 显示优化过程中2D变量的轨迹

Defined in :numref:`subsec_gd-learningrate`""""""
d2l.set_figsize()
d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')
x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),
d2l.arange(-3.0, 1.0, 0.1), indexing='ij')
d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')
d2l.plt.xlabel('x1')
d2l.plt.ylabel('x2')

d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
'76e5be1548fd8222e5074cf0faae75edff8cf93f')

def get_data_ch11(batch_size=10, n=1500):
Defined in :numref:`sec_minibatches`, 显示所有边界框, 显示所有边界框

Defined in :numref:`sec_anchor`""""""
def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj

labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))

def box_iou(boxes1, boxes2):
计算两个锚框或边界框列表中成对的交并比, 显示矩阵热图, 显示矩阵热图

Defined in :numref:`sec_attention-cues`""""""
d2l.use_svg_display()
num_rows, num_cols = matrices.shape[0], matrices.shape[1]
fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
sharex=True, sharey=True, squeeze=False)
for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)
if i == num_rows - 1:
ax.set_xlabel(xlabel)
if j == 0:
ax.set_ylabel(ylabel)
if titles:
ax.set_title(titles[j])
fig.colorbar(pcm, ax=axes, shrink=0.6);

def masked_softmax(X, valid_lens):
通过在最后一个轴上掩蔽元素来执行softmax操作, 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0, 未知词元的索引为0, 构建从RGB到VOC类别索引的映射, 构建从RGB到VOC类别索引的映射

Defined in :numref:`sec_semantic_segmentation`""""""
colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
for i, colormap in enumerate(VOC_COLORMAP):
colormap2label[
(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
return colormap2label

def voc_label_indices(colormap, colormap2label):
将VOC标签中的RGB值映射到它们的类别索引, 构造一个PyTorch数据迭代器, 根据n个采样权重在, 根据n个采样权重在{1,...,n}中随机抽取
def __init__(self, sampling_weights):
Defined in :numref:`sec_word2vec_data`, 根据带有预测偏移量的锚框来预测边界框, 正在从, 正确预测数、预测总数, 残差连接后进行层规范化, 残差连接后进行层规范化

Defined in :numref:`sec_transformer`""""""
def __init__(self, normalized_shape, dropout, **kwargs):
super(AddNorm, self).__init__(**kwargs)
self.dropout = nn.Dropout(dropout)
self.ln = nn.LayerNorm(normalized_shape)

def forward(self, X, Y):
return self.ln(self.dropout(Y) + X)

class EncoderBlock(nn.Module):
Transformer编码器块, 每个中心点都将有“boxes_per_pixel”个锚框，, 然后如此复制第二项，然后诸如此类。, 生成y=Xw+b+噪声, 生成“boxes_per_pixel”个高和宽，, 生成以每个像素为中心具有不同形状的锚框, 生成锚框的所有中心点, 用GPU训练模型, 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引, 用于加载SNLI数据集的自定义数据集, 用于序列到序列学习的循环神经网络编码器, 用于序列到序列学习的循环神经网络编码器

Defined in :numref:`sec_seq2seq`""""""
def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
dropout=0, **kwargs):
super(Seq2SeqEncoder, self).__init__(**kwargs)
# 嵌入层
self.embedding = nn.Embedding(vocab_size, embed_size)
self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
dropout=dropout)

def forward(self, X, *args):
# 输出'X'的形状：(batch_size,num_steps,embed_size)
X = self.embedding(X)
# 在循环神经网络模型中，第一个轴对应于时间步
X = X.permute(1, 0, 2)
# 如果未提及状态，则默认为0
output, state = self.rnn(X)
# output的形状:(num_steps,batch_size,num_hiddens)
# state的形状:(num_layers,batch_size,num_hiddens)
return output, state

def sequence_mask(X, valid_len, value=0):
在序列中屏蔽不相关的项, 用于测量运行时间, 用于测量运行时间
def __init__(self, description='Done'):
Defined in :numref:`sec_hybridize`, 用多GPU进行小批量训练, 用多GPU进行小批量训练

Defined in :numref:`sec_image_augmentation`""""""
if isinstance(X, list):
# 微调BERT中所需
X = [x.to(devices[0]) for x in X]
else:
X = X.to(devices[0])
y = y.to(devices[0])
net.train()
trainer.zero_grad()
pred = net(X)
l = loss(pred, y)
l.sum().backward()
trainer.step()
train_loss_sum = l.sum()
train_acc_sum = d2l.accuracy(pred, y)
return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
devices=d2l.try_all_gpus()):
用多GPU进行模型训练, 用多GPU进行模型训练, 用定制的训练机优化2D目标函数, 稍加修改的ResNet-18模型, 线性回归模型, 线性回归模型

Defined in :numref:`sec_linear_scratch`""""""
return d2l.matmul(X, w) + b

def squared_loss(y_hat, y):
均方损失, 经过变换后，输出的queries，keys，values　的形状:, 绘制列表长度对的直方图, 绘制图像列表, 绘制数据点, 统计词元的频率, 统计词元的频率

Defined in :numref:`sec_text_preprocessing`""""""
# 这里的tokens是1D列表或2D列表
if len(tokens) == 0 or isinstance(tokens[0], list):
# 将词元列表展平成一个列表
tokens = [token for line in tokens for token in line]
return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
返回时光机器数据集的词元索引列表和词表, 缓存k个随机采样结果, 编码器-解码器架构的基本编码器接口, 编码器-解码器架构的基本解码器接口, 编码器-解码器架构的基本解码器接口

Defined in :numref:`sec_encoder-decoder`""""""
def __init__(self, **kwargs):
super(Decoder, self).__init__(**kwargs)

def init_state(self, enc_outputs, *args):
raise NotImplementedError

def forward(self, X, state):
raise NotImplementedError

class EncoderDecoder(nn.Module):
编码器-解码器架构的基类, 编码器-解码器架构的基类, 缩放点积注意力, 获取输入序列的词元及其片段索引, 裁剪梯度, 裁剪梯度

Defined in :numref:`sec_rnn_scratch`""""""
if isinstance(net, nn.Module):
params = [p for p in net.parameters() if p.requires_grad]
else:
params = net.params
norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
if norm > theta:
for param in params:
param.grad[:] *= theta / norm

def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
训练网络一个迭代周期（定义见第8章）, 要形成“中心词-上下文词”对，每个句子至少需要有2个词, 计算BLEU, 计算两个锚框或边界框列表中成对的交并比, 计算二维互相关运算, 计算在指定数据集上模型的精度, 计算预测正确的数量, 计算预测正确的数量

Defined in :numref:`sec_softmax_scratch`""""""
if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
y_hat = d2l.argmax(y_hat, axis=1)
cmp = d2l.astype(y_hat, y.dtype) == y
return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))

def evaluate_accuracy(net, data_iter):
计算在指定数据集上模型的精度, 训练序列到序列模型, 训练损失之和,词元数量, 训练损失之和，训练准确率之和，样本数, 训练损失总和，词元数量, 训练数据集中样本最少的类别中的样本数, 训练模型一个迭代周期（定义见第3章）, 训练模型一个迭代周期（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
# 将模型设置为训练模式
if isinstance(net, torch.nn.Module):
net.train()
# 训练损失总和、训练准确度总和、样本数
metric = Accumulator(3)
for X, y in train_iter:
# 计算梯度并更新参数
y_hat = net(X)
l = loss(y_hat, y)
if isinstance(updater, torch.optim.Optimizer):
# 使用PyTorch内置的优化器和损失函数
updater.zero_grad()
l.mean().backward()
updater.step()
else:
# 使用定制的优化器和损失函数
l.sum().backward()
updater(X.shape[0])
metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
# 返回训练损失和训练精度
return metric[0] / metric[2], metric[1] / metric[2]

class Animator:
在动画中绘制数据, 训练模型（定义见第3章）, 训练模型（定义见第8章）, 训练模型（定义见第8章）

Defined in :numref:`sec_rnn_scratch`""""""
loss = nn.CrossEntropyLoss()
animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
legend=['train'], xlim=[10, num_epochs])
# 初始化
if isinstance(net, nn.Module):
updater = torch.optim.SGD(net.parameters(), lr)
else:
updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
# 训练和预测
for epoch in range(num_epochs):
ppl, speed = train_epoch_ch8(
net, train_iter, loss, updater, device, use_random_iter)
if (epoch + 1) % 10 == 0:
print(predict('time traveller'))
animator.add(epoch + 1, [ppl])
print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
print(predict('time traveller'))
print(predict('traveller'))

class RNNModel(nn.Module):
循环神经网络模型, 训练网络一个迭代周期（定义见第8章）, 记录多次运行时间, 记录多次运行时间
def __init__(self):
Defined in :numref:`subsec_linear_model`, 设置matplotlib的图表大小, 设置matplotlib的轴, 设置matplotlib的轴

Defined in :numref:`sec_calculus`""""""
axes.set_xlabel(xlabel)
axes.set_ylabel(ylabel)
axes.set_xscale(xscale)
axes.set_yscale(yscale)
axes.set_xlim(xlim)
axes.set_ylim(ylim)
if legend:
axes.legend(legend)
axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
ylim=None, xscale='linear', yscale='linear',
fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
绘制数据点, 设置transpose_b=True为了交换keys的最后两个维度, 评估给定数据集上模型的损失, 词元/秒, 词元化“英语－法语”数据数据集, 词元化“英语－法语”数据数据集

Defined in :numref:`sec_machine_translation`""""""
source, target = [], []
for i, line in enumerate(text.split('\n')):
if num_examples and i > num_examples:
break
parts = line.split('\t')
if len(parts) == 2:
source.append(parts[0].split(' '))
target.append(parts[1].split(' '))
return source, target

def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
绘制列表长度对的直方图, 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层, 读取IMDb评论数据集文本序列和标签, 读取fname来给标签字典返回一个文件名, 读取所有VOC图像并标注, 读取香蕉检测数据集中的图像和标签, 读取香蕉检测数据集中的图像和标签

Defined in :numref:`sec_object-detection-dataset`""""""
data_dir = d2l.download_extract('banana-detection')
csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
else 'bananas_val', 'label.csv')
csv_data = pd.read_csv(csv_fname)
csv_data = csv_data.set_index('img_name')
images, targets = [], []
for img_name, target in csv_data.iterrows():
images.append(torchvision.io.read_image(
os.path.join(data_dir, 'bananas_train' if is_train else
'bananas_val', 'images', f'{img_name}')))
# 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
# 其中所有图像都具有相同的香蕉类（索引为0）
targets.append(list(target))
return images, torch.tensor(targets).unsqueeze(1) / 256

class BananasDataset(torch.utils.data.Dataset):
一个用于加载香蕉检测数据集的自定义数据集, 跳过文件头行(列名), 载入“英语－法语”数据集, 载入“英语－法语”数据集

Defined in :numref:`sec_machine_translation`""""""
data_dir = d2l.download_extract('fra-eng')
with open(os.path.join(data_dir, 'fra.txt'), 'r',
encoding='utf-8') as f:
return f.read()

def preprocess_nmt(text):
预处理“英语－法语”数据集, 返回Fashion-MNIST数据集的文本标签, 返回Fashion-MNIST数据集的文本标签

Defined in :numref:`sec_fashion_mnist`""""""
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
绘制图像列表, 返回带有负采样的跳元模型的小批量样本, 返回平均时间, 返回平均时间
return sum(self.times) / len(self.times)

def sum(self):
返回时间总和, 返回所有可用的GPU，如果没有GPU，则返回[cpu, 返回所有可用的GPU，如果没有GPU，则返回[cpu(),]

Defined in :numref:`sec_use_gpu`""""""
devices = [torch.device(f'cuda:{i}')
for i in range(torch.cuda.device_count())]
return devices if devices else [torch.device('cpu')]

def corr2d(X, K):
计算二维互相关运算, 返回数据迭代器和IMDb评论数据集的词表, 返回数据迭代器和IMDb评论数据集的词表

Defined in :numref:`sec_sentiment`""""""
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
train_data = read_imdb(data_dir, True)
test_data = read_imdb(data_dir, False)
train_tokens = d2l.tokenize(train_data[0], token='word')
test_tokens = d2l.tokenize(test_data[0], token='word')
vocab = d2l.Vocab(train_tokens, min_freq=5)
train_features = torch.tensor([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])
test_features = torch.tensor([d2l.truncate_pad(
vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])
train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),
batch_size)
test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),
batch_size,
is_train=False)
return train_iter, test_iter, vocab

def predict_sentiment(net, vocab, sequence):
预测文本序列的情感, 返回时光机器数据集的词元索引列表和词表, 返回时光机器数据集的迭代器和词表, 返回时光机器数据集的迭代器和词表

Defined in :numref:`sec_language_model`""""""
data_iter = SeqDataLoader(
batch_size, num_steps, use_random_iter, max_tokens)
return data_iter, data_iter.vocab

class RNNModelScratch:
从零开始实现的循环神经网络模型, 返回时间总和, 返回累计时间, 返回累计时间
return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
生成y=Xw+b+噪声, 返回翻译数据集的迭代器和词表, 返回翻译数据集的迭代器和词表

Defined in :numref:`subsec_mt_data_loading`""""""
text = preprocess_nmt(read_data_nmt())
source, target = tokenize_nmt(text, num_examples)
src_vocab = d2l.Vocab(source, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
tgt_vocab = d2l.Vocab(target, min_freq=2,
reserved_tokens=['<pad>', '<bos>', '<eos>'])
src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)
data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
data_iter = d2l.load_array(data_arrays, batch_size)
return data_iter, src_vocab, tgt_vocab

class Encoder(nn.Module):
编码器-解码器架构的基本编码器接口, 返回负采样中的噪声词, 返回负采样中的噪声词

Defined in :numref:`sec_word2vec_data`""""""
# 索引为1、2、...（索引0是词表中排除的未知标记）
sampling_weights = [counter[vocab.to_tokens(i)]**0.75
for i in range(1, len(vocab))]
all_negatives, generator = [], RandomGenerator(sampling_weights)
for contexts in all_contexts:
negatives = []
while len(negatives) < len(contexts) * K:
neg = generator.draw()
# 噪声词不能是上下文词
if neg not in contexts:
negatives.append(neg)
all_negatives.append(negatives)
return all_negatives

def batchify(data):
返回带有负采样的跳元模型的小批量样本, 返回跳元模型中的中心词和上下文词, 逆转transpose_qkv函数的操作, 通过在最后一个轴上掩蔽元素来执行softmax操作, 那么batch_idx是np.array（[0,0,0,1,1,1]）, 错误：未知词元类型：, 除以2来获得半高和半宽, 随机裁剪特征和标签图像, 随机裁剪特征和标签图像

Defined in :numref:`sec_semantic_segmentation`""""""
rect = torchvision.transforms.RandomCrop.get_params(
feature, (height, width))
feature = torchvision.transforms.functional.crop(feature, *rect)
label = torchvision.transforms.functional.crop(label, *rect)
return feature, label

class VOCSegDataset(torch.utils.data.Dataset):
一个用于加载VOC数据集的自定义数据集, 预处理“英语－法语”数据集, 预测num_preds步, 预测前提和假设之间的逻辑关系, 预测文本序列的情感, 预测标签（定义见第3章）, 预测标签（定义见第3章）

Defined in :numref:`sec_softmax_scratch`""""""
for X, y in test_iter:
break
trues = d2l.get_fashion_mnist_labels(y)
preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))
titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
d2l.show_images(
d2l.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])

def evaluate_loss(net, data_iter, loss):
评估给定数据集上模型的损失, 预热期, 验证集中每个类别的样本数, ，否则返回cpu"
https://github.com/d2l-ai/d2l-zh,__init__.py,0,0.0,1,25.0,2,50.0,0,0.0,1,25.0,0,0.0,0,0,1,1,,__version__,,,,__version__,"Saved source code for ""Dive into Deep Learing"" (https://d2l.ai).

Please import d2l by one of the following ways:

from d2l import mxnet as d2l  # Use MXNet as the backend
from d2l import torch as d2l  # Use PyTorch as the backend
from d2l import tensorflow as d2l  # Use TensorFlow as the backend
from d2l import paddle as d2l  # Use Paddle as the backend","2.0.0, Saved source code for ""Dive into Deep Learing"" (https://d2l.ai).

Please import d2l by one of the following ways:

from d2l import mxnet as d2l  # Use MXNet as the backend
from d2l import torch as d2l  # Use PyTorch as the backend
from d2l import tensorflow as d2l  # Use TensorFlow as the backend
from d2l import paddle as d2l  # Use Paddle as the backend

",,"Saved source code for ""Dive into Deep Learing"" (https://d2l.ai).

Please import d2l by one of the following ways:

from d2l import mxnet as d2l  # Use MXNet as the backend
from d2l import torch as d2l  # Use PyTorch as the backend
from d2l import tensorflow as d2l  # Use TensorFlow as the backend
from d2l import paddle as d2l  # Use Paddle as the backend

",
https://github.com/d2l-ai/d2l-zh,main.py,0,0.0,39,42.86,30,32.97,6,6.59,14,15.38,2,2.2,7,0,20,0,,"NUM_UNNUMBERED_CHAPS, TOC2_START_CHAP_NO, UNNUMBERED, __name__, _delete_discussions_title, _edit_titlepage, _sec_to_chap, _startswith_unnumbered, _unnumber_chaps_and_secs, append, ch2_reached, chap_name, delete_lines, deletes, enumerate, i, l, line, lines, longest_balanced_braces, main, num_chaps, open, os, pdf_dir, preface_reached, re, read, regex, replace, smanual, src, startswith, sys, tex_file, tgt, to_delete, unnum, write","os.path.dirname, os.path.join, re.split, regex.findall, sys.argv","_delete_discussions_title, _edit_titlepage, _sec_to_chap, _startswith_unnumbered, _unnumber_chaps_and_secs, delete_lines, main",,"NUM_UNNUMBERED_CHAPS, TOC2_START_CHAP_NO, UNNUMBERED, ch2_reached, chap_name, deletes, i, l, line, lines, longest_balanced_braces, num_chaps, pdf_dir, preface_reached, smanual, src, tex_file, tgt, to_delete, unnum",,", 
, Chapter \ref, Section \ref, \@date, \addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
, \chapter*{, \chapter{, \section, \section{小结, \section{练习, \sphinxincludegraphics, \subsection, \subsection{小结, \subsection{练习, \subsubsection, \{(?>[^{}]|(?R))*\}, __main__, index:, r, section*{, section*{Discussion, section{, section{Discussion, sphinxmanual.cls, w, {Section \ref, {|}, }
, }\addcontentsline{toc}{chapter}{","0, 1, 3, 5, False, True","# If label is of chap*/index.md title, its numref is Chapter X instead of Section X, # Note that there can be multiple {Section } in one line, # Preface, Installation, and Notation are unnumbered chapters, # Prelimilaries, # Remove date, # Set tocdepth to 2 after Chap 1, # Since we inserted '\n' in some lines[i], re-build the list, # Unnumber all sections in unnumbered chapters, # Unnumber summary, references, exercises, qr code in numbered chapters, # Unnumber unnumbered chapters, # e.g., {Section \ref{\detokenize{chapter_dlc/index:chap-dlc}}} matches, # {Section \ref{\detokenize{chapter_prelim/nd:sec-nd}}} does not match, #_edit_titlepage(pdf_dir), #lines = _delete_discussions_title(lines)","小结, 练习"
https://github.com/d2l-ai/d2l-zh,utils.py,0,0.0,355,60.68,161,27.52,40,6.84,29,4.96,0,0.0,66,4,110,54,,"Activation, ArrayDataset, BatchNorm, Benchmark, Block, Compose, Conv2D, DataLoader, Dataset, Dense, FashionMNIST, GlobalAvgPool2D, IPython, ImageDetIter, K, L2Loss, Normal, RNNModel, Rectangle, Residual, Resize, Sequential, SoftmaxCrossEntropyLoss, T, ToTensor, Trainer, VOCSegDataset, VOC_CLASSES, VOC_COLORMAP, Vocabulary, X, Xs, Y, _, __enter__, __exit__, __getitem__, __init__, __len__, _data, _download_pikachu, _get_batch, _make_list, acc_sum, add, add_patch, append, argmax, args, array, as_in_context, asnumpy, asscalar, astype, attach_grad, axes, backward, batch, batch_i, batch_indices, batch_len, batch_size, bbox, bbox_to_rect, bboxes, begin_state, blk, bn1, bn2, char, char_to_idx, clipping_theta, collect_params, collections, color, colormap, colormap2label, colors, concat, contour, conv1, conv2, conv3, corpus_chars, corpus_indices, corr2d, count_tokens, counter, crop_size, ctx, ctxes, d2lzh, data, data_dir, data_iter, data_iter_consecutive, data_iter_fn, data_iter_random, data_len, dataset, default_values, dense, detach, dict, dot, download, download_imdb, download_voc_pascal, dtype, edge_size, enumerate, epoch, epoch_size, eval_loss, evaluate_accuracy, example_indices, extractall, feature, features, figs, figsize, filter, first_block, fixed_crop, fname, folder, folder_name, forward, gdata, get_data_ch7, get_fashion_mnist_labels, get_params, get_tokenized_imdb, get_vocab_imdb, get_xaxis, get_yaxis, gloss, grad, grad_clipping, gutils, height, hyperparams, idx, idx_to_char, images, img, imgs, imread, imshow, indices, init_rnn_state, initialize, inputs, int, is_random_iter, is_train, isinstance, items, j, k, kwargs, l_sum, label, labels, lbl, legend, len, linreg, list, load_data_fashion_mnist, load_data_jay_lyrics, load_data_pikachu, load_data_time_machine, lower, lr, math, matplotlib, max, max_l, mean, min, mkdir_if_not_exist, mnist_test, mnist_train, model, mxnet, norm, normal, normalize_image, num_channels, num_chars, num_classes, num_cols, num_epochs, num_examples, num_hiddens, num_residuals, num_rows, num_steps, num_workers, numpy, obj, one_hot, open, os, output, outputs, pad, param, params, plot, plt, pos, pred_len, pred_period, predict_rnn, predict_rnn_gluon, predict_sentiment, prefix, prefixes, preprocess_imdb, print, random, random_crop, range, rcParams, read, read_imdb, read_voc_images, record, rect, relu, replace, res, reshape, resize, resnet18, resnet_block, review, rgb_mean, rgb_std, rnn, rnn_layer, root, root_url, s_x1, s_x2, sample, samples, scale, score, self, semilogy, sentence, set, set_figsize, set_matplotlib_formats, set_title, sgd, sha1, shape, show, show_bboxes, show_fashion_mnist, show_images, show_trace_2d, size, split, split_and_load, sqrt, squared_loss, st, start, state, states, std, step, str, strides, subplots, sum, super, sys, take, tarfile, test_acc, test_iter, text, text_color, text_labels, theta, time, tk, to_indices, to_onehot, tok, token, token_counter, token_to_idx, tokenized_data, tokenizer, train, train_2d, train_acc_sum, train_and_predict_rnn, train_and_predict_rnn_gluon, train_ch3, train_ch5, train_ch7, train_gluon_ch7, train_iter, train_l_sum, trainer, trainer_fn, trainer_hyperparams, trainer_name, transform_first, transformer, transforms, transpose, try_all_gpus, try_gpu, tuple, txt_fname, url, use_1x1conv, use_svg_display, v, val_iter, vision, voc_dir, voc_label_indices, voc_rand_crop, vocab, vocab_size, w, wait_to_read, width, x1, x2, x2_vals, x_label, x_vals, xlabel, xy, y2_vals, y_hat, y_hats, y_label, y_vals, ylabel, zeros, zin, zip, zipfile","IPython.display, collections.Counter, d2lzh.text, math.exp, matplotlib.pyplot, mx.Context, mx.base.MXNetError, mx.cpu, mx.gpu, mxnet.autograd, mxnet.gluon, mxnet.gluon.data, mxnet.gluon.loss, mxnet.gluon.nn, mxnet.gluon.utils, mxnet.image, mxnet.init, mxnet.nd, np.arange, np.genfromtxt, np.linspace, np.meshgrid, os.listdir, os.makedirs, os.path.exists, os.path.expanduser, os.path.join, random.shuffle, sys.platform.startswith, tarfile.open, time.time, zipfile.ZipFile","__enter__, __exit__, __getitem__, __init__, __len__, _data, _download_pikachu, _get_batch, _make_list, bbox_to_rect, begin_state, corr2d, count_tokens, data_iter, data_iter_consecutive, data_iter_random, download_imdb, download_voc_pascal, eval_loss, evaluate_accuracy, filter, forward, get_data_ch7, get_fashion_mnist_labels, get_tokenized_imdb, get_vocab_imdb, grad_clipping, linreg, load_data_fashion_mnist, load_data_jay_lyrics, load_data_pikachu, load_data_time_machine, mkdir_if_not_exist, normalize_image, pad, predict_rnn, predict_rnn_gluon, predict_sentiment, preprocess_imdb, read_imdb, read_voc_images, resnet18, resnet_block, semilogy, set_figsize, sgd, show_bboxes, show_fashion_mnist, show_images, show_trace_2d, squared_loss, to_onehot, tokenizer, train, train_2d, train_and_predict_rnn, train_and_predict_rnn_gluon, train_ch3, train_ch5, train_ch7, train_gluon_ch7, try_all_gpus, try_gpu, use_svg_display, voc_label_indices, voc_rand_crop","Benchmark, RNNModel, Residual, VOCSegDataset","VOC_CLASSES, VOC_COLORMAP, X, Xs, Y, _, acc_sum, axes, batch, batch_i, batch_indices, batch_len, batch_size, bbox, blk, char, char_to_idx, color, colormap, colors, corpus_chars, corpus_indices, counter, ctx, ctxes, data, data_dir, data_iter, data_iter_fn, data_len, dataset, epoch, epoch_size, example_indices, feature, features, figs, figsize, fname, folder_name, i, idx, idx_to_char, images, img, indices, inputs, j, k, l, l_sum, label, labels, lbl, loss, ls, max_l, mnist_test, mnist_train, net, norm, num_examples, num_workers, obj, output, outputs, param, params, prefix, rect, res, review, root, root_url, s_x1, s_x2, sample, score, sentence, sha1, st, start, state, test_acc, test_iter, text_color, text_labels, tk, tok, token, token_counter, tokenized_data, train_acc_sum, train_iter, train_l_sum, trainer, transformer, txt_fname, url, v, val_iter, voc_dir, vocab_size, w, x1, x2, y, y_hat, y_hats, zin","Assign label indices for Pascal VOC2012 Dataset., Benchmark programs., Clip the gradient., Compute 2D cross-correlation., Convert bounding box to matplotlib format., Count tokens in the data set., Download the IMDB data set for sentiment analysis., Download the Pascal VOC2012 Dataset., Download the fashion mnist dataset and then load into memory., Download the pikachu dataest and then load into memory., Evaluate accuracy of a model on the given data set., Get text label for fashion mnist., Get the data set used in Chapter 7., Get the tokenized IMDB data set for sentiment analysis., Get the vocab for the IMDB data set for sentiment analysis., If GPU is available, return mx.gpu(0); else return mx.cpu()., Iterate through a data set., Linear regression., Load the Jay Chou lyric data set (available in the Chinese book)., Load the time machine data set (available in the English book)., Make a directory if it does not exist., Mini-batch stochastic gradient descent., Optimize the objective function of 2d variables with a customized trainer., Plot Fashion-MNIST images with labels., Plot a list of images., Plot x and log(y)., Precit next chars with a Gluon RNN model, Predict next chars with a RNN model, Predict the sentiment of a given sentence., Preprocess the IMDB data set for sentiment analysis., RNN model., Random cropping for images of the Pascal VOC2012 Dataset., Read VOC images., Read the IMDB data set for sentiment analysis., Represent inputs with one-hot encoding., Return all available GPUs, or [mx.cpu()] if there is no GPU., Return features and labels on ctx., Sample mini-batches in a consecutive order from sequential data., Sample mini-batches in a random order from sequential data., Set matplotlib figure size., Show bounding boxes., Show the trace of 2d variables during optimization., Squared loss., The Pascal VOC2012 Dataset., The ResNet-18 model., The residual block., Train a linear regression model with a given Gluon trainer., Train a linear regression model., Train an Gluon RNN model and predict the next item in the sequence., Train an RNN model and predict the next item in the sequence., Train and evaluate a model with CPU or GPU., Train and evaluate a model with CPU., Train and evaluate a model., Use svg format to display plot in jupyter",", 	, 
, ,  ,  -,  examples, #1f77b4, #ff7f0e, %s/ImageSets/Segmentation/%s, %s/JPEGImages/%s.jpg, %s/SegmentationClass/%s.png, %stime: %.4f sec, -o, ../data, ../data/VOCdevkit/VOC2012, ../data/aclImdb/, ../data/airfoil_self_noise.dat, ../data/jaychou_lyrics.txt.zip, ../data/pikachu, ../data/timemachine.txt, .mxnet, 01ada507287d82875905620988597833ad4e0903, 4e443f8a2eca6b1dac8a6c57641b67dd40621a49, :, <pad>, Assign label indices for Pascal VOC2012 Dataset., Benchmark programs., Clip the gradient., Compute 2D cross-correlation., Convert bounding box to matplotlib format., Count tokens in the data set., Download the IMDB data set for sentiment analysis., Download the Pascal VOC2012 Dataset., Download the fashion mnist dataset and then load into memory., Download the pikachu dataest and then load into memory., Evaluate accuracy of a model on the given data set., Get text label for fashion mnist., Get the data set used in Chapter 7., Get the tokenized IMDB data set for sentiment analysis., Get the vocab for the IMDB data set for sentiment analysis., If GPU is available, return mx.gpu(0); else return mx.cpu()., Iterate through a data set., Linear regression., Load the Jay Chou lyric data set (available in the Chinese book)., Load the time machine data set (available in the English book)., Make a directory if it does not exist., Mini-batch stochastic gradient descent., Optimize the objective function of 2d variables with a customized trainer., Plot Fashion-MNIST images with labels., Plot a list of images., Plot x and log(y)., Precit next chars with a Gluon RNN model, Predict next chars with a RNN model, Predict the sentiment of a given sentence., Preprocess the IMDB data set for sentiment analysis., RNN model., Random cropping for images of the Pascal VOC2012 Dataset., Read VOC images., Read the IMDB data set for sentiment analysis., Represent inputs with one-hot encoding., Return all available GPUs, or [mx.cpu()] if there is no GPU., Return features and labels on ctx., Sample mini-batches in a consecutive order from sequential data., Sample mini-batches in a random order from sequential data., Set matplotlib figure size., Show bounding boxes., Show the trace of 2d variables during optimization., Squared loss., The Pascal VOC2012 Dataset., The ResNet-18 model., The residual block., Train a linear regression model with a given Gluon trainer., Train a linear regression model., Train an Gluon RNN model and predict the next item in the sequence., Train an RNN model and predict the next item in the sequence., Train and evaluate a model with CPU or GPU., Train and evaluate a model with CPU., Train and evaluate a model., Use svg format to display plot in jupyter, VOCdevkit/VOC2012, aeroplane, ankle boot, b, background, bag, bicycle, bird, boat, bottle, bus, car, cat, center, chair, coat, cow, d6c33f799b4d058e82f2cb5bd9a976f69d72d520, datasets, dcf7318b2602c06428b9988470c731621716c393, diningtable, dog, dress, e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8, epoch, epoch %d, loss %.4f, train acc %.3f, test acc %.3f, epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec, epoch %d, perplexity %f, time %.2f sec, epoch %d, x1 %f, x2 %f, fashion-mnist, figure.figsize, float32, g, horse, http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar, https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/pikachu/, int32, jaychou_lyrics.txt, k, learning_rate, loss, loss: %f, %f sec per epoch, m, momentum, motorbike, neg, negative, person, pos, positive, potted plant, pullover, r, rb, read , relu, sandal, sgd, sheep, shirt, sneaker, sofa, svg, t-shirt, train, train.idx, train.rec, train.txt, training on, trouser, tv/monitor, utf-8, val.rec, val.txt, w, wd, win32, x1, x2, ~","0, 0.0, 0.01, 0.1, 0.224, 0.225, 0.229, 0.406, 0.456, 0.485, 0.95, 1, 1.0, 10, 100, 10000, 12, 128, 16, 192, 2, 2.5, 20, 200, 255, 256, 28, 3, 3.0, 3.5, 4, 5, 5.5, 500, 512, 64, 9, False, None, True","#1f77b4'), #ff7f0e'), Compute 2D cross-correlation.
h, w = K.shape
Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
for i in range(Y.shape[0]):
for j in range(Y.shape[1]):
Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
return Y


def count_tokens(samples):
Count tokens in the data set., Convert bounding box to matplotlib format.
return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],
height=bbox[3]-bbox[1], fill=False, edgecolor=color,
linewidth=2)


class Benchmark():
Benchmark programs., Download the Pascal VOC2012 Dataset.
voc_dir = os.path.join(data_dir, 'VOCdevkit/VOC2012')
url = ('http://host.robots.ox.ac.uk/pascal/VOC/voc2012'
'/VOCtrainval_11-May-2012.tar')
sha1 = '4e443f8a2eca6b1dac8a6c57641b67dd40621a49'
fname = gutils.download(url, data_dir, sha1_hash=sha1)
with tarfile.open(fname, 'r') as f:
f.extractall(data_dir)
return voc_dir


def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):
Evaluate accuracy of a model on the given data set., Get text label for fashion mnist.
text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
return [text_labels[int(i)] for i in labels]


def get_tokenized_imdb(data):
Get the tokenized IMDB data set for sentiment analysis., Get the vocab for the IMDB data set for sentiment analysis.
tokenized_data = get_tokenized_imdb(data)
counter = collections.Counter([tk for st in tokenized_data for tk in st])
return text.vocab.Vocabulary(counter, min_freq=5,
reserved_tokens=['<pad>'])


def grad_clipping(params, theta, ctx):
Clip the gradient., Iterate through a data set.
num_examples = len(features)
indices = list(range(num_examples))
random.shuffle(indices)
for i in range(0, num_examples, batch_size):
j = nd.array(indices[i: min(i + batch_size, num_examples)])
yield features.take(j), labels.take(j)


def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):
Sample mini-batches in a consecutive order from sequential data., Linear regression.
return nd.dot(X, w) + b


def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(
'~', '.mxnet', 'datasets', 'fashion-mnist')):
Download the fashion mnist dataset and then load into memory., Load the Jay Chou lyric data set (available in the Chinese book).
with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip') as zin:
with zin.open('jaychou_lyrics.txt') as f:
corpus_chars = f.read().decode('utf-8')
corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ')
corpus_chars = corpus_chars[0:10000]
idx_to_char = list(set(corpus_chars))
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])
vocab_size = len(char_to_idx)
corpus_indices = [char_to_idx[char] for char in corpus_chars]
return corpus_indices, char_to_idx, idx_to_char, vocab_size


def load_data_pikachu(batch_size, edge_size=256):
Download the pikachu dataest and then load into memory., Load the time machine data set (available in the English book).
with open('../data/timemachine.txt') as f:
corpus_chars = f.read()
corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ').lower()
corpus_chars = corpus_chars[0:10000]
idx_to_char = list(set(corpus_chars))
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])
vocab_size = len(char_to_idx)
corpus_indices = [char_to_idx[char] for char in corpus_chars]
return corpus_indices, char_to_idx, idx_to_char, vocab_size


def _make_list(obj, default_values=None):
if obj is None:
obj = default_values
elif not isinstance(obj, (list, tuple)):
obj = [obj]
return obj


def mkdir_if_not_exist(path):
Make a directory if it does not exist., Plot a list of images.
figsize = (num_cols * scale, num_rows * scale)
_, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
for i in range(num_rows):
for j in range(num_cols):
axes[i][j].imshow(imgs[i * num_cols + j].asnumpy())
axes[i][j].axes.get_xaxis().set_visible(False)
axes[i][j].axes.get_yaxis().set_visible(False)
return axes


def show_trace_2d(f, res):
Show the trace of 2d variables during optimization., Predict next chars with a RNN model
state = init_rnn_state(1, num_hiddens, ctx)
output = [char_to_idx[prefix[0]]]
for t in range(num_chars + len(prefix) - 1):
X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)
(Y, state) = rnn(X, state, params)
if t < len(prefix) - 1:
output.append(char_to_idx[prefix[t + 1]])
else:
output.append(int(Y[0].argmax(axis=1).asscalar()))
return ''.join([idx_to_char[i] for i in output])


def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,
char_to_idx):
Precit next chars with a Gluon RNN model, Predict the sentiment of a given sentence.
sentence = nd.array(vocab.to_indices(sentence), ctx=try_gpu())
label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)
return 'positive' if label.asscalar() == 1 else 'negative'


def preprocess_imdb(data, vocab):
Preprocess the IMDB data set for sentiment analysis., RNN model.
def __init__(self, rnn_layer, vocab_size, **kwargs):
super(RNNModel, self).__init__(**kwargs)
self.rnn = rnn_layer
self.vocab_size = vocab_size
self.dense = nn.Dense(vocab_size)

def forward(self, inputs, state):
X = nd.one_hot(inputs.T, self.vocab_size)
Y, state = self.rnn(X, state)
output = self.dense(Y.reshape((-1, Y.shape[-1])))
return output, state

def begin_state(self, *args, **kwargs):
return self.rnn.begin_state(*args, **kwargs)


def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,
legend=None, figsize=(3.5, 2.5)):
Plot x and log(y)., Random cropping for images of the Pascal VOC2012 Dataset.
feature, rect = image.random_crop(feature, (width, height))
label = image.fixed_crop(label, *rect)
return feature, label


class VOCSegDataset(gdata.Dataset):
The Pascal VOC2012 Dataset., Read the IMDB data set for sentiment analysis.
data = []
for label in ['pos', 'neg']:
folder_name = os.path.join('../data/aclImdb/', folder, label)
for file in os.listdir(folder_name):
with open(os.path.join(folder_name, file), 'rb') as f:
review = f.read().decode('utf-8').replace('\n', '').lower()
data.append([review, 1 if label == 'pos' else 0])
random.shuffle(data)
return data


def read_voc_images(root='../data/VOCdevkit/VOC2012', is_train=True):
Read VOC images., Return all available GPUs, or [mx.cpu()] if there is no GPU.
ctxes = []
try:
for i in range(16):
ctx = mx.gpu(i)
_ = nd.array([0], ctx=ctx)
ctxes.append(ctx)
except mx.base.MXNetError:
pass
if not ctxes:
ctxes = [mx.cpu()]
return ctxes


def try_gpu():
If GPU is available, return mx.gpu(0); else return mx.cpu()., Return features and labels on ctx.
features, labels = batch
if labels.dtype != features.dtype:
labels = labels.astype(features.dtype)
return (gutils.split_and_load(features, ctx),
gutils.split_and_load(labels, ctx), features.shape[0])


def get_data_ch7():
Get the data set used in Chapter 7., Sample mini-batches in a random order from sequential data.
num_examples = (len(corpus_indices) - 1) // num_steps
epoch_size = num_examples // batch_size
example_indices = list(range(num_examples))
random.shuffle(example_indices)

def _data(pos):
return corpus_indices[pos : pos + num_steps]

for i in range(epoch_size):
i = i * batch_size
batch_indices = example_indices[i : i + batch_size]
X = nd.array(
[_data(j * num_steps) for j in batch_indices], ctx=ctx)
Y = nd.array(
[_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)
yield X, Y


def download_imdb(data_dir='../data'):
Download the IMDB data set for sentiment analysis., Set matplotlib figure size.
use_svg_display()
plt.rcParams['figure.figsize'] = figsize


def sgd(params, lr, batch_size):
Mini-batch stochastic gradient descent., Show bounding boxes.
labels = _make_list(labels)
colors = _make_list(colors, ['b', 'g', 'r', 'm', 'k'])
for i, bbox in enumerate(bboxes):
color = colors[i % len(colors)]
rect = bbox_to_rect(bbox.asnumpy(), color)
axes.add_patch(rect)
if labels and len(labels) > i:
text_color = 'k' if color == 'w' else 'w'
axes.text(rect.xy[0], rect.xy[1], labels[i],
va='center', ha='center', fontsize=9, color=text_color,
bbox=dict(facecolor=color, lw=0))


def show_fashion_mnist(images, labels):
Plot Fashion-MNIST images with labels., Squared loss.
return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2


def to_onehot(X, size):
Represent inputs with one-hot encoding., The residual block.
def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
super(Residual, self).__init__(**kwargs)
self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
strides=strides)
self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
if use_1x1conv:
self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
strides=strides)
else:
self.conv3 = None
self.bn1 = nn.BatchNorm()
self.bn2 = nn.BatchNorm()

def forward(self, X):
Y = nd.relu(self.bn1(self.conv1(X)))
Y = self.bn2(self.conv2(Y))
if self.conv3:
X = self.conv3(X)
return nd.relu(Y + X)


def resnet18(num_classes):
The ResNet-18 model., Train a linear regression model.
net, loss = linreg, squared_loss
w, b = nd.random.normal(scale=0.01, shape=(features.shape[1], 1)), nd.zeros(1)
w.attach_grad()
b.attach_grad()

def eval_loss():
return loss(net(features, w, b), labels).mean().asscalar()

ls = [eval_loss()]
data_iter = gdata.DataLoader(
gdata.ArrayDataset(features, labels), batch_size, shuffle=True)
for _ in range(num_epochs):
start = time.time()
for batch_i, (X, y) in enumerate(data_iter):
with autograd.record():
l = loss(net(X, w, b), y).mean()
l.backward()
trainer_fn([w, b], states, hyperparams)
if (batch_i + 1) * batch_size % 100 == 0:
ls.append(eval_loss())
print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))
set_figsize()
plt.plot(np.linspace(0, num_epochs, len(ls)), ls)
plt.xlabel('epoch')
plt.ylabel('loss')


def train_gluon_ch7(trainer_name, trainer_hyperparams, features, labels,
batch_size=10, num_epochs=2):
Train a linear regression model with a given Gluon trainer., Train an RNN model and predict the next item in the sequence.
if is_random_iter:
data_iter_fn = data_iter_random
else:
data_iter_fn = data_iter_consecutive
params = get_params()
loss = gloss.SoftmaxCrossEntropyLoss()

for epoch in range(num_epochs):
if not is_random_iter:
state = init_rnn_state(batch_size, num_hiddens, ctx)
l_sum, n, start = 0.0, 0, time.time()
data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)
for X, Y in data_iter:
if is_random_iter:
state = init_rnn_state(batch_size, num_hiddens, ctx)
else:
for s in state:
s.detach()
with autograd.record():
inputs = to_onehot(X, vocab_size)
(outputs, state) = rnn(inputs, state, params)
outputs = nd.concat(*outputs, dim=0)
y = Y.T.reshape((-1,))
l = loss(outputs, y).mean()
l.backward()
grad_clipping(params, clipping_theta, ctx)
sgd(params, lr, 1)
l_sum += l.asscalar() * y.size
n += y.size

if (epoch + 1) % pred_period == 0:
print('epoch %d, perplexity %f, time %.2f sec' % (
epoch + 1, math.exp(l_sum / n), time.time() - start))
for prefix in prefixes:
print(' -', predict_rnn(
prefix, pred_len, rnn, params, init_rnn_state,
num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))


def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,
corpus_indices, idx_to_char, char_to_idx,
num_epochs, num_steps, lr, clipping_theta,
batch_size, pred_period, pred_len, prefixes):
Train an Gluon RNN model and predict the next item in the sequence., Train and evaluate a model with CPU.
for epoch in range(num_epochs):
train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
for X, y in train_iter:
with autograd.record():
y_hat = net(X)
l = loss(y_hat, y).sum()
l.backward()
if trainer is None:
sgd(params, lr, batch_size)
else:
trainer.step(batch_size)
y = y.astype('float32')
train_l_sum += l.asscalar()
train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
n += y.size
test_acc = evaluate_accuracy(test_iter, net)
print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
% (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))


def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,
num_epochs):
Train and evaluate a model with CPU or GPU., Train and evaluate a model.
print('training on', ctx)
if isinstance(ctx, mx.Context):
ctx = [ctx]
for epoch in range(num_epochs):
train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()
for i, batch in enumerate(train_iter):
Xs, ys, batch_size = _get_batch(batch, ctx)
with autograd.record():
y_hats = [net(X) for X in Xs]
ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]
for l in ls:
l.backward()
trainer.step(batch_size)
train_l_sum += sum([l.sum().asscalar() for l in ls])
n += sum([l.size for l in ls])
train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()
for y_hat, y in zip(y_hats, ys)])
m += sum([y.size for y in ys])
test_acc = evaluate_accuracy(test_iter, net, ctx)
print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '
'time %.1f sec'
% (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,
time.time() - start))


def train_2d(trainer):
Optimize the objective function of 2d variables with a customized trainer., Use svg format to display plot in jupyter
display.set_matplotlib_formats('svg')


def voc_label_indices(colormap, colormap2label):
Assign label indices for Pascal VOC2012 Dataset.",
https://github.com/d2l-ai/d2l-zh,__init__.py,0,0.0,2,66.67,1,33.33,0,0.0,0,0.0,0,0.0,0,0,1,0,,"__version__, utils","text, utils.*",,,__version__,,1.0.0,,,
https://github.com/d2l-ai/d2l-zh,embedding.py,0,0.0,54,60.67,27,30.34,4,4.49,4,4.49,0,0.0,9,1,19,2,,"DATA_URL, PRETRAINED_FILE, TokenEmbedding, __init__, __len__, _load_embedding, append, array, base_dir, cache_dir, create, data_dir, download, download_extract, elem, elems, embedding_name, enumerate, ext, extractall, float, fname, folder, fp, get, get_pretrained_file_names, get_vecs_by_tokens, idx, idx_to_token, idx_to_vec, indices, isinstance, len, line, lower, mkdir_if_not_exist, mxnet, os, path, pretrained_file_name, rstrip, self, sha1, str, tarfile, token, token_to_idx, tokens, unknown_idx, url, utils, vecs, vocabulary, zipfile","mxnet.gluon, mxnet.nd, os.makedirs, os.path.dirname, os.path.exists, os.path.join, os.path.splitext, tarfile.open, zipfile.ZipFile","__init__, __len__, _load_embedding, create, download, download_extract, get_pretrained_file_names, get_vecs_by_tokens, mkdir_if_not_exist",TokenEmbedding,"DATA_URL, PRETRAINED_FILE, base_dir, data_dir, elem, elems, ext, fname, fp, idx, idx_to_token, idx_to_vec, indices, line, path, sha1, token, url, vecs","Download and extract a zip/tar file., Token Embedding."," , .., .gz, .tar, .zip, 0b8703943ccdb6eb788e6f091b8946e82231bc4d, <unk>, Download and extract a zip/tar file., Only zip/tar files can be extracted, Token Embedding., b5116e234e9eb9076672cfeabf5469f3eec904fa, c1816da3821ae9f43899be655002f6c723e91b88, cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a, data, fasttext, glove, glove.42B.300d.zip, glove.42b.300d.txt, glove.6B.100d.zip, glove.6B.50d.zip, glove.6b.100d.txt, glove.6b.50d.txt, http://d2l-data.s3-accelerate.amazonaws.com/, r, vec.txt, wiki.en, wiki.en.zip","0, 1, False, None","# GloVe website: https://nlp.stanford.edu/projects/glove/, # Skip header information, such as the top row in fastText, # fastText website: https://fasttext.cc/, Download and extract a zip/tar file.
fname = download(embedding_name, pretrained_file_name)
base_dir = os.path.dirname(fname)
data_dir, ext = os.path.splitext(fname)
if ext == '.zip':
fp = zipfile.ZipFile(fname, 'r')
elif ext in ('.tar', '.gz'):
fp = tarfile.open(fname, 'r')
else:
assert False, 'Only zip/tar files can be extracted'
fp.extractall(base_dir)
if folder:
return os.path.join(base_dir, folder)
else:
return data_dir

def get_pretrained_file_names(embedding_name=None):
if embedding_name is not None:
return PRETRAINED_FILE[embedding_name].keys()
else:
return PRETRAINED_FILE

def create(embedding_name, pretrained_file_name, vocabulary=None):
return TokenEmbedding(embedding_name, pretrained_file_name.lower(), vocabulary)

class TokenEmbedding:
Token Embedding.",
https://github.com/d2l-ai/d2l-zh,vocab.py,0,0.0,27,81.82,1,3.03,4,12.12,1,3.03,0,0.0,3,1,4,0,,"Vocabulary, __init__, __len__, append, counter, dict, freq, get, idx_to_token, isinstance, items, len, list, min_freq, reserved_tokens, self, sort, sorted, to_indices, token, token_freqs, token_to_idx, tokens, tuple, uniq_tokens, unk, x",,"__init__, __len__, to_indices",Vocabulary,"freq, reserved_tokens, token, uniq_tokens",,<unk>,"0, 1, None, True",# Sort according to frequencies,
https://github.com/d2l-ai/d2l-zh,__init__.py,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,,,"embedding, vocab",,,,,,,,
https://github.com/d2l-ai/d2l-zh,print_versions.py,0,0.0,18,36.0,21,42.0,3,6.0,8,16.0,0,0.0,0,0,1,0,,"d2lbook, flax, framework_name, gpytorch, gym, jax, jaxlib, len, mxnet, os, print, syne_tune, sys, tensorflow, tensorflow_datasets, tensorflow_probability, torch, torchvision","d2lbook.__version__, flax.__version__, gpytorch.__version__, gym.__version__, jax.__version__, jaxlib.__version__, mxnet.__version__, os.system, syne_tune.__version__, sys.argv, tensorflow.__version__, tensorflow_datasets.__version__, tensorflow_probability.__version__, torch.__version__, torchvision.__version__",,,framework_name,,"*, D2L Framework Version Details, Framework Name: , MXNet version: , d2lbook version: , flax version: , gpytorch version: , gym version: , jax, jax version: , jaxlib version: , mxnet, nvcc --version, pytorch, syne_tune version: , tensorflow, tensorflow version: , tensorflow_datasets version: , tensorflow_probability version: , torch version: , torchvision version: ","1, 10, None","# Assume using d2l-builder docker container, # Here all the frameworks are installed and no CUDA support, # Print CUDA version, # Print JAX versions, # Print MXNet versions, # Print PyTorch versions, # Print TensorFlow versions, # Print d2lbook version",
https://github.com/hankcs/HanLP,setup.py,0,0.0,21,27.63,48,63.16,3,3.95,4,5.26,0,0.0,0,0,9,0,,"FASTTEXT, TOKENIZERS, __file__, exec, extras_require, file, fp, list, long_description, major, minor, open, os, read, setuptools, sum, sys, sys_version_info, this_dir, values, version","os.path.abspath, os.path.dirname, os.path.join, setuptools.find_packages, setuptools.setup, sys.platform, sys.version_info",,,"FASTTEXT, TOKENIZERS, extras_require, file, fp, long_description, sys_version_info, this_dir, version",,">=3.6, Apache License 2.0, Development Status :: 4 - Beta, HanLP: Han Language Processing, Intended Audience :: Developers, Intended Audience :: Science/Research, License :: OSI Approved :: Apache Software License, Operating System :: OS Independent, Programming Language :: Python :: 3.10, Programming Language :: Python :: 3.6, Programming Language :: Python :: 3.7, Programming Language :: Python :: 3.8, Programming Language :: Python :: 3.9, README.md, Topic :: Scientific/Engineering :: Artificial Intelligence, Topic :: Text Processing :: Linguistic, __version__, amr, corpus,machine-learning,NLU,NLP, darwin, docs, fasttext, fasttext-wheel==0.9.2, full, hankcs, hankcshe@gmail.com, hanlp, hanlp-common>=0.0.22, hanlp-downloader, hanlp-trie>=0.0.4, https://github.com/hankcs/HanLP, networkx>=2.5.1, penman==1.2.1, perin-parser>=0.0.12, pynvml, sentencepiece>=0.1.91, tensorflow>=2.6.0,<2.14, termcolor, tests*, text/markdown, tf, tokenizers==0.10.3, toposort==1.5, torch>=1.6.0, transformers>=4.1.1, utf-8, version.py, win32","3, 6, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26, # Essential for tokenization_bert_japanese",
https://github.com/hankcs/HanLP,conf.py,0,0.0,44,26.35,71,42.51,5,2.99,47,28.14,0,0.0,0,0,40,0,,"author, autoclass_content, bibtex_default_style, blog_baseurl, blog_path, blog_post_pattern, copyright, datetime, exclude_patterns, execution_show_tb, extensions, fontawesome_included, hanlp, html_baseurl, html_copy_source, html_favicon, html_logo, html_sidebars, html_sourcelink_suffix, html_static_path, html_theme, html_theme_options, html_title, intersphinx_mapping, jupyter_execute_notebooks, language, locale_dirs, master_doc, myst_admonition_enable, myst_deflist_enable, myst_url_schemes, nb_render_priority, nitpick_ignore, numfig, os, panels_add_bootstrap_css, post_auto_excerpt, post_auto_image, project, release, sys, templates_path, thebe_config, version","datetime.datetime, datetime.now, hanlp.__version__, os.environ, os.path.abspath, sys.path.append",,,"author, autoclass_content, bibtex_default_style, blog_baseurl, blog_path, blog_post_pattern, copyright, exclude_patterns, execution_show_tb, extensions, fontawesome_included, html_baseurl, html_copy_source, html_favicon, html_logo, html_sidebars, html_sourcelink_suffix, html_static_path, html_theme, html_theme_options, html_title, intersphinx_mapping, jupyter_execute_notebooks, language, locale_dirs, master_doc, myst_admonition_enable, myst_deflist_enable, myst_url_schemes, nb_render_priority, nitpick_ignore, numfig, panels_add_bootstrap_css, post_auto_excerpt, post_auto_image, project, release, templates_path, thebe_config, version",,", , hankcs, .., ../plugins/hanlp_common, ../plugins/hanlp_restful, ../plugins/hanlp_trie, .DS_Store, 2020-, HanLP, HanLP Documentation, READTHEDOCS, Thumbs.db, _build, _static, _static/favicon.png, _static/logo.png, _templates, ablog, application/javascript, application/vnd.jupyter.widget-view+json, both, cache, docs, docutils.nodes.document, docutils.parsers.rst.directives.body.Sidebar, en, gettext, hankcs, http, https, https://docs.python.org/3.8, https://github.com/binder-examples/jupyter-stacks-datascience, https://github.com/hankcs/HanLP, https://hanlp.hankcs.com/docs/, https://www.sphinx-doc.org/en/3.x, image/jpeg, image/png, image/svg+xml, index, locale/, mailto, master, myst_nb, path_to_docs, py:class, python, reference/blog, reference/blog/*.md, repository_branch, repository_url, sphinx, sphinx.ext.autodoc, sphinx.ext.intersphinx, sphinx.ext.napoleon, sphinx.ext.viewcode, sphinx_astrorefs, sphinx_book_theme, sphinx_copybutton, sphinx_thebe, sphinx_togglebutton, sphinxcontrib.bibtex, text/html, text/latex, text/markdown, text/plain, theme_dev_mode, unsrtalpha, use_download_button, use_edit_page_button, use_issues_button, use_repository_button","1, 2, False, None, True","#, #     ""archives.html"",, #     ""categories.html"",, #     ""colab_url"": ""https://colab.research.google.com/"",, #     ""notebook_interface"": ""jupyterlab"",, #     ""postcard.html"",, #     ""recentposts.html"",, #     ""sbt-sidebar-footer.html"",, #     ""sbt-sidebar-nav.html"",, #     ""sidebar-search-bs.html"",, #     ""tagcloud.html"",, #     ""thebe"": True,, #     # ""binderhub_url"": ""https://mybinder.org"",, #     # ""jupyterhub_url"": ""https://datahub.berkeley.edu"",  # For testing, # ""extra_footer"": ""<a href='https://google.com'>Test</a>"",  # DEPRECATED KEY, # ""extra_navbar"": ""<a href='https://google.com'>Test</a>"",, # ""home_page_in_toc"": True,, # ""launch_buttons"": {, # ""reference/blog/*"": [, # ""repository_branch"": ""gh-pages"",  # For testing, # ""single_page"": True,, # -- ABlog config -------------------------------------------------, # -- General configuration ---------------------------------------------------, # -- Options for HTML output -------------------------------------------------, # -- Project information -----------------------------------------------------, # Add any Sphinx extension module names here, as strings. They can be, # Add any paths that contain custom static files (such as style sheets) here,, # Add any paths that contain templates here, relative to this directory., # For testing, # List of patterns, relative to source directory, that match files and, # Localization, # The full version, including alpha/beta/rc tags., # The language for content autogenerated by Sphinx. Refer to documentation, # The short X.Y version., # The theme to use for HTML and HTML Help pages.  See the documentation for, # This pattern also affects html_static_path and html_extra_path., # ], # a list of builtin themes., # astrophysics style, similar to ACL, # bibtex, # directories to ignore when looking for source files., # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom, # for a list of supported languages., # ones., # relative to this directory. They are copied after the builtin static files,, # so a file named ""default.css"" will overwrite the builtin ""default.css""., # },",
https://github.com/hankcs/HanLP,version.py,0,0.0,3,37.5,2,25.0,0,0.0,3,37.5,0,0.0,0,1,1,0,,"Exception, NotCompatible, __version__",,,NotCompatible,__version__,,"2.1.0-beta.64, HanLP version",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,19,67.86,3,10.71,1,3.57,5,17.86,0,0.0,2,0,2,2,,"ALL, Component, Pipeline, common, component, components, get, hanlp, hanlp_common, kwargs, load, ls_resource_in_module, pipeline, pipes, pretrained, save_dir, str, utils, verbose","hanlp.utils.component_util.load_from_meta_file, hanlp.version.__version__, hanlp_common.constant.HANLP_VERBOSE","load, pipeline",,"save_dir, verbose","Creates a pipeline of components. It's made for bundling `KerasComponents`. For `TorchComponent`, use
:class:`~hanlp.components.mtl.multi_task_learning.MultiTaskLearning` instead.

Args:
  *pipes: Components if pre-defined any.

Returns:
  hanlp.components.pipeline.Pipeline: A pipeline, which is a list of components in order., Load a pretrained component from an identifier.

Args:
  save_dir (str): The identifier to the saved component. It could be a remote URL or a local path.
  verbose: ``True`` to print loading progress.
  **kwargs: Arguments passed to :func:`hanlp.common.torch_component.TorchComponent.load`, e.g.,
    ``devices`` is a useful argument to specify which GPU devices a PyTorch component will use.

Examples::

    import hanlp
    # Load component onto the 0-th GPU.
    hanlp.load(..., devices=0)
    # Load component onto the 0-th and 1-st GPUs using data parallelization.
    hanlp.load(..., devices=[0, 1])

.. Note::
    A component can have dependencies on other components or resources, which will be recursively loaded. So it's
    common to see multiple downloading messages per single load.

Returns:
  hanlp.common.component.Component: A pretrained component.","Creates a pipeline of components. It's made for bundling `KerasComponents`. For `TorchComponent`, use
    :class:`~hanlp.components.mtl.multi_task_learning.MultiTaskLearning` instead.

    Args:
      *pipes: Components if pre-defined any.

    Returns:
      hanlp.components.pipeline.Pipeline: A pipeline, which is a list of components in order.

    , Load a pretrained component from an identifier.

    Args:
      save_dir (str): The identifier to the saved component. It could be a remote URL or a local path.
      verbose: ``True`` to print loading progress.
      **kwargs: Arguments passed to :func:`hanlp.common.torch_component.TorchComponent.load`, e.g.,
        ``devices`` is a useful argument to specify which GPU devices a PyTorch component will use.

    Examples::

        import hanlp
        # Load component onto the 0-th GPU.
        hanlp.load(..., devices=0)
        # Load component onto the 0-th and 1-st GPUs using data parallelization.
        hanlp.load(..., devices=[0, 1])

    .. Note::
        A component can have dependencies on other components or resources, which will be recursively loaded. So it's
        common to see multiple downloading messages per single load.

    Returns:
      hanlp.common.component.Component: A pretrained component.

    , meta.json",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 18:05, Creates a pipeline of components. It's made for bundling `KerasComponents`. For `TorchComponent`, use
:class:`~hanlp.components.mtl.multi_task_learning.MultiTaskLearning` instead.

Args:
*pipes: Components if pre-defined any.

Returns:
hanlp.components.pipeline.Pipeline: A pipeline, which is a list of components in order.

, Load a pretrained component from an identifier.

Args:
save_dir (str): The identifier to the saved component. It could be a remote URL or a local path.
verbose: ``True`` to print loading progress.
**kwargs: Arguments passed to :func:`hanlp.common.torch_component.TorchComponent.load`, e.g.,
``devices`` is a useful argument to specify which GPU devices a PyTorch component will use.

Examples::

import hanlp
# Load component onto the 0-th GPU.
hanlp.load(..., devices=0)
# Load component onto the 0-th and 1-st GPUs using data parallelization.
hanlp.load(..., devices=[0, 1])

.. Note::
A component can have dependencies on other components or resources, which will be recursively loaded. So it's
common to see multiple downloading messages per single load.

Returns:
hanlp.common.component.Component: A pretrained component.

",
https://github.com/hankcs/HanLP,test_config_tracker.py,0,0.0,14,77.78,3,16.67,1,5.56,0,0.0,0,0.0,2,2,1,0,,"MyClass, TestConfigTracker, __init__, __name__, assertEqual, config, get, hanlp, locals, obj, self, super, test_init, unittest","hanlp.common.structure.ConfigTracker, unittest.TestCase, unittest.main","__init__, test_init","MyClass, TestConfigTracker",obj,,"__main__, i_need_this, yes",None,,
https://github.com/hankcs/HanLP,test_mtl.py,0,0.0,38,27.54,56,40.58,7,5.07,0,0.0,37,26.81,12,1,9,0,,"TestMultiTaskLearning, __name__, assertDictEqual, assertEqual, assertSequenceEqual, b, config, dict_combine, dict_force, hanlp, hanlp_common, mtl, multiprocessing, num_proc, output_spans, pool, pre_tokenized_sents, results, self, sent, starmap, task, test_emoji, test_mtl_empty_str, test_mtl_multiple_sents, test_mtl_single_sent, test_sdp_as_the_first_task, test_skip_tok, test_space, test_threading, test_tok_offset, test_transform, test_unicode_removed_by_hf, text, to_dict, tok, tokenize, unittest","hanlp.load, hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, hanlp_common.document.Document, multiprocessing.dummy.Pool, unittest.TestCase, unittest.main","test_emoji, test_mtl_empty_str, test_mtl_multiple_sents, test_mtl_single_sent, test_sdp_as_the_first_task, test_skip_tok, test_space, test_threading, test_tok_offset, test_transform, test_unicode_removed_by_hf, tokenize",TestMultiTaskLearning,"b, mtl, num_proc, pool, pre_tokenized_sents, results, sent, task, tok",,",  ,  ͜,  ͡,  ͡ , (, ( ͡° ͜ʖ ͡ °), ( ͡° ͜ʖ ͡ °)你好, ), 2个空格, Agt, Pat, Pro, Root, __main__, good, iPad, iPad  Pro, iPad Pro, id, sdp, tok, tok*, tok/fine, °, ʖ, ͡, 一个, 不同, 人, 你, 先去, 吃, 和, 商品, 商品 和服务, 商品和服务, 好, 如何, 如何评价iPad Pro ？iPad  Pro有2个空格, 我, 我先去看医生, 我的用户ID跟你的用户id不同, 有, 服务, 生命, 用户, 用户ID, 的, 研究, 研究生命, 评价, 词, 跟, 鱼, ？","0, 1, 2, 8, False, None, True",,"2个空格, Pro有2个空格, °, ʖ, ͜ʖ, ͡, ͡°, 一个, 不同, 人, 你, 你好, 先去, 吃, 和, 和服务, 商品, 商品和服务, 好, 如何, 如何评价iPad, 我, 我先去看医生, 我的用户ID跟你的用户id不同, 有, 服务, 生命, 用户, 用户ID, 的, 研究, 研究生命, 评价, 词, 跟, 鱼, ？iPad"
https://github.com/hankcs/HanLP,test_pipeline.py,0,0.0,10,71.43,3,21.43,0,0.0,0,0.0,1,7.14,1,1,3,0,,"TestPipeLine, __name__, append, copied_pipe, copy, hanlp, pipe, test_copy, test_text, unittest","hanlp.pipeline, hanlp.utils.rules.split_sentence, unittest.TestCase, unittest.main",test_copy,TestPipeLine,"copied_pipe, pipe, test_text",,", __main__, 今天天气真好。我要去散步。",,,今天天气真好。我要去散步。
https://github.com/hankcs/HanLP,test_rules.py,0,0.0,8,34.78,8,34.78,0,0.0,3,13.04,4,17.39,1,1,0,0,,"TestRules, __name__, assertListEqual, hanlp, list, self, test_eos, unittest","hanlp.utils.rules.split_sentence, unittest.TestCase, unittest.main",test_eos,TestRules,,,"Go to hankcs.com., Go to hankcs.com. Yes., Yes., __main__, 他说：“加油。”, 他说：“加油。”谢谢, 叶, 谢谢",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-03-22 17:17","他说：“加油。”, 他说：“加油。”谢谢, 叶, 谢谢"
https://github.com/hankcs/HanLP,test_string_util.py,0,0.0,10,52.63,3,15.79,2,10.53,3,15.79,1,5.26,1,1,3,0,,"TestStringUtility, __name__, each, hanlp, len, set, test_enumerate_tokenization, text, toks, unittest","hanlp.utils.string_util.possible_tokenization, unittest.TestCase, unittest.main",test_enumerate_tokenization,TestStringUtility,"each, text, toks",,", __main__, 商品和服务","1, 2","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-03-22 17:17",商品和服务
https://github.com/hankcs/HanLP,__init__.py,0,0.0,4,44.44,1,11.11,0,0.0,4,44.44,0,0.0,1,0,1,1,,"__file__, cdroot, os, root","os.chdir, os.pardir, os.path.abspath, os.path.dirname, os.path.join",cdroot,,root,"cd to project root, so models are saved in the root folder","
    cd to project root, so models are saved in the root folder
    ",,"
cd to project root, so models are saved in the root folder
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 23:43",
https://github.com/hankcs/HanLP,setup.py,0,0.0,8,20.51,26,66.67,1,2.56,4,10.26,0,0.0,0,0,3,0,,"__file__, file, long_description, open, os, read, setuptools, this_dir","os.path.abspath, os.path.dirname, os.path.join, setuptools.find_packages, setuptools.setup",,,"file, long_description, this_dir",,"0.0.22, >=3.6, Apache License 2.0, Development Status :: 3 - Alpha, HanLP: Han Language Processing, Intended Audience :: Developers, Intended Audience :: Science/Research, License :: OSI Approved :: Apache Software License, Operating System :: OS Independent, Programming Language :: Python :: 3 :: Only, README.md, Topic :: Scientific/Engineering :: Artificial Intelligence, Topic :: Text Processing :: Linguistic, corpus,machine-learning,NLU,NLP, docs, full, hankcs, hankcshe@gmail.com, hanlp_common, https://github.com/hankcs/HanLP, networkx, penman==0.6.2, phrasetree>=0.0.9, tests*, text/markdown, utf-8",True,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26, # These AMR dependencies might not be necessary for most people.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-16 22:20",
https://github.com/hankcs/HanLP,setup.py,0,0.0,8,22.86,23,65.71,1,2.86,3,8.57,0,0.0,0,0,3,0,,"__file__, file, long_description, open, os, read, setuptools, this_dir","os.path.abspath, os.path.dirname, os.path.join, setuptools.find_packages, setuptools.setup",,,"file, long_description, this_dir",,"0.0.1, >=3.6, Apache License 2.0, Development Status :: 3 - Alpha, HanLP: Han Language Processing, Intended Audience :: Developers, Intended Audience :: Science/Research, License :: OSI Approved :: Apache Software License, Operating System :: OS Independent, Programming Language :: Python :: 3 :: Only, README.md, Topic :: Scientific/Engineering :: Artificial Intelligence, Topic :: Text Processing :: Linguistic, corpus,machine-learning,NLU,NLP, docs, hankcs, hankcshe@gmail.com, hanlp_common, hanlp_demo, https://github.com/hankcs/HanLP, tests*, text/markdown, utf-8",True,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26",
https://github.com/hankcs/HanLP,setup.py,0,0.0,8,22.86,23,65.71,1,2.86,3,8.57,0,0.0,0,0,3,0,,"__file__, file, long_description, open, os, read, setuptools, this_dir","os.path.abspath, os.path.dirname, os.path.join, setuptools.find_packages, setuptools.setup",,,"file, long_description, this_dir",,"0.0.23, >=3.6, Apache License 2.0, Development Status :: 3 - Alpha, HanLP: Han Language Processing, Intended Audience :: Developers, Intended Audience :: Science/Research, License :: OSI Approved :: Apache Software License, Operating System :: OS Independent, Programming Language :: Python :: 3 :: Only, README.md, Topic :: Scientific/Engineering :: Artificial Intelligence, Topic :: Text Processing :: Linguistic, corpus,machine-learning,NLU,NLP, docs, hankcs, hankcshe@gmail.com, hanlp_common, hanlp_restful, https://github.com/hankcs/HanLP, tests*, text/markdown, utf-8",True,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26",
https://github.com/hankcs/HanLP,setup.py,0,0.0,8,22.86,23,65.71,1,2.86,3,8.57,0,0.0,0,0,3,0,,"__file__, file, long_description, open, os, read, setuptools, this_dir","os.path.abspath, os.path.dirname, os.path.join, setuptools.find_packages, setuptools.setup",,,"file, long_description, this_dir",,"0.0.5, >=3.6, Apache License 2.0, Development Status :: 3 - Alpha, HanLP: Han Language Processing, Intended Audience :: Developers, Intended Audience :: Science/Research, License :: OSI Approved :: Apache Software License, Operating System :: OS Independent, Programming Language :: Python :: 3 :: Only, README.md, Topic :: Scientific/Engineering :: Artificial Intelligence, Topic :: Text Processing :: Linguistic, corpus,machine-learning,NLU,NLP, docs, hankcs, hankcshe@gmail.com, hanlp_common, hanlp_trie, https://github.com/hankcs/HanLP, tests*, text/markdown, utf-8",True,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:26",
https://github.com/hankcs/HanLP,dictionary.py,0,0.0,62,74.7,9,10.84,3,3.61,9,10.84,0,0.0,7,3,27,7,,"DictInterface, TrieDict, TupleTrieDict, __init__, _d, _value, abc, append, begin, config, data, dict, dictionary, dst, end, enumerate, found, hanlp_common, hanlp_trie, head, idx, info, int, isinstance, items, k, label, len, list, merge_batch, new_data, new_data_belongs, new_outputs, offset, outputs, parse_longest, parts, pop, pre_start, property, range, segments, self, sent, spans, split, split_batch, start, state, staticmethod, sum, super, text, to, token, tokenize, transit, tuple, typing, v, value, zip","abc.ABC, abc.abstractmethod, hanlp_common.configurable.Configurable, hanlp_common.reflection.classpath_of, hanlp_trie.trie.Trie, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Optional, typing.Sequence, typing.Tuple, typing.Union","__init__, config, merge_batch, parse_longest, split, split_batch, tokenize","DictInterface, TrieDict, TupleTrieDict","_d, begin, dictionary, dst, end, found, head, i, idx, info, k, label, new_data, new_data_belongs, offset, outputs, parts, pre_start, segments, sent, spans, start, state, to, token, v, value","A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with
a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each
key is assigned with a boolean value ``True``.

Args:
    dictionary: A custom dictionary of string-value pairs., A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with
a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each
key is assigned with a boolean value ``True``. In comparison to ``TrieDict``, ``TupleTrieDict`` additionally
supports serializing/deserializing tuple-as-keys dict.

Args:
    dictionary: A custom dictionary of string-value pairs., A handy method to perform longest-prefix-matching on a batch of sentences. It tokenize each sentence, record
the chunks being either a key in the dict or a span outside of the dict. The spans are then packed into a new
batch and returned along with the following information:

    - which sentence a span belongs to
    - the matched keys along with their spans and values.

This method bridges the gap between statistical models and rule-based gazetteers.
It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.merge_batch`.

Args:
    data: A batch of sentences.

Returns:
    A tuple of the new batch, the belonging information and the keys., A helper method to merge the outputs of split batch back by concatenating the output per span with the key
used to split it. It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.split_batch`.

Args:
    data: Split batch.
    new_outputs: Outputs of the split batch.
    new_data_belongs: Belonging information.
    parts: The keys.

Returns:
    Merged outputs., Implement this method to tokenize a piece of text into a list of non-intersect spans, each span is a tuple
of ``(begin_offset, end_offset, label)``, where label is some properties related to this span and downstream
tasks have the freedom to define what kind of labels they want.

Args:
    text: The text to be tokenized.

Returns:
      A list of tokens., Like the :meth:`str.split`, this method splits a piece of text into chunks by taking the keys in this
dictionary as delimiters. It performs longest-prefix-matching on text and split it whenever a longest key is
matched. Unlike the :meth:`str.split`, it inserts matched keys into the results list right after where they are
found. So that the text can be restored by joining chunks in the results list.

Args:
    text: A piece of text.

Returns:
    A list of chunks, each chunk is a span of ``(begin_offset, end_offset, label)``, where label is some
    properties related to this span and downstream tasks., Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
its tail. By definition, the matches won't overlap with each other.

Args:
    text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
        The trie will transit on either types properly, which means a list of str simply defines a list of
        transition criteria while a str defines each criterion as a character.

Returns:
    A tuple of ``(begin, end, value)``.","
        A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with
        a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each
        key is assigned with a boolean value ``True``.

        Args:
            dictionary: A custom dictionary of string-value pairs.
        , 
        A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with
        a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each
        key is assigned with a boolean value ``True``. In comparison to ``TrieDict``, ``TupleTrieDict`` additionally
        supports serializing/deserializing tuple-as-keys dict.

        Args:
            dictionary: A custom dictionary of string-value pairs.
        ,  A handy method to perform longest-prefix-matching on a batch of sentences. It tokenize each sentence, record
        the chunks being either a key in the dict or a span outside of the dict. The spans are then packed into a new
        batch and returned along with the following information:

            - which sentence a span belongs to
            - the matched keys along with their spans and values.

        This method bridges the gap between statistical models and rule-based gazetteers.
        It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.merge_batch`.

        Args:
            data: A batch of sentences.

        Returns:
            A tuple of the new batch, the belonging information and the keys.
        ,  A helper method to merge the outputs of split batch back by concatenating the output per span with the key
        used to split it. It's used in conjunction with :meth:`~hanlp_trie.dictionary.TrieDict.split_batch`.

        Args:
            data: Split batch.
            new_outputs: Outputs of the split batch.
            new_data_belongs: Belonging information.
            parts: The keys.

        Returns:
            Merged outputs.
        , Implement this method to tokenize a piece of text into a list of non-intersect spans, each span is a tuple
        of ``(begin_offset, end_offset, label)``, where label is some properties related to this span and downstream
        tasks have the freedom to define what kind of labels they want.

        Args:
            text: The text to be tokenized.

        Returns:
              A list of tokens.

        , Like the :meth:`str.split`, this method splits a piece of text into chunks by taking the keys in this
        dictionary as delimiters. It performs longest-prefix-matching on text and split it whenever a longest key is
        matched. Unlike the :meth:`str.split`, it inserts matched keys into the results list right after where they are
        found. So that the text can be restored by joining chunks in the results list.

        Args:
            text: A piece of text.

        Returns:
            A list of chunks, each chunk is a span of ``(begin_offset, end_offset, label)``, where label is some
            properties related to this span and downstream tasks.
        , Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
        its tail. By definition, the matches won't overlap with each other.

        Args:
            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
                The trie will transit on either types properly, which means a list of str simply defines a list of
                transition criteria while a str defines each criterion as a character.

        Returns:
            A tuple of ``(begin, end, value)``.

        , classpath, dictionary","0, 1, None","
new_data, new_data_belongs, parts = [], [], []
for idx, sent in enumerate(data):
parts.append([])
found = self.tokenize(sent)
if found:
pre_start = 0
for start, end, info in found:
if start > pre_start:
new_data.append(sent[pre_start:start])
new_data_belongs.append(idx)
pre_start = end
parts[idx].append((start, end, info))
if pre_start != len(sent):
new_data.append(sent[pre_start:])
new_data_belongs.append(idx)
else:
new_data.append(sent)
new_data_belongs.append(idx)
return new_data, new_data_belongs, parts

@staticmethod
def merge_batch(data, new_outputs, new_data_belongs, parts):
 A helper method to merge the outputs of split batch back by concatenating the output per span with the key, 
outputs = []
segments = []
for idx in range(len(data)):
segments.append([])
for o, b in zip(new_outputs, new_data_belongs):
dst = segments[b]
dst.append(o)
for s, p, sent in zip(segments, parts, data):
s: list = s
if p:
dst = []
offset = 0
for start, end, info in p:
while offset < start:
head = s.pop(0)
offset += sum(len(token) for token in head)
dst += head
if isinstance(info, list):
dst += info
elif isinstance(info, str):
dst.append(info)
else:
dst.append(sent[start:end])
offset = end
if s:
assert len(s) == 1
dst += s[0]
outputs.append(dst)
else:
outputs.append(s[0])
return outputs

@property
def config(self):
return {
'classpath': classpath_of(self),
'dictionary': dict(self.items())
}


class TupleTrieDict(TrieDict):
def __init__(self, dictionary: Optional[Union[Dict[Iterable[str], Any], Iterable[str]]] = None) -> None:
r""""""
A dict-like structure for fast custom dictionary strategies in tokenization and tagging. It is built with
a dict of key-value pairs or a set of strings. When a set is passed in, it will be turned into a dict where each
key is assigned with a boolean value ``True``. In comparison to ``TrieDict``, ``TupleTrieDict`` additionally
supports serializing/deserializing tuple-as-keys dict.

Args:
dictionary: A custom dictionary of string-value pairs.
, 
super().__init__(dictionary)

def tokenize(self, text: Union[str, Sequence[str]]) -> List[Tuple[int, int, Any]]:
return self.parse_longest(text)

def split_batch(self, data: List[str]) -> Tuple[List[str], List[int], List[List[Tuple[int, int, Any]]]]:
 A handy method to perform longest-prefix-matching on a batch of sentences. It tokenize each sentence, record, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 17:53, Implement this method to tokenize a piece of text into a list of non-intersect spans, each span is a tuple
of ``(begin_offset, end_offset, label)``, where label is some properties related to this span and downstream
tasks have the freedom to define what kind of labels they want.

Args:
text: The text to be tokenized.

Returns:
A list of tokens.

, Like the :meth:`str.split`, this method splits a piece of text into chunks by taking the keys in this
dictionary as delimiters. It performs longest-prefix-matching on text and split it whenever a longest key is
matched. Unlike the :meth:`str.split`, it inserts matched keys into the results list right after where they are
found. So that the text can be restored by joining chunks in the results list.

Args:
text: A piece of text.

Returns:
A list of chunks, each chunk is a span of ``(begin_offset, end_offset, label)``, where label is some
properties related to this span and downstream tasks.
, Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
its tail. By definition, the matches won't overlap with each other.

Args:
text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
The trie will transit on either types properly, which means a list of str simply defines a list of
transition criteria while a str defines each criterion as a character.

Returns:
A tuple of ``(begin, end, value)``.

",
https://github.com/hankcs/HanLP,trie.py,0,0.0,52,73.24,6,8.45,5,7.04,8,11.27,0,0.0,14,2,13,5,,"Node, Trie, __bool__, __contains__, __delitem__, __getitem__, __init__, __len__, __setitem__, _children, _get_or_add_child, _size, _value, _walk, append, bool, char, child, dic, dict, end, found, get, int, isinstance, items, j, k, key, leaf, len, object, ordered, parse, parse_longest, prefix, prefix_new, range, self, sorted, state, str, super, text, to, tokens, transit, tuple, typing, update, v, value","typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Optional, typing.Sequence, typing.Tuple, typing.Union","__bool__, __contains__, __delitem__, __getitem__, __init__, __len__, __setitem__, _get_or_add_child, _walk, items, parse, parse_longest, transit, update","Node, Trie","char, child, end, found, i, j, k, leaf, prefix_new, state, to, v, value","A node in a trie tree.

Args:
    value: The value associated with this node., A referential implementation of the trie (:cite:`10.1145/1457838.1457895`) structure. It stores a dict by
assigning each key/value pair a :class:`~hanlp_trie.trie.Node` in a trie tree. It provides get/set/del/items
methods just like a :class:`dict` does. Additionally, it also provides longest-prefix-matching and keywords
lookup against a piece of text, which are very helpful in rule-based Natural Language Processing.

Args:
    tokens: A set of keys or a dict mapping., Keywords lookup which takes a piece of text as input, and lookup all occurrences of keywords in it. These
occurrences can overlap with each other.

Args:
    text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
        The trie will transit on either types properly, which means a list of str simply defines a list of
        transition criteria while a str defines each criterion as a character.

Returns:
    A tuple of ``(begin, end, value)``., Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
its tail. By definition, the matches won't overlap with each other.

Args:
    text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
        The trie will transit on either types properly, which means a list of str simply defines a list of
        transition criteria while a str defines each criterion as a character.

Returns:
    A tuple of ``(begin, end, value)``., Transit the state of a Deterministic Finite Automata (DFA) with key.

Args:
    key: A sequence of criterion (tokens or characters) used to transit to a new state.

Returns:
    A new state if the transition succeeded, otherwise ``None``.",", A node in a trie tree.

        Args:
            value: The value associated with this node.
        , A referential implementation of the trie (:cite:`10.1145/1457838.1457895`) structure. It stores a dict by
        assigning each key/value pair a :class:`~hanlp_trie.trie.Node` in a trie tree. It provides get/set/del/items
        methods just like a :class:`dict` does. Additionally, it also provides longest-prefix-matching and keywords
        lookup against a piece of text, which are very helpful in rule-based Natural Language Processing.

        Args:
            tokens: A set of keys or a dict mapping.
        , Keywords lookup which takes a piece of text as input, and lookup all occurrences of keywords in it. These
        occurrences can overlap with each other.

        Args:
            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
                The trie will transit on either types properly, which means a list of str simply defines a list of
                transition criteria while a str defines each criterion as a character.

        Returns:
            A tuple of ``(begin, end, value)``.
        , Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
        its tail. By definition, the matches won't overlap with each other.

        Args:
            text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
                The trie will transit on either types properly, which means a list of str simply defines a list of
                transition criteria while a str defines each criterion as a character.

        Returns:
            A tuple of ``(begin, end, value)``.

        , Transit the state of a Deterministic Finite Automata (DFA) with key.

        Args:
            key: A sequence of criterion (tokens or characters) used to transit to a new state.

        Returns:
            A new state if the transition succeeded, otherwise ``None``.

        ","0, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-04 23:46, A node in a trie tree.

Args:
value: The value associated with this node.
, A referential implementation of the trie (:cite:`10.1145/1457838.1457895`) structure. It stores a dict by
assigning each key/value pair a :class:`~hanlp_trie.trie.Node` in a trie tree. It provides get/set/del/items
methods just like a :class:`dict` does. Additionally, it also provides longest-prefix-matching and keywords
lookup against a piece of text, which are very helpful in rule-based Natural Language Processing.

Args:
tokens: A set of keys or a dict mapping.
, Keywords lookup which takes a piece of text as input, and lookup all occurrences of keywords in it. These
occurrences can overlap with each other.

Args:
text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
The trie will transit on either types properly, which means a list of str simply defines a list of
transition criteria while a str defines each criterion as a character.

Returns:
A tuple of ``(begin, end, value)``.
, Longest-prefix-matching which tries to match the longest keyword sequentially from the head of the text till
its tail. By definition, the matches won't overlap with each other.

Args:
text: A piece of text. In HanLP's design, it doesn't really matter whether this is a str or a list of str.
The trie will transit on either types properly, which means a list of str simply defines a list of
transition criteria while a str defines each criterion as a character.

Returns:
A tuple of ``(begin, end, value)``.

, Transit the state of a Deterministic Finite Automata (DFA) with key.

Args:
key: A sequence of criterion (tokens or characters) used to transit to a new state.

Returns:
A new state if the transition succeeded, otherwise ``None``.

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,2,40.0,0,0.0,0,0.0,3,60.0,0,0.0,0,0,0,0,,"dictionary, trie","dictionary.DictInterface, dictionary.TrieDict, trie.Trie",,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 17:48",
https://github.com/hankcs/HanLP,test_trie.py,0,0.0,24,50.0,13,27.08,5,10.42,0,0.0,6,12.5,6,1,8,0,,"TestTrie, __name__, assertEqual, assert_results_valid, begin, build_small_trie, end, hanlp_trie, items, len, list, parse, parse_longest, parse_longest_result, parse_result, results, self, test_items, test_len, test_parse, test_parse_longest, text, unittest, value","hanlp_trie.Trie, unittest.TestCase, unittest.main","assert_results_valid, build_small_trie, test_items, test_len, test_parse, test_parse_longest",TestTrie,"begin, end, items, parse_longest_result, parse_result, text, trie, value",,"&, __main__, and, business, goods, kimono, service, 务, 和, 和服, 商品, 商品和服务, 服务","0, 2, 3, 4, 5",,"务, 和, 和服, 商品, 商品和服务, 服务"
https://github.com/hankcs/HanLP,test_trie_dict.py,0,0.0,28,40.0,17,24.29,11,15.71,0,0.0,14,20.0,5,1,8,0,,"TestTrieDict, __name__, assertEqual, assertFalse, assertSequenceEqual, assertTrue, bool, data, hanlp_trie, list, merge_batch, new_data, new_data_belongs, parts, predictions, self, setUp, split_batch, super, test_empty_dict, test_split_batch, test_tokenize, test_tokenize_2, text, tokenize, trie_dict, unittest, x","hanlp_trie.TrieDict, unittest.TestCase, unittest.main","setUp, test_empty_dict, test_split_batch, test_tokenize, test_tokenize_2",TestTrieDict,"data, new_data, new_data_belongs, parts, predictions, t, trie_dict, x",,"2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, __main__, important, one, 一, 个, 也, 二, 很, 次世代, 生产环境, 第, 第一个词语很重要，第二个词语也很重要, 词, 语, 重要, ，","1, 15, 16, 18, 19, 21, 24, 6, 8, None, True",,"1为生产环境带来次世代最先进的多语种NLP技术。, 2021年HanLPv2, 一, 个, 也, 二, 很, 次世代, 生产环境, 第, 第一个词语很重要，第二个词语也很重要, 词, 语, 重要"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 18:05",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,76,25.25,52,17.28,7,2.33,28,9.3,138,45.85,23,1,10,16,,"HanLPClient, ImportError, NotImplementedError, __call__, __init__, _add_headers, _auth, _fire_request, _language, _post, _send_get, _send_get_json, _send_post, _send_post_json, _timeout, _url, _verify, about, abstract_meaning_representation, abstractive_summarization, add_header, auth, bool, check_hostname, coarse, coreference_resolution, ctx, dict, extractive_summarization, float, form, grammatical_error_correction, hanlp_common, headers, info, int, items, iter, json, k, keyphrase_extraction, language, language_identification, model, next, object, os, parse, prob, requests, response, self, semantic_textual_similarity, sentiment_analysis, skip_tasks, speakers, ssl, status_code, str, super, target_style, tasks, text, text_classification, text_style_transfer, timeout, tokenize, tokens, topk, typing, urllib, v, values, verify, verify_mode, visualization","hanlp_common.document.Document, json.dumps, json.loads, os.getenv, requests.post, ssl.CERT_NONE, ssl.create_default_context, typing.Any, typing.Dict, typing.List, typing.Optional, typing.Tuple, typing.Union, urllib.error.HTTPError, urllib.parse.urlencode, urllib.request.Request, urllib.request.urlopen","__call__, __init__, _add_headers, _fire_request, _post, _send_get, _send_get_json, _send_post, _send_post_json, about, abstract_meaning_representation, abstractive_summarization, coreference_resolution, extractive_summarization, grammatical_error_correction, keyphrase_extraction, language_identification, parse, semantic_textual_similarity, sentiment_analysis, text_classification, text_style_transfer, tokenize",HanLPClient,"auth, ctx, doc, headers, info, k, language, request, response, v","A shortcut of :meth:`~hanlp_restful.HanLPClient.parse`., Abstract Meaning Representation (AMR) captures “who is doing what to whom” in a sentence. Each sentence is
represented as a rooted, directed, acyclic graph consisting of nodes (concepts) and edges (relations).

Args:
    text: A document (str), or a list of sentences (List[str]).
    tokens: A list of sentences where each sentence is a list of tokens.
    language: The language of input text or tokens. ``None`` to use the default language on server.
    visualization: Set to `dot` or `svg` to obtain coresspodning visualization.

Returns:
    Graphs in meaning represenation format.

Examples::

    HanLP.abstract_meaning_representation('男孩希望女孩相信他。')
    HanLP.abstract_meaning_representation('The boy wants the girl to believe him.',
                                          language='en')

.. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=%E7%94%B7%E5%AD%A9%20%E5%B8%8C%E6%9C%9B%20%E5%A5%B3%E5%AD%A9%20%E7%9B%B8%E4%BF%A1%20%E4%BB%96%20%E3%80%82&language=zh&scale=1
    :alt: Abstract Meaning Representation

.. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=The%20boy%20wants%20the%20girl%20to%20believe%20him%20.&language=en&scale=1
    :alt: Abstract Meaning Representation, Abstractive Summarization is the task of generating a short and concise summary that captures the
salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that
may not appear in the source text.

Args:
    text: The text content of the document.
    language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
    Summarization.

Examples::

    HanLP.abstractive_summarization('''
    每经AI快讯，2月4日，长江证券研究所金属行业首席分析师王鹤涛表示，2023年海外经济衰退，美债现处于历史高位，
    黄金的趋势是值得关注的；在国内需求修复的过程中，看好大金属品种中的铜铝钢。
    此外，在细分的小品种里，建议关注两条主线，一是新能源，比如锂、钴、镍、稀土，二是专精特新主线。（央视财经）
    ''')
    # Output:
    '长江证券：看好大金属品种中的铜铝钢', Args:
    url (str): An API endpoint to a service provider.
    auth (str): An auth key licenced from a service provider.
    language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.
        Contact the service provider for the list of languages supported.
        Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.
        Leave ``None`` to use the default language on server.
    timeout (int): Maximum waiting time in seconds for a request.
    verify (bool): ``True`` to enable SSL cert verification. You can also pass ``verify`` the path to a CA_BUNDLE
        file or directory with certificates of trusted CAs (``requests`` required)., Coreference resolution is the task of clustering mentions in text that refer to the same underlying
real world entities.

Args:
    text: A piece of text, usually a document without tokenization.
    tokens: A list of sentences where each sentence is a list of tokens.
    speakers: A list of speakers where each speaker is a ``str`` representing the speaker's ID, e.g., ``Tom``.
    language: The language of input text. ``None`` to use the default language.

Returns:
    When ``text`` is specified, return the clusters and tokens. Otherwise just the clusters, In this case, you need to ``sum(tokens, [])`` in order to match the span indices with tokens

Examples::

    HanLP.coreference_resolution('我姐送我她的猫。我很喜欢它。')
    # Output:
    {'clusters': [
                  [['我', 0, 1], ['我', 3, 4], ['我', 8, 9]], # 指代说话人
                  [['我姐', 0, 2], ['她', 4, 5]],             # 指代说话人的姐姐
                  [['她的猫', 4, 7], ['它', 11, 12]]],        # 指代说话人的姐姐的猫
     'tokens': ['我', '姐', '送', '我', '她', '的', '猫', '。',
                '我', '很', '喜欢', '它', '。']}

    HanLP.coreference_resolution(
    tokens=[['我', '姐', '送', '我', '她', '的', '猫', '。'],
            ['我', '很', '喜欢', '它', '。']])
    # Output:
                 [
                  [['我', 0, 1], ['我', 3, 4], ['我', 8, 9]], # 指代说话人
                  [['我姐', 0, 2], ['她', 4, 5]],             # 指代说话人的姐姐
                  [['她的猫', 4, 7], ['它', 11, 12]]],        # 指代说话人的姐姐的猫

.. image:: https://file.hankcs.com/img/coref_demo_small.png
    :alt: Coreference resolution visualization, Get the information about server and your client.

Returns:
    A dict containing your rate limit and server version etc., Grammatical Error Correction (GEC) is the task of correcting different kinds of errors in text such as
spelling, punctuation, grammatical, and word choice errors.

Args:
    text: Text potentially containing different kinds of errors such as spelling, punctuation,
        grammatical, and word choice errors.
    language: The language of input text. ``None`` to use the default language.

Returns:
    Corrected text.

Examples::

    HanLP.grammatical_error_correction(['每个青年都应当有远大的报复。',
                                        '有的同学对语言很兴趣。'])
    # Output:
    [
        '每个青年都应当有远大的抱负。',
        '有的同学对语言很有兴趣。'
    ], Identify the language of a given text.

Args:
    text: A document or a list of documents.
    topk: ``True`` or ``int`` to return the top-k languages.
    prob: Return also probabilities.

Returns:

    Identified language in `ISO 639-1 codes`_.

Examples::

    HanLP.language_identification(
    'In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques.')
    'en'
    lang, prob = HanLP.language_identification(
    '2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。', prob=True)
    ('ja', 0.9976244568824768)
    HanLP.language_identification(
    '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', topk=2)
    ['zh', 'ja']
    HanLP.language_identification(
    '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', topk=3, prob=True)
    {'zh': 0.3952908217906952, 'en': 0.37189167737960815, 'ja': 0.056213412433862686}

.. _ISO 639-1 codes:
   https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes, Keyphrase extraction aims to identify keywords or phrases reflecting the main topics of a document.

Args:
    text: The text content of the document. Preferably the concatenation of the title and the content.
    topk: The number of top-K ranked keywords or keyphrases.
    language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
    A dictionary containing each keyword or keyphrase and its ranking score :math:`s`, :math:`s \in [0, 1]`.

Examples::

    HanLP.keyphrase_extraction(
        '自然语言处理是一门博大精深的学科，掌握理论才能发挥出HanLP的全部性能。 '
        '《自然语言处理入门》是一本配套HanLP的NLP入门书，助你零起点上手自然语言处理。', topk=3)
    # Output:
    {'自然语言处理': 0.800000011920929,
     'HanLP的全部性能': 0.5258446335792542,
     '一门博大精深的学科': 0.421421080827713}, Parse a piece of text.

Args:
    text: A document (str), or a list of sentences (List[str]).
    tokens: A list of sentences where each sentence is a list of tokens.
    tasks: The tasks to predict. Use ``tasks=[...]`` to run selected tasks only. Dependent tasks will be
        automatically selected.
    skip_tasks: The tasks to skip. Use ``skip_tasks='tok/fine'`` to enable coarse tokenization for all tasks.
        Use ``tasks=['tok/coarse', ...]`` and ``skip_tasks='tok/fine'`` to enable coarse tokenization for
        selected tasks.
    language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
    A :class:`~hanlp_common.document.Document`.

Examples::

    # Use tasks=[...] to run selected tasks only
    HanLP('晓美焰来到自然语义科技公司', tasks=['pos', 'ner'])

    # Use skip_tasks='tok/fine' to enable coarse tokenization for all tasks
    HanLP('晓美焰来到自然语义科技公司', skip_tasks='tok/fine')

    # Use tasks=['tok/coarse', ...] and skip_tasks='tok/fine' to enable
    # coarse tokenization for selected tasks
    HanLP('晓美焰来到自然语义科技公司', tasks=['tok/coarse','pos'],skip_tasks='tok/fine')


Raises:
    HTTPError: Any errors happening on the Internet side or the server side. Refer to the ``code`` and ``msg``
        of the exception for more details. A list of common errors :

- ``400 Bad Request`` indicates that the server cannot process the request due to a client
  fault (e.g., text too long, language unsupported).
- ``401 Unauthorized`` indicates that the request lacks **valid** ``auth`` credentials for the API.
- ``422 Unprocessable Entity`` indicates that the content type of the request entity is not in
  proper json format.
- ``429 Too Many Requests`` indicates the user has sent too many requests in a given
  amount of time (""rate limiting"")., Semantic textual similarity deals with determining how similar two pieces of texts are.

Args:
    text: A pair or pairs of text.
    language: The language of input text. ``None`` to use the default language.

Returns:
    Similarities.

Examples::

    HanLP.semantic_textual_similarity([
        ('看图猜一电影名', '看图猜电影'),
        ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用'),
        ('北京到上海的动车票', '上海到北京的动车票'),
    ])
    # Output:
    [
        0.9764469, # Similarity of ('看图猜一电影名', '看图猜电影')
        0.0,       # Similarity of ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用')
        0.0034587  # Similarity of ('北京到上海的动车票', '上海到北京的动车票')
    ], Sentiment analysis is the task of classifying the polarity of a given text. For instance,
a text-based tweet can be categorized into either ""positive"", ""negative"", or ""neutral"".

Args:
    text: A document or a list of documents.
    language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.
        Contact the service provider for the list of languages supported.
        Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.
        Leave ``None`` to use the default language on server.

Returns:

    Sentiment polarity as a numerical value which measures how positive the sentiment is.

Examples::

    HanLP.language_identification('''“这是一部男人必看的电影。”人人都这么说。但单纯从性别区分，就会让这电影变狭隘。
    《肖申克的救赎》突破了男人电影的局限，通篇几乎充满令人难以置信的温馨基调，而电影里最伟大的主题是“希望”。
    当我们无奈地遇到了如同肖申克一般囚禁了心灵自由的那种囹圄，我们是无奈的老布鲁克，灰心的瑞德，还是智慧的安迪？
    运用智慧，信任希望，并且勇敢面对恐惧心理，去打败它？
    经典的电影之所以经典，因为他们都在做同一件事——让你从不同的角度来欣赏希望的美好。''')
    0.9505730271339417, Single document summarization is the task of selecting a subset of the sentences which best
represents a summary of the document, with a balance of salience and redundancy.

Args:
    text: The text content of the document.
    topk: The maximum number of top-K ranked sentences. Note that due to Trigram Blocking tricks, the actual
        number of returned sentences could be less than ``topk``.
    language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
    A dictionary containing each sentence and its ranking score :math:`s \in [0, 1]`.

Examples::

    HanLP.extractive_summarization('''
    据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。
    据供应链消息人士称，生产厂的订单拉动情况正在慢慢转强，这会提高MacBook Pro机型的供应量，并缩短苹果客户在过去几周所经历的延长交货时间。
    仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。
    据分析师郭明錤表示，广达是高端MacBook Pro的唯一供应商，自防疫封控依赖，MacBook Pro大部分型号交货时间增加了三到五周，
    一些高端定制型号的MacBook Pro配置要到6月底到7月初才能交货。
    尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。
    苹果上周表示，防疫措施和元部件短缺将继续使其难以生产足够的产品来满足消费者的强劲需求，这最终将影响苹果6月份的收入。
    ''')
    # Output:
    {'据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。': 0.9999,
     '仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。': 0.5800,
     '尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。': 0.5422}, Split a document into sentences and tokenize them. Note that it is always faster to tokenize a whole
document than to tokenize each sentence one by one. So avoid calling this method sentence by sentence but put
sentences into a ``list`` and pass them to the ``text`` argument.

Args:
    text: A document (``str``), or a list of sentences (``List[str]``).
    coarse: Whether to perform coarse-grained or fine-grained tokenization.
    language: The language of input text. ``None`` to use the default language.

Returns:
    A list of tokenized sentences.

Examples::

    # Avoid tokenizing sentence by sentence, it is expensive:
    HanLP.tokenize('商品和服务。')
    [['商品', '和', '服务', '。']]
    HanLP.tokenize('阿婆主来到北京立方庭参观自然语义科技公司')
    [['阿婆主', '来到', '北京', '立方庭', '参观', '自然', '语义', '科技', '公司']]

    # Instead, the following codes are much faster:
    HanLP.tokenize('商品和服务。阿婆主来到北京立方庭参观自然语义科技公司')
    [['商品', '和', '服务', '。'],
     ['阿婆主', '来到', '北京', '立方庭', '参观', '自然', '语义', '科技', '公司']]

    # To tokenize with coarse-grained standard:
    HanLP.tokenize('商品和服务。阿婆主来到北京立方庭参观自然语义科技公司', coarse=True)
    [['商品', '和', '服务', '。'],
     ['阿婆主', '来到', '北京', '立方庭', '参观', '自然语义科技公司']]

    # To tokenize pre-segmented sentences:
    HanLP.tokenize(['商品和服务。', '当下雨天地面积水分外严重'])
    [['商品', '和', '服务', '。'],
     ['当', '下雨天', '地面', '积水', '分', '外', '严重']]

    # Multilingual tokenization by specifying language='mul':
    HanLP.tokenize(
        ['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques
         'to production environment.',
         '2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。',
         '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。'], language='mul')
    [['In', '2021', ',', 'HanLPv2.1', 'delivers', 'state-of-the-art', 'multilingual',
      'NLP', 'techniques', 'to', 'production', 'environment', '.'],
     ['2021', '年', '、', 'HanLPv2.1', 'は', '次', '世代', 'の', '最', '先端', '多',
      '言語', 'NLP', '技術', 'を', '本番', '環境', 'に', '導入', 'します', '。'],
     ['2021', '年', 'HanLPv2.1', '为', '生产', '环境', '带来', '次世代', '最', '先进的',
      '多', '语种', 'NLP', '技术', '。']], Text classification is the task of assigning a sentence or document an appropriate category.
The categories depend on the chosen dataset and can range from topics.

Args:
    text: A document or a list of documents.
    model: The model to use for prediction.
    topk: ``True`` or ``int`` to return the top-k labels.
    prob: Return also probabilities.

Returns:

    Classification results., Text style transfer aims to change the style of the input text to the target style while preserving its
content.

Args:
    text: Source text.
    target_style: Target style.
    language: The language of input text. ``None`` to use the default language.

Returns:
    Text or a list of text of the target style.

Examples::

    HanLP.text_style_transfer(['国家对中石油抱有很大的期望.', '要用创新去推动高质量的发展。'],
                              target_style='gov_doc')
    # Output:
    [
        '国家对中石油寄予厚望。',
        '要以创新驱动高质量发展。'
    ]

    HanLP.text_style_transfer('我看到了窗户外面有白色的云和绿色的森林',
                              target_style='modern_poetry')
    # Output:
    '我看见窗外的白云绿林'","

        Args:
            url (str): An API endpoint to a service provider.
            auth (str): An auth key licenced from a service provider.
            language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.
                Contact the service provider for the list of languages supported.
                Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.
                Leave ``None`` to use the default language on server.
            timeout (int): Maximum waiting time in seconds for a request.
            verify (bool): ``True`` to enable SSL cert verification. You can also pass ``verify`` the path to a CA_BUNDLE
                file or directory with certificates of trusted CAs (``requests`` required).
        , 
        A shortcut of :meth:`~hanlp_restful.HanLPClient.parse`.
        , 
        Identify the language of a given text.

        Args:
            text: A document or a list of documents.
            topk: ``True`` or ``int`` to return the top-k languages.
            prob: Return also probabilities.

        Returns:

            Identified language in `ISO 639-1 codes`_.

        Examples::

            HanLP.language_identification(
            'In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques.')
            'en'
            lang, prob = HanLP.language_identification(
            '2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。', prob=True)
            ('ja', 0.9976244568824768)
            HanLP.language_identification(
            '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', topk=2)
            ['zh', 'ja']
            HanLP.language_identification(
            '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', topk=3, prob=True)
            {'zh': 0.3952908217906952, 'en': 0.37189167737960815, 'ja': 0.056213412433862686}

        .. _ISO 639-1 codes:
           https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes
        , 
        Parse a piece of text.

        Args:
            text: A document (str), or a list of sentences (List[str]).
            tokens: A list of sentences where each sentence is a list of tokens.
            tasks: The tasks to predict. Use ``tasks=[...]`` to run selected tasks only. Dependent tasks will be
                automatically selected.
            skip_tasks: The tasks to skip. Use ``skip_tasks='tok/fine'`` to enable coarse tokenization for all tasks.
                Use ``tasks=['tok/coarse', ...]`` and ``skip_tasks='tok/fine'`` to enable coarse tokenization for
                selected tasks.
            language: The language of input text or tokens. ``None`` to use the default language on server.

        Returns:
            A :class:`~hanlp_common.document.Document`.

        Examples::

            # Use tasks=[...] to run selected tasks only
            HanLP('晓美焰来到自然语义科技公司', tasks=['pos', 'ner'])

            # Use skip_tasks='tok/fine' to enable coarse tokenization for all tasks
            HanLP('晓美焰来到自然语义科技公司', skip_tasks='tok/fine')

            # Use tasks=['tok/coarse', ...] and skip_tasks='tok/fine' to enable
            # coarse tokenization for selected tasks
            HanLP('晓美焰来到自然语义科技公司', tasks=['tok/coarse','pos'],skip_tasks='tok/fine')


        Raises:
            HTTPError: Any errors happening on the Internet side or the server side. Refer to the ``code`` and ``msg``
                of the exception for more details. A list of common errors :

        - ``400 Bad Request`` indicates that the server cannot process the request due to a client
          fault (e.g., text too long, language unsupported).
        - ``401 Unauthorized`` indicates that the request lacks **valid** ``auth`` credentials for the API.
        - ``422 Unprocessable Entity`` indicates that the content type of the request entity is not in
          proper json format.
        - ``429 Too Many Requests`` indicates the user has sent too many requests in a given
          amount of time (""rate limiting"").

        , 
        Sentiment analysis is the task of classifying the polarity of a given text. For instance,
        a text-based tweet can be categorized into either ""positive"", ""negative"", or ""neutral"".

        Args:
            text: A document or a list of documents.
            language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.
                Contact the service provider for the list of languages supported.
                Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.
                Leave ``None`` to use the default language on server.

        Returns:

            Sentiment polarity as a numerical value which measures how positive the sentiment is.

        Examples::

            HanLP.language_identification('''“这是一部男人必看的电影。”人人都这么说。但单纯从性别区分，就会让这电影变狭隘。
            《肖申克的救赎》突破了男人电影的局限，通篇几乎充满令人难以置信的温馨基调，而电影里最伟大的主题是“希望”。
            当我们无奈地遇到了如同肖申克一般囚禁了心灵自由的那种囹圄，我们是无奈的老布鲁克，灰心的瑞德，还是智慧的安迪？
            运用智慧，信任希望，并且勇敢面对恐惧心理，去打败它？
            经典的电影之所以经典，因为他们都在做同一件事——让你从不同的角度来欣赏希望的美好。''')
            0.9505730271339417
        , 
        Text classification is the task of assigning a sentence or document an appropriate category.
        The categories depend on the chosen dataset and can range from topics.

        Args:
            text: A document or a list of documents.
            model: The model to use for prediction.
            topk: ``True`` or ``int`` to return the top-k labels.
            prob: Return also probabilities.

        Returns:

            Classification results.
        ,  Abstractive Summarization is the task of generating a short and concise summary that captures the
        salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that
        may not appear in the source text.

        Args:
            text: The text content of the document.
            language: The language of input text or tokens. ``None`` to use the default language on server.

        Returns:
            Summarization.

        Examples::

            HanLP.abstractive_summarization('''
            每经AI快讯，2月4日，长江证券研究所金属行业首席分析师王鹤涛表示，2023年海外经济衰退，美债现处于历史高位，
            黄金的趋势是值得关注的；在国内需求修复的过程中，看好大金属品种中的铜铝钢。
            此外，在细分的小品种里，建议关注两条主线，一是新能源，比如锂、钴、镍、稀土，二是专精特新主线。（央视财经）
            ''')
            # Output:
            '长江证券：看好大金属品种中的铜铝钢'
        ,  Coreference resolution is the task of clustering mentions in text that refer to the same underlying
        real world entities.

        Args:
            text: A piece of text, usually a document without tokenization.
            tokens: A list of sentences where each sentence is a list of tokens.
            speakers: A list of speakers where each speaker is a ``str`` representing the speaker's ID, e.g., ``Tom``.
            language: The language of input text. ``None`` to use the default language.

        Returns:
            When ``text`` is specified, return the clusters and tokens. Otherwise just the clusters, In this case, you need to ``sum(tokens, [])`` in order to match the span indices with tokens

        Examples::

            HanLP.coreference_resolution('我姐送我她的猫。我很喜欢它。')
            # Output:
            {'clusters': [
                          [['我', 0, 1], ['我', 3, 4], ['我', 8, 9]], # 指代说话人
                          [['我姐', 0, 2], ['她', 4, 5]],             # 指代说话人的姐姐
                          [['她的猫', 4, 7], ['它', 11, 12]]],        # 指代说话人的姐姐的猫
             'tokens': ['我', '姐', '送', '我', '她', '的', '猫', '。',
                        '我', '很', '喜欢', '它', '。']}

            HanLP.coreference_resolution(
            tokens=[['我', '姐', '送', '我', '她', '的', '猫', '。'],
                    ['我', '很', '喜欢', '它', '。']])
            # Output:
                         [
                          [['我', 0, 1], ['我', 3, 4], ['我', 8, 9]], # 指代说话人
                          [['我姐', 0, 2], ['她', 4, 5]],             # 指代说话人的姐姐
                          [['她的猫', 4, 7], ['它', 11, 12]]],        # 指代说话人的姐姐的猫

        .. image:: https://file.hankcs.com/img/coref_demo_small.png
            :alt: Coreference resolution visualization
        ,  Grammatical Error Correction (GEC) is the task of correcting different kinds of errors in text such as
        spelling, punctuation, grammatical, and word choice errors.

        Args:
            text: Text potentially containing different kinds of errors such as spelling, punctuation,
                grammatical, and word choice errors.
            language: The language of input text. ``None`` to use the default language.

        Returns:
            Corrected text.

        Examples::

            HanLP.grammatical_error_correction(['每个青年都应当有远大的报复。',
                                                '有的同学对语言很兴趣。'])
            # Output:
            [
                '每个青年都应当有远大的抱负。',
                '有的同学对语言很有兴趣。'
            ]

        ,  Keyphrase extraction aims to identify keywords or phrases reflecting the main topics of a document.

        Args:
            text: The text content of the document. Preferably the concatenation of the title and the content.
            topk: The number of top-K ranked keywords or keyphrases.
            language: The language of input text or tokens. ``None`` to use the default language on server.

        Returns:
            A dictionary containing each keyword or keyphrase and its ranking score :math:`s`, :math:`s \in [0, 1]`.

        Examples::

            HanLP.keyphrase_extraction(
                '自然语言处理是一门博大精深的学科，掌握理论才能发挥出HanLP的全部性能。 '
                '《自然语言处理入门》是一本配套HanLP的NLP入门书，助你零起点上手自然语言处理。', topk=3)
            # Output:
            {'自然语言处理': 0.800000011920929,
             'HanLP的全部性能': 0.5258446335792542,
             '一门博大精深的学科': 0.421421080827713}
        ,  Semantic textual similarity deals with determining how similar two pieces of texts are.

        Args:
            text: A pair or pairs of text.
            language: The language of input text. ``None`` to use the default language.

        Returns:
            Similarities.

        Examples::

            HanLP.semantic_textual_similarity([
                ('看图猜一电影名', '看图猜电影'),
                ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用'),
                ('北京到上海的动车票', '上海到北京的动车票'),
            ])
            # Output:
            [
                0.9764469, # Similarity of ('看图猜一电影名', '看图猜电影')
                0.0,       # Similarity of ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用')
                0.0034587  # Similarity of ('北京到上海的动车票', '上海到北京的动车票')
            ]
        ,  Single document summarization is the task of selecting a subset of the sentences which best
        represents a summary of the document, with a balance of salience and redundancy.

        Args:
            text: The text content of the document.
            topk: The maximum number of top-K ranked sentences. Note that due to Trigram Blocking tricks, the actual
                number of returned sentences could be less than ``topk``.
            language: The language of input text or tokens. ``None`` to use the default language on server.

        Returns:
            A dictionary containing each sentence and its ranking score :math:`s \in [0, 1]`.

        Examples::

            HanLP.extractive_summarization('''
            据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。
            据供应链消息人士称，生产厂的订单拉动情况正在慢慢转强，这会提高MacBook Pro机型的供应量，并缩短苹果客户在过去几周所经历的延长交货时间。
            仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。
            据分析师郭明錤表示，广达是高端MacBook Pro的唯一供应商，自防疫封控依赖，MacBook Pro大部分型号交货时间增加了三到五周，
            一些高端定制型号的MacBook Pro配置要到6月底到7月初才能交货。
            尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。
            苹果上周表示，防疫措施和元部件短缺将继续使其难以生产足够的产品来满足消费者的强劲需求，这最终将影响苹果6月份的收入。
            ''')
            # Output:
            {'据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。': 0.9999,
             '仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。': 0.5800,
             '尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。': 0.5422}
        ,  Split a document into sentences and tokenize them. Note that it is always faster to tokenize a whole
        document than to tokenize each sentence one by one. So avoid calling this method sentence by sentence but put
        sentences into a ``list`` and pass them to the ``text`` argument.

        Args:
            text: A document (``str``), or a list of sentences (``List[str]``).
            coarse: Whether to perform coarse-grained or fine-grained tokenization.
            language: The language of input text. ``None`` to use the default language.

        Returns:
            A list of tokenized sentences.

        Examples::

            # Avoid tokenizing sentence by sentence, it is expensive:
            HanLP.tokenize('商品和服务。')
            [['商品', '和', '服务', '。']]
            HanLP.tokenize('阿婆主来到北京立方庭参观自然语义科技公司')
            [['阿婆主', '来到', '北京', '立方庭', '参观', '自然', '语义', '科技', '公司']]

            # Instead, the following codes are much faster:
            HanLP.tokenize('商品和服务。阿婆主来到北京立方庭参观自然语义科技公司')
            [['商品', '和', '服务', '。'],
             ['阿婆主', '来到', '北京', '立方庭', '参观', '自然', '语义', '科技', '公司']]

            # To tokenize with coarse-grained standard:
            HanLP.tokenize('商品和服务。阿婆主来到北京立方庭参观自然语义科技公司', coarse=True)
            [['商品', '和', '服务', '。'],
             ['阿婆主', '来到', '北京', '立方庭', '参观', '自然语义科技公司']]

            # To tokenize pre-segmented sentences:
            HanLP.tokenize(['商品和服务。', '当下雨天地面积水分外严重'])
            [['商品', '和', '服务', '。'],
             ['当', '下雨天', '地面', '积水', '分', '外', '严重']]

            # Multilingual tokenization by specifying language='mul':
            HanLP.tokenize(
                ['In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques
                 'to production environment.',
                 '2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。',
                 '2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。'], language='mul')
            [['In', '2021', ',', 'HanLPv2.1', 'delivers', 'state-of-the-art', 'multilingual',
              'NLP', 'techniques', 'to', 'production', 'environment', '.'],
             ['2021', '年', '、', 'HanLPv2.1', 'は', '次', '世代', 'の', '最', '先端', '多',
              '言語', 'NLP', '技術', 'を', '本番', '環境', 'に', '導入', 'します', '。'],
             ['2021', '年', 'HanLPv2.1', '为', '生产', '环境', '带来', '次世代', '最', '先进的',
              '多', '语种', 'NLP', '技术', '。']]
        ,  Text style transfer aims to change the style of the input text to the target style while preserving its
        content.

        Args:
            text: Source text.
            target_style: Target style.
            language: The language of input text. ``None`` to use the default language.

        Returns:
            Text or a list of text of the target style.

        Examples::

            HanLP.text_style_transfer(['国家对中石油抱有很大的期望.', '要用创新去推动高质量的发展。'],
                                      target_style='gov_doc')
            # Output:
            [
                '国家对中石油寄予厚望。',
                '要以创新驱动高质量发展。'
            ]

            HanLP.text_style_transfer('我看到了窗户外面有白色的云和绿色的森林',
                                      target_style='modern_poetry')
            # Output:
            '我看见窗外的白云绿林'
        , . Please set language=""zh""., /about, /abstract_meaning_representation, /abstractive_summarization, /coreference_resolution, /extractive_summarization, /grammatical_error_correction, /keyphrase_extraction, /parse, /semantic_textual_similarity, /sentiment_analysis, /text_classification, /text_style_transfer, ?, Abstract Meaning Representation (AMR) captures “who is doing what to whom” in a sentence. Each sentence is
        represented as a rooted, directed, acyclic graph consisting of nodes (concepts) and edges (relations).

        Args:
            text: A document (str), or a list of sentences (List[str]).
            tokens: A list of sentences where each sentence is a list of tokens.
            language: The language of input text or tokens. ``None`` to use the default language on server.
            visualization: Set to `dot` or `svg` to obtain coresspodning visualization.

        Returns:
            Graphs in meaning represenation format.

        Examples::

            HanLP.abstract_meaning_representation('男孩希望女孩相信他。')
            HanLP.abstract_meaning_representation('The boy wants the girl to believe him.',
                                                  language='en')

        .. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=%E7%94%B7%E5%AD%A9%20%E5%B8%8C%E6%9C%9B%20%E5%A5%B3%E5%AD%A9%20%E7%9B%B8%E4%BF%A1%20%E4%BB%96%20%E3%80%82&language=zh&scale=1
            :alt: Abstract Meaning Representation

        .. image:: https://hanlp.hankcs.com/backend/v2/amr_svg?tokens=The%20boy%20wants%20the%20girl%20to%20believe%20him%20.&language=en&scale=1
            :alt: Abstract Meaning Representation

        , At least one of text or tokens has to be specified., Authorization, Basic , Coarse tokenization not supported for , Get the information about server and your client.

        Returns:
            A dict containing your rate limit and server version etc.

        , HANLP_AUTH, Text has to be non-empty., Text has to be specified., language, lid, model, prob, skip_tasks, speakers, target_style, tasks, text, tok, tok/coarse, tokens, topk, visualization, zh","10, 200, 3, 60, False, None, True","

Args:
url (str): An API endpoint to a service provider.
auth (str): An auth key licenced from a service provider.
language (str): The default language for each :func:`~hanlp_restful.HanLPClient.parse` call.
Contact the service provider for the list of languages supported.
Conventionally, ``zh`` is used for Chinese and ``mul`` for multilingual.
Leave ``None`` to use the default language on server.
timeout (int): Maximum waiting time in seconds for a request.
verify (bool): ``True`` to enable SSL cert verification. You can also pass ``verify`` the path to a CA_BUNDLE
file or directory with certificates of trusted CAs (``requests`` required).
, 
A shortcut of :meth:`~hanlp_restful.HanLPClient.parse`.
, 
Parse a piece of text.

Args:
text: A document (str), or a list of sentences (List[str]).
tokens: A list of sentences where each sentence is a list of tokens.
tasks: The tasks to predict. Use ``tasks=[...]`` to run selected tasks only. Dependent tasks will be
automatically selected.
skip_tasks: The tasks to skip. Use ``skip_tasks='tok/fine'`` to enable coarse tokenization for all tasks.
Use ``tasks=['tok/coarse', ...]`` and ``skip_tasks='tok/fine'`` to enable coarse tokenization for
selected tasks.
language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
A :class:`~hanlp_common.document.Document`.

Examples::

# Use tasks=[...] to run selected tasks only
HanLP('晓美焰来到自然语义科技公司', tasks=['pos', 'ner'])

# Use skip_tasks='tok/fine' to enable coarse tokenization for all tasks
HanLP('晓美焰来到自然语义科技公司', skip_tasks='tok/fine')

# Use tasks=['tok/coarse', ...] and skip_tasks='tok/fine' to enable
# coarse tokenization for selected tasks
HanLP('晓美焰来到自然语义科技公司', tasks=['tok/coarse','pos'],skip_tasks='tok/fine')


Raises:
HTTPError: Any errors happening on the Internet side or the server side. Refer to the ``code`` and ``msg``
of the exception for more details. A list of common errors :

- ``400 Bad Request`` indicates that the server cannot process the request due to a client
fault (e.g., text too long, language unsupported).
- ``401 Unauthorized`` indicates that the request lacks **valid** ``auth`` credentials for the API.
- ``422 Unprocessable Entity`` indicates that the content type of the request entity is not in
proper json format.
- ``429 Too Many Requests`` indicates the user has sent too many requests in a given
amount of time (""rate limiting"").

, 
Text classification is the task of assigning a sentence or document an appropriate category.
The categories depend on the chosen dataset and can range from topics.

Args:
text: A document or a list of documents.
model: The model to use for prediction.
topk: ``True`` or ``int`` to return the top-k labels.
prob: Return also probabilities.

Returns:

Classification results.
, 
assert text or tokens, 'At least one of text or tokens has to be specified.'
return self._send_post_json(self._url + '/abstract_meaning_representation', {
'text': text,
'tokens': tokens,
'language': language or self._language,
'visualization': visualization,
})

def keyphrase_extraction(
self,
text: str,
topk: int = 10,
language: str = None,
) -> Dict[str, float]:
 Keyphrase extraction aims to identify keywords or phrases reflecting the main topics of a document., 
assert text, 'Text has to be specified.'
return self._send_post_json(self._url + '/keyphrase_extraction', {
'text': text,
'language': language or self._language,
'topk': topk,
})

def extractive_summarization(
self,
text: str,
topk: int = 3,
language: str = None,
) -> Dict[str, float]:
 Single document summarization is the task of selecting a subset of the sentences which best, 
language = language or self._language
if coarse and language and language != 'zh':
raise NotImplementedError(f'Coarse tokenization not supported for {language}. Please set language=""zh"".')
doc = self.parse(text=text, tasks='tok/coarse' if coarse is True else 'tok', language=language)
return next(iter(doc.values()))

def abstract_meaning_representation(self,
text: Union[str, List[str]] = None,
tokens: List[List[str]] = None,
language: str = None,
visualization: str = None,
) -> List[Dict]:
Abstract Meaning Representation (AMR) captures “who is doing what to whom” in a sentence. Each sentence is, 
response = self._send_post_json(self._url + '/coreference_resolution',
{'text': text, 'tokens': tokens, 'speakers': speakers,
'language': language or self._language})
return response

def tokenize(self, text: Union[str, List[str]], coarse: Optional[bool] = None, language=None) -> List[List[str]]:
 Split a document into sentences and tokenize them. Note that it is always faster to tokenize a whole, 
response = self._send_post_json(self._url + '/sentiment_analysis',
{'text': text, 'language': language or self._language})
return response

def language_identification(self, text: Union[str, List[str]], topk=False, prob=False) -> Union[
str, Dict[str, float], List[Union[str, Dict[str, float]]]]:
,  Grammatical Error Correction (GEC) is the task of correcting different kinds of errors in text such as
spelling, punctuation, grammatical, and word choice errors.

Args:
text: Text potentially containing different kinds of errors such as spelling, punctuation,
grammatical, and word choice errors.
language: The language of input text. ``None`` to use the default language.

Returns:
Corrected text.

Examples::

HanLP.grammatical_error_correction(['每个青年都应当有远大的报复。',
'有的同学对语言很兴趣。'])
# Output:
[
'每个青年都应当有远大的抱负。',
'有的同学对语言很有兴趣。'
]

,  Semantic textual similarity deals with determining how similar two pieces of texts are.

Args:
text: A pair or pairs of text.
language: The language of input text. ``None`` to use the default language.

Returns:
Similarities.

Examples::

HanLP.semantic_textual_similarity([
('看图猜一电影名', '看图猜电影'),
('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用'),
('北京到上海的动车票', '上海到北京的动车票'),
])
# Output:
[
0.9764469, # Similarity of ('看图猜一电影名', '看图猜电影')
0.0,       # Similarity of ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用')
0.0034587  # Similarity of ('北京到上海的动车票', '上海到北京的动车票')
]
,  Text style transfer aims to change the style of the input text to the target style while preserving its
content.

Args:
text: Source text.
target_style: Target style.
language: The language of input text. ``None`` to use the default language.

Returns:
Text or a list of text of the target style.

Examples::

HanLP.text_style_transfer(['国家对中石油抱有很大的期望.', '要用创新去推动高质量的发展。'],
target_style='gov_doc')
# Output:
[
'国家对中石油寄予厚望。',
'要以创新驱动高质量发展。'
]

HanLP.text_style_transfer('我看到了窗户外面有白色的云和绿色的森林',
target_style='modern_poetry')
# Output:
'我看见窗外的白云绿林'
, # -*- coding:utf-8 -*-, # Author: hankcs, # Avoid tokenizing sentence by sentence, it is expensive:, # Date: 2020-11-29 17:48, # Instead, the following codes are much faster:, # Multilingual tokenization by specifying language='mul':, # Output:, # To tokenize pre-segmented sentences:, # To tokenize with coarse-grained standard:, # noinspection PyUnresolvedReferences, # 指代说话人, # 指代说话人的姐姐, # 指代说话人的姐姐的猫, )
# Output:
'长江证券：看好大金属品种中的铜铝钢'
, )
# Output:
{'据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。': 0.9999,
'仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。': 0.5800,
'尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。': 0.5422}
, Get the information about server and your client.

Returns:
A dict containing your rate limit and server version etc.

","
Parse a piece of text.

Args:
text: A document (str), or a list of sentences (List[str]).
tokens: A list of sentences where each sentence is a list of tokens.
tasks: The tasks to predict. Use ``tasks=[...]`` to run selected tasks only. Dependent tasks will be
automatically selected.
skip_tasks: The tasks to skip. Use ``skip_tasks='tok/fine'`` to enable coarse tokenization for all tasks.
Use ``tasks=['tok/coarse', ...]`` and ``skip_tasks='tok/fine'`` to enable coarse tokenization for
selected tasks.
language: The language of input text or tokens. ``None`` to use the default language on server.

Returns:
A :class:`~hanlp_common.document.Document`.

Examples::

# Use tasks=[...] to run selected tasks only
HanLP('晓美焰来到自然语义科技公司', tasks=['pos', 'ner'])

# Use skip_tasks='tok/fine' to enable coarse tokenization for all tasks
HanLP('晓美焰来到自然语义科技公司', skip_tasks='tok/fine')

# Use tasks=['tok/coarse', ...] and skip_tasks='tok/fine' to enable
# coarse tokenization for selected tasks
HanLP('晓美焰来到自然语义科技公司', tasks=['tok/coarse','pos'],skip_tasks='tok/fine')


Raises:
HTTPError: Any errors happening on the Internet side or the server side. Refer to the ``code`` and ``msg``
of the exception for more details. A list of common errors :

- ``400 Bad Request`` indicates that the server cannot process the request due to a client
fault (e.g., text too long, language unsupported).
- ``401 Unauthorized`` indicates that the request lacks **valid** ``auth`` credentials for the API.
- ``422 Unprocessable Entity`` indicates that the content type of the request entity is not in
proper json format.
- ``429 Too Many Requests`` indicates the user has sent too many requests in a given
amount of time (""rate limiting"").

, 
language = language or self._language
if coarse and language and language != 'zh':
raise NotImplementedError(f'Coarse tokenization not supported for {language}. Please set language=""zh"".')
doc = self.parse(text=text, tasks='tok/coarse' if coarse is True else 'tok', language=language)
return next(iter(doc.values()))

def abstract_meaning_representation(self,
text: Union[str, List[str]] = None,
tokens: List[List[str]] = None,
language: str = None,
visualization: str = None,
) -> List[Dict]:
Abstract Meaning Representation (AMR) captures “who is doing what to whom” in a sentence. Each sentence is, '''“这是一部男人必看的电影。”人人都这么说。但单纯从性别区分，就会让这电影变狭隘。, '2021年, '2021年、HanLPv2, 'HanLP的全部性能', '、', '。', '。'], '。']], '《自然语言处理入门》是一本配套HanLP的NLP入门书，助你零起点上手自然语言处理。', 'します', 'に', 'の', 'は', 'を', '一门博大精深的学科', '上海到北京的动车票', '下雨天', '世代', '严重']], '为', '仍有许多苹果笔记本用户在等待3月和4月订购的MacBook, '先端', '先进的', '公司']], '分', '北京', '北京到上海的动车票', '参观', '和', '商品和服务。', '商品和服务。阿婆主来到北京立方庭参观自然语义科技公司', '喜欢', '国家对中石油寄予厚望。', '地面', '外', '多', '她', '姐', '它', '導入', '尽管MacBook, '带来', '年', '当下雨天地面积水分外严重'], '很', '我', '我姐送我她的猫。我很喜欢它。', '我看到了窗户外面有白色的云和绿色的森林', '我看见窗外的白云绿林', '技术', '技術', '据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。', '无线上网卡和无线路由器怎么用', '无线路由器怎么无线上网', '晓美焰来到自然语义科技公司', '最', '有的同学对语言很兴趣。'], '有的同学对语言很有兴趣。', '服务', '本番', '来到', '次', '次世代', '每个青年都应当有远大的抱负。', '猫', '环境', '環境', '生产', '男孩希望女孩相信他。', '的', '看图猜一电影名', '看图猜电影', '科技', '积水', '立方庭', '自然', '自然语义科技公司']], '自然语言处理', '自然语言处理是一门博大精深的学科，掌握理论才能发挥出HanLP的全部性能。, '要以创新驱动高质量发展。', '要用创新去推动高质量的发展。'], '言語', '语义', '语种', '送', '长江证券：看好大金属品种中的铜铝钢', '阿婆主来到北京立方庭参观自然语义科技公司', )
# Output:
'长江证券：看好大金属品种中的铜铝钢'
, )
# Output:
{'据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。': 0.9999,
'仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。': 0.5800,
'尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。': 0.5422}
, 1は次世代の最先端多言語NLP技術を本番環境に導入します。', 1为生产环境带来次世代最先进的多语种NLP技术。', 1为生产环境带来次世代最先进的多语种NLP技术。'], Grammatical Error Correction (GEC) is the task of correcting different kinds of errors in text such as
spelling, punctuation, grammatical, and word choice errors.

Args:
text: Text potentially containing different kinds of errors such as spelling, punctuation,
grammatical, and word choice errors.
language: The language of input text. ``None`` to use the default language.

Returns:
Corrected text.

Examples::

HanLP.grammatical_error_correction(['每个青年都应当有远大的报复。',
'有的同学对语言很兴趣。'])
# Output:
[
'每个青年都应当有远大的抱负。',
'有的同学对语言很有兴趣。'
]

, Pro大部分型号交货时间增加了三到五周，, Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。, Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。', Pro机型的供应量，并缩短苹果客户在过去几周所经历的延长交货时间。, Pro的唯一供应商，自防疫封控依赖，MacBook, Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。, Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。', Pro配置要到6月底到7月初才能交货。, Semantic textual similarity deals with determining how similar two pieces of texts are.

Args:
text: A pair or pairs of text.
language: The language of input text. ``None`` to use the default language.

Returns:
Similarities.

Examples::

HanLP.semantic_textual_similarity([
('看图猜一电影名', '看图猜电影'),
('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用'),
('北京到上海的动车票', '上海到北京的动车票'),
])
# Output:
[
0.9764469, # Similarity of ('看图猜一电影名', '看图猜电影')
0.0,       # Similarity of ('无线路由器怎么无线上网', '无线上网卡和无线路由器怎么用')
0.0034587  # Similarity of ('北京到上海的动车票', '上海到北京的动车票')
]
, Text style transfer aims to change the style of the input text to the target style while preserving its
content.

Args:
text: Source text.
target_style: Target style.
language: The language of input text. ``None`` to use the default language.

Returns:
Text or a list of text of the target style.

Examples::

HanLP.text_style_transfer(['国家对中石油抱有很大的期望.', '要用创新去推动高质量的发展。'],
target_style='gov_doc')
# Output:
[
'国家对中石油寄予厚望。',
'要以创新驱动高质量发展。'
]

HanLP.text_style_transfer('我看到了窗户外面有白色的云和绿色的森林',
target_style='modern_poetry')
# Output:
'我看见窗外的白云绿林'
, ['商品和服务。', ['国家对中石油抱有很大的期望, ['她', ['它', ['当', ['我', ['每个青年都应当有远大的报复。', ['阿婆主', [['商品', [['她的猫', [['我', [['我姐', [['阿婆主', tokens=[['我', whom”, “who, 《肖申克的救赎》突破了男人电影的局限，通篇几乎充满令人难以置信的温馨基调，而电影里最伟大的主题是“希望”。, 一些高端定制型号的MacBook, 仍有许多苹果笔记本用户在等待3月和4月订购的MacBook, 尽管MacBook, 当我们无奈地遇到了如同肖申克一般囚禁了心灵自由的那种囹圄，我们是无奈的老布鲁克，灰心的瑞德，还是智慧的安迪？, 指代说话人, 指代说话人的姐姐, 指代说话人的姐姐的猫, 据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。, 据供应链消息人士称，生产厂的订单拉动情况正在慢慢转强，这会提高MacBook, 据分析师郭明錤表示，广达是高端MacBook, 此外，在细分的小品种里，建议关注两条主线，一是新能源，比如锂、钴、镍、稀土，二是专精特新主线。（央视财经）, 每经AI快讯，2月4日，长江证券研究所金属行业首席分析师王鹤涛表示，2023年海外经济衰退，美债现处于历史高位，, 经典的电影之所以经典，因为他们都在做同一件事——让你从不同的角度来欣赏希望的美好。''', 苹果上周表示，防疫措施和元部件短缺将继续使其难以生产足够的产品来满足消费者的强劲需求，这最终将影响苹果6月份的收入。, 运用智慧，信任希望，并且勇敢面对恐惧心理，去打败它？, 黄金的趋势是值得关注的；在国内需求修复的过程中，看好大金属品种中的铜铝钢。"
https://github.com/hankcs/HanLP,test_client.py,0,0.0,28,19.18,58,39.73,3,2.05,1,0.68,56,38.36,11,1,3,0,,"HanLP, TestClient, __name__, abstract_meaning_representation, coreference_resolution, doc, extractive_summarization, hanlp_restful, keyphrase_extraction, parse, print, self, setUp, test_abstract_meaning_representation, test_coreference_resolution, test_extractive_summarization, test_keyphrase_extraction, test_raw_text, test_sents, test_sents_mul, test_text_style_transfer, test_tokenize, test_tokens, text, text_style_transfer, tokenize, tokens, unittest","hanlp_restful.HanLPClient, unittest.TestCase, unittest.main","setUp, test_abstract_meaning_representation, test_coreference_resolution, test_extractive_summarization, test_keyphrase_extraction, test_raw_text, test_sents, test_sents_mul, test_text_style_transfer, test_tokenize, test_tokens",TestClient,"doc, text, tokens",,"
        据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。
        据供应链消息人士称，生产厂的订单拉动情况正在慢慢转强，这会提高MacBook Pro机型的供应量，并缩短苹果客户在过去几周所经历的延长交货时间。
        仍有许多苹果笔记本用户在等待3月和4月订购的MacBook Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。
        据分析师郭明錤表示，广达是高端MacBook Pro的唯一供应商，自防疫封控依赖，MacBook Pro大部分型号交货时间增加了三到五周，
        一些高端定制型号的MacBook Pro配置要到6月底到7月初才能交货。
        尽管MacBook Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。
        苹果上周表示，防疫措施和元部件短缺将继续使其难以生产足够的产品来满足消费者的强劲需求，这最终将影响苹果6月份的收入。
            , 2021年, 2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。, 2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, HanLPv2.1, In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environment., NLP, The boy wants the girl to believe him., __main__, dep, dot, en, gov_doc, https://hanlp.hankcs.com/api, modern_poetry, mul, ner*, srl, svg, 。, 与, 世代, 为, 他, 先进, 公司, 华为, 商品和服务。, 商品和服务。阿婆主来到北京立方庭参观自然语义科技公司, 国家对中石油抱有很大的期望., 多语种, 女孩, 希望, 带来, 当下雨天地面积水分外严重, 我姐送我她的猫。我很喜欢它。, 我看到了窗户外面有白色的云和绿色的森林, 打工人，打工魂，打工都是人上人, 技术, 最, 次, 特朗普, 环境, 生产, 电话, 男孩, 男孩希望女孩相信他。, 的, 相信, 自然语言处理是一门博大精深的学科，掌握理论才能发挥出HanLP的全部性能。 《自然语言处理入门》是一本配套HanLP的NLP入门书，助你零起点上手自然语言处理。, 英, 苹果, 讨论, 通, 阿婆主来到北京立方庭参观自然语义科技公司。, 首相","3, None, True",# Fill in your auth,"1は次世代の最先端多言語NLP技術を本番環境に導入します。, 1为生产环境带来次世代最先进的多语种NLP技术。, 1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。, 2021年, 2021年HanLPv2, 2021年、HanLPv2, Pro大部分型号交货时间增加了三到五周，, Pro机型到货，由于苹果的供应问题，他们的发货时间被大大推迟了。, Pro机型的供应量，并缩短苹果客户在过去几周所经历的延长交货时间。, Pro的唯一供应商，自防疫封控依赖，MacBook, Pro的生产逐渐恢复，但供应问题预计依然影响2022年第三季度的产品销售。, Pro配置要到6月底到7月初才能交货。, 《自然语言处理入门》是一本配套HanLP的NLP入门书，助你零起点上手自然语言处理。, 一些高端定制型号的MacBook, 与, 世代, 为, 仍有许多苹果笔记本用户在等待3月和4月订购的MacBook, 他, 先进, 公司, 华为, 商品和服务。, 商品和服务。阿婆主来到北京立方庭参观自然语义科技公司, 国家对中石油抱有很大的期望, 多语种, 女孩, 尽管MacBook, 希望, 带来, 当下雨天地面积水分外严重, 我姐送我她的猫。我很喜欢它。, 我看到了窗户外面有白色的云和绿色的森林, 打工人，打工魂，打工都是人上人, 技术, 据DigiTimes报道，在上海疫情趋缓，防疫管控开始放松后，苹果供应商广达正在逐步恢复其中国工厂的MacBook产品生产。, 据供应链消息人士称，生产厂的订单拉动情况正在慢慢转强，这会提高MacBook, 据分析师郭明錤表示，广达是高端MacBook, 最, 次, 特朗普, 环境, 生产, 电话, 男孩, 男孩希望女孩相信他。, 的, 相信, 自然语言处理是一门博大精深的学科，掌握理论才能发挥出HanLP的全部性能。, 英, 苹果, 苹果上周表示，防疫措施和元部件短缺将继续使其难以生产足够的产品来满足消费者的强劲需求，这最终将影响苹果6月份的收入。, 讨论, 通, 阿婆主来到北京立方庭参观自然语义科技公司。, 首相"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 18:05",
https://github.com/hankcs/HanLP,block_windows.py,0,0.0,1,20.0,1,20.0,0,0.0,3,60.0,0,0.0,0,0,0,0,,hanlp,hanlp.utils.io_util.windows,,,,,Windows is not supported for this script. Please run it on Linux systems.,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-07-28 21:38",
https://github.com/hankcs/HanLP,sent_split.py,0,0.0,4,33.33,2,16.67,0,0.0,4,33.33,2,16.67,0,0,2,0,,"hanlp, output, print, split_sent","hanlp.load, hanlp.pretrained.eos.UD_CTB_EOS_MUL",,,"output, split_sent",,"
, 3.14 is pi. “你好！！！”——他说。劇場版「Fate/stay night [HF]」最終章公開カウントダウン！",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 14:23, # See also https://hanlp.hankcs.com/docs/api/hanlp/components/eos.html","[HF]」最終章公開カウントダウン！, “你好！！！”——他说。劇場版「Fate/stay"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 17:48",
https://github.com/hankcs/HanLP,demo_amr.py,0,0.0,3,42.86,1,14.29,0,0.0,3,42.86,0,0.0,0,0,2,0,,"amr_parser, hanlp, print","hanlp.load, hanlp.pretrained.amr.AMR3_SEQ2SEQ_BART_LARGE",,,"amr, amr_parser",,The boy wants the girl to believe him.,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-01-25 19:09",
https://github.com/hankcs/HanLP,demo_dep.py,0,0.0,5,23.81,13,61.9,0,0.0,3,14.29,0,0.0,0,0,3,0,,"hanlp, print, sent, syntactic_parser, tree","hanlp.load, hanlp.pretrained.dep.PTB_BIAFFINE_DEP_EN",,,"sent, syntactic_parser, tree",,"., ?, DT, IN, Is, NN, VBZ, chamber, future, music, of, the, this",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 17:55",
https://github.com/hankcs/HanLP,demo_lm.py,0,0.0,5,50.0,2,20.0,0,0.0,3,30.0,0,0.0,0,0,1,0,,"generate_text, hanlp, list, lm, print","hanlp.load, hanlp.pretrained.rnnlm.FLAIR_LM_FW_WMT11_EN_TF",,,lm,,", hello",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-02-11 09:14",
https://github.com/hankcs/HanLP,demo_ner.py,0,0.0,3,20.0,9,60.0,0,0.0,3,20.0,0,0.0,0,0,1,0,,"hanlp, print, recognizer","hanlp.load, hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_CASED_EN",,,recognizer,,"., House, Obama, President, White, at, is, speaking, the",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-03 22:50",
https://github.com/hankcs/HanLP,demo_pipeline.py,0,0.0,10,47.62,7,33.33,1,4.76,3,14.29,0,0.0,0,0,8,0,,"deployed, doc, hanlp, pipeline, print, save, semantic_parser, syntactic_parser, tagger, text","hanlp.load, hanlp.pipeline, hanlp.pretrained.dep.PTB_BIAFFINE_DEP_EN, hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN, hanlp.pretrained.sdp.SEMEVAL15_PAS_BIAFFINE_EN, hanlp.utils.lang.en.english_tokenizer.tokenize_english, hanlp.utils.rules.split_sentence",,,"deployed, doc, pipeline, semantic_parser, syntactic_parser, tagger, text, tokenizer",,"Jobs and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer.
Together the duo gained fame and wealth a year later with the Apple II.
, en.json, part_of_speech_tags, semantic_dependencies, sentences, syntactic_dependencies, tokens",False,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-04 21:05",
https://github.com/hankcs/HanLP,demo_pos.py,0,0.0,3,13.64,16,72.73,0,0.0,3,13.64,0,0.0,0,0,1,0,,"hanlp, print, tagger","hanlp.load, hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN",,,tagger,,"., 2, ?, I, Is, a, bank, banked, chamber, dollars, future, in, music, of, the, this",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-03 22:16",
https://github.com/hankcs/HanLP,demo_sdp.py,0,0.0,6,23.08,13,50.0,0,0.0,7,26.92,0,0.0,0,0,3,0,,"hanlp, hanlp_common, print, semantic_parser, sent, tree","hanlp.load, hanlp.pretrained.sdp.SEMEVAL15_PAS_BIAFFINE_EN, hanlp_common.conll.CoNLLSentence",,,"semantic_parser, sent, tree",,"., ?, DT, IN, Is, NN, VBZ, chamber, future, music, of, the, this",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-03 15:26, # semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_DM_BIAFFINE_EN), # semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_PSD_BIAFFINE_EN), # semeval15 offers three independent annotations over the Penn Treebank (PTB), # type:CoNLLSentence",
https://github.com/hankcs/HanLP,demo_sentiment_analysis.py,0,0.0,4,44.44,2,22.22,0,0.0,3,33.33,0,0.0,0,0,1,0,,"classifier, hanlp, predict, print",hanlp.load,,,classifier,,"I feel lucky, SST2_ALBERT_BASE_EN",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 03:52",
https://github.com/hankcs/HanLP,demo_tok.py,0,0.0,3,42.86,1,14.29,0,0.0,3,42.86,0,0.0,0,0,1,0,,"hanlp, print, text",hanlp.utils.lang.en.english_tokenizer.tokenize_english,,,text,,"Don't go gentle into that good night.
",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-02 19:41",
https://github.com/hankcs/HanLP,train_sst2_albert_base.py,0,0.0,8,44.44,7,38.89,0,0.0,3,16.67,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, load, os, print, save_dir, tests","hanlp.components.classifiers.transformer_classifier_tf.TransformerClassifierTF, hanlp.datasets.glu.glue.STANFORD_SENTIMENT_TREEBANK_2_DEV, hanlp.datasets.glu.glue.STANFORD_SENTIMENT_TREEBANK_2_TEST, hanlp.datasets.glu.glue.STANFORD_SENTIMENT_TREEBANK_2_TRAIN, os.path.join, tests.cdroot",,,"classifier, save_dir",,"Model saved in , albert-base-v2, data, it' s a charming and often affecting journey, model, sst, sst2_albert_base",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 17:41",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 17:55",
https://github.com/hankcs/HanLP,demo_mtl.py,0,0.0,5,38.46,2,15.38,0,0.0,3,23.08,3,23.08,0,0,1,0,,"HanLP, hanlp, hanlp_common, pretty_print, print","hanlp.load, hanlp.pretrained.mtl.NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA, hanlp_common.document.Document",,,HanLP,,"2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, 奈須きのこは1973年11月28日に千葉県円空山で生まれ、ゲーム制作会社「ノーツ」の設立者だ。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-17 22:30","1は次世代の最先端多言語NLP技術を本番環境に導入します。, 2021年、HanLPv2, 奈須きのこは1973年11月28日に千葉県円空山で生まれ、ゲーム制作会社「ノーツ」の設立者だ。"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-17 22:30",
https://github.com/hankcs/HanLP,demo_lid.py,0,0.0,6,26.09,6,26.09,3,13.04,4,17.39,4,17.39,0,0,4,0,,"hanlp, lang, lid, print, prob, text","hanlp.load, hanlp.pretrained.classifiers.LID_176_FASTTEXT_BASE",,,"lang, lid, prob, text",,"
2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。
In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.
,  language identified with probability , .3%, 2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments.","2, 3, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-09-28 16:49, # For a combination of languages, predict top-k languages with probabilities:","1は次世代の最先端多言語NLP技術を本番環境に導入します。, 1为生产环境带来次世代最先进的多语种NLP技术。, 2021年, 2021年、HanLPv2"
https://github.com/hankcs/HanLP,demo_lid_restful.py,0,0.0,3,18.75,5,31.25,1,6.25,3,18.75,4,25.0,0,0,1,0,,"hanlp_restful, language_identification, print",hanlp_restful.HanLPClient,,,HanLP,,"2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environment., https://hanlp.hankcs.com/api, mul",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-09-28 16:49","1は次世代の最先端多言語NLP技術を本番環境に導入します。, 1为生产环境带来次世代最先进的多语种NLP技术。, 2021年, 2021年、HanLPv2"
https://github.com/hankcs/HanLP,demo_mtl.py,0,0.0,5,33.33,3,20.0,0,0.0,3,20.0,4,26.67,0,0,1,0,,"HanLP, hanlp, hanlp_common, pretty_print, print","hanlp.load, hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE, hanlp_common.document.Document",,,HanLP,,"2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environment.",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 13:51","1は次世代の最先端多言語NLP技術を本番環境に導入します。, 1为生产环境带来次世代最先进的多语种NLP技术。, 2021年, 2021年、HanLPv2"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 22:25",
https://github.com/hankcs/HanLP,demo_amr.py,0,0.0,3,9.09,18,54.55,1,3.03,6,18.18,5,15.15,0,0,1,0,,"hanlp, parser, print","hanlp.load, hanlp.pretrained.amr.MRP2020_AMR_ENG_ZHO_XLM_BASE",,,parser,,"., The, believe, boy, eng, girl, he, him, the, to, want, wants, 。, 他, 女孩, 希望, 男孩, 相信",False,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-04-12 22:19, # For Chinese:, # For English:, # It's suggested to also feed the lemma for stabler performance.","他, 女孩, 希望, 男孩, 相信"
https://github.com/hankcs/HanLP,demo_custom_dict.py,0,0.0,7,11.67,19,31.67,1,1.67,10,16.67,23,38.33,0,0,1,0,,"HanLP, config, dict, dict_combine, dict_force, hanlp, print","hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.components.mtl.tasks.tok.tag_tok.TaggingTokenization, hanlp.load, hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH",,,HanLP,,"2个空格, Pro, dictionary, iPad, tok/fine, 不挂词典:
, 合并模式:
, 和, 和服, 和服务, 商品和服务项目, 如何评价iPad Pro ？iPad  Pro有2个空格, 强制校正:
, 强制模式:
, 服务, 服务项目, 正向匹配商品和服务、任何和服务必按上述切分, 空格匹配：
, 词典内容：
",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-15 22:26, # See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html, # 加载多任务模型, # 含有空格、制表符等（Transformer tokenizer去掉的字符）的词语需要用tuple的形式提供, # 慎用，详见《自然语言处理入门》第二章, # 聪明的用户请继续阅读：tuple词典中的字符串其实等价于该字符串的所有可能的切分方式, # 获取分词任务（以tok开头的任务都是分词任务，以细分标准为例）, # 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php","2个空格, Pro有2个空格, 不挂词典, 加载多任务模型, 合并模式, 含有空格、制表符等（Transformer tokenizer去掉的字符）的词语需要用tuple的形式提供, 和, 和服, 和服务, 商品和服务项目, 如何评价iPad, 强制校正, 强制模式, 慎用，详见《自然语言处理入门》第二章, 服务, 服务项目, 正向匹配商品和服务、任何和服务必按上述切分, 空格匹配：, 聪明的用户请继续阅读：tuple词典中的字符串其实等价于该字符串的所有可能的切分方式, 获取分词任务（以tok开头的任务都是分词任务，以细分标准为例）, 词典内容：, 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php, ？iPad"
https://github.com/hankcs/HanLP,demo_custom_dict_stl.py,0,0.0,4,10.0,13,32.5,1,2.5,7,17.5,15,37.5,0,0,0,0,,"dict_combine, dict_force, hanlp, print","hanlp.components.tokenizers.transformer.TransformerTaggingTokenizer, hanlp.load",,,,,"https://file.hankcs.com/hanlp/tok/coarse_electra_small_20220220_013548.zip, 不挂词典:
, 合并模式:
, 川普, 川普通电话, 强制校正:
, 强制模式:
, 电话, 美国总统, 通, 银川普通人与川普通电话讲四川普通话, 首相和川普通电话, 首相和川普通电话，川普是美国总统。",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-15 22:26, # See also https://hanlp.hankcs.com/docs/api/hanlp/components/tokenizers/transformer.html, # 加载一个旧版本单任务模型演示分词错误（最新版已经修复）：, # 慎用，详见《自然语言处理入门》第二章, # 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php","不挂词典, 加载一个旧版本单任务模型演示分词错误（最新版已经修复）：, 合并模式, 川普, 川普通电话, 强制校正, 强制模式, 慎用，详见《自然语言处理入门》第二章, 电话, 美国总统, 通, 银川普通人与川普通电话讲四川普通话, 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php, 首相和川普通电话, 首相和川普通电话，川普是美国总统。"
https://github.com/hankcs/HanLP,demo_del_tasks.py,0,0.0,8,36.36,4,18.18,0,0.0,7,31.82,3,13.64,0,0,2,0,,"HanLP, hanlp, hanlp_common, keys, list, pretty_print, print, tasks","hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.load, hanlp.pretrained.mtl.OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, hanlp_common.document.Document",,,"HanLP, tasks",,"2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, pos, tok, up主来到北京立方庭参观自然语义科技公司。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-02-03 13:28, # HanLP.load('path/to/new/component'), # HanLP.save('path/to/new/component'), # Pick what you need from what we have, # You can save it as a new component","1为生产环境带来次世代最先进的多语种NLP技术。, 2021年HanLPv2, up主来到北京立方庭参观自然语义科技公司。"
https://github.com/hankcs/HanLP,demo_document.py,0,0.0,9,9.89,51,56.04,7,7.69,13,14.29,11,12.09,0,0,2,0,,"count_sentences, get_by_prefix, hanlp_common, pretty_print, pretty_text, print, squeeze, str, to_pretty",hanlp_common.document.Document,,,"doc, pretty_text",,"

,  sentence(s), ARG0, ARG1, FAC, GPE, It has , LOCATION, NN, NR, ORG, ORGANIZATION, PERSON, PRED, PU, VV, compound, conj, dep, dobj, n, name, ner, ner/msra, ner/ontonotes, nr, ns, nsubj, nz, pos/ctb, pos/pku, punct, root, srl, tok, tok/coarse, tok/fine, v, w, 。, 公司, 北京, 北京立方庭, 参观, 晓美焰, 来到, 科技, 立方庭, 自然, 自然语义科技公司, 语义","0, 1, 2, 3, 4, 5, 9","# -*- coding:utf-8 -*-, # Access an annotation by its task name, # Access the n-th sentence, # Author: hankcs, # Create a document from a dict, # Create a document or get a document from HanLP.parse, # Date: 2022-10-26 23:40, # Get number of sentences, # Get the first annotation for NER, # Pretty print it right in your console or notebook, # Pretty print using a different NER annotation, # To save the pretty prints in a str, # print(doc) or str(doc) to get its JSON representation","公司, 北京, 北京立方庭, 参观, 晓美焰, 来到, 科技, 立方庭, 自然, 自然语义科技公司, 语义"
https://github.com/hankcs/HanLP,demo_mlm.py,0,0.0,3,25.0,3,25.0,0,0.0,4,33.33,2,16.67,0,0,1,0,,"hanlp, load, print",hanlp.components.lm.mlm.MaskedLanguageModel,,,mlm,,"bert-base-chinese, 巴黎是[MASK][MASK]的首都。, 生活的真谛是[MASK]。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Batching is always faster, # Date: 2022-01-29 21:11","巴黎是[MASK][MASK]的首都。, 生活的真谛是[MASK]。"
https://github.com/hankcs/HanLP,demo_mtl.py,0,0.0,5,20.83,2,8.33,0,0.0,9,37.5,8,33.33,0,0,1,0,,"HanLP, hanlp, hanlp_common, pretty_print, print","hanlp.load, hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH, hanlp_common.document.Document",,,HanLP,,"2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 阿婆主来到北京立方庭参观自然语义科技公司。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # CLOSE是自然语义标注的闭源语料库，BASE是中号模型，ZH中文, # Date: 2020-12-31 13:51, # doc.pretty_print(ner='ner/ontonotes', pos='pku'), # 即时可视化，防止换行请最大化窗口，推荐在Jupyter Notebook里调用, # 指定可视化OntoNotes标准的NER, # 返回类型Document是dict的子类，打印出来兼容JSON, # 默认执行全部任务","1为生产环境带来次世代最先进的多语种NLP技术。, 2021年HanLPv2, CLOSE是自然语义标注的闭源语料库，BASE是中号模型，ZH中文, 即时可视化，防止换行请最大化窗口，推荐在Jupyter Notebook里调用, 指定可视化OntoNotes标准的NER, 返回类型Document是dict的子类，打印出来兼容JSON, 阿婆主来到北京立方庭参观自然语义科技公司。, 默认执行全部任务"
https://github.com/hankcs/HanLP,demo_ner_dict.py,0,0.0,7,23.33,10,33.33,0,0.0,6,20.0,7,23.33,0,0,2,0,,"HanLP, dict_tags, dict_whitelist, doc, hanlp, pretty_print, print","hanlp.components.mtl.tasks.ner.tag_ner.TaggingNamedEntityRecognition, hanlp.load, hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH, hanlp.utils.io_util.get_resource",,,"HanLP, doc",,"2021年测试高血压是138，时间是午饭后2点45，低血压是44, O, S-PERSON, TIME, ner/msra, 他在浙江金华出生，他的名字叫金华。, 午饭后, 叫, 名字, 金华",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-04-29 11:06, # HanLP.save(get_resource(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH)), # See https://hanlp.hankcs.com/docs/api/hanlp/components/mtl/tasks/ner/tag_ner.html, # 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php","2021年测试高血压是138，时间是午饭后2点45，低血压是44, 他在浙江金华出生，他的名字叫金华。, 午饭后, 叫, 名字, 金华, 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php"
https://github.com/hankcs/HanLP,demo_parse_constituency.py,0,0.0,26,40.0,18,27.69,3,4.62,8,12.31,10,15.38,1,0,13,0,,"con, flat, hanlp, hanlp_common, height, insert, isinstance, items, k, label, merge_pos_into_con, offset, pos, pretty_print, print, set_label, squeeze, str, subtree, subtrees, tag, tags, tok, tree, v, zip","hanlp.load, hanlp.pipeline, hanlp.pretrained.constituency.CTB9_CON_FULL_TAG_ELECTRA_SMALL, hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL, hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH, hanlp_common.document.Document",merge_pos_into_con,,"con, doc, flat, k, nlp, offset, pos, subtree, tag, tags, tok, tree, v",,"*, 2021年, 2021年HanLPv2.1带来最先进的多语种NLP技术。, HanLPv2.1, NLP, The pipeline looks like this: , _, con, pos, tok, 。, 先进, 多, 带来, 技术, 最, 的, 语种","0, 1, 2","# -*- coding:utf-8 -*-, # ATTENTION: Pipelines are usually slower than MTL but they are more flexible., # Author: hankcs, # Date: 2022-01-18 11:09, # If you need to parse raw text, simply add a tokenizer into this pipeline., # The first level of non-terminals are PoS tags. So usually a PoS model is piped., # The rest of this tutorial is written for clever users., # To speed up, parse multiple sentences at once, and use a GPU.","1带来最先进的多语种NLP技术。, 2021年, 2021年HanLPv2, 先进, 多, 带来, 技术, 最, 的, 语种"
https://github.com/hankcs/HanLP,demo_pipeline.py,0,0.0,5,19.23,12,46.15,1,3.85,6,23.08,2,7.69,0,0,2,0,,"HanLP, doc, hanlp, pretty_print, print","hanlp.load, hanlp.pipeline, hanlp.utils.rules.split_sentence",,,"HanLP, doc",,"2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。, CTB9_CON_ELECTRA_SMALL, CTB9_DEP_ELECTRA_SMALL, CTB9_POS_ELECTRA_SMALL, CTB9_TOK_ELECTRA_SMALL, MSRA_NER_ELECTRA_SMALL_ZH, con, dep, ner, pos, sentences, tok",False,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 20:47, # Pipeline allows blending multiple callable functions no matter they are a rule, a TensorFlow component or a PyTorch, # one. However, it's slower than the MTL framework., # pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ALBERT_BASE)  # In case both tf and torch are used, load tf first.","1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。, 2021年HanLPv2"
https://github.com/hankcs/HanLP,demo_pos_dict.py,0,0.0,5,15.15,12,36.36,0,0.0,6,18.18,10,30.3,0,0,1,0,,"HanLP, dict_tags, hanlp, print, tests","hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.components.mtl.tasks.pos.TransformerTagging, hanlp.components.mtl.tasks.tok.tag_tok.TaggingTokenization, hanlp.load, hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, tests.cdroot",,,HanLP,,"HanLP, HanLP为生产环境带来次世代最先进的多语种NLP技术。, pos/ctb, state-of-the-art-tool, 动词, 名词, 希望, 我的希望是希望张晚霞的背影被晚霞映红。, 根据上下文自定义词性:, 的, 自定义单个词性:, 补语成分",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-15 22:26, # Demonstrates custom dict in part-of-speech tagging, # See also https://hanlp.hankcs.com/docs/api/hanlp/components/taggers/transformer_tagger.html, # 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php","HanLP为生产环境带来次世代最先进的多语种NLP技术。, 动词, 名词, 希望, 我的希望是希望张晚霞的背影被晚霞映红。, 根据上下文自定义词性, 的, 自定义单个词性, 补语成分, 需要算法基础才能理解，初学者可参考 http://nlp.hankcs.com/book.php"
https://github.com/hankcs/HanLP,demo_sts.py,0,0.0,3,16.67,6,33.33,0,0.0,3,16.67,6,33.33,0,0,1,0,,"hanlp, print, sim","hanlp.load, hanlp.pretrained.sts.STS_ELECTRA_BASE_ZH",,,sim,,"上海到北京的动车票, 北京到上海的动车票, 无线上网卡和无线路由器怎么用, 无线路由器怎么无线上网, 看图猜一电影名, 看图猜电影",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-24 13:15","上海到北京的动车票, 北京到上海的动车票, 无线上网卡和无线路由器怎么用, 无线路由器怎么无线上网, 看图猜一电影名, 看图猜电影"
https://github.com/hankcs/HanLP,demo_word2vec.py,0,0.0,5,15.62,10,31.25,2,6.25,4,12.5,11,34.38,0,0,2,0,,"hanlp, most_similar, print, torch, word2vec","hanlp.load, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_PKU, torch.nn.functional.cosine_similarity",,,"vec, word2vec",,"上海, 优秀, 但是在doc2vec模式下OOV也可以进行相似度计算：, 先进, 国家图书馆推出2022年春节主题活动, 水果, 甚至可以处理短文本：, 获取语义最相似的词语：, 非常寒冷, 非常寒冷是OOV所以无法获取：","0, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-12 18:33, # print(word2vec.most_similar(['上海', '寒冷'])) # batching更快","print(word2vec.most_similar(['上海', '寒冷'])) # batching更快, 上海, 优秀, 但是在doc2vec模式下OOV也可以进行相似度计算：, 先进, 国家图书馆推出2022年春节主题活动, 水果, 甚至可以处理短文本：, 获取语义最相似的词语：, 非常寒冷, 非常寒冷是OOV所以无法获取："
https://github.com/hankcs/HanLP,train_sota_bert_pku.py,0,0.0,6,28.57,3,14.29,8,38.1,4,19.05,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, print, save_dir, tests","hanlp.common.dataset.SortingSamplerBuilder, hanlp.components.tokenizers.transformer.TransformerTaggingTokenizer, hanlp.datasets.tokenization.sighan2005.pku.SIGHAN2005_PKU_TEST, hanlp.datasets.tokenization.sighan2005.pku.SIGHAN2005_PKU_TRAIN_ALL, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , bert-base-chinese, data/model/cws/sighan2005_pku_bert_base_96.7","0.01, 0.1, 1660853059, 1e-06, 3, 300, 32, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Conventionally, no devset is used. See Tian et al. (2020)., # Date: 2020-08-11 02:47",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 13:51",
https://github.com/hankcs/HanLP,demo_classifier.py,0,0.0,7,43.75,2,12.5,2,12.5,4,25.0,1,6.25,0,0,2,0,,"classifier, file_to_inputs, hanlp, outputs, predict, print, transform","hanlp.datasets.classification.sentiment.CHNSENTICORP_ERNIE_TEST, hanlp.load",,,"classifier, outputs",,"CHNSENTICORP_BERT_BASE_ZH, 前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！","5, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 03:52, # predict a whole file in batch mode",前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！
https://github.com/hankcs/HanLP,demo_client.py,0,0.0,27,46.55,14,24.14,4,6.9,5,8.62,8,13.79,0,0,9,0,,"MakeNdarray, Predict, PredictRequest, PredictionServiceStub, Y_to_outputs, channel, grpc, hanlp, inputs, inputs_to_dataset, iter, list, model_spec, name, next, outputs, print, request, result, samples, signature_name, stub, tagger, tensorflow, tensorflow_core, tensorflow_serving, transform","grpc.insecure_channel, hanlp.common.keras_component.KerasComponent, hanlp.load, hanlp.pretrained.pos.CTB5_POS_RNN, tensorflow_core.python.framework.tensor_util, tensorflow_serving.apis.predict_pb2, tensorflow_serving.apis.prediction_service_pb2_grpc, tf.float32, tf.make_tensor_proto",,,"channel, inputs, prediction, request, result, samples, stub, tagger, transform",,"ctb5_pos_rnn_20191229_015325, dense, embedding_input, localhost, serving_default, {host}:{port}, 和, 和平, 商品, 希望, 我, 是, 服务, 的","0, 10.0, 8500, True","# -*- coding:utf-8 -*-, # 10 secs timeout, # Author: hankcs, # Date: 2020-01-08 04:43, # pip3 install tensorflow-serving-api-gpu","和, 和平, 商品, 希望, 我, 是, 服务, 的"
https://github.com/hankcs/HanLP,demo_cws.py,0,0.0,24,48.0,12,24.0,2,4.0,5,10.0,7,14.0,1,0,11,0,,"append, dic, end, extend, finditer, flat, group, hanlp, keys, len, offset, pred, print, re, sents, split_by_dic, start, str, tag, text, tokenizer, word, words, zip","hanlp.load, hanlp.pretrained.tok.LARGE_ALBERT_BASE, re.compile",split_by_dic,,"dic, flat, offset, p, pred, sents, tag, text, tokenizer, word, words",,"(, ), HanLP支援臺灣正體、香港繁體，具有新詞辨識能力的中文斷詞系統, NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。, custom_dict, smart, |, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。, 商品和服务, 聪明人, 自定义词典, 萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。","0, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 21:25, # However, you should use some trie trees for production, # We use regular expression for the sake of simplicity.","HanLP支援臺灣正體、香港繁體，具有新詞辨識能力的中文斷詞系統, NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。, 商品和服务, 聪明人, 自定义词典, 萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。"
https://github.com/hankcs/HanLP,demo_cws_trie.py,0,0.0,25,56.82,11,25.0,2,4.55,3,6.82,3,6.82,2,0,13,0,,"append, each, end, hanlp, hanlp_trie, items, len, merge_parts, offsets, parse_longest, parts, pre_start, print, sents, sorted, split_sents, start, str, text, tokenizer, update, value, words, x, zip","hanlp.load, hanlp.pipeline, hanlp_trie.trie.Trie","merge_parts, split_sents",,"each, end, items, offsets, pre_start, sents, start, text, tokenizer, trie, value, words, x",,"LARGE_ALBERT_BASE, NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。, custom_dict, merged, offsets, parts, smart, tokens, words, 聪明人, 自定义词典","0, 1","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 21:25","NLP统计模型没有加规则，聪明人知道自己加。英文、数字、自定义词典统统都是规则。, 聪明人, 自定义词典"
https://github.com/hankcs/HanLP,demo_dep.py,0,0.0,5,26.32,7,36.84,0,0.0,3,15.79,4,21.05,0,0,3,0,,"hanlp, print, sent, syntactic_parser, tree","hanlp.load, hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH",,,"sent, syntactic_parser, tree",,"CD, NN, VV, 两, 头, 烧, 蜡烛",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 21:25","两, 头, 烧, 蜡烛"
https://github.com/hankcs/HanLP,demo_fasttext.py,0,0.0,5,29.41,3,17.65,1,5.88,5,29.41,3,17.65,0,0,2,0,,"fasttext, hanlp, print, torch, vec","hanlp.load, hanlp.pretrained.fasttext.FASTTEXT_WIKI_300_ZH, torch.nn.functional.cosine_similarity",,,"fasttext, vec",,"今天, 单词, 词语",0,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-12 18:33, # PyTorch, otherwise don't bother to use this., # fasttext is a `torch.nn.Module`. Unless you know how to code in","今天, 单词, 词语"
https://github.com/hankcs/HanLP,demo_multiprocess.py,0,0.0,11,40.74,4,14.81,2,7.41,8,29.63,2,7.41,1,0,3,0,,"__name__, hanlp, job, map, multiprocessing, num_proc, pool, print, range, tokenizer, worker","hanlp.load, hanlp.pretrained.tok.LARGE_ALBERT_BASE, multiprocessing.Pool, multiprocessing.set_start_method",worker,,"num_proc, pool, tokenizer",,"__main__, spawn, 号进程的任务, 给","2, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-02-15 11:30, # Important! The python multiprocessing package defaults to just call fork when creating a child process., # See https://github.com/tensorflow/tensorflow/issues/8220#issuecomment-302826884, # See https://sefiks.com/2019/03/20/tips-and-tricks-for-gpu-and-multiprocessing-in-tensorflow/, # This cannot work when the child process calls async code (i.e TensorFlow is multithreaded)., # only spawn works with TensorFlow","号进程的任务, 给"
https://github.com/hankcs/HanLP,demo_ner.py,0,0.0,5,41.67,2,16.67,0,0.0,3,25.0,2,16.67,0,0,1,0,,"hanlp, list, predict, print, recognizer","hanlp.load, hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH",,,recognizer,,"上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。, 萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 19:52","上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。, 萨哈夫说，伊拉克将同联合国销毁伊拉克大规模杀伤性武器特别委员会继续保持合作。"
https://github.com/hankcs/HanLP,demo_pipeline.py,0,0.0,11,37.93,11,37.93,1,3.45,3,10.34,3,10.34,0,0,8,0,,"deployed, doc, hanlp, pipeline, print, save, semantic_parser, syntactic_parser, tagger, text, tokenizer","hanlp.load, hanlp.pipeline, hanlp.utils.rules.split_sentence",,,"deployed, doc, pipeline, semantic_parser, syntactic_parser, tagger, text, tokenizer",,"CTB7_BIAFFINE_DEP_ZH, CTB9_POS_ALBERT_BASE, HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。
HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。
, LARGE_ALBERT_BASE, SEMEVAL16_TEXT_BIAFFINE_ZH, part_of_speech_tags, semantic_dependencies, sentences, syntactic_dependencies, tokens, zh.json",False,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 03:24","HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。, HanLP是一系列模型与算法组成的自然语言处理工具包，目标是普及自然语言处理在生产环境中的应用。, 内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版。"
https://github.com/hankcs/HanLP,demo_pos.py,0,0.0,4,12.12,13,39.39,0,0.0,3,9.09,13,39.39,0,0,1,0,,"hanlp, predict, print, tagger","hanlp.load, hanlp.pretrained.pos.CTB9_POS_ALBERT_BASE",,,tagger,,"世界, 和平, 地, 希望, 快, 我, 批处理, 支持, 是, 更, 的, 速度, 预测",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 21:25","世界, 和平, 地, 希望, 快, 我, 批处理, 支持, 是, 更, 的, 速度, 预测"
https://github.com/hankcs/HanLP,demo_sdp.py,0,0.0,4,21.05,8,42.11,0,0.0,3,15.79,4,21.05,0,0,2,0,,"hanlp, print, semantic_parser, sent",hanlp.load,,,"semantic_parser, sent",,"CD, NN, SEMEVAL16_NEWS_BIAFFINE_ZH, VV, 两, 头, 烧, 蜡烛",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 23:55","两, 头, 烧, 蜡烛"
https://github.com/hankcs/HanLP,demo_serving.py,0,0.0,4,36.36,1,9.09,0,0.0,3,27.27,3,27.27,0,0,1,0,,"hanlp, print, serve, tagger","hanlp.common.keras_component.KerasComponent, hanlp.load, hanlp.pretrained.pos.CTB5_POS_RNN",,,tagger,,商品 和 服务,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-06 20:23","和, 商品, 服务"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 20:36",
https://github.com/hankcs/HanLP,finetune_ner.py,0,0.0,12,30.77,11,28.21,2,5.13,7,17.95,7,17.95,0,0,6,0,,"HanLP, fit, hanlp, open, os, out, print, save_dir, tests, write, your_development_corpus, your_training_corpus","hanlp.components.ner.transformer_ner.TransformerNamedEntityRecognizer, hanlp.load, hanlp.pipeline, hanlp.pretrained.ner.MSRA_NER_ELECTRA_SMALL_ZH, hanlp.pretrained.tok.FINE_ELECTRA_SMALL_ZH, os.makedirs, os.path.dirname, os.path.exists, tests.cdroot",,,"HanLP, ner, out, save_dir, your_development_corpus, your_training_corpus",,"Load fine-tuned model, Start fine-tuning , data/ner/finetune/model, data/ner/finetune/word_to_iobes.tsv, hfl/chinese-electra-180g-small-discriminator, ner, tok, w, 晓美焰来到北京立方庭参观自然语义科技公司。, 训练	B-NLP
语料	E-NLP
为	O
IOBES	O
格式	O
, 训练语料为IOBES格式","50, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2023-10-18 18:49, # Since the corpus is small, overfit it, # Use a different one in reality, # You MUST set the same parameters with the fine-tuning model:, 训练\tB-NLP
语料\tE-NLP
为\tO
IOBES\tO
格式\tO
","为, 晓美焰来到北京立方庭参观自然语义科技公司。, 格式, 训练, 训练\tB-NLP
语料\tE-NLP
为\tO
IOBES\tO
格式\tO
, 训练语料为IOBES格式, 语料"
https://github.com/hankcs/HanLP,open_base.py,0,0.0,7,18.42,14,36.84,13,34.21,3,7.89,1,2.63,0,0,3,0,,"evaluate, fit, hanlp, hanlp_demo, load, save_dir, tests","hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.transform.NormalizeCharacter, hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.components.mtl.tasks.constituency.CRFConstituencyParsing, hanlp.components.mtl.tasks.dep.BiaffineDependencyParsing, hanlp.components.mtl.tasks.ner.tag_ner.TaggingNamedEntityRecognition, hanlp.components.mtl.tasks.pos.TransformerTagging, hanlp.components.mtl.tasks.sdp.BiaffineSemanticDependencyParsing, hanlp.components.mtl.tasks.srl.bio_srl.SpanBIOSemanticRoleLabeling, hanlp.components.mtl.tasks.tok.tag_tok.TaggingTokenization, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_DEV, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_TEST, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_CWS_DEV, hanlp.datasets.parsing.ctb8.CTB8_CWS_TEST, hanlp.datasets.parsing.ctb8.CTB8_CWS_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_POS_DEV, hanlp.datasets.parsing.ctb8.CTB8_POS_TEST, hanlp.datasets.parsing.ctb8.CTB8_POS_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_SD330_DEV, hanlp.datasets.parsing.ctb8.CTB8_SD330_TEST, hanlp.datasets.parsing.ctb8.CTB8_SD330_TRAIN, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_DEV_CONLLU, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TEST_CONLLU, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TRAIN_CONLLU, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_DEV, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_TEST, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_TRAIN, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbedding, hanlp.layers.transformers.relative_transformer.RelativeTransformerEncoder, hanlp.utils.lang.zh.char_table.HANLP_CHAR_TABLE_JSON, hanlp.utils.log_util.cprint, hanlp_demo.block_windows, tests.cdroot",,,"mtl, save_dir, tasks",,"BMES, Model saved in [cyan], [/cyan], con, data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base, dep, hfl/chinese-electra-180g-base-discriminator, ner, pos, sdp, srl, tok, token, 华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动","0.001, 0.1, 1, 2, 2048, 30, 32, 510, 512, 5e-05, 768, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-03 14:24",华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动
https://github.com/hankcs/HanLP,open_small.py,0,0.0,7,17.95,15,38.46,13,33.33,3,7.69,1,2.56,0,0,3,0,,"evaluate, fit, hanlp, hanlp_demo, load, save_dir, tests","hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.transform.NormalizeCharacter, hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.components.mtl.tasks.constituency.CRFConstituencyParsing, hanlp.components.mtl.tasks.dep.BiaffineDependencyParsing, hanlp.components.mtl.tasks.ner.tag_ner.TaggingNamedEntityRecognition, hanlp.components.mtl.tasks.pos.TransformerTagging, hanlp.components.mtl.tasks.sdp.BiaffineSemanticDependencyParsing, hanlp.components.mtl.tasks.srl.bio_srl.SpanBIOSemanticRoleLabeling, hanlp.components.mtl.tasks.tok.tag_tok.TaggingTokenization, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST, hanlp.datasets.ner.msra.MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_DEV, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_TEST, hanlp.datasets.parsing.ctb8.CTB8_BRACKET_LINE_NOEC_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_CWS_DEV, hanlp.datasets.parsing.ctb8.CTB8_CWS_TEST, hanlp.datasets.parsing.ctb8.CTB8_CWS_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_POS_DEV, hanlp.datasets.parsing.ctb8.CTB8_POS_TEST, hanlp.datasets.parsing.ctb8.CTB8_POS_TRAIN, hanlp.datasets.parsing.ctb8.CTB8_SD330_DEV, hanlp.datasets.parsing.ctb8.CTB8_SD330_TEST, hanlp.datasets.parsing.ctb8.CTB8_SD330_TRAIN, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_DEV_CONLLU, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TEST_CONLLU, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TRAIN_CONLLU, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_DEV, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_TEST, hanlp.datasets.srl.ontonotes5.chinese.ONTONOTES5_CONLL12_CHINESE_TRAIN, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbedding, hanlp.layers.transformers.relative_transformer.RelativeTransformerEncoder, hanlp.utils.lang.zh.char_table.HANLP_CHAR_TABLE_JSON, hanlp.utils.log_util.cprint, hanlp_demo.block_windows, tests.cdroot",,,"mtl, save_dir, tasks",,"BMES, Model saved in [cyan], Model will be saved in [cyan], [/cyan], con, data/model/mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small, dep, hfl/chinese-electra-180g-small-discriminator, ner, pos, sdp, srl, tok, token, 华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动","0.001, 0.1, 1, 128, 1280, 256, 30, 32, 510, 512, 5e-05, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-03 14:24",华纳音乐旗下的新垣结衣在12月21日于日本武道馆举办歌手出道活动
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-31 20:12",
https://github.com/hankcs/HanLP,finetune_msra_ner_albert.py,0,0.0,10,55.56,4,22.22,0,0.0,3,16.67,1,5.56,0,0,2,0,,"evaluate, fit, hanlp, list, load, predict, print, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH, tests.cdroot",,,"recognizer, save_dir",,"Model saved in , albert_base_zh, data/model/ner/finetune_ner_albert_base_zh_msra, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15",上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。
https://github.com/hankcs/HanLP,train_chnsenticorp_bert.py,0,0.0,8,50.0,3,18.75,1,6.25,3,18.75,1,6.25,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.classifiers.transformer_classifier_tf.TransformerClassifierTF, hanlp.components.classifiers.transformer_classifier_tf.TransformerTextTransform, hanlp.datasets.classification.sentiment.CHNSENTICORP_ERNIE_DEV, hanlp.datasets.classification.sentiment.CHNSENTICORP_ERNIE_TEST, hanlp.datasets.classification.sentiment.CHNSENTICORP_ERNIE_TRAIN, tests.cdroot",,,"classifier, save_dir",,"bert-base-chinese, data/model/classification/chnsenticorp_bert_base, 前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！",0,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 21:01",前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！
https://github.com/hankcs/HanLP,train_conll03_ner_bert.py,0,0.0,9,45.0,6,30.0,2,10.0,3,15.0,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tagger, tests","hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.datasets.ner.conll03.CONLL03_EN_DEV, hanlp.datasets.ner.conll03.CONLL03_EN_TEST, hanlp.datasets.ner.conll03.CONLL03_EN_TRAIN, tests.cdroot",,,"save_dir, tagger",,"Model saved in , West Indian all-rounder Phil Simmons eats apple ., accuracy, bert-base-cased, data/model/ner/ner_conll03_bert_base_cased_en, f1","32, False","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-25 21:34",
https://github.com/hankcs/HanLP,train_conll03_ner_flair.py,0,0.0,9,20.93,16,37.21,11,25.58,7,16.28,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, predict, print, save_dir, tagger, tensorflow, tests","hanlp.components.ner.ner_tf.RNNNamedEntityRecognizerTF, hanlp.datasets.ner.conll03.CONLL03_EN_TEST, hanlp.datasets.ner.conll03.CONLL03_EN_TRAIN, hanlp.pretrained.glove.GLOVE_6B_100D, hanlp.pretrained.rnnlm.FLAIR_LM_BW_WMT11_EN_TF, hanlp.pretrained.rnnlm.FLAIR_LM_FW_WMT11_EN_TF, tests.cdroot, tf.keras.optimizers.Adam",,,"save_dir, tagger",,"HanLP>ContextualStringEmbedding, HanLP>Word2VecEmbedding, West Indian all-rounder Phil Simmons eats apple ., backward_model_path, class_name, config, crf, data/model/conll03-ner-rnn-flair, embeddings_initializer, expand_vocab, f1, filepath, forward_model_path, lowercase, trainable, zero","0.1, 0.5, 0.9, 0.999, 100, 1e-08, 2, 256, 32, False, True","#                       ['Not', 'this', 'year', '.']])), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-25 21:34, # [['DT', 'VBZ', 'DT', 'JJ', 'NN'], ['RB', 'DT', 'NN', '.']], # print(tagger.predict([['This', 'is', 'an', 'old', 'story'],, # tagger.load(save_dir)",
https://github.com/hankcs/HanLP,train_ctb5_dep.py,0,0.0,9,18.75,25,52.08,2,4.17,3,6.25,9,18.75,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineDependencyParserTF, hanlp.datasets.parsing.ctb5.CTB5_DEP_DEV, hanlp.datasets.parsing.ctb5.CTB5_DEP_TEST, hanlp.datasets.parsing.ctb5.CTB5_DEP_TRAIN, hanlp.pretrained.word2vec.CTB5_FASTTEXT_300_CN, tests.cdroot",,,"parser, save_dir, sentence",,"CD, HanLP>Word2VecEmbedding, M, NN, NR, VV, class_name, config, data/model/dep/biaffine_ctb, embeddings_initializer, expand_vocab, filepath, lowercase, normalize, trainable, zero, 三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 18:33","三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾"
https://github.com/hankcs/HanLP,train_ctb5_pos_rnn.py,0,0.0,6,40.0,6,40.0,0,0.0,3,20.0,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, print, save_dir, tests","hanlp.components.taggers.pos_tf.RNNPartOfSpeechTaggerTF, hanlp.datasets.pos.ctb5.CTB5_POS_DEV, hanlp.datasets.pos.ctb5.CTB5_POS_TEST, hanlp.datasets.pos.ctb5.CTB5_POS_TRAIN, hanlp.pretrained.fasttext.FASTTEXT_WIKI_300_ZH, tests.cdroot",,,"save_dir, tagger",,"HanLP>FastTextEmbedding, Model saved in , class_name, config, data/model/pos/ctb5_pos_rnn_fasttext, filepath",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:46",
https://github.com/hankcs/HanLP,train_ctb7_dep.py,0,0.0,9,18.75,25,52.08,2,4.17,3,6.25,9,18.75,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineDependencyParserTF, hanlp.datasets.parsing.ctb5.CIP_W2V_100_CN, hanlp.datasets.parsing.ctb7.CTB7_DEP_DEV, hanlp.datasets.parsing.ctb7.CTB7_DEP_TEST, hanlp.datasets.parsing.ctb7.CTB7_DEP_TRAIN, tests.cdroot",,,"parser, save_dir, sentence",,"CD, HanLP>Word2VecEmbedding, M, NN, NR, VV, class_name, config, data/model/dep/biaffine_ctb7, embeddings_initializer, expand_vocab, filepath, lowercase, normalize, trainable, zero, 三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 18:33","三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾"
https://github.com/hankcs/HanLP,train_ctb9_pos_albert.py,0,0.0,7,24.14,10,34.48,4,13.79,3,10.34,5,17.24,0,0,2,0,,"evaluate, fit, hanlp, load, print, save_dir, tests","hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF, tests.cdroot",,,"save_dir, tagger",,"Model saved in , data/model/pos/ctb9_albert_base, data/pos/ctb9/test.tsv, data/pos/ctb9/train.tsv, uer/albert-base-chinese-cluecorpussmall, 和平, 希望, 我, 是, 的","0.1, 130, 20, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15","和平, 希望, 我, 是, 的"
https://github.com/hankcs/HanLP,train_ctb9_pos_electra.py,0,0.0,7,24.14,10,34.48,4,13.79,3,10.34,5,17.24,0,0,2,0,,"evaluate, fit, hanlp, load, print, save_dir, tests","hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF, tests.cdroot",,,"save_dir, tagger",,"Model saved in , data/model/pos/ctb9_electra_small_zh_epoch_20, data/pos/ctb9/test.tsv, data/pos/ctb9/train.tsv, hfl/chinese-electra-small-discriminator, 和平, 希望, 我, 是, 的","0.1, 130, 20, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15","和平, 希望, 我, 是, 的"
https://github.com/hankcs/HanLP,train_msra_ner_albert.py,0,0.0,10,45.45,6,27.27,1,4.55,4,18.18,1,4.55,0,0,2,0,,"evaluate, fit, hanlp, list, load, predict, print, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, tests.cdroot",,,"recognizer, save_dir",,"Model saved in , accuracy, data/model/ner/msra_ner_albert_base, f1, uer/albert-base-chinese-cluecorpussmall, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。",5e-05,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15, # Use accuracy to speed up training",上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。
https://github.com/hankcs/HanLP,train_msra_ner_bert.py,0,0.0,10,47.62,6,28.57,0,0.0,4,19.05,1,4.76,0,0,2,0,,"evaluate, fit, hanlp, list, load, predict, print, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, tests.cdroot",,,"recognizer, save_dir",,"Model saved in , accuracy, bert-base-chinese, data/model/ner/ner_bert_base_msra_1, f1, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15, # accuracy is faster",上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。
https://github.com/hankcs/HanLP,train_msra_ner_electra.py,0,0.0,10,45.45,6,27.27,1,4.55,4,18.18,1,4.55,0,0,2,0,,"evaluate, fit, hanlp, list, load, predict, print, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, tests.cdroot",,,"recognizer, save_dir",,"Model saved in , accuracy, data/model/ner/ner_electra_small_zh_msra_sparse_categorical_crossentropy, f1, hfl/chinese-electra-small-discriminator, 上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。",5e-05,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15, # Use accuracy to speed up training",上海华安工业（集团）公司董事长谭旭光和秘书张晚霞来到美国纽约现代艺术博物馆参观。
https://github.com/hankcs/HanLP,train_msra_ner_ngram_conv.py,0,0.0,6,31.58,8,42.11,2,10.53,3,15.79,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.NgramConvNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_MSR, tests.cdroot",,,"recognizer, save_dir",,"HanLP>Word2VecEmbedding, class_name, config, data/model/ner/msra_ner_ngram_conv, expand_vocab, filepath, lowercase, trainable","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15",
https://github.com/hankcs/HanLP,train_msra_ner_rnn.py,0,0.0,6,50.0,1,8.33,2,16.67,3,25.0,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, recognizer, save_dir, tests","hanlp.components.ner.ner_tf.RNNNamedEntityRecognizerTF, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_DEV, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TEST, hanlp.datasets.ner.msra.MSRA_NER_CHAR_LEVEL_TRAIN, hanlp.pretrained.word2vec.RADICAL_CHAR_EMBEDDING_100, tests.cdroot",,,"recognizer, save_dir",,data/model/ner/msra_ner_rnn,"100, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:15",
https://github.com/hankcs/HanLP,train_ptb_dep_biaffine_albert.py,0,0.0,7,29.17,3,12.5,7,29.17,7,29.17,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, load, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, tests.cdroot",,,"parser, save_dir",,"Model saved in , albert-xxlarge-v2, data/model/dep/ptb_albert3","0.002, 0.1, 0.33, 150, 1e-05, 256, False","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # early_stopping_patience=10,, # output = f'{save_dir}/test.predict.conll', # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_biaffine_bert.py,0,0.0,6,23.08,3,11.54,1,3.85,16,61.54,0,0.0,0,0,2,0,,"evaluate, hanlp, load, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, tests.cdroot",,,"parser, save_dir",,"Model saved in , data/model/dep/ptb_bert_1e-5, tarjan",False,"#            # early_stopping_patience=10,, #            ), #            batch_size=3000,, #            learning_rate=2e-3,, #            learning_rate_transformer=1e-5,, #            samples_per_batch=150,, #            token_mapping=PTB_TOKEN_MAPPING,, #            transformer_dropout=.33,, #            warmup_steps_ratio=.1,, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # output = f'{save_dir}/test.predict.conll', # parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',, # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_biaffine_bert_96.6.py,0,0.0,9,36.0,6,24.0,1,4.0,9,36.0,0,0.0,0,0,5,0,,"evaluate, hanlp, las, load, output, print, save_dir, tests, uas","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, tests.cdroot",,,"las, output, parser, save_dir, uas",," LAS: , .4f, /test.predict.conll, Model saved in , Official UAS: , data/model/dep/ptb_bert_96.61",False,"#            ), #            batch_size=3000,, #            samples_per_batch=150,, #            token_mapping=PTB_TOKEN_MAPPING,, #            warmup_steps_ratio=.1,, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',",
https://github.com/hankcs/HanLP,train_ptb_dep_biaffine_bert_positional.py,0,0.0,5,20.83,2,8.33,8,33.33,9,37.5,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, tests.cdroot",,,"parser, save_dir",,"bert-base-uncased, data/model/dep/ptb_bert_positional_diff_lr","0.0001, 0.1, 0.33, 128, 150, 1e-05, 3000, False","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # early_stopping_patience=10,, # output = f'{save_dir}/test.predict.conll', # parser.load(save_dir), # print(f'Model saved in {save_dir}'), # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_sa_albert.py,0,0.0,6,16.22,2,5.41,1,2.7,28,75.68,0,0.0,0,0,2,0,,"evaluate, hanlp, load, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.components.parsers.biaffine_parser_tf.StructuralAttentionDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, hanlp.pretrained.glove.GLOVE_840B_300D, tests.cdroot",,,"parser, save_dir",,"Model saved in , data/model/dep/ptb_sa_glove",False,"#                                 # 'embeddings_initializer': 'zero',, #                                 'cpu': False, #                                 'expand_vocab': False,, #                                 'filepath': GLOVE_840B_300D,, #                                 'lowercase': True,, #                                 'trainable': False,, #                             'config': {, #                             }}, #            # alpha=1,, #            # early_stopping_patience=10,, #            # learning_rate=2e-3,, #            # learning_rate_transformer=1e-5,, #            # num_decoder_layers=2,, #            ), #            batch_size=3000,, #            masked_lm_dropout=.33,, #            masked_lm_embed={'class_name': 'HanLP>Word2VecEmbedding',, #            samples_per_batch=150,, #            token_mapping=PTB_TOKEN_MAPPING,, #            transformer_dropout=.33,, #            warmup_steps_ratio=.1,, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # output = f'{save_dir}/test.predict.conll', # parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',, # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_sa_albert_topk.py,0,0.0,7,26.92,3,11.54,7,26.92,9,34.62,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, load, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.components.parsers.biaffine_parser_tf.StructuralAttentionDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, hanlp.pretrained.glove.GLOVE_840B_300D, tests.cdroot",,,"parser, save_dir",,"Model saved in , bert-base-uncased, data/model/dep/ptb_sa_topk","0.002, 0.1, 0.33, 150, 1e-05, 3000, False","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # alpha=1,, # early_stopping_patience=10,, # num_decoder_layers=2,, # output = f'{save_dir}/test.predict.conll', # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_sa_bert.py,0,0.0,6,16.22,2,5.41,1,2.7,28,75.68,0,0.0,0,0,2,0,,"evaluate, hanlp, load, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.components.parsers.biaffine_parser_tf.StructuralAttentionDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, hanlp.pretrained.glove.GLOVE_840B_300D, tests.cdroot",,,"parser, save_dir",,"Model saved in , data/model/dep/ptb_sa_glove",False,"#                                 # 'embeddings_initializer': 'zero',, #                                 'cpu': False, #                                 'expand_vocab': False,, #                                 'filepath': GLOVE_840B_300D,, #                                 'lowercase': True,, #                                 'trainable': False,, #                             'config': {, #                             }}, #            # alpha=1,, #            # early_stopping_patience=10,, #            # learning_rate=2e-3,, #            # learning_rate_transformer=1e-5,, #            # num_decoder_layers=2,, #            ), #            batch_size=3000,, #            masked_lm_dropout=.33,, #            masked_lm_embed={'class_name': 'HanLP>Word2VecEmbedding',, #            samples_per_batch=150,, #            token_mapping=PTB_TOKEN_MAPPING,, #            transformer_dropout=.33,, #            warmup_steps_ratio=.1,, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # output = f'{save_dir}/test.predict.conll', # parser.fit(PTB_SD330_TRAIN, PTB_SD330_DEV, save_dir, 'bert-base-uncased',, # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_dep_sa_pos_bert.py,0,0.0,6,20.0,6,20.0,8,26.67,10,33.33,0,0.0,0,0,2,0,,"evaluate, fit, hanlp, print, save_dir, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineTransformerDependencyParserTF, hanlp.components.parsers.biaffine_parser_tf.StructuralAttentionDependencyParserTF, hanlp.datasets.parsing.ptb.PTB_SD330_DEV, hanlp.datasets.parsing.ptb.PTB_SD330_TEST, hanlp.datasets.parsing.ptb.PTB_SD330_TRAIN, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.metrics.parsing.conllx_eval, hanlp.pretrained.glove.GLOVE_840B_300D, tests.cdroot",,,"parser, save_dir",,"Model saved in , bert-base-uncased, data/model/dep/ptb_sa_bert_joint_pos, data/ptb-dep/dev.conllx, data/ptb-dep/test.conllx, data/ptb-dep/train.conllx","0.002, 0.1, 0.33, 150, 1e-05, 256, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-07 23:48, # alpha=1,, # early_stopping_patience=10,, # num_decoder_layers=2,, # output = f'{save_dir}/test.predict.conll', # parser.load(save_dir), # print(f'Official UAS: {uas:.4f} LAS: {las:.4f}'), # uas, las = conllx_eval.evaluate(PTB_SD330_TEST, output)",
https://github.com/hankcs/HanLP,train_ptb_pos_rnn_fasttext.py,0,0.0,10,23.81,21,50.0,7,16.67,4,9.52,0,0.0,0,0,3,0,,"evaluate, fit, hanlp, load, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.taggers.pos_tf.RNNPartOfSpeechTaggerTF, hanlp.pretrained.fasttext.FASTTEXT_CC_300_EN, tests.cdroot, tf.keras.optimizers.SGD",,,"optimizer, save_dir, tagger",,"., HanLP>FastTextEmbedding, Model saved in , Not, This, Thistime, an, class_name, config, data/model/pos/ptb_pos_rnn_fasttext, data/ptb-pos/dev.tsv, data/ptb-pos/test.tsv, data/ptb-pos/train.tsv, dinner, filepath, for, is, old, story, this, year","0.015, 0.05, 0.5, 10, 100, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-25 21:34, # optimizer = 'adam'",
https://github.com/hankcs/HanLP,train_semeval15_dm.py,0,0.0,9,20.93,28,65.12,2,4.65,4,9.3,0,0.0,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.pretrained.glove.GLOVE_6B_100D, tests.cdroot",,,"parser, save_dir, sentence",,"., ?, DT, HanLP>Word2VecEmbedding, IN, Is, NN, VBZ, chamber, class_name, config, data/model/sdp/semeval15_biaffine_dm, data/semeval15/en.dm.dev.conll, data/semeval15/en.dm.train.conll, data/semeval15/en.id.dm.auto.conllu, data/semeval15/en.ood.dm.auto.conllu, embeddings_initializer, expand_vocab, filepath, future, lowercase, music, normalize, of, the, this, trainable, zero","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 18:26, # disable variational dropout during evaluation so as to use CudaLSTM",
https://github.com/hankcs/HanLP,train_semeval15_pas.py,0,0.0,9,20.93,28,65.12,2,4.65,4,9.3,0,0.0,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.pretrained.glove.GLOVE_6B_100D, tests.cdroot",,,"parser, save_dir, sentence",,"., ?, DT, HanLP>Word2VecEmbedding, IN, Is, NN, VBZ, chamber, class_name, config, data/model/sdp/semeval15_biaffine_pas, data/semeval15/en.id.pas.conll, data/semeval15/en.ood.pas.conll, data/semeval15/en.pas.dev.conll, data/semeval15/en.pas.train.conll, embeddings_initializer, expand_vocab, filepath, future, lowercase, music, normalize, of, the, this, trainable, zero","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 18:26, # disable variational dropout during evaluation so as to use CudaLSTM",
https://github.com/hankcs/HanLP,train_semeval15_psd.py,0,0.0,9,20.93,28,65.12,2,4.65,4,9.3,0,0.0,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.pretrained.glove.GLOVE_6B_100D, tests.cdroot",,,"parser, save_dir, sentence",,"., ?, DT, HanLP>Word2VecEmbedding, IN, Is, NN, VBZ, chamber, class_name, config, data/model/sdp/semeval15_biaffine_psd, data/semeval15/en.id.psd.conll, data/semeval15/en.ood.psd.conll, data/semeval15/en.psd.dev.conll, data/semeval15/en.psd.train.conll, embeddings_initializer, expand_vocab, filepath, future, lowercase, music, normalize, of, the, this, trainable, zero","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 18:26, # disable variational dropout during evaluation so as to use CudaLSTM",
https://github.com/hankcs/HanLP,train_semeval16_news.py,0,0.0,9,18.75,25,52.08,2,4.17,3,6.25,9,18.75,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.datasets.parsing.semeval16.SEMEVAL2016_NEWS_DEV, hanlp.datasets.parsing.semeval16.SEMEVAL2016_NEWS_TEST, hanlp.datasets.parsing.semeval16.SEMEVAL2016_NEWS_TRAIN, hanlp.pretrained.word2vec.SEMEVAL16_EMBEDDINGS_300_NEWS_CN, hanlp.utils.tf_util.nice, tests.cdroot",,,"parser, save_dir, sentence",,"CD, HanLP>Word2VecEmbedding, M, NN, NR, VV, class_name, config, data/model/sdp/semeval16-news, embeddings_initializer, expand_vocab, filepath, lowercase, normalize, trainable, zero, 三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 23:20","三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾"
https://github.com/hankcs/HanLP,train_semeval16_text.py,0,0.0,9,18.75,25,52.08,2,4.17,3,6.25,9,18.75,0,0,3,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, sentence, tests","hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_DEV, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TEST, hanlp.datasets.parsing.semeval16.SEMEVAL2016_TEXT_TRAIN, hanlp.pretrained.word2vec.SEMEVAL16_EMBEDDINGS_300_TEXT_CN, hanlp.utils.tf_util.nice, tests.cdroot",,,"parser, save_dir, sentence",,"CD, HanLP>Word2VecEmbedding, M, NN, NR, VV, class_name, config, data/model/sdp/semeval16-text, embeddings_initializer, expand_vocab, filepath, lowercase, normalize, trainable, zero, 三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾","False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 23:20","三十万, 中国, 企业, 外商, 家, 批准, 投资, 设立, 逾"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-26 23:25",
https://github.com/hankcs/HanLP,train_ctb6_cws_albert.py,0,0.0,8,38.1,6,28.57,2,9.52,3,14.29,2,9.52,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TRAIN, tests.cdroot",,,"save_dir, tokenizer",,"/home/ubuntu/hankcs/laser/data/transformer/albert_base_tf2, Model saved in , data/model/cws_bert_albert_ctb6, f1, 中央民族乐团离开北京前往维也纳, 商品和服务","3, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_ctb6_cws_bert.py,0,0.0,8,42.11,6,31.58,0,0.0,3,15.79,2,10.53,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TRAIN, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , chinese_L-12_H-768_A-12, data/model/cws_bert_base_ctb6, f1, 中央民族乐团离开北京前往维也纳, 商品和服务",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_ctb6_cws_convseg.py,0,0.0,9,29.03,11,35.48,6,19.35,3,9.68,2,6.45,0,0,3,0,,"evaluate, fit, hanlp, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TRAIN, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , class_name, config, data/model/cws/ctb6_cws, expand_vocab, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0, 0.001, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_large_bert_cws.py,0,0.0,8,36.36,8,36.36,1,4.55,3,13.64,2,9.09,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , accuracy, bert-base-chinese, data/cws/large/all.txt, data/model/cws_bert_base_100million, f1, 中央民族乐团离开北京前往维也纳, 商品和服务",32,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_large_conv_cws.py,0,0.0,9,29.03,11,35.48,6,19.35,3,9.68,2,6.45,0,0,3,0,,"evaluate, fit, hanlp, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.datasets.cws.ctb.CTB6_CWS_DEV, hanlp.datasets.cws.ctb.CTB6_CWS_TEST, hanlp.datasets.cws.ctb.CTB6_CWS_TRAIN, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , class_name, config, data/model/cws/ctb6_cws, expand_vocab, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0, 0.001, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 21:58","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_large_cws_albert.py,0,0.0,8,33.33,8,33.33,3,12.5,3,12.5,2,8.33,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , accuracy, data/cws/large/all.txt, data/model/large_corpus_cws_albert_base, f1, uer/albert-base-chinese-cluecorpussmall, 中央民族乐团离开北京前往维也纳, 商品和服务","128, 3, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_large_cws_electra.py,0,0.0,8,33.33,8,33.33,3,12.5,3,12.5,2,8.33,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.ctb6.CTB6_CWS_DEV, hanlp.datasets.tokenization.ctb6.CTB6_CWS_TEST, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , accuracy, data/cws/large/all.txt, data/model/large_corpus_cws_electra_small, f1, hfl/chinese-electra-small-discriminator, 中央民族乐团离开北京前往维也纳, 商品和服务","10, 128, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_large_rnn_cws.py,0,0.0,10,28.57,14,40.0,6,17.14,3,8.57,2,5.71,0,0,3,0,,"evaluate, fit, hanlp, load, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.RNNTokenizerTF, hanlp.datasets.cws.ctb.CTB6_CWS_DEV, hanlp.datasets.cws.ctb.CTB6_CWS_TEST, hanlp.pretrained.word2vec.RADICAL_CHAR_EMBEDDING_100, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , accuracy, class_name, config, data/cws/large/all.txt, data/model/cws/large_rnn_cws, expand_vocab, f1, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0.001, 1e-08, 5, 64, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_msr_cws_albert.py,0,0.0,8,36.36,6,27.27,3,13.64,3,13.64,2,9.09,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tests","hanlp.components.tokenizers.tok.TransformerTokenizer, hanlp.datasets.cws.ctb.CTB6_CWS_TEST, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TRAIN, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_VALID, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , albert_base_zh, data/model/msr_cws_albert_base, f1, 中央民族乐团离开北京前往维也纳, 商品和服务","10, 150, 5e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:22","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_msr_cws_bert.py,0,0.0,7,36.84,6,31.58,0,0.0,4,21.05,2,10.53,0,0,2,0,,"evaluate, fit, hanlp, predict, print, save_dir, tests","hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_DEV, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TEST, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TRAIN, tests.cdroot",,,"save_dir, tokenizer",,"Model saved in , bert-base-chinese, data/model/cws_bert_base_msra, f1, 中央民族乐团离开北京前往维也纳, 商品和服务",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39, # tagger.load(save_dir)","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_msr_cws_ngram_conv.py,0,0.0,8,25.81,11,35.48,7,22.58,3,9.68,2,6.45,0,0,2,0,,"evaluate, fit, hanlp, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_DEV, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TEST, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TRAIN, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, tests.cdroot, tf.keras.optimizers.Adam",,,"save_dir, tokenizer",,"HanLP>Word2VecEmbedding, class_name, config, data/model/cws/convseg-msr-nocrf-noembed, expand_vocab, f1, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0, 0.001, 100, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_msr_cws_ngram_conv_embed.py,0,0.0,9,28.12,11,34.38,7,21.88,3,9.38,2,6.25,0,0,2,0,,"evaluate, fit, hanlp, load, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok.NgramConvTokenizer, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TEST, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_TRAIN, hanlp.datasets.tokenization.sighan2005.msr.SIGHAN2005_MSR_VALID, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_MSR, tests.cdroot, tf.keras.optimizers.Adam",,,"save_dir, tokenizer",,"HanLP>Word2VecEmbedding, class_name, config, data/model/cws/convseg-msr-nocrf-noembed, expand_vocab, f1, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0.001, 1e-08, 3, 4, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_pku980106_conv_cws.py,0,0.0,9,27.27,13,39.39,6,18.18,3,9.09,2,6.06,0,0,3,0,,"evaluate, fit, hanlp, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.pretrained.word2vec.RADICAL_CHAR_EMBEDDING_100, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , class_name, config, data/cws/pku98/199801-06-seg.txt, data/cws/pku98/test_pku98_name_merged.txt, data/model/cws/pku98_6m_conv_ngram, expand_vocab, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0, 0.001, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_pku980106_rnn_cws.py,0,0.0,9,28.12,13,40.62,5,15.62,3,9.38,2,6.25,0,0,3,0,,"evaluate, fit, hanlp, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.RNNTokenizerTF, hanlp.pretrained.word2vec.RADICAL_CHAR_EMBEDDING_100, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , class_name, config, data/cws/pku98/199801-06-seg.txt, data/cws/pku98/pku98_test.txt, data/model/cws/pku_6m_rnn_cws, expand_vocab, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0.001, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39","中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,train_pku_conv_cws.py,0,0.0,9,25.71,11,31.43,6,17.14,5,14.29,4,11.43,0,0,3,0,,"evaluate, fit, hanlp, optimizer, predict, print, save_dir, tensorflow, tests","hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.datasets.tokenization.sighan2005.SIGHAN2005_PKU_DEV, hanlp.datasets.tokenization.sighan2005.SIGHAN2005_PKU_TEST, hanlp.datasets.tokenization.sighan2005.SIGHAN2005_PKU_TRAIN, hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_CHAR, hanlp.utils.tf_util.nice, tests.cdroot, tf.keras.optimizers.Adam",,,"optimizer, save_dir, tokenizer",,"HanLP>Word2VecEmbedding, Model saved in , class_name, config, data/model/cws/sighan2005-pku-convseg, expand_vocab, filepath, lowercase, trainable, 中央民族乐团离开北京前往维也纳, 商品和服务","0, 0.001, 1e-08, 5, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:39, # print(tagger.predict('中央民族乐团离开北京前往维也纳')), # print(tagger.tag(list('中央民族乐团离开北京前往维也纳')))","print(tagger.predict('中央民族乐团离开北京前往维也纳')), print(tagger.tag(list('中央民族乐团离开北京前往维也纳'))), 中央民族乐团离开北京前往维也纳, 商品和服务"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 20:55",
https://github.com/hankcs/HanLP,mul_base.py,0,0.0,17,27.42,25,40.32,13,20.97,3,4.84,4,6.45,1,0,5,0,,"__name__, dev, doc, evaluate, fit, hanlp, items, load, main, pretty_print, save_config, save_dir, tasks, tests, transformer, trn, tst","hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.transform.NormalizeToken, hanlp.components.mtl.multi_task_learning.MultiTaskLearning, hanlp.components.mtl.tasks.tok.tag_tok.TaggingTokenization, hanlp.components.mtl.tasks.ud.UniversalDependenciesParsing, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.datasets.parsing.ud.ud210m.UD_210_MULTILINGUAL_DEV, hanlp.datasets.parsing.ud.ud210m.UD_210_MULTILINGUAL_TEST, hanlp.datasets.parsing.ud.ud210m.UD_210_MULTILINGUAL_TRAIN, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbedding, hanlp.utils.log_util.cprint, tests.cdroot",main,,"doc, mtl, save_dir, tasks, transformer",,"	, 'd, 'll, 'm, 're, 's, 've, 2021年 HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。, 2021年、HanLPv2.1は次世代の最先端多言語NLP技術を本番環境に導入します。, BMES, In 2021, HanLPv2.1 delivers state-of-the-art multilingual NLP techniques to production environments., Model saved in [cyan], Model will be saved in [cyan], [/cyan], __main__, data/cache/ud/mtl, data/model/mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12, data/mtl/mul/tok/dev.tsv, data/mtl/mul/tok/test.tsv, data/mtl/mul/tok/train.tsv, n't, nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large, tok, token, ud","0.001, 0.2, 0.5, 1, 128, 12800, 256, 30, 512, 5e-05, 8, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-03 14:24","1は次世代の最先端多言語NLP技術を本番環境に導入します。, 1为生产环境带来次世代最先进的多语种NLP技术。, 2021年, 2021年、HanLPv2"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2023-02-21 19:40",
https://github.com/hankcs/HanLP,amr.py,0,0.0,286,66.67,86,20.05,9,2.1,48,11.19,0,0.0,72,5,127,1,,"AMR, AMRGraph, AMRNode, DEFAULT_OOV_TOKEN, DEFAULT_PADDING_TOKEN, Exception, GraphRepair, KeyError, ModuleNotFoundError, QUOTED_RE, SourceCopyVocabulary, Triples, WORDSENSE_RE, _G, __eq__, __hash__, __init__, __repr__, __str__, _build_extras, _graph, _num_copies, _sort_attributes, _span, _src_tags, _src_tokens, _t, _top, _triples, _update_penman_graph, abstract_attribute, abstract_map, abstract_node, add, add_attribute, add_edge, add_node, add_node_attribute, add_source_side_tags_to_target_side, all, all_list, amr, amr_codec, any, append, attr, attribute_priority, attributes, attrmatch, attrs, bert_tokenizer, bos, child, child_node, child_relation, children, classmethod, cls, collections, copy, copy_idx, copy_index, copy_of, copy_offset, coref_index, corefs, correct_multiroot, coverage1, coverage2, decode, dfs, do, edge, edge_counter, edge_label_priority, edge_set, edges, encode, enumerate, eos, exit, fields, filter, find_similar_token, find_span_indexes, found, from_lists, from_prediction, get, get_attr_priority, get_copy_map, get_edges, get_frame_attributes, get_list_data, get_list_node, get_name_node_type, get_name_node_wiki, get_named_entity_span, get_nodes, get_senseless_attributes, get_special_tok_list, get_src_tokens, get_subtree, get_tgt_tokens, get_token_from_idx, get_token_idx, gr, graph, group, groups, hanlp_common, hash, head_index, head_indices, head_label, head_labels, head_tags, head_variable, heads, id, identifier, idx, idx_to_token, in_edges, index, index_sequence, indexes, indices, instance, instances1, instances2, invalid_indices, invert_relation, inverted, is_abstract_token, is_attribute_edge, is_attribute_value, is_date_node, is_english_punct, is_name_node, is_named_entity, is_similar, isalpha, isinstance, items, json, kept_target, key, label, large, lemmas, len, list, list_tokens, logger, logging, max, max_depth, max_tgt_length, misc, modifier, modifier_variable, ner, ner_tags, networkx, new, next, node, node_list, node_to_idx, nodes, normalize_number, old, ops, other, pad_token, parent, parent_node, part, parts, penman, penman_graph, pos, pos_tag_lut, pos_tags, prediction, prev, property, range, raw_graph_string, re, relation, remove, remove_attribute, remove_edge, remove_node, remove_node_attribute, remove_node_ops, remove_redundant_edges, remove_span, remove_subtree, remove_unknown_nodes, removed_nodes, repaired_items, replace, replace_attribute, replace_copy, replace_node_attribute, replace_span, repr, ret, root, self, sentence, set, set_name_node_wiki, set_src_tokens, sim_token, small, sort, sort_edges, source, span, split, src_copy_indices, src_copy_invalid_ids, src_copy_map, src_copy_vocab, src_idx, src_indices, src_must_copy_tags, src_pos_tags, src_tag, src_token, src_token_ids, src_token_idx, src_token_subword_index, src_tokens, startswith, staticmethod, str, subtree_instances1, subtree_instances2, sum, super, tag, tag_counter, tag_lut, target, text, tgt_copy_indices, tgt_copy_map, tgt_copy_mask, tgt_pos_tags, tgt_tags, tgt_token, tgt_token_counter, tgt_tokens, token, token_idx, token_to_idx, tokenize, tokens, top, top_variable, traceback, trim_very_long_tgt_tokens, triples, type, unk_token, update, update_edge_label, update_info, value, var, variable, variable_map, variable_to_node, variables, variables_count, visited, visited_children, vocab_size, wiki, y, zip","collections.Counter, collections.defaultdict, hanlp_common.io.eprint, json.dumps, logging.getLogger, nx.DiGraph, penman.AMRCodec, penman.AMRCodec._deinversions.pop, penman.AMRCodec._inversions.pop, penman.Graph, penman.Triple, penman.alphanum_order, re.compile, re.search, re.split, re.sub, traceback.print_exc","__eq__, __hash__, __init__, __repr__, __str__, _build_extras, _sort_attributes, _update_penman_graph, abstract_attribute, abstract_node, add_attribute, add_edge, add_node, add_node_attribute, add_source_side_tags_to_target_side, attributes, copy, correct_multiroot, decode, dfs, do, find_similar_token, find_span_indexes, from_lists, from_prediction, get_attr_priority, get_copy_map, get_edges, get_frame_attributes, get_list_data, get_list_node, get_name_node_type, get_name_node_wiki, get_named_entity_span, get_nodes, get_senseless_attributes, get_special_tok_list, get_src_tokens, get_subtree, get_tgt_tokens, get_token_from_idx, get_token_idx, index_sequence, instance, is_abstract_token, is_attribute_edge, is_attribute_value, is_date_node, is_english_punct, is_name_node, is_named_entity, is_similar, normalize_number, ops, remove_attribute, remove_edge, remove_node, remove_node_attribute, remove_node_ops, remove_redundant_edges, remove_span, remove_subtree, remove_unknown_nodes, replace_attribute, replace_node_attribute, replace_span, set_name_node_wiki, set_src_tokens, sort_edges, trim_very_long_tgt_tokens, update_edge_label, update_info","AMR, AMRGraph, AMRNode, GraphRepair, SourceCopyVocabulary","DEFAULT_OOV_TOKEN, DEFAULT_PADDING_TOKEN, G, QUOTED_RE, Triples, WORDSENSE_RE, _graph, _span, _t, amr_codec, attr, attribute_priority, attributes, attrmatch, attrs, child, child_node, child_relation, children, copy, copy_idx, copy_index, copy_offset, coref_index, corefs, coverage1, coverage2, edge, edge_counter, edge_label_priority, edge_set, edges, fields, found, gr, graph, group, groups, head_index, head_indices, head_label, head_labels, head_tags, head_variable, heads, i, identifier, index, indices, instance, invalid_indices, kept_target, key, label, large, logger, modifier, modifier_variable, ner, next, node, node_list, node_to_idx, nodes, ops, parent, parent_node, part, parts, pos, pos_tag_lut, prev, relation, removed_nodes, ret, sentence, sim_token, small, source, span, src_copy_indices, src_copy_invalid_ids, src_copy_map, src_copy_vocab, src_idx, src_indices, src_must_copy_tags, src_pos_tags, src_tag, src_token, src_token_ids, src_token_idx, src_token_subword_index, src_tokens, subtree_instances1, subtree_instances2, t, tag, tag_counter, tag_lut, target, text, tgt_copy_indices, tgt_copy_map, tgt_copy_mask, tgt_pos_tags, tgt_tags, tgt_token, tgt_token_counter, tgt_tokens, token, token_idx, top, top_variable, triples, v, value, var, variable, variable_map, variables, variables_count, visited, visited_children, wiki, x, y","Edge labels such as ARGx, ARGx-of, and 'opx' should only appear at most once
in each node's outgoing edges.",", 
, 
	:{} {}, 
        Edge labels such as ARGx, ARGx-of, and 'opx' should only appear at most once
        in each node's outgoing edges.
        ,  ,  / , ""{}"", # ::{} {}, (^"".*""$|^[^a-zA-Z]+$), ,, -, -\d\d$, 0, @@PADDING@@, @@UNKNOWN@@, AMR support requires the full version which can be installed via:
pip install hanlp_common[full], ARG6, O, [/:\\()], ^"".*""$, ^(ARG|op|snt), ^([A-Z]+_)+\d+$, ^([A-Z]+|DATE_ATTRS|SCORE_ENTITY|ORDINAL_ENTITY)_\d+$, ^[,.?!:;""\'-(){}\[\]]$, ^\d+,\d+$, ^\d0*$, ^_QUANTITY_\d+$, ^op\d+$, _, _copy_{}, amr, coref, corefs, date-entity, day, decade, domain, frequency, graph, head_indices, head_labels, head_tags, heads, instance, label, li, misc, mod, mod name time location degree poss domain quant manner unit purpose topic condition part-of compared-to duration source ord beneficiary concession direction frequency consist-of example medium location-of manner-of quant-of time-of instrument prep-in destination accompanier prep-with extent instrument-of age path concession-of subevent-of prep-as prep-to prep-against prep-on prep-for degree-of prep-under part condition-of prep-without topic-of season duration-of poss-of prep-from prep-at range purpose-of source-of subevent example-of value path-of scale conj-as-if prep-into prep-by prep-on-behalf-of medium-of prep-among calendar beneficiary-of prep-along-with extent-of age-of frequency-of dayperiod accompanier-of destination-of prep-amid prep-toward prep-in-addition-to ord-of name-of weekday direction-of prep-out-of timezone subset-of, mode, month, name, nodes, op, op\d+, pos_tag_lut, poss, quant, remove-redundant-edge, remove-unknown-node, root, snt, src_copy_indices, src_copy_invalid_ids, src_copy_map, src_copy_vocab, src_must_copy_tags, src_pos_tags, src_token_ids, src_token_subword_index, src_tokens, string-entity, tgt_copy_indices, tgt_copy_map, tgt_copy_mask, tgt_pos_tags, tgt_tokens, time, tokens, unit, value, vv, vv1, vv{}, wiki, year","0, 0.8, 1, 2, 5, 6, False, None, True","
Edge labels such as ARGx, ARGx-of, and 'opx' should only appear at most once
in each node's outgoing edges.
, #, #     (t.startswith(token) and len(token) > 3) or, #     re.sub('ied$', 'y', t) == token or, #     re.sub('ly$', '', t) == token, #     re.sub('ly$', 'le', t) == token or, #     re.sub('tion$', 'te', t) == token or, #     re.sub('tive$', 'te', t) == token or, #     return tokens[i], #     token + 'd' == t or, #     token + 'ed' == t or, # ):, # .lower()), # ::{} {}'.format(k, v)), # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Add quotes if there's a backslash., # Build edge triples and other attribute triples., # Build the variable map from variable to instance., # Copyright (c) 2019 Sheng Zhang, # Disable inverting ':mod' relation., # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # Refine attributes because there's a bug in penman.attributes(), # SOFTWARE., # See https://github.com/goodmami/penman/issues/29, # Set the coreferred target to 0 if no coref is available., # Source Copy, # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # Target side Coreference, # The above copyright notice and this permission notice shall be included in all, # `name`, `ARGx`, and `ARGx-of` should only appear once., # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # if (token == t or, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # or label.startswith('ARG'):, # remove quotes, # remove redundant edges., # self._sort_attributes(), # t = t.lower(), # the target of `opx' should only appear once., # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,configurable.py,0,0.0,23,65.71,6,17.14,1,2.86,5,14.29,0,0.0,3,2,4,2,,"AutoConfigurable, Configurable, __dict__, __repr__, cls, config, deserialized_config, dict, from_config, get, hanlp_common, hasattr, isinstance, items, k, object, pop, property, repr, self, startswith, staticmethod, v","hanlp_common.reflection.classpath_of, hanlp_common.reflection.str_to_type","__repr__, config, from_config","AutoConfigurable, Configurable","cls, deserialized_config, k, v","Build an object from config.

Args:
  config: A ``dict`` holding parameters for its constructor. It has to contain a `classpath` key,
            which has a classpath str as its value. ``classpath`` will determine the type of object
            being deserialized.
  kwargs: Arguments not used.

Returns: A deserialized object., The config of this object, which are public properties. If any properties needs to be excluded from this config,
simply declare it with prefix ``_``.","
        The config of this object, which are public properties. If any properties needs to be excluded from this config,
        simply declare it with prefix ``_``.
        ,  doesn't contain classpath field, Build an object from config.

        Args:
          config: A ``dict`` holding parameters for its constructor. It has to contain a `classpath` key,
                    which has a classpath str as its value. ``classpath`` will determine the type of object
                    being deserialized.
          kwargs: Arguments not used.

        Returns: A deserialized object.

        , _, classpath, config",None,"
The config of this object, which are public properties. If any properties needs to be excluded from this config,
simply declare it with prefix ``_``.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-16 22:24, Build an object from config.

Args:
config: A ``dict`` holding parameters for its constructor. It has to contain a `classpath` key,
which has a classpath str as its value. ``classpath`` will determine the type of object
being deserialized.
kwargs: Arguments not used.

Returns: A deserialized object.

",
https://github.com/hankcs/HanLP,conll.py,0,0.0,85,51.83,50,30.49,8,4.88,17,10.37,4,2.44,12,4,32,13,,"CoNLLSentence, CoNLLSentenceList, CoNLLUWord, CoNLLWord, __init__, __str__, alignment, all, append, arc, arrows, cell_per_word, cells, conll, conllu, cpos, deprel, deps, di, dict, dj, each, enumerate, extend, extras, f, feats, form, from_dict, from_file, from_str, get_pos, hanlp_common, has_lem, has_pos, head, headings, hi, hj, id, int, isinstance, isprojective, lemma, len, line, list, lj, misc, nonempty_fields, open, pair, pairs, path, pdeprel, phead, pos, prev_id, projective, property, read, rj, rows, sanitize_conll_int_value, self, sequence, sorted, split, src, startswith, staticmethod, strip, super, text, to_markdown, to_tree, typing, upos, value, values, word, words, x, xpos, zip","hanlp_common.structure.SerializableDict, hanlp_common.visualization.make_table, hanlp_common.visualization.markdown_table, hanlp_common.visualization.pretty_tree_horizontal, typing.List, typing.Union","__init__, __str__, from_dict, from_file, from_str, get_pos, isprojective, nonempty_fields, projective, sanitize_conll_int_value, to_markdown, to_tree","CoNLLSentence, CoNLLSentenceList, CoNLLUWord, CoNLLWord","alignment, arc, arrows, cell_per_word, cells, deprel, deps, di, dj, each, f, h, has_lem, has_pos, head, headings, hi, hj, line, lj, pair, pairs, prev_id, rj, rows, src, text, tree, values, word, words, x","A list of :class:`~hanlp_common.conll.CoNLLWord` or :class:`~hanlp_common.conll.CoNLLUWord`. It is a sub-class
of :class:`list` and its words can be accessed in the same way as accessing list elements.

Args:
    words (list[Union[CoNLLWord, CoNLLUWord]]): A list of words., Build a CoNLLSentence from CoNLL-X format str

Args:
  conll (str): CoNLL-X or CoNLL-U format string
  conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
    A :class:`~hanlp_common.conll.CoNLLSentence`., Build a CoNLLSentence from ``.conllx`` or ``.conllu`` file

Args:
  path: Path to the file.
  conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
    A :class:`~hanlp_common.conll.CoNLLSentence`., Build a CoNLLSentence from a dict.

Args:
    d: A dict storing a list for each field, where each index corresponds to a token.
    conllu: ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
    A :class:`~hanlp_common.conll.CoNLLSentence`., Checks if a dependency tree is projective.
This also works for partial annotation.

Besides the obvious crossing arcs, the examples below illustrate two non-projective cases
which are hard to detect in the scenario of partial annotation.

Args:
    sequence (list[int]):
        A list of head indices.

Returns:
    ``True`` if the tree is projective, ``False`` otherwise.

Examples:
    >>> isprojective([2, -1, 1])  # -1 denotes un-annotated cases
    False
    >>> isprojective([3, -1, 2])
    False, CoNLL (:cite:`buchholz-marsi-2006-conll`) format template, see http://anthology.aclweb.org/W/W06/W06-2920.pdf

Args:
    id (int):
        Token counter, starting at 1 for each new sentence.
    form (str):
        Word form or punctuation symbol.
    lemma (str):
        Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
    cpos (str):
        Coarse-grained part-of-speech tag, where the tagset depends on the treebank.
    pos (str):
        Fine-grained part-of-speech tag, where the tagset depends on the treebank.
    feats (str):
        Unordered set of syntactic and/or morphological features (depending on the particular treebank),
        or an underscore if not available.
    head (Union[int, List[int]]):
        Head of the current token, which is either a value of ID,
        or zero (’0’) if the token links to the virtual root node of the sentence.
    deprel (Union[str, List[str]]):
        Dependency relation to the HEAD.
    phead (int):
        Projective head of current token, which is either a value of ID or zero (’0’),
        or an underscore if not available.
    pdeprel (str):
        Dependency relation to the PHEAD, or an underscore if not available., CoNLL-U format template, see https://universaldependencies.org/format.html

Args:

    id (Union[int, str]):
        Token counter, starting at 1 for each new sentence.
    form (Union[str, None]):
        Word form or punctuation symbol.
    lemma (str):
        Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
    upos (str):
        Universal part-of-speech tag.
    xpos (str):
        Language-specific part-of-speech tag; underscore if not available.
    feats (str):
        List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.
    head (int):
        Head of the current token, which is either a value of ID,
        or zero (’0’) if the token links to the virtual root node of the sentence.
    deprel (str):
        Dependency relation to the HEAD.
    deps (Union[List[Tuple[int, str], str]):
        Projective head of current token, which is either a value of ID or zero (’0’),
        or an underscore if not available.
    misc (str):
        Dependency relation to the PHEAD, or an underscore if not available., Convert into a pretty tree string which can be printed to show the tree structure.

Args:
    extras: Extra table to be aligned to this tree.

Returns:
    A pretty tree string along with extra table if passed any., Convert into markdown string.

Args:
    headings: ``auto`` to automatically detect the word type. When passed a list of string, they are treated as
                headings for each field.

Returns:
    A markdown representation of this sentence., Get the precisest pos for this word.

Returns: ``self.pos`` or ``self.cpos``., Get the precisest pos for this word.

Returns: ``self.xpos`` or ``self.upos``, Get the values of nonempty fields as a list., ``True`` if this tree is projective.","	, 
, 

, 
        A list of :class:`~hanlp_common.conll.CoNLLWord` or :class:`~hanlp_common.conll.CoNLLUWord`. It is a sub-class
        of :class:`list` and its words can be accessed in the same way as accessing list elements.

        Args:
            words (list[Union[CoNLLWord, CoNLLUWord]]): A list of words.
        , 
        Get the precisest pos for this word.

        Returns: ``self.pos`` or ``self.cpos``.

        , 
        Get the precisest pos for this word.

        Returns: ``self.xpos`` or ``self.upos``

        , 
        Get the values of nonempty fields as a list.
        , 
        ``True`` if this tree is projective.
        , 
    Checks if a dependency tree is projective.
    This also works for partial annotation.

    Besides the obvious crossing arcs, the examples below illustrate two non-projective cases
    which are hard to detect in the scenario of partial annotation.

    Args:
        sequence (list[int]):
            A list of head indices.

    Returns:
        ``True`` if the tree is projective, ``False`` otherwise.

    Examples:
        >>> isprojective([2, -1, 1])  # -1 denotes un-annotated cases
        False
        >>> isprojective([3, -1, 2])
        False
    , #, -, :, <, >, Build a CoNLLSentence from CoNLL-X format str

        Args:
          conll (str): CoNLL-X or CoNLL-U format string
          conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

        Returns:
            A :class:`~hanlp_common.conll.CoNLLSentence`.
        , Build a CoNLLSentence from ``.conllx`` or ``.conllu`` file

        Args:
          path: Path to the file.
          conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

        Returns:
            A :class:`~hanlp_common.conll.CoNLLSentence`.
        , Build a CoNLLSentence from a dict.

        Args:
            d: A dict storing a list for each field, where each index corresponds to a token.
            conllu: ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

        Returns:
            A :class:`~hanlp_common.conll.CoNLLSentence`.
        , CPOS, CoNLL (:cite:`buchholz-marsi-2006-conll`) format template, see http://anthology.aclweb.org/W/W06/W06-2920.pdf

        Args:
            id (int):
                Token counter, starting at 1 for each new sentence.
            form (str):
                Word form or punctuation symbol.
            lemma (str):
                Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
            cpos (str):
                Coarse-grained part-of-speech tag, where the tagset depends on the treebank.
            pos (str):
                Fine-grained part-of-speech tag, where the tagset depends on the treebank.
            feats (str):
                Unordered set of syntactic and/or morphological features (depending on the particular treebank),
                or an underscore if not available.
            head (Union[int, List[int]]):
                Head of the current token, which is either a value of ID,
                or zero (’0’) if the token links to the virtual root node of the sentence.
            deprel (Union[str, List[str]]):
                Dependency relation to the HEAD.
            phead (int):
                Projective head of current token, which is either a value of ID or zero (’0’),
                or an underscore if not available.
            pdeprel (str):
                Dependency relation to the PHEAD, or an underscore if not available.
        , CoNLL-U format template, see https://universaldependencies.org/format.html

        Args:

            id (Union[int, str]):
                Token counter, starting at 1 for each new sentence.
            form (Union[str, None]):
                Word form or punctuation symbol.
            lemma (str):
                Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
            upos (str):
                Universal part-of-speech tag.
            xpos (str):
                Language-specific part-of-speech tag; underscore if not available.
            feats (str):
                List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.
            head (int):
                Head of the current token, which is either a value of ID,
                or zero (’0’) if the token links to the virtual root node of the sentence.
            deprel (str):
                Dependency relation to the HEAD.
            deps (Union[List[Tuple[int, str], str]):
                Projective head of current token, which is either a value of ID or zero (’0’),
                or an underscore if not available.
            misc (str):
                Dependency relation to the PHEAD, or an underscore if not available.
        , Convert into a pretty tree string which can be printed to show the tree structure.

        Args:
            extras: Extra table to be aligned to this tree.

        Returns:
            A pretty tree string along with extra table if passed any.
        , Convert into markdown string.

        Args:
            headings: ``auto`` to automatically detect the word type. When passed a list of string, they are treated as
                        headings for each field.

        Returns:
            A markdown representation of this sentence.
        , DEPREL, DEPS, Dep Tree, FEATS, FORM, HEAD, ID, LEMMA, Lemma, MISC, PDEPREL, PHEAD, POS, PoS, Relation, Token, UPOS, When head is a list, deprel has to be a list, When head is a list, deprel has to match its length, When head is a list, deps has to be None, XPOS, ^, _, auto, from, to, |, ⎮","0, 1, 6, 7, 8, False, None, True","
A list of :class:`~hanlp_common.conll.CoNLLWord` or :class:`~hanlp_common.conll.CoNLLUWord`. It is a sub-class
of :class:`list` and its words can be accessed in the same way as accessing list elements.

Args:
words (list[Union[CoNLLWord, CoNLLUWord]]): A list of words.
, 
Get the precisest pos for this word.

Returns: ``self.pos`` or ``self.cpos``.

, 
Get the precisest pos for this word.

Returns: ``self.xpos`` or ``self.upos``

, 
Get the values of nonempty fields as a list.
, 
arrows = []
for word in self:  # type: Union[CoNLLWord, CoNLLUWord]
if word.head:
arrows.append({'from': word.head - 1, 'to': word.id - 1})
tree = pretty_tree_horizontal(arrows)
rows = [['Dep Tree', 'Token', 'Relation']]
has_lem = all(x.lemma for x in self)
has_pos = all(x.get_pos() for x in self)
if has_lem:
rows[0].append('Lemma')
if has_pos:
rows[0].append('PoS')
if extras:
rows[0].extend(extras[0])
for i, (word, arc) in enumerate(zip(self, tree)):
cell_per_word = [arc]
cell_per_word.append(word.form)
cell_per_word.append(word.deprel)
if has_lem:
cell_per_word.append(word.lemma)
if has_pos:
cell_per_word.append(word.get_pos())
if extras:
cell_per_word.extend(extras[i + 1])
rows.append(cell_per_word)
return make_table(rows, insert_header=True)

@property
def projective(self):
, 
cells = [str(word).split('\t') for word in self]
if headings == 'auto':
if isinstance(self[0], CoNLLWord):
headings = ['ID', 'FORM', 'LEMMA', 'CPOS', 'POS', 'FEATS', 'HEAD', 'DEPREL', 'PHEAD', 'PDEPREL']
else:  # conllu
headings = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']
for each in cells:
# if '|' in each[8]:
# each[8] = f'`{each[8]}`'
each[8] = each[8].replace('|', '⎮')
alignment = [('^', '>'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '>'), ('^', '<'),
('^', '<'), ('^', '<')]
text = markdown_table(headings, cells, alignment=alignment)
return text

def to_tree(self, extras: List[str] = None) -> str:
Convert into a pretty tree string which can be printed to show the tree structure., 
return isprojective([x.head for x in self])


class CoNLLSentenceList(list):

def __str__(self) -> str:
return '\n\n'.join(str(x) for x in self)


def sanitize_conll_int_value(value: Union[str, int]):
if value is None or isinstance(value, int):
return value
if value == '_':
return None
if isinstance(value, str):
return int(value)
return value


def isprojective(sequence):
r""""""
Checks if a dependency tree is projective.
This also works for partial annotation.

Besides the obvious crossing arcs, the examples below illustrate two non-projective cases
which are hard to detect in the scenario of partial annotation.

Args:
sequence (list[int]):
A list of head indices.

Returns:
``True`` if the tree is projective, ``False`` otherwise.

Examples:
>>> isprojective([2, -1, 1])  # -1 denotes un-annotated cases
False
>>> isprojective([3, -1, 2])
False
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-19 20:50, # type: CoNLLUWord, #'):, Build a CoNLLSentence from CoNLL-X format str

Args:
conll (str): CoNLL-X or CoNLL-U format string
conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
A :class:`~hanlp_common.conll.CoNLLSentence`.
, Build a CoNLLSentence from ``.conllx`` or ``.conllu`` file

Args:
path: Path to the file.
conllu:  ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
A :class:`~hanlp_common.conll.CoNLLSentence`.
, Build a CoNLLSentence from a dict.

Args:
d: A dict storing a list for each field, where each index corresponds to a token.
conllu: ``True`` to build :class:`~hanlp_common.conll.CoNLLUWord` for each token.

Returns:
A :class:`~hanlp_common.conll.CoNLLSentence`.
, CoNLL (:cite:`buchholz-marsi-2006-conll`) format template, see http://anthology.aclweb.org/W/W06/W06-2920.pdf

Args:
id (int):
Token counter, starting at 1 for each new sentence.
form (str):
Word form or punctuation symbol.
lemma (str):
Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
cpos (str):
Coarse-grained part-of-speech tag, where the tagset depends on the treebank.
pos (str):
Fine-grained part-of-speech tag, where the tagset depends on the treebank.
feats (str):
Unordered set of syntactic and/or morphological features (depending on the particular treebank),
or an underscore if not available.
head (Union[int, List[int]]):
Head of the current token, which is either a value of ID,
or zero (’0’) if the token links to the virtual root node of the sentence.
deprel (Union[str, List[str]]):
Dependency relation to the HEAD.
phead (int):
Projective head of current token, which is either a value of ID or zero (’0’),
or an underscore if not available.
pdeprel (str):
Dependency relation to the PHEAD, or an underscore if not available.
, CoNLL-U format template, see https://universaldependencies.org/format.html

Args:

id (Union[int, str]):
Token counter, starting at 1 for each new sentence.
form (Union[str, None]):
Word form or punctuation symbol.
lemma (str):
Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
upos (str):
Universal part-of-speech tag.
xpos (str):
Language-specific part-of-speech tag; underscore if not available.
feats (str):
List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.
head (int):
Head of the current token, which is either a value of ID,
or zero (’0’) if the token links to the virtual root node of the sentence.
deprel (str):
Dependency relation to the HEAD.
deps (Union[List[Tuple[int, str], str]):
Projective head of current token, which is either a value of ID or zero (’0’),
or an underscore if not available.
misc (str):
Dependency relation to the PHEAD, or an underscore if not available.
","
cells = [str(word).split('\t') for word in self]
if headings == 'auto':
if isinstance(self[0], CoNLLWord):
headings = ['ID', 'FORM', 'LEMMA', 'CPOS', 'POS', 'FEATS', 'HEAD', 'DEPREL', 'PHEAD', 'PDEPREL']
else:  # conllu
headings = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']
for each in cells:
# if '|' in each[8]:
# each[8] = f'`{each[8]}`'
each[8] = each[8].replace('|', '⎮')
alignment = [('^', '>'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '<'), ('^', '>'), ('^', '<'),
('^', '<'), ('^', '<')]
text = markdown_table(headings, cells, alignment=alignment)
return text

def to_tree(self, extras: List[str] = None) -> str:
Convert into a pretty tree string which can be printed to show the tree structure., CoNLL (:cite:`buchholz-marsi-2006-conll`) format template, see http://anthology.aclweb.org/W/W06/W06-2920.pdf

Args:
id (int):
Token counter, starting at 1 for each new sentence.
form (str):
Word form or punctuation symbol.
lemma (str):
Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
cpos (str):
Coarse-grained part-of-speech tag, where the tagset depends on the treebank.
pos (str):
Fine-grained part-of-speech tag, where the tagset depends on the treebank.
feats (str):
Unordered set of syntactic and/or morphological features (depending on the particular treebank),
or an underscore if not available.
head (Union[int, List[int]]):
Head of the current token, which is either a value of ID,
or zero (’0’) if the token links to the virtual root node of the sentence.
deprel (Union[str, List[str]]):
Dependency relation to the HEAD.
phead (int):
Projective head of current token, which is either a value of ID or zero (’0’),
or an underscore if not available.
pdeprel (str):
Dependency relation to the PHEAD, or an underscore if not available.
, CoNLL-U format template, see https://universaldependencies.org/format.html

Args:

id (Union[int, str]):
Token counter, starting at 1 for each new sentence.
form (Union[str, None]):
Word form or punctuation symbol.
lemma (str):
Lemma or stem (depending on the particular treebank) of word form, or an underscore if not available.
upos (str):
Universal part-of-speech tag.
xpos (str):
Language-specific part-of-speech tag; underscore if not available.
feats (str):
List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.
head (int):
Head of the current token, which is either a value of ID,
or zero (’0’) if the token links to the virtual root node of the sentence.
deprel (str):
Dependency relation to the HEAD.
deps (Union[List[Tuple[int, str], str]):
Projective head of current token, which is either a value of ID or zero (’0’),
or an underscore if not available.
misc (str):
Dependency relation to the PHEAD, or an underscore if not available.
, ’0’"
https://github.com/hankcs/HanLP,constant.py,0,0.0,15,36.59,20,48.78,1,2.44,5,12.2,0,0.0,0,0,12,0,,"BOS, CLS, EOS, HANLP_URL, HANLP_VERBOSE, IDX, IPYTHON, NULL, NameError, PAD, PRED, ROOT, UNK, get_ipython, os","os.environ.get, os.getenv",,,"BOS, CLS, EOS, HANLP_URL, HANLP_VERBOSE, IDX, IPYTHON, NULL, PAD, PRED, ROOT, UNK",,"1, <bos>, <eos>, <null>, <pad>, <unk>, Enable verbose or not., HANLP_IPYTHON, HANLP_URL, HANLP_VERBOSE, Key for index., PRED, Padding token., Resource URL., Unknown token., [CLS], _idx_, https://file.hankcs.com/hanlp/, true, yes",False,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 22:41, Key for index.
HANLP_URL = os.getenv('HANLP_URL', 'https://file.hankcs.com/hanlp/')
Resource URL., Padding token.
UNK = '<unk>'
Unknown token.",
https://github.com/hankcs/HanLP,document.py,0,0.0,126,55.75,58,25.66,8,3.54,20,8.85,14,6.19,15,1,63,11,,"Document, IPython, NotImplementedError, __init__, __str__, _dep, _i, _k, _ner, _srl, _to_doc_without_spans, _tok, _type, _v, anno_per_sent, annotations, any, append, args, arrow, block, block_, cells, center, cols, con_samples, condense, conlls, count_sentences, dep, dict, each, ensure_ascii, enumerate, extend, extras, extras_, fea, flat, form, get, get_by_prefix, getattr, graphs, group, hanlp, hanlp_common, header, height, html, indent, int, isinstance, items, join, json, key, kwargs, label, labels, lang, leaves, lem, len, length, level_len, line, lines, ls, name, ner, ner_per_sample, ner_samples, nonempty_fields, num_cols, offset, p_index, pas, penman, phrasetree, pos, pretty_print, pretty_text, print, projective, range, re, replace, repr, results, row, sample, sdp, self, sent, sent_new_line, show_header, spans, split, sq, squeeze, srl, srl_per_sample, srl_samples, start_offsets, str, subtree, subtrees, super, task, text, text_, to_conll, to_dict, to_html, to_json, to_pretty, to_tree, tok, tokens, translate, triples, typing, values, warnings, zip","IPython.core.display.HTML, IPython.core.display.display, hanlp.utils.lang.zh.localization, hanlp_common.amr.AMRGraph, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLSentenceList, hanlp_common.conll.CoNLLUWord, hanlp_common.constant.IPYTHON, hanlp_common.constant.PRED, hanlp_common.util.collapse_json, hanlp_common.util.prefix_match, hanlp_common.visualization.list_to_tree, hanlp_common.visualization.make_table, hanlp_common.visualization.render_labeled_span, hanlp_common.visualization.tree_to_list, json.dumps, penman.Graph, phrasetree.tree.Tree, re.sub, typing.List, typing.Union","__init__, __str__, _to_doc_without_spans, condense, count_sentences, get, get_by_prefix, pretty_print, squeeze, to_conll, to_dict, to_html, to_json, to_pretty, translate",Document,"_dep, _ner, _srl, _tok, _type, _v, anno_per_sent, annotations, arrow, block, cells, cols, con_samples, conlls, d, dep, each, extras, fea, flat, graphs, header, height, html, key, labels, lem, length, level_len, line, lines, ls, name, ner, ner_per_sample, ner_samples, num_cols, offset, p_index, pas, pos, results, row, sample, sdp, self, sent, sent_new_line, spans, sq, srl_per_sample, srl_samples, start_offsets, subtree, task, text, text_, tok, tokens, translate, tree, triples, v","A dict structure holding parsed annotations. A document is a subclass of ``dict`` and it supports every
interface of ``dict``\. Additionally, it supports interfaces to deal with various linguistic structures. Its
``str`` and ``dict`` representations are made to be compatible with JSON serialization.

Args:
    *args: An iterator of key-value pairs.
    **kwargs: Arguments from ``**`` operator.

Examples::

    # Create a document
    doc = Document(
        tok=[[""晓美焰"", ""来到"", ""北京"", ""立方庭"", ""参观"", ""自然"", ""语义"", ""科技"", ""公司""]],
        pos=[[""NR"", ""VV"", ""NR"", ""NR"", ""VV"", ""NN"", ""NN"", ""NN"", ""NN""]],
        ner=[[[""晓美焰"", ""PERSON"", 0, 1], [""北京立方庭"", ""LOCATION"", 2, 4],
              [""自然语义科技公司"", ""ORGANIZATION"", 5, 9]]],
        dep=[[[2, ""nsubj""], [0, ""root""], [4, ""name""], [2, ""dobj""], [2, ""conj""],
              [9, ""compound""], [9, ""compound""], [9, ""compound""], [5, ""dobj""]]]
    )

    # print(doc) or str(doc) to get its JSON representation
    print(doc)

    # Access an annotation by its task name
    print(doc['tok'])

    # Get number of sentences
    print(f'It has {doc.count_sentences()} sentence(s)')

    # Access the n-th sentence
    print(doc.squeeze(0)['tok'])

    # Pretty print it right in your console or notebook
    doc.pretty_print()

    # To save the pretty prints in a str
    pretty_text: str = '\n\n'.join(doc.to_pretty()), Convert to :class:`~hanlp_common.conll.CoNLLSentence`.

Args:
    tok (str): Field name for tok.
    lem (str): Field name for lem.
    pos (str): Field name for upos.
    fea (str): Field name for feats.
    dep (str): Field name for dependency parsing.
    sdp (str): Field name for semantic dependency parsing.

Returns:
    A :class:`~hanlp_common.conll.CoNLLSentence` representation., Convert to a json compatible dict.

Returns:
    A dict representation., Convert to a pretty text representation which can be printed to visualize linguistic structures.

Args:
    tok: Token key.
    lem: Lemma key.
    pos: Part-of-speech key.
    dep: Dependency parse tree key.
    sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
    ner: Named entity key.
    srl: Semantic role labeling key.
    con: Constituency parsing key.
    show_header: ``True`` to include a header which indicates each field with its name.
    html: ``True`` to output HTML format so that non-ASCII characters can align correctly.

Returns:
    A pretty string., Convert to json string.

Args:
    ensure_ascii: ``False`` to allow for non-ascii text.
    indent: Indent per nested structure.

Returns:
    A text representation in ``str``., Count number of sentences in this document.

Returns:
    Number of sentences., Get value by the prefix of a key.

Args:
    prefix: The prefix of a key. If multiple keys are matched, only the first one will be used.

Returns:
    The value assigned with the matched key., Print a pretty text representation which visualizes linguistic structures.

Args:
    tok: Token key.
    lem: Lemma key.
    pos: Part-of-speech key.
    dep: Dependency parse tree key.
    sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
    ner: Named entity key.
    srl: Semantic role labeling key.
    con: Constituency parsing key.
    show_header: ``True`` to print a header which indicates each field with its name.
    html: ``True`` to output HTML format so that non-ASCII characters can align correctly., Remove the spans attached to tokens and return a new document.

Args:
    tok: The key to tokens.

Returns:
    A new document or itself., Squeeze the dimension of each field into one. It's intended to convert a nested document like ``[[sent_i]]``
to ``[sent_i]``. When there are multiple sentences, only the ``i-th`` one will be returned. Note this is not an
inplace operation.

Args:
    i: Keep the element at ``index`` for all ``list``\s.

Returns:
    A squeezed document with only one sentence., Translate tags for each annotation. This is an inplace operation.

.. Attention:: Note that the translated document might not print well in terminal due to non-ASCII characters.

Args:
    lang: Target language to be translated to.
    tok: Token key.
    pos: Part-of-speech key.
    dep: Dependency parse tree key.
    sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
    ner: Named entity key.
    srl: Semantic role labeling key.

Returns:
    The translated document.",", 	, 
, 

, 
        Convert to :class:`~hanlp_common.conll.CoNLLSentence`.

        Args:
            tok (str): Field name for tok.
            lem (str): Field name for lem.
            pos (str): Field name for upos.
            fea (str): Field name for feats.
            dep (str): Field name for dependency parsing.
            sdp (str): Field name for semantic dependency parsing.

        Returns:
            A :class:`~hanlp_common.conll.CoNLLSentence` representation.

        , 
        Convert to a pretty text representation which can be printed to visualize linguistic structures.

        Args:
            tok: Token key.
            lem: Lemma key.
            pos: Part-of-speech key.
            dep: Dependency parse tree key.
            sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
            ner: Named entity key.
            srl: Semantic role labeling key.
            con: Constituency parsing key.
            show_header: ``True`` to include a header which indicates each field with its name.
            html: ``True`` to output HTML format so that non-ASCII characters can align correctly.

        Returns:
            A pretty string.

        , 
        Count number of sentences in this document.

        Returns:
            Number of sentences.
        , 
        Get value by the prefix of a key.

        Args:
            prefix: The prefix of a key. If multiple keys are matched, only the first one will be used.

        Returns:
            The value assigned with the matched key.
        , 
        Print a pretty text representation which visualizes linguistic structures.

        Args:
            tok: Token key.
            lem: Lemma key.
            pos: Part-of-speech key.
            dep: Dependency parse tree key.
            sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
            ner: Named entity key.
            srl: Semantic role labeling key.
            con: Constituency parsing key.
            show_header: ``True`` to print a header which indicates each field with its name.
            html: ``True`` to output HTML format so that non-ASCII characters can align correctly.

        , 
        Remove the spans attached to tokens and return a new document.

        Args:
            tok: The key to tokens.

        Returns:
            A new document or itself.

        , 
        Squeeze the dimension of each field into one. It's intended to convert a nested document like ``[[sent_i]]``
        to ``[sent_i]``. When there are multiple sentences, only the ``i-th`` one will be returned. Note this is not an
        inplace operation.

        Args:
            i: Keep the element at ``index`` for all ``list``\s.

        Returns:
            A squeezed document with only one sentence.

        , 
        Translate tags for each annotation. This is an inplace operation.

        .. Attention:: Note that the translated document might not print well in terminal due to non-ASCII characters.

        Args:
            lang: Target language to be translated to.
            tok: Token key.
            pos: Part-of-speech key.
            dep: Dependency parse tree key.
            sdp: Semantic dependency tree/graph key. SDP visualization has not been implemented yet.
            ner: Named entity key.
            srl: Semantic role labeling key.

        Returns:
            The translated document.

        ,  ,   ─,  ──, &nbsp;, ([►─])([\w-]*)(\s+)([│├]), . Please contribute to our translation at https://github.com/hankcs/HanLP, /, :, </div>, </pre>, <br>, <div style=""display: table; padding-bottom: 1rem;"">, <pre style=""display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;"">, A dict structure holding parsed annotations. A document is a subclass of ``dict`` and it supports every
        interface of ``dict``\. Additionally, it supports interfaces to deal with various linguistic structures. Its
        ``str`` and ``dict`` representations are made to be compatible with JSON serialization.

        Args:
            *args: An iterator of key-value pairs.
            **kwargs: Arguments from ``**`` operator.

        Examples::

            # Create a document
            doc = Document(
                tok=[[""晓美焰"", ""来到"", ""北京"", ""立方庭"", ""参观"", ""自然"", ""语义"", ""科技"", ""公司""]],
                pos=[[""NR"", ""VV"", ""NR"", ""NR"", ""VV"", ""NN"", ""NN"", ""NN"", ""NN""]],
                ner=[[[""晓美焰"", ""PERSON"", 0, 1], [""北京立方庭"", ""LOCATION"", 2, 4],
                      [""自然语义科技公司"", ""ORGANIZATION"", 5, 9]]],
                dep=[[[2, ""nsubj""], [0, ""root""], [4, ""name""], [2, ""dobj""], [2, ""conj""],
                      [9, ""compound""], [9, ""compound""], [9, ""compound""], [5, ""dobj""]]]
            )

            # print(doc) or str(doc) to get its JSON representation
            print(doc)

            # Access an annotation by its task name
            print(doc['tok'])

            # Get number of sentences
            print(f'It has {doc.count_sentences()} sentence(s)')

            # Access the n-th sentence
            print(doc.squeeze(0)['tok'])

            # Pretty print it right in your console or notebook
            doc.pretty_print()

            # To save the pretty prints in a str
            pretty_text: str = '\n\n'.join(doc.to_pretty())

        , Convert to a json compatible dict.

        Returns:
            A dict representation.
        , Convert to json string.

        Args:
            ensure_ascii: ``False`` to allow for non-ascii text.
            indent: Indent per nested structure.

        Returns:
            A text representation in ``str``.

        , NER, No translation for , PA, PoS, SRL, Token, Type, amr, con, dep, fea, lem, ner, pos, sdp, srl, tok, zh, |, ─, ─ ─, ─\1►, ───, ────, ───►, │, ┤, ┼, ╟──►, ►(─+)►","0, 1, 2, 3, 4, False, None, True","
Count number of sentences in this document.

Returns:
Number of sentences.
, 
Get value by the prefix of a key.

Args:
prefix: The prefix of a key. If multiple keys are matched, only the first one will be used.

Returns:
The value assigned with the matched key.
, 
Remove the spans attached to tokens and return a new document.

Args:
tok: The key to tokens.

Returns:
A new document or itself.

, 
d = dict(self)
for k, v in self.items():
if v == [] or v is None:
continue
if k == 'con':
if not isinstance(v, Tree) and not isinstance(v[0], Tree):
continue
flat = isinstance(v, Tree)
if flat:
v = [v]
ls = []
for each in v:
if isinstance(each, Tree):
ls.append(tree_to_list(each))
if flat:
ls = ls[0]
d[k] = ls
return d

def __str__(self) -> str:
return self.to_json()

def to_conll(self, tok='tok', lem='lem', pos='pos', fea='fea', dep='dep', sdp='sdp') -> Union[
CoNLLSentence, List[CoNLLSentence]]:
, 
d = self.to_dict()
text = json.dumps(d, ensure_ascii=ensure_ascii, indent=indent, default=lambda o: repr(o))
text = collapse_json(text, 4)
return text

def to_dict(self):
Convert to a json compatible dict., 
if lang == 'zh':
from hanlp.utils.lang.zh import localization
else:
raise NotImplementedError(f'No translation for {lang}. '
f'Please contribute to our translation at https://github.com/hankcs/HanLP')
flat = isinstance(self[tok][0], str)
for task, name in zip(['pos', 'ner', 'dep', 'sdp', 'srl'], [pos, ner, dep, sdp, srl]):
annotations = self.get(name, None)
if not annotations:
continue
if flat:
annotations = [annotations]
translate: dict = getattr(localization, name, None)
if not translate:
continue
for anno_per_sent in annotations:
for i, v in enumerate(anno_per_sent):
if task == 'ner' or task == 'dep':
v[1] = translate.get(v[1], v[1])
else:
anno_per_sent[i] = translate.get(v, v)
return self

def squeeze(self, i=0):
r""""""
Squeeze the dimension of each field into one. It's intended to convert a nested document like ``[[sent_i]]``
to ``[sent_i]``. When there are multiple sentences, only the ``i-th`` one will be returned. Note this is not an
inplace operation.

Args:
i: Keep the element at ``index`` for all ``list``\s.

Returns:
A squeezed document with only one sentence.

, 
results = []
tok = prefix_match(tok, self)
pos = prefix_match(pos, self)
ner = prefix_match(ner, self)
conlls = self.to_conll(tok=tok, lem=lem, pos=pos, dep=dep, sdp=sdp)
flat = isinstance(conlls, CoNLLSentence)
if flat:
conlls: List[CoNLLSentence] = [conlls]

def condense(block_, extras_=None):
text_ = make_table(block_, insert_header=False)
text_ = [x.split('\t', 1) for x in text_.split('\n')]
text_ = [[x[0], x[1].replace('\t', '')] for x in text_]
if extras_:
for r, s in zip(extras_, text_):
r.extend(s)
return text_

for i, conll in enumerate(conlls):
conll: CoNLLSentence = conll
tokens = [x.form for x in conll]
length = len(conll)
extras = [[] for j in range(length + 1)]
if ner in self:
ner_samples = self[ner]
if flat:
ner_samples = [ner_samples]
ner_per_sample = ner_samples[i]
# For nested NER, use the longest span
start_offsets = [None for i in range(length)]
for ent, label, b, e in ner_per_sample:
if not start_offsets[b] or e > start_offsets[b][-1]:
start_offsets[b] = (ent, label, b, e)
ner_per_sample = [y for y in start_offsets if y]
header = ['Token', 'NER', 'Type']
block = [[] for _ in range(length + 1)]
_ner = []
_type = []
offset = 0
for ent, label, b, e in ner_per_sample:
render_labeled_span(b, e, _ner, _type, label, offset)
offset = e
if offset != length:
_ner.extend([''] * (length - offset))
_type.extend([''] * (length - offset))
if any(_type):
block[0].extend(header)
for j, (_s, _t) in enumerate(zip(_ner, _type)):
block[j + 1].extend((tokens[j], _s, _t))
text = condense(block, extras)

if srl in self:
srl_samples = self[srl]
if flat:
srl_samples = [srl_samples]
srl_per_sample = srl_samples[i]
for k, pas in enumerate(srl_per_sample):
if not pas:
continue
block = [[] for _ in range(length + 1)]
header = ['Token', 'SRL', f'PA{k + 1}']
_srl = []
_type = []
offset = 0
p_index = None
for _, label, b, e in pas:
render_labeled_span(b, e, _srl, _type, label, offset)
offset = e
if label == PRED:
p_index = b
if len(_srl) != length:
_srl.extend([''] * (length - offset))
_type.extend([''] * (length - offset))
if p_index is not None:
_srl[p_index] = '╟──►'
# _type[j] = 'V'
if len(block) != len(_srl) + 1:
# warnings.warn(f'Unable to visualize overlapped spans: {pas}')
continue
block[0].extend(header)
while len(_srl) < length:
_srl.append('')
while len(_type) < length:
_type.append('')
for j, (_s, _t) in enumerate(zip(_srl, _type)):
block[j + 1].extend((tokens[j], _s, _t))
text = condense(block, extras)
if con in self:
con_samples: Tree = self[con]
if flat:
con_samples: List[Tree] = [con_samples]
tree = con_samples[i]
block = [[] for _ in range(length + 1)]
block[0].extend(('Token', 'PoS'))
for j, t in enumerate(tree.pos()):
block[j + 1].extend(t)

for height in range(2, tree.height() + (0 if len(tree) == 1 else 1)):
offset = 0
spans = []
labels = []
for k, subtree in enumerate(tree.subtrees(lambda x: x.height() == height)):
subtree: Tree = subtree
b, e = offset, offset + len(subtree.leaves())
if height >= 3:
b, e = subtree[0].center, subtree[-1].center + 1
subtree.center = b + (e - b) // 2
render_labeled_span(b, e, spans, labels, subtree.label(), offset, unidirectional=True)
offset = e
if len(spans) != length:
spans.extend([''] * (length - len(spans)))
if len(labels) != length:
labels.extend([''] * (length - len(labels)))
if height < 3:
continue
block[0].extend(['', f'{height}'])
for j, (_s, _t) in enumerate(zip(spans, labels)):
block[j + 1].extend((_s, _t))
# check short arrows and increase their length
for j, arrow in enumerate(spans):
if not arrow:
# -1 current tag ; -2 arrow to current tag ; -3 = prev tag ; -4 = arrow to prev tag
if block[j + 1][-3] or block[j + 1][-4] == '───►':
if height > 3:
if block[j + 1][-3]:
block[j + 1][-1] = block[j + 1][-3]
block[j + 1][-2] = '───►'
else:
block[j + 1][-1] = '────'
block[j + 1][-2] = '────'
block[j + 1][-3] = '────'
if block[j + 1][-4] == '───►':
block[j + 1][-4] = '────'
else:
block[j + 1][-1] = '────'
if block[j + 1][-1] == '────':
block[j + 1][-2] = '────'
if not block[j + 1][-4]:
block[j + 1][-4] = '────'
# If the root label is shorter than the level number, extend it to the same length
level_len = len(block[0][-1])
for row in block[1:]:
if row[-1] and len(row[-1]) < level_len:
row[-1] = row[-1] + ' ' * (level_len - len(row[-1]))

text = condense(block)
# Cosmetic issues
for row in text[1:]:
while '  ─' in row[1]:
row[1] = row[1].replace('  ─', ' ──')
row[1] = row[1].replace('─ ─', '───')
row[1] = re.sub(r'([►─])([\w-]*)(\s+)([│├])', lambda
m: f'{m.group(1)}{m.group(2)}{""─"" * len(m.group(3))}{""┤"" if m.group(4) == ""│"" else ""┼""}',
row[1])
row[1] = re.sub(r'►(─+)►', r'─\1►', row[1])
for r, s in zip(extras, text):
r.extend(s)
# warnings.warn('Unable to visualize non-projective trees.')
if dep in self and conll.projective:
text = conll.to_tree(extras)
if not show_header:
text = text.split('\n')
text = '\n'.join(text[2:])
results.append(text)
elif any(extras):
results.append(make_table(extras, insert_header=True))
else:
results.append(' '.join(['/'.join(str(f) for f in x.nonempty_fields) for x in conll]))
if html:
def to_html(pretty_text: str) -> str:
lines = [x for x in pretty_text.split('\n') if x]
cells = []
for line in lines:
cells.append(line.split('\t'))

num_cols = len(cells[0])
cols = []

for i in range(num_cols):
cols.append([])
for row in cells:
cols[-1].append(row[i])

html = '<div style=""display: table; padding-bottom: 1rem;"">'
for i, each in enumerate(cols):
html += '<pre style=""display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,' \
'Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;"">'
if i != len(cols) - 1:
each = [x + ' ' for x in each]
html += '<br>'.join([x.replace(' ', '&nbsp;') for x in each])
html += '</pre>'
html += '</div>'
return html

results = [to_html(x) for x in results]
if flat:
return results[0]
return results

def pretty_print(self, tok='tok', lem='lem', pos='pos', dep='dep', sdp='sdp', ner='ner', srl='srl', con='con',
show_header=True, html=IPYTHON):
, 
results = self.to_pretty(tok, lem, pos, dep, sdp, ner, srl, con, show_header, html=html)
if isinstance(results, str):
results = [results]
if html and IPYTHON:
from IPython.core.display import display, HTML
display(HTML('<br>'.join(results)))
else:
sent_new_line = '\n\n' if any('\n' in x for x in results) else '\n'
print(sent_new_line.join(results))

def translate(self, lang, tok='tok', pos='pos', dep='dep', sdp='sdp', ner='ner', srl='srl'):
, 
super().__init__(*args, **kwargs)
for k, v in list(self.items()):
if not v:
continue
if k == 'con':
if isinstance(v, Tree) or isinstance(v[0], Tree):
continue
flat = isinstance(v[0], str)
if flat:
v = [v]
ls = []
for each in v:
if not isinstance(each, Tree):
ls.append(list_to_tree(each))
if flat:
ls = ls[0]
self[k] = ls
elif k == 'amr':
from hanlp_common.amr import AMRGraph
import penman
if isinstance(v, AMRGraph) or isinstance(v[0], AMRGraph):
continue
flat = isinstance(v[0][0], str)
if flat:
v = [v]
graphs = [AMRGraph(penman.Graph(triples)) for triples in v]
if flat:
graphs = graphs[0]
self[k] = graphs

def to_json(self, ensure_ascii=False, indent=2) -> str:
Convert to json string., 
tok = prefix_match(tok, self)
lem = prefix_match(lem, self)
pos = prefix_match(pos, self)
fea = prefix_match(fea, self)
dep = prefix_match(dep, self)
sdp = prefix_match(sdp, self)
results = CoNLLSentenceList()
if not tok or not self[tok]:
return results
self = self._to_doc_without_spans(tok)
flat = isinstance(self[tok][0], str)
if flat:
d = Document((k, [v]) for k, v in self.items())
else:
d = self
for sample in [dict(zip(d, t)) for t in zip(*d.values())]:
def get(_k, _i):
_v = sample.get(_k, None)
if not _v:
return None
return _v[_i]

sent = CoNLLSentence()

for i, _tok in enumerate(sample[tok]):
_dep = get(dep, i)
if not _dep:
_dep = (None, None)
sent.append(
CoNLLUWord(i + 1, form=_tok, lemma=get(lem, i), upos=get(pos, i), feats=get(fea, i), head=_dep[0],
deprel=_dep[1],
deps=None if not get(sdp, i) else '|'.join(f'{x[0]}:{x[1]}' for x in get(sdp, i))))
results.append(sent)
if flat:
return results[0]
return results

def to_pretty(self, tok='tok', lem='lem', pos='pos', dep='dep', sdp='sdp', ner='ner', srl='srl', con='con',
show_header=True, html=False) -> Union[str, List[str]]:
, # -*- coding:utf-8 -*-, # Access an annotation by its task name, # Access the n-th sentence, # Author: hankcs, # Create a document, # Date: 2019-12-31 04:16, # Get number of sentences, # Pretty print it right in your console or notebook, # To save the pretty prints in a str, # print(doc) or str(doc) to get its JSON representation","
results = []
tok = prefix_match(tok, self)
pos = prefix_match(pos, self)
ner = prefix_match(ner, self)
conlls = self.to_conll(tok=tok, lem=lem, pos=pos, dep=dep, sdp=sdp)
flat = isinstance(conlls, CoNLLSentence)
if flat:
conlls: List[CoNLLSentence] = [conlls]

def condense(block_, extras_=None):
text_ = make_table(block_, insert_header=False)
text_ = [x.split('\t', 1) for x in text_.split('\n')]
text_ = [[x[0], x[1].replace('\t', '')] for x in text_]
if extras_:
for r, s in zip(extras_, text_):
r.extend(s)
return text_

for i, conll in enumerate(conlls):
conll: CoNLLSentence = conll
tokens = [x.form for x in conll]
length = len(conll)
extras = [[] for j in range(length + 1)]
if ner in self:
ner_samples = self[ner]
if flat:
ner_samples = [ner_samples]
ner_per_sample = ner_samples[i]
# For nested NER, use the longest span
start_offsets = [None for i in range(length)]
for ent, label, b, e in ner_per_sample:
if not start_offsets[b] or e > start_offsets[b][-1]:
start_offsets[b] = (ent, label, b, e)
ner_per_sample = [y for y in start_offsets if y]
header = ['Token', 'NER', 'Type']
block = [[] for _ in range(length + 1)]
_ner = []
_type = []
offset = 0
for ent, label, b, e in ner_per_sample:
render_labeled_span(b, e, _ner, _type, label, offset)
offset = e
if offset != length:
_ner.extend([''] * (length - offset))
_type.extend([''] * (length - offset))
if any(_type):
block[0].extend(header)
for j, (_s, _t) in enumerate(zip(_ner, _type)):
block[j + 1].extend((tokens[j], _s, _t))
text = condense(block, extras)

if srl in self:
srl_samples = self[srl]
if flat:
srl_samples = [srl_samples]
srl_per_sample = srl_samples[i]
for k, pas in enumerate(srl_per_sample):
if not pas:
continue
block = [[] for _ in range(length + 1)]
header = ['Token', 'SRL', f'PA{k + 1}']
_srl = []
_type = []
offset = 0
p_index = None
for _, label, b, e in pas:
render_labeled_span(b, e, _srl, _type, label, offset)
offset = e
if label == PRED:
p_index = b
if len(_srl) != length:
_srl.extend([''] * (length - offset))
_type.extend([''] * (length - offset))
if p_index is not None:
_srl[p_index] = '╟──►'
# _type[j] = 'V'
if len(block) != len(_srl) + 1:
# warnings.warn(f'Unable to visualize overlapped spans: {pas}')
continue
block[0].extend(header)
while len(_srl) < length:
_srl.append('')
while len(_type) < length:
_type.append('')
for j, (_s, _t) in enumerate(zip(_srl, _type)):
block[j + 1].extend((tokens[j], _s, _t))
text = condense(block, extras)
if con in self:
con_samples: Tree = self[con]
if flat:
con_samples: List[Tree] = [con_samples]
tree = con_samples[i]
block = [[] for _ in range(length + 1)]
block[0].extend(('Token', 'PoS'))
for j, t in enumerate(tree.pos()):
block[j + 1].extend(t)

for height in range(2, tree.height() + (0 if len(tree) == 1 else 1)):
offset = 0
spans = []
labels = []
for k, subtree in enumerate(tree.subtrees(lambda x: x.height() == height)):
subtree: Tree = subtree
b, e = offset, offset + len(subtree.leaves())
if height >= 3:
b, e = subtree[0].center, subtree[-1].center + 1
subtree.center = b + (e - b) // 2
render_labeled_span(b, e, spans, labels, subtree.label(), offset, unidirectional=True)
offset = e
if len(spans) != length:
spans.extend([''] * (length - len(spans)))
if len(labels) != length:
labels.extend([''] * (length - len(labels)))
if height < 3:
continue
block[0].extend(['', f'{height}'])
for j, (_s, _t) in enumerate(zip(spans, labels)):
block[j + 1].extend((_s, _t))
# check short arrows and increase their length
for j, arrow in enumerate(spans):
if not arrow:
# -1 current tag ; -2 arrow to current tag ; -3 = prev tag ; -4 = arrow to prev tag
if block[j + 1][-3] or block[j + 1][-4] == '───►':
if height > 3:
if block[j + 1][-3]:
block[j + 1][-1] = block[j + 1][-3]
block[j + 1][-2] = '───►'
else:
block[j + 1][-1] = '────'
block[j + 1][-2] = '────'
block[j + 1][-3] = '────'
if block[j + 1][-4] == '───►':
block[j + 1][-4] = '────'
else:
block[j + 1][-1] = '────'
if block[j + 1][-1] == '────':
block[j + 1][-2] = '────'
if not block[j + 1][-4]:
block[j + 1][-4] = '────'
# If the root label is shorter than the level number, extend it to the same length
level_len = len(block[0][-1])
for row in block[1:]:
if row[-1] and len(row[-1]) < level_len:
row[-1] = row[-1] + ' ' * (level_len - len(row[-1]))

text = condense(block)
# Cosmetic issues
for row in text[1:]:
while '  ─' in row[1]:
row[1] = row[1].replace('  ─', ' ──')
row[1] = row[1].replace('─ ─', '───')
row[1] = re.sub(r'([►─])([\w-]*)(\s+)([│├])', lambda
m: f'{m.group(1)}{m.group(2)}{""─"" * len(m.group(3))}{""┤"" if m.group(4) == ""│"" else ""┼""}',
row[1])
row[1] = re.sub(r'►(─+)►', r'─\1►', row[1])
for r, s in zip(extras, text):
r.extend(s)
# warnings.warn('Unable to visualize non-projective trees.')
if dep in self and conll.projective:
text = conll.to_tree(extras)
if not show_header:
text = text.split('\n')
text = '\n'.join(text[2:])
results.append(text)
elif any(extras):
results.append(make_table(extras, insert_header=True))
else:
results.append(' '.join(['/'.join(str(f) for f in x.nonempty_fields) for x in conll]))
if html:
def to_html(pretty_text: str) -> str:
lines = [x for x in pretty_text.split('\n') if x]
cells = []
for line in lines:
cells.append(line.split('\t'))

num_cols = len(cells[0])
cols = []

for i in range(num_cols):
cols.append([])
for row in cells:
cols[-1].append(row[i])

html = '<div style=""display: table; padding-bottom: 1rem;"">'
for i, each in enumerate(cols):
html += '<pre style=""display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,' \
'Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;"">'
if i != len(cols) - 1:
each = [x + ' ' for x in each]
html += '<br>'.join([x.replace(' ', '&nbsp;') for x in each])
html += '</pre>'
html += '</div>'
return html

results = [to_html(x) for x in results]
if flat:
return results[0]
return results

def pretty_print(self, tok='tok', lem='lem', pos='pos', dep='dep', sdp='sdp', ner='ner', srl='srl', con='con',
show_header=True, html=IPYTHON):
, [│├], [►─], 公司, 北京, 北京立方庭, 参观, 晓美焰, 来到, 科技, 立方庭, 自然, 自然语义科技公司, 语义"
https://github.com/hankcs/HanLP,io.py,0,0.0,33,71.74,6,13.04,4,8.7,3,6.52,0,0.0,6,0,6,0,,"args, cls, default, dict, dirname, ensure_ascii, eprint, f, file_extension, filename, filename_is_json, float, indent, int, item, json, kwargs, list, load_json, load_pickle, open, os, out, path, pickle, print, repr, save_json, save_pickle, src, str, sys, typing","json.dump, json.load, os.makedirs, os.path.dirname, os.path.splitext, pickle.dump, pickle.load, sys.stderr, typing.Union","eprint, filename_is_json, load_json, load_pickle, save_json, save_pickle",,"dirname, f, file_extension, filename, out, src",,".json, .jsonl, rb, utf-8, w, wb","2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-16 22:38",
https://github.com/hankcs/HanLP,reflection.py,0,0.0,24,61.54,8,20.51,1,2.56,6,15.38,0,0.0,5,0,5,3,,"__class__, __module__, __name__, class_name, classpath, classpath_of, cls, cls_name, endswith, getattr, importlib, inspect, kwargs, len, module_name, module_path_of, obj, object_from_classpath, rsplit, startswith, str, str_to_type, type_object, type_to_str","importlib.import_module, inspect.getmodule, inspect.isfunction","classpath_of, module_path_of, object_from_classpath, str_to_type, type_to_str",,"class_name, classpath, cls, cls_name, module_name","convert a type object to class path in str format

Args:
  type_object: type

Returns:
  class path, convert class path in str format to a type

Args:
  classpath: class path

Returns:
  type, get the full class path of object

Args:
  obj: return:

Returns:","'>, ., <class ', convert a type object to class path in str format

    Args:
      type_object: type

    Returns:
      class path

    , convert class path in str format to a type

    Args:
      classpath: class path

    Returns:
      type

    , get the full class path of object

    Args:
      obj: return:

    Returns:

    , illegal input, {0}.{1}",1,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 16:41, convert a type object to class path in str format

Args:
type_object: type

Returns:
class path

, convert class path in str format to a type

Args:
classpath: class path

Returns:
type

, get the full class path of object

Args:
obj: return:

Returns:

",
https://github.com/hankcs/HanLP,structure.py,0,0.0,38,73.08,6,11.54,3,5.77,5,9.62,0,0.0,11,2,2,3,,"Serializable, SerializableDict, __dict__, __getattr__, __getitem__, __setattr__, __setitem__, clear, collections, copy_from, dict, ensure_ascii, fmt, hanlp_common, indent, isinstance, item, items, json, key, load, load_json, load_pickle, object, path, repr, save, save_json, save_pickle, self, sort, sorted, startswith, str, to_dict, to_json, update, value","collections.OrderedDict, hanlp_common.io.filename_is_json, hanlp_common.io.load_json, hanlp_common.io.load_pickle, hanlp_common.io.save_json, hanlp_common.io.save_pickle, json.dumps","__getattr__, __setattr__, copy_from, load, load_json, load_pickle, save, save_json, save_pickle, to_dict, to_json","Serializable, SerializableDict","d, item","A super class for save/load operations., Load from path

Args:
  path(str): file path

Returns:, Save to path

Args:
  path:

Returns:","A super class for save/load operations., Load from path

        Args:
          path(str): file path

        Returns:


        , Save to path

        Args:
          path:

        Returns:


        , __, json, jsonl","2, False, None","
save_pickle(self, path)

def load_pickle(self, path):
Load from path, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-19 20:56, A super class for save/load operations.

def save(self, path, fmt=None):
if not fmt:
if filename_is_json(path):
self.save_json(path)
else:
self.save_pickle(path)
elif fmt in ['json', 'jsonl']:
self.save_json(path)
else:
self.save_pickle(path)

def load(self, path, fmt=None):
if not fmt:
if filename_is_json(path):
self.load_json(path)
else:
self.load_pickle(path)
elif fmt in ['json', 'jsonl']:
self.load_json(path)
else:
self.load_pickle(path)

def save_pickle(self, path):
Save to path",
https://github.com/hankcs/HanLP,util.py,0,0.0,93,72.66,22,17.19,7,5.47,6,4.69,0,0.0,18,1,30,4,,"DummyContext, __enter__, __exit__, add, append, batch, bool, collapse_json, consume_keys_from_dict, consumed, d, descending, dfs, dict, each, endswith, excludes, extend, frame, from_iterable, get, graph, indent, infer_space_after, initial, inplace, inspect, int, isdebugging, isinstance, item, items, iterable, itertools, k_fold, keys, kwargs, last_token, len, line, list, list_is_list_of_lists, locals, math, max, merge_dict, merge_list_of_dict, merge_locals_kwargs, nd, num_samples, object, order, out, overwrite, path, pending, pop, powerset, prefix_match, q, quote_count, range, reorder, rstrip, samples, seen, sent, set, set_tuple_with, sizes, sorted, sources, split_dict, splitlines, start, startswith, str, strip, sublevel, target, text, token, topological_sort, total, trn, tst, tuple, typing, update, v, values, vs, whitespace_after","inspect.stack, itertools.chain, itertools.combinations, math.ceil, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Optional, typing.Tuple, typing.Union","__enter__, __exit__, collapse_json, consume_keys_from_dict, dfs, infer_space_after, isdebugging, k_fold, list_is_list_of_lists, merge_dict, merge_list_of_dict, merge_locals_kwargs, powerset, prefix_match, reorder, set_tuple_with, split_dict, topological_sort",DummyContext,"batch, consumed, each, frame, initial, item, kwargs, last_token, line, nd, num_samples, order, out, path, pending, q, quote_count, s, samples, seen, sizes, stack, sublevel, t, token, trn, tst, v, vs, whitespace_after","Compacts a string of json data by collapsing whitespace after the
specified indent level

NOTE: will not produce correct results when indent level is not a multiple
of the json indent level

Args:
  text: 
  indent:  (Default value = 12)

Returns:, Merging the provided dict with other kvs

Args:
  d: 
  kwargs: 
  d: dict: 
  overwrite:  (Default value = False)
  inplace:  (Default value = False)
  **kwargs: 

Returns:, See Also https://stackoverflow.com/questions/333995/how-to-detect-that-python-code-is-being-executed-through-the-debugger, powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)

Args:
    iterable:

Returns:",", 
, 
    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)

    Args:
        iterable:

    Returns:

    ,  , !, "", ', (, ), ,, ., :, ;, ?, Compacts a string of json data by collapsing whitespace after the
    specified indent level
    
    NOTE: will not produce correct results when indent level is not a multiple
    of the json indent level

    Args:
      text: 
      indent:  (Default value = 12)

    Returns:

    , Merging the provided dict with other kvs

    Args:
      d: 
      kwargs: 
      d: dict: 
      overwrite:  (Default value = False)
      inplace:  (Default value = False)
      **kwargs: 

    Returns:

    
    , See Also https://stackoverflow.com/questions/333995/how-to-detect-that-python-code-is-being-executed-through-the-debugger, __class__, kwargs, n't, pydevd.py, self","0, 1, 12, 2, False, None, True","
nd = dict([(k, v) for k, v in d.items()] + [(k, v) for k, v in kwargs.items() if overwrite or k not in d])
if inplace:
d.update(nd)
return d
return nd


def merge_locals_kwargs(locals: dict, kwargs: dict = None, excludes=('self', 'kwargs', '__class__')):
if not kwargs:
kwargs = dict()
return merge_dict(dict((k, v) for k, v in list(locals.items())
if k not in excludes), **kwargs)


def infer_space_after(sent: List[str]):
last_token = None
quote_count: int = 0
# infer whitespace after field
whitespace_after = [True] * len(sent)
for token in range(len(sent)):
if sent[token] == '""':
quote_count += 1
if quote_count % 2 != 0:
whitespace_after[token] = False
elif last_token is not None:
whitespace_after[last_token] = False

if last_token is not None:

if sent[token] in [""."", "":"", "","", "";"", "")"", ""n't"", ""!"", ""?""]:
whitespace_after[last_token] = False

if sent[token].startswith(""'""):
whitespace_after[last_token] = False

if sent[token] in [""(""]:
whitespace_after[token] = False

last_token = token
return whitespace_after


def collapse_json(text, indent=12):
Compacts a string of json data by collapsing whitespace after the, 
powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)

Args:
iterable:

Returns:

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-27 19:09, See Also https://stackoverflow.com/questions/333995/how-to-detect-that-python-code-is-being-executed-through-the-debugger
for frame in inspect.stack():
if frame[1].endswith(""pydevd.py""):
return True
return False


def list_is_list_of_lists(sent: Union[Any, List[Any]]) -> Optional[bool]:
if not sent:
return None
return isinstance(sent[0], list)


def set_tuple_with(t: Tuple, v, at=0) -> Tuple:
t = list(t)
t[at] = v
return tuple(t)


def consume_keys_from_dict(keys: Iterable, d: dict) -> dict:
consumed = {}
for k in keys:
if k in d:
consumed[k] = d.pop(k)
return consumed


def merge_dict(d: dict, overwrite=False, inplace=False, **kwargs):
Merging the provided dict with other kvs",
https://github.com/hankcs/HanLP,visualization.py,0,0.0,110,53.92,61,29.9,8,3.92,25,12.25,0,0.0,11,0,57,3,,"L, _, __name__, _do_print_debug_info, _start_end, alignment, append, arr, arr_chars, arr_i, arrow, arrow_index, arrows, arrows_with_deps, b, begin, cell_align, center, ch, child, close, col_widths, collections, column, column_widths, columns, dst, end, enumerate, evalute_field, extend, extended_align, field, field_spec, field_widths, fields, file, fmt, format, getattr, getvalue, goes_up, head, heading_align, heading_template, heading_widths, headings, height, insert, insert_header, io, isinstance, j, label, labels, left_rule, len, line, lines, list, list_to_tree, main, make_table, markdown_table, max, max_len, min, mn, mx, num_arrows_left, num_columns, num_deps, o_end, o_mn, o_mx, o_start, offset, other, phrasetree, pprint, pretty_tree_horizontal, range, record, records, render_arrows, render_labeled_span, render_span, reversed, right_rule, row, row_template, rows, rstrip, ruling, sent_len, sorted, spans, src, start, str, text, tree_to_list, tuple, type, unidirectional, w, width, write, x, zip","collections.defaultdict, io.StringIO, phrasetree.tree.Tree, pprint.pprint","_start_end, evalute_field, list_to_tree, main, make_table, markdown_table, pretty_tree_horizontal, render_arrows, render_labeled_span, render_span, tree_to_list",,"_, arr, arr_chars, arr_i, arrow, arrow_index, arrows_with_deps, cell_align, center, ch, child, col_widths, column, column_widths, columns, dst, end, extended_align, field, field_widths, fields, file, fmt, goes_up, head, heading_align, heading_template, heading_widths, height, j, left_rule, line, lines, max_len, mn, mx, num_arrows_left, num_columns, num_deps, o_end, o_mn, o_mx, o_start, other, record, right_rule, row, row_template, rows, ruling, sent_len, src, start, text, w, width, x","Evalute a field of a record using the type of the field_spec as a guide.

Args:
  record:
  field_spec:

Returns:, Generate a Doxygen-flavor Markdown table from records.
See https://stackoverflow.com/questions/13394140/generate-markdown-tables

file -- Any object with a 'write' method that takes a single string
    parameter.
records -- Iterable.  Rows will be generated from this.
fields -- List of fields for each row.  Each entry may be an integer,
    string or a function.  If the entry is an integer, it is assumed to be
    an index of each record.  If the entry is a string, it is assumed to be
    a field of each record.  If the entry is a function, it is called with
    the record and its return value is taken as the value of the field.
headings -- List of column headings.
alignment - List of pairs alignment characters.  The first of the pair
    specifies the alignment of the header, (Doxygen won't respect this, but
    it might look good, the second specifies the alignment of the cells in
    the column.

    Possible alignment characters are:
        '<' = Left align
        '>' = Right align (default for cells)
        '^' = Center (default for column headings)

Args:
  headings:
  records:
  fields:  (Default value = None)
  alignment:  (Default value = None)
  file:  (Default value = None)

Returns:, Print the dependency tree horizontally

Args:
  arrows: 
  _do_print_debug_info:  (Default value = False)

Returns:",", 	, 
,  ,   height = %d,   │,   ├►,  |,  | , %%-%ds, %d is over %d, -, :, <, >, Arrow %d: ""%s"" -> ""%s"", Evalute a field of a record using the type of the field_spec as a guide.

    Args:
      record:
      field_spec:

    Returns:

    , Generate a Doxygen-flavor Markdown table from records.
    See https://stackoverflow.com/questions/13394140/generate-markdown-tables

    file -- Any object with a 'write' method that takes a single string
        parameter.
    records -- Iterable.  Rows will be generated from this.
    fields -- List of fields for each row.  Each entry may be an integer,
        string or a function.  If the entry is an integer, it is assumed to be
        an index of each record.  If the entry is a string, it is assumed to be
        a field of each record.  If the entry is a function, it is called with
        the record and its return value is taken as the value of the field.
    headings -- List of column headings.
    alignment - List of pairs alignment characters.  The first of the pair
        specifies the alignment of the header, (Doxygen won't respect this, but
        it might look good, the second specifies the alignment of the cells in
        the column.

        Possible alignment characters are:
            '<' = Left align
            '>' = Right align (default for cells)
            '^' = Center (default for column headings)

    Args:
      headings:
      records:
      fields:  (Default value = None)
      alignment:  (Default value = None)
      file:  (Default value = None)

    Returns:

    , Print the dependency tree horizontally

    Args:
      arrows: 
      _do_print_debug_info:  (Default value = False)

    Returns:

    , Rendering arrow %d: ""%s"" -> ""%s"", ^, __main__, arrows:, arrows_with_deps:, e, en, ens, ensw, enw, es, esw, ew, from, height, n, ns, num_deps, num_deps_left, s, to, underset, w, {:, | , }, ─, ───►, ──┐, ──┘, ──┴►, │, ┌, └, ├, ┬, ┴, ┼, ►, ◄─┐, ◄─┘, ◄─┴►","0, 1, 12, 2, 3, 7, False, None","#           {'from': 4, 'to': 3}], #     '  │',, #     '  ├►',, #     '◄─┐',, #     '◄─┘',, # -*- coding:utf-8 -*-, # Check the height needed., # Compute the table cell data, # Convert the character lists into strings., # Draw the adjoining vertical line., # Draw the incoming dst line., # Draw the outgoing src line., # Fill out any missing alignment characters., # Modified from https://github.com/tylerneylon/explacy, # Render the arrows in characters. Some heights will be raised to make room for arrowheads., # Set the base height; these may increase to allow room for arrowheads after this., # Update arrows_with_deps., # ])), # arrows = [{'from': 1, 'to': 0}, {'from': 2, 'to': 1}, {'from': 2, 'to': 4}, {'from': 2, 'to': 5},, # lines = pretty_tree_horizontal(arrows), # print('\n'.join([, # print('\n'.join(lines)), Evalute a field of a record using the type of the field_spec as a guide.

Args:
record:
field_spec:

Returns:

, Generate a Doxygen-flavor Markdown table from records.
See https://stackoverflow.com/questions/13394140/generate-markdown-tables

file -- Any object with a 'write' method that takes a single string
parameter.
records -- Iterable.  Rows will be generated from this.
fields -- List of fields for each row.  Each entry may be an integer,
string or a function.  If the entry is an integer, it is assumed to be
an index of each record.  If the entry is a string, it is assumed to be
a field of each record.  If the entry is a function, it is called with
the record and its return value is taken as the value of the field.
headings -- List of column headings.
alignment - List of pairs alignment characters.  The first of the pair
specifies the alignment of the header, (Doxygen won't respect this, but
it might look good, the second specifies the alignment of the cells in
the column.

Possible alignment characters are:
'<' = Left align
'>' = Right align (default for cells)
'^' = Center (default for column headings)

Args:
headings:
records:
fields:  (Default value = None)
alignment:  (Default value = None)
file:  (Default value = None)

Returns:

, Print the dependency tree horizontally

Args:
arrows:
_do_print_debug_info:  (Default value = False)

Returns:

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-16 22:21",
https://github.com/hankcs/HanLP,fine_csv_logger.py,0,0.0,51,69.86,11,15.07,4,5.48,7,9.59,0,0.0,6,2,9,0,,"FineCSVLogger, StreamTableFormatter, _, __init__, append, bars, c, cell, cells, close, col_widths, copy, dict, enumerate, epoch, filename, flush, format_cell, format_row, formatter, get, getattr, headers, io, isinstance, join, keys, len, list, logs, max, min_width, model, numpy, object, on_epoch_end, on_train_begin, on_train_end, open, out, self, separator, sorted, str, super, tensorflow, typing, values, width, write, zip","io.TextIOWrapper, np.float, np.float32, tf.keras.callbacks.History, typing.List","__init__, format_cell, format_row, on_epoch_end, on_train_begin, on_train_end","FineCSVLogger, StreamTableFormatter","_, bars, c, cell, cells, headers, logs, values, width",,", 
, ,, -, NA, a, epoch, stop_training, w, {:>{}.4f}, {:>{}}","0, 1, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-05 02:12, # We set NA so that csv parsers do not fail for this last epoch., # bars for markdown style, # feed them twice to decide the actual width, # print headers and bars",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-05 02:10",
https://github.com/hankcs/HanLP,component.py,0,0.0,13,56.52,3,13.04,2,8.7,5,21.74,0,0.0,2,1,0,2,,"Component, NotImplementedError, __call__, __class__, __name__, abc, args, hanlp_common, inspect, kwargs, predict, self, typing","abc.ABC, abc.abstractmethod, hanlp_common.configurable.Configurable, inspect.stack, typing.Any","__call__, predict",Component,,"A shortcut for :func:`~hanlp.common.component.predict`.

Args:
  *args: Any type of data subject to sub-classes
  **kwargs: Additional arguments

Returns: Any predicted annotations., Predict on data. This is the base class for all components, including rule based and statistical ones.

Args:
  *args: Any type of data subject to sub-classes
  **kwargs: Additional arguments

Returns: Any predicted annotations.","
        A shortcut for :func:`~hanlp.common.component.predict`.

        Args:
          *args: Any type of data subject to sub-classes
          **kwargs: Additional arguments

        Returns: Any predicted annotations.

        , %s.%s(), Predict on data. This is the base class for all components, including rule based and statistical ones.

        Args:
          *args: Any type of data subject to sub-classes
          **kwargs: Additional arguments

        Returns: Any predicted annotations.

        ","0, 3","
A shortcut for :func:`~hanlp.common.component.predict`.

Args:
*args: Any type of data subject to sub-classes
**kwargs: Additional arguments

Returns: Any predicted annotations.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 14:45, Predict on data. This is the base class for all components, including rule based and statistical ones.

Args:
*args: Any type of data subject to sub-classes
**kwargs: Additional arguments

Returns: Any predicted annotations.

",
https://github.com/hankcs/HanLP,dataset.py,0,0.0,170,59.86,55,19.37,8,2.82,51,17.96,0,0.0,28,14,55,26,,"BucketSampler, CachedDataLoader, DeviceDataLoader, KMeansSampler, KMeansSamplerBuilder, PadSequenceDataLoader, PrefetchDataLoader, SamplerBuilder, SortingSampler, SortingSamplerBuilder, TableDataset, TransformSequentialDataset, Transformable, TransformableDataset, __call__, __getitem__, __init__, __iter__, __len__, __repr__, _batchify, _build_cache, _fire_process, _prefetch_generator, abc, any, append, append_transform, batch, batch_indices, batch_max_tokens, batch_sampler, batch_size, batchify, bool, bucket, buckets, build, cache, chars, chunks, close, collate_fn, copy, criterion, cur, data, data_, dataloader, dataset, delimiter, device, drop_last, dst, each, effective_tokens, endswith, enumerate, field, filename, filepath, float, generate_idx, get, good_ones, gradient_accumulation, hanlp, hanlp_common, headers, ids, idx, index, indices, info, inner_is_iterable, inplace, insert, insert_transform, int, isinstance, items, j, k_fold, kwargs, len, lengths, load_data, load_file, log, logger, logging, math, max, max_seq_len, max_word_len, min, mini_batch, multiprocessing_context, n_buckets, num_pruned, num_tokens, num_workers, object, open, os, outputs, pad, pad_data, pad_dict, pin_memory, prefetch, prev, process, property, prune, purge_cache, put, queue, random, range, range_fn, ratios, raw_batch, round, safe_pad_token_idx, sample, sampler, samples, scale, self, setter, should_load_file, shuffle, size, size_after, size_before, sizes, slice, split, split_sizes, start, staticmethod, strip, subset, sum, super, tempfile, tensorize, terminate, test_indices, timeout, timer, to, tolist, torch, train_indices, transform, transform_sample, typing, use_effective_tokens, vars, verbose, vocab_key, vocabs, warnings, words, worker_init_fn, x, z, zip","abc.ABC, abc.abstractmethod, copy.copy, hanlp.common.transform.EmbeddingNamedTransform, hanlp.common.transform.TransformList, hanlp.common.transform.VocabDict, hanlp.common.vocab.Vocab, hanlp.components.parsers.alg.kmeans, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.read_cells, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.dtype_of, hanlp_common.configurable.AutoConfigurable, hanlp_common.constant.HANLP_VERBOSE, hanlp_common.constant.IDX, hanlp_common.util.isdebugging, hanlp_common.util.k_fold, hanlp_common.util.merge_list_of_dict, logging.Logger, math.ceil, mp.Process, mp.Queue, os.path.isfile, os.remove, random.shuffle, tempfile.NamedTemporaryFile, torch.Tensor, torch.arange, torch.argsort, torch.bool, torch.load, torch.long, torch.nn.utils.rnn.pad_sequence, torch.randperm, torch.save, torch.tensor, torch.utils.data.DataLoader, torch.utils.data.Dataset, torch.utils.data.Sampler, torch.utils.data.dataset.IterableDataset, torch.zeros, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union, warnings.warn","__call__, __getitem__, __init__, __iter__, __len__, __repr__, _build_cache, _fire_process, _prefetch_generator, append_transform, batchify, build, close, collate_fn, insert_transform, k_fold, load_data, load_file, pad_data, prune, purge_cache, scale, should_load_file, shuffle, split, subset, tensorize, transform_sample","BucketSampler, CachedDataLoader, DeviceDataLoader, KMeansSampler, KMeansSamplerBuilder, PadSequenceDataLoader, PrefetchDataLoader, SamplerBuilder, SortingSampler, SortingSamplerBuilder, TableDataset, TransformSequentialDataset, Transformable, TransformableDataset","batch, batch_max_tokens, batch_size, bucket, buckets, cache, chars, chunks, collate_fn, cur, data, data_, dataset, device, drop_last, dtype, each, effective_tokens, field, filename, generate_idx, good_ones, ids, idx, indices, inner_is_iterable, j, max_seq_len, max_word_len, mini_batch, n_buckets, num_pruned, num_tokens, num_workers, outputs, pad, prefetch, prev, range_fn, ratios, raw_batch, sample, shuffle, size, size_after, size_before, split_sizes, test_indices, timer, train_indices, transform, vocab_key, words, x, z","A :class:`~torch.utils.data.Dataset` which can be applied with a list of transform functions.

Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler., A bucket sampler which groups samples using KMeans on their lengths.

Args:
    lengths: Lengths of each sample, usually measured by number of tokens.
    batch_max_tokens: Maximum tokens per batch.
    batch_size: Maximum samples per batch.
    shuffle: ``True`` to shuffle batches. Samples in the same batch won't be shuffled since the ordered sequence
            is helpful to speed up RNNs.
    n_buckets: Number of buckets. Clusters in terms of KMeans., A bucketing based sampler which groups samples into buckets then creates batches from each bucket.

Args:
    buckets: A dict of which keys are some statistical numbers of each bucket, and values are the indices of
        samples in each bucket.
    batch_max_tokens: Maximum tokens per batch.
    batch_size: Maximum samples per batch.
    shuffle: ``True`` to shuffle batches and samples in a batch., A dataloader commonly used for NLP tasks. It offers the following convenience.

- Bachify each field of samples into a :class:`~torch.Tensor` if the field name satisfies the following criterion.
    - Name ends with _id, _ids, _count, _offset, _span, mask
    - Name is in `pad` dict.

- Pad each field according to field name, the vocabs and pad dict.
- Move :class:`~torch.Tensor` onto device.

Args:
    dataset: A :class:`~torch.utils.data.Dataset` to be bachified.
    batch_size: Max size of each batch.
    shuffle: ``True`` to shuffle batches.
    sampler: A :class:`~torch.utils.data.Sampler` to sample samples from data.
    batch_sampler: A :class:`~torch.utils.data.Sampler` to sample batches form all batches.
    num_workers: Number of workers for multi-thread loading. Note that multi-thread loading aren't always
        faster.
    collate_fn: A function to perform batchifying. It must be set to ``None`` in order to make use of the
         features this class offers.
    pin_memory: If samples are loaded in the Dataset on CPU and would like to be pushed to
            the GPU, enabling pin_memory can speed up the transfer. It's not useful since most data field are
            not in Tensor type.
    drop_last: Drop the last batch since it could be half-empty.
    timeout: For multi-worker loading, set a timeout to wait for a worker.
    worker_init_fn: Init function for multi-worker.
    multiprocessing_context: Context for multiprocessing.
    pad: A dict holding field names and their padding values.
    vocabs: A dict of vocabs so padding value can be fetched from it.
    device: The device tensors will be moved onto.
    **kwargs: Other arguments will be passed to :meth:`torch.utils.data.Dataset.__init__`, A dataloader wrapper which speeds up bachifying using multi-processing. It works best for dataloaders
of which the bachify takes very long time. But it introduces extra GPU memory consumption since prefetched
batches are stored in a ``Queue`` on GPU.

.. Caution::

    PrefetchDataLoader only works in spawn mode with the following initialization code:

    Examples::

        if __name__ == '__main__':
            import torch

            torch.multiprocessing.set_start_method('spawn')

    And these 2 lines **MUST** be put into ``if __name__ == '__main__':`` block.

Args:
    dataloader: A :class:`~torch.utils.data.DatasetLoader` to be prefetched.
    prefetch: Number of batches to prefetch.
    batchify: A bachify function called on each batch of samples. In which case, the inner dataloader shall
            return samples without really bachify them., A intermediate step between constructor and calling the actual file loading method.

Args:
    data: If data is a file, this method calls :meth:`~hanlp.common.dataset.TransformableDataset.load_file`
        to load it.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler.

Returns: Loaded samples., A sampler which sorts samples according to their lengths. It takes a continuous chunk of sorted samples to
make a batch. The effective batch size is determined by ``batch_size``, ``batch_max_tokens`` and
``use_effective_tokens``.

Args:
    lengths: Lengths of each sample, usually measured by number of tokens.
    batch_max_tokens: Maximum tokens per batch.
    use_effective_tokens: Whether to calculate the effective number of tokens after padding when applying the
        ``batch_max_tokens``.
    batch_size: Maximum samples per batch.
    shuffle: ``True`` to shuffle batches and samples in a batch., An object which can be transformed with a list of functions. It is the final result of an object being passed
through a list of functions, while these functions are kept in a list.

Args:
    transform: A transform function or a list of functions., Append a transform to its list of transforms.

Args:
    transform: A new transform to be appended.

Returns:
    Itself., Apply transforms to a sample.

Args:
    sample: A sample, which is a ``dict`` holding features.
    inplace: ``True`` to apply transforms inplace.

.. Attention::
    If any transform modifies existing features, it will modify again and again when ``inplace=True``.
    For example, if a transform insert a ``BOS`` token to a list inplace, and it is called twice,
    then 2 ``BOS`` will be inserted which might not be an intended result.

Returns:
    Transformed sample., Build a ``Sampler`` given statistics of samples and other arguments.

Args:
    lengths: The lengths of samples.
    shuffle: ``True`` to shuffle batches. Note samples in each mini-batch are not necessarily shuffled.
    gradient_accumulation: Number of mini-batches per update step.
    **kwargs: Other arguments to be passed to the constructor of the sampler., Builds a :class:`~hanlp.common.dataset.KMeansSampler`.

Args:
    batch_max_tokens: Maximum tokens per batch.
    batch_size: Maximum samples per batch.
    n_buckets: Number of buckets. Clusters in terms of KMeans., Builds a :class:`~hanlp.common.dataset.SortingSampler`.

Args:
    batch_max_tokens: Maximum tokens per batch.
    use_effective_tokens: Whether to calculate effective number of tokens when applying the `batch_max_tokens`.
    batch_size: Maximum samples per batch., Close this dataloader and terminates internal processes and queue. It's recommended to call this method to
ensure a program can gracefully shutdown., Create a subset given indices of samples.

Args:
    indices: Indices of samples.

Returns:
    TransformableDataset: The a subset of this dataset., Determines whether data is a filepath.

Args:
    data: Data to check.

Returns: ``True`` to indicate it's a filepath., Get the index-th sample in this dataset.

Args:
    index: Either a integer index of a list of indices.

Returns: Either a sample or or list of samples depending on how many indices are passed in., Insert a transform to a certain position.

Args:
    index: A certain position.
    transform: A new transform.

Returns:
    Itself., Perform k-fold sampling.

Args:
    k (int): Number of folds.
    i (int): The i-th fold.

Returns:
    TransformableDataset: The i-th fold subset of this dataset., Perform the actual padding for a given data.

Args:
    data: Data to be padded.
    pad: Padding value.
    dtype: Data type.
    device: Device to be moved onto.

Returns:
    torch.Tensor: A ``torch.Tensor``., Prune (to discard) samples according to a criterion.

Args:
    criterion: A functions takes a sample as input and output ``True`` if the sample needs to be pruned.
    logger: If any, log statistical messages using it.

Returns:
    int: Size before pruning., Purges all cache. If cache is not enabled, this method enables it.
        , Scale down the ``batch_size`` and ``batch_max_tokens`` to :math:`\frac{1}{\text{gradient_accumulation}}`
of them respectively.

Args:
    gradient_accumulation: Number of mini-batches per update step.

Returns:
    tuple(int,int): batch_size, batch_max_tokens, Shuffle this dataset inplace.
        , Split dataset into subsets.

Args:
    *ratios: The ratios for each subset. They can be any type of numbers which will be normalized. For example,
            ``8, 1, 1`` are equivalent to ``0.8, 0.1, 0.1``.

Returns:
    list[TransformableDataset]: A list of subsets., The actual file loading logic.

Args:
    filepath: The path to a dataset.",",  (,  ...,  A dataloader commonly used for NLP tasks. It offers the following convenience.

        - Bachify each field of samples into a :class:`~torch.Tensor` if the field name satisfies the following criterion.
            - Name ends with _id, _ids, _count, _offset, _span, mask
            - Name is in `pad` dict.

        - Pad each field according to field name, the vocabs and pad dict.
        - Move :class:`~torch.Tensor` onto device.

        Args:
            dataset: A :class:`~torch.utils.data.Dataset` to be bachified.
            batch_size: Max size of each batch.
            shuffle: ``True`` to shuffle batches.
            sampler: A :class:`~torch.utils.data.Sampler` to sample samples from data.
            batch_sampler: A :class:`~torch.utils.data.Sampler` to sample batches form all batches.
            num_workers: Number of workers for multi-thread loading. Note that multi-thread loading aren't always
                faster.
            collate_fn: A function to perform batchifying. It must be set to ``None`` in order to make use of the
                 features this class offers.
            pin_memory: If samples are loaded in the Dataset on CPU and would like to be pushed to
                    the GPU, enabling pin_memory can speed up the transfer. It's not useful since most data field are
                    not in Tensor type.
            drop_last: Drop the last batch since it could be half-empty.
            timeout: For multi-worker loading, set a timeout to wait for a worker.
            worker_init_fn: Init function for multi-worker.
            multiprocessing_context: Context for multiprocessing.
            pad: A dict holding field names and their padding values.
            vocabs: A dict of vocabs so padding value can be fetched from it.
            device: The device tensors will be moved onto.
            **kwargs: Other arguments will be passed to :meth:`torch.utils.data.Dataset.__init__`
        ,  A dataloader wrapper which speeds up bachifying using multi-processing. It works best for dataloaders
        of which the bachify takes very long time. But it introduces extra GPU memory consumption since prefetched
        batches are stored in a ``Queue`` on GPU.

        .. Caution::

            PrefetchDataLoader only works in spawn mode with the following initialization code:

            Examples::

                if __name__ == '__main__':
                    import torch

                    torch.multiprocessing.set_start_method('spawn')

            And these 2 lines **MUST** be put into ``if __name__ == '__main__':`` block.

        Args:
            dataloader: A :class:`~torch.utils.data.DatasetLoader` to be prefetched.
            prefetch: Number of batches to prefetch.
            batchify: A bachify function called on each batch of samples. In which case, the inner dataloader shall
                    return samples without really bachify them.
        ,  Get the index-th sample in this dataset.

        Args:
            index: Either a integer index of a list of indices.

        Returns: Either a sample or or list of samples depending on how many indices are passed in.

        ,  [blink][yellow]...[/yellow][/blink],  instead.,  samples: , )[/yellow] samples out of , ., .1%, A :class:`~torch.utils.data.Dataset` which can be applied with a list of transform functions.

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.
        , A bucket sampler which groups samples using KMeans on their lengths.

        Args:
            lengths: Lengths of each sample, usually measured by number of tokens.
            batch_max_tokens: Maximum tokens per batch.
            batch_size: Maximum samples per batch.
            shuffle: ``True`` to shuffle batches. Samples in the same batch won't be shuffled since the ordered sequence
                    is helpful to speed up RNNs.
            n_buckets: Number of buckets. Clusters in terms of KMeans.
        , A bucketing based sampler which groups samples into buckets then creates batches from each bucket.

        Args:
            buckets: A dict of which keys are some statistical numbers of each bucket, and values are the indices of
                samples in each bucket.
            batch_max_tokens: Maximum tokens per batch.
            batch_size: Maximum samples per batch.
            shuffle: ``True`` to shuffle batches and samples in a batch.
        , A intermediate step between constructor and calling the actual file loading method.

        Args:
            data: If data is a file, this method calls :meth:`~hanlp.common.dataset.TransformableDataset.load_file`
                to load it.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.

        Returns: Loaded samples.

        , A sampler which sorts samples according to their lengths. It takes a continuous chunk of sorted samples to
        make a batch. The effective batch size is determined by ``batch_size``, ``batch_max_tokens`` and
        ``use_effective_tokens``.

        Args:
            lengths: Lengths of each sample, usually measured by number of tokens.
            batch_max_tokens: Maximum tokens per batch.
            use_effective_tokens: Whether to calculate the effective number of tokens after padding when applying the
                ``batch_max_tokens``.
            batch_size: Maximum samples per batch.
            shuffle: ``True`` to shuffle batches and samples in a batch.
        , An object which can be transformed with a list of functions. It is the final result of an object being passed
        through a list of functions, while these functions are kept in a list.

        Args:
            transform: A transform function or a list of functions.
        , Append a transform to its list of transforms.

        Args:
            transform: A new transform to be appended.

        Returns:
            Itself.

        , Apply transforms to a sample.

        Args:
            sample: A sample, which is a ``dict`` holding features.
            inplace: ``True`` to apply transforms inplace.

        .. Attention::
            If any transform modifies existing features, it will modify again and again when ``inplace=True``.
            For example, if a transform insert a ``BOS`` token to a list inplace, and it is called twice,
            then 2 ``BOS`` will be inserted which might not be an intended result.

        Returns:
            Transformed sample.
        , As you did not pass in `headers` to `TableDataset`, the first line is regarded as headers. However, the length for some headers are too long (>32), which might be wrong. To make sure, pass `headers=...` explicitly., Build a ``Sampler`` given statistics of samples and other arguments.

        Args:
            lengths: The lengths of samples.
            shuffle: ``True`` to shuffle batches. Note samples in each mini-batch are not necessarily shuffled.
            gradient_accumulation: Number of mini-batches per update step.
            **kwargs: Other arguments to be passed to the constructor of the sampler.
        , Builds a :class:`~hanlp.common.dataset.KMeansSampler`.

        Args:
            batch_max_tokens: Maximum tokens per batch.
            batch_size: Maximum samples per batch.
            n_buckets: Number of buckets. Clusters in terms of KMeans.
        , Builds a :class:`~hanlp.common.dataset.SortingSampler`.

        Args:
            batch_max_tokens: Maximum tokens per batch.
            use_effective_tokens: Whether to calculate effective number of tokens when applying the `batch_max_tokens`.
            batch_size: Maximum samples per batch.
        , Caching , Close this dataloader and terminates internal processes and queue. It's recommended to call this method to
            ensure a program can gracefully shutdown.
        , Create a subset given indices of samples.

        Args:
            indices: Indices of samples.

        Returns:
            TransformableDataset: The a subset of this dataset.
        , Determines whether data is a filepath.

        Args:
            data: Data to check.

        Returns: ``True`` to indicate it's a filepath.

        , Insert a transform to a certain position.

        Args:
            index: A certain position.
            transform: A new transform.

        Returns:
            Itself.

        , Invalid split , None transform not allowed, Perform k-fold sampling.

        Args:
            k (int): Number of folds.
            i (int): The i-th fold.

        Returns:
            TransformableDataset: The i-th fold subset of this dataset.

        , Perform the actual padding for a given data.

        Args:
            data: Data to be padded.
            pad: Padding value.
            dtype: Data type.
            device: Device to be moved onto.

        Returns:
            torch.Tensor: A ``torch.Tensor``.
        , Prune (to discard) samples according to a criterion.

        Args:
            criterion: A functions takes a sample as input and output ``True`` if the sample needs to be pruned.
            logger: If any, log statistical messages using it.

        Returns:
            int: Size before pruning.
        , Pruned [yellow], Purges all cache. If cache is not enabled, this method enables it.
        , Scale down the ``batch_size`` and ``batch_max_tokens`` to :math:`\frac{1}{\text{gradient_accumulation}}`
        of them respectively.

        Args:
            gradient_accumulation: Number of mini-batches per update step.

        Returns:
            tuple(int,int): batch_size, batch_max_tokens
        , Shuffle this dataset inplace.
        , Split dataset into subsets.

        Args:
            *ratios: The ratios for each subset. They can be any type of numbers which will be normalized. For example,
                    ``8, 1, 1`` are equivalent to ``0.8, 0.1, 0.1``.

        Returns:
            list[TransformableDataset]: A list of subsets.
        , The actual file loading logic.

        Args:
            filepath: The path to a dataset.
        , TransformDataset expects each sample to be a dict but got , _count, _id, _ids, _length, _mask, _offset, _score, _span, auto, batch_size has to be specified when batch_sampler is None, hanlp-cache-, maxsize, rb, wb","0, 1, 10, 2, 32, False, None, True","
batch_size = self.batch_size
batch_max_tokens = self.batch_max_tokens
if gradient_accumulation:
if batch_size:
batch_size //= gradient_accumulation
if batch_max_tokens:
batch_max_tokens //= gradient_accumulation
return batch_size, batch_max_tokens


class SortingSamplerBuilder(SortingSampler, SamplerBuilder):
# noinspection PyMissingConstructor
def __init__(self, batch_size=None, batch_max_tokens=None, use_effective_tokens=False) -> None:
Builds a :class:`~hanlp.common.dataset.SortingSampler`., 
self.use_effective_tokens = use_effective_tokens
self.batch_max_tokens = batch_max_tokens
self.batch_size = batch_size

def build(self, lengths: List[int], shuffle=False, gradient_accumulation=1, **kwargs) -> Sampler:
batch_size, batch_max_tokens = self.scale(gradient_accumulation)
return SortingSampler(lengths, batch_size, batch_max_tokens, shuffle)

def __len__(self) -> int:
return 1


class KMeansSamplerBuilder(KMeansSampler, SamplerBuilder):
# noinspection PyMissingConstructor
def __init__(self, batch_max_tokens, batch_size=None, n_buckets=1):
Builds a :class:`~hanlp.common.dataset.KMeansSampler`.,  A dataloader commonly used for NLP tasks. It offers the following convenience.

- Bachify each field of samples into a :class:`~torch.Tensor` if the field name satisfies the following criterion.
- Name ends with _id, _ids, _count, _offset, _span, mask
- Name is in `pad` dict.

- Pad each field according to field name, the vocabs and pad dict.
- Move :class:`~torch.Tensor` onto device.

Args:
dataset: A :class:`~torch.utils.data.Dataset` to be bachified.
batch_size: Max size of each batch.
shuffle: ``True`` to shuffle batches.
sampler: A :class:`~torch.utils.data.Sampler` to sample samples from data.
batch_sampler: A :class:`~torch.utils.data.Sampler` to sample batches form all batches.
num_workers: Number of workers for multi-thread loading. Note that multi-thread loading aren't always
faster.
collate_fn: A function to perform batchifying. It must be set to ``None`` in order to make use of the
features this class offers.
pin_memory: If samples are loaded in the Dataset on CPU and would like to be pushed to
the GPU, enabling pin_memory can speed up the transfer. It's not useful since most data field are
not in Tensor type.
drop_last: Drop the last batch since it could be half-empty.
timeout: For multi-worker loading, set a timeout to wait for a worker.
worker_init_fn: Init function for multi-worker.
multiprocessing_context: Context for multiprocessing.
pad: A dict holding field names and their padding values.
vocabs: A dict of vocabs so padding value can be fetched from it.
device: The device tensors will be moved onto.
**kwargs: Other arguments will be passed to :meth:`torch.utils.data.Dataset.__init__`
,  A dataloader wrapper which speeds up bachifying using multi-processing. It works best for dataloaders
of which the bachify takes very long time. But it introduces extra GPU memory consumption since prefetched
batches are stored in a ``Queue`` on GPU.

.. Caution::

PrefetchDataLoader only works in spawn mode with the following initialization code:

Examples::

if __name__ == '__main__':
import torch

torch.multiprocessing.set_start_method('spawn')

And these 2 lines **MUST** be put into ``if __name__ == '__main__':`` block.

Args:
dataloader: A :class:`~torch.utils.data.DatasetLoader` to be prefetched.
prefetch: Number of batches to prefetch.
batchify: A bachify function called on each batch of samples. In which case, the inner dataloader shall
return samples without really bachify them.
,  Get the index-th sample in this dataset.

Args:
index: Either a integer index of a list of indices.

Returns: Either a sample or or list of samples depending on how many indices are passed in.

, #     assert len(index) == 1, #     data = self.load_list(data), #     index = index[0], #     return data, # -*- coding:utf-8 -*-, # Author: hankcs, # DON'T use `torch.chunk` which may return wrong number of chunks, # Date: 2020-05-09 20:27, # assert any([batch_size, batch_max_tokens]), 'At least one of batch_size and batch_max_tokens is required', # assert data_, f'No samples loaded from {data}', # def load_list(self, data: list) -> List[Dict[str, Any]]:, # elif isinstance(data, list):, # guess some common fields to pad, # if batch_max_tokens:, # if isinstance(index, (list, tuple)):, # if shuffle, shuffle both the buckets and samples in each bucket, # no need to pad, # noinspection PyArgumentList, # noinspection PyMethodMayBeStatic, # noinspection PyMissingConstructor, # noinspection PyTypeChecker, # print(len(max(self.batch_indices, key=len))), # range [1, len(bucket)], # self.batch_max_tokens = batch_max_tokens, # the number of chunks in each bucket, which is clipped by, # this sequence is longer than  batch_max_tokens, A :class:`~torch.utils.data.Dataset` which can be applied with a list of transform functions.

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.
, A bucket sampler which groups samples using KMeans on their lengths.

Args:
lengths: Lengths of each sample, usually measured by number of tokens.
batch_max_tokens: Maximum tokens per batch.
batch_size: Maximum samples per batch.
shuffle: ``True`` to shuffle batches. Samples in the same batch won't be shuffled since the ordered sequence
is helpful to speed up RNNs.
n_buckets: Number of buckets. Clusters in terms of KMeans.
, A bucketing based sampler which groups samples into buckets then creates batches from each bucket.

Args:
buckets: A dict of which keys are some statistical numbers of each bucket, and values are the indices of
samples in each bucket.
batch_max_tokens: Maximum tokens per batch.
batch_size: Maximum samples per batch.
shuffle: ``True`` to shuffle batches and samples in a batch.
, A intermediate step between constructor and calling the actual file loading method.

Args:
data: If data is a file, this method calls :meth:`~hanlp.common.dataset.TransformableDataset.load_file`
to load it.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.

Returns: Loaded samples.

, A sampler which sorts samples according to their lengths. It takes a continuous chunk of sorted samples to
make a batch. The effective batch size is determined by ``batch_size``, ``batch_max_tokens`` and
``use_effective_tokens``.

Args:
lengths: Lengths of each sample, usually measured by number of tokens.
batch_max_tokens: Maximum tokens per batch.
use_effective_tokens: Whether to calculate the effective number of tokens after padding when applying the
``batch_max_tokens``.
batch_size: Maximum samples per batch.
shuffle: ``True`` to shuffle batches and samples in a batch.
, An object which can be transformed with a list of functions. It is the final result of an object being passed
through a list of functions, while these functions are kept in a list.

Args:
transform: A transform function or a list of functions.
, Append a transform to its list of transforms.

Args:
transform: A new transform to be appended.

Returns:
Itself.

, Apply transforms to a sample.

Args:
sample: A sample, which is a ``dict`` holding features.
inplace: ``True`` to apply transforms inplace.

.. Attention::
If any transform modifies existing features, it will modify again and again when ``inplace=True``.
For example, if a transform insert a ``BOS`` token to a list inplace, and it is called twice,
then 2 ``BOS`` will be inserted which might not be an intended result.

Returns:
Transformed sample.
, Build a ``Sampler`` given statistics of samples and other arguments.

Args:
lengths: The lengths of samples.
shuffle: ``True`` to shuffle batches. Note samples in each mini-batch are not necessarily shuffled.
gradient_accumulation: Number of mini-batches per update step.
**kwargs: Other arguments to be passed to the constructor of the sampler.
, Close this dataloader and terminates internal processes and queue. It's recommended to call this method to
ensure a program can gracefully shutdown.
, Create a subset given indices of samples.

Args:
indices: Indices of samples.

Returns:
TransformableDataset: The a subset of this dataset.
, Determines whether data is a filepath.

Args:
data: Data to check.

Returns: ``True`` to indicate it's a filepath.

, Insert a transform to a certain position.

Args:
index: A certain position.
transform: A new transform.

Returns:
Itself.

, Perform k-fold sampling.

Args:
k (int): Number of folds.
i (int): The i-th fold.

Returns:
TransformableDataset: The i-th fold subset of this dataset.

, Perform the actual padding for a given data.

Args:
data: Data to be padded.
pad: Padding value.
dtype: Data type.
device: Device to be moved onto.

Returns:
torch.Tensor: A ``torch.Tensor``.
, Prune (to discard) samples according to a criterion.

Args:
criterion: A functions takes a sample as input and output ``True`` if the sample needs to be pruned.
logger: If any, log statistical messages using it.

Returns:
int: Size before pruning.
, Purges all cache. If cache is not enabled, this method enables it.
, Shuffle this dataset inplace.
, Split dataset into subsets.

Args:
*ratios: The ratios for each subset. They can be any type of numbers which will be normalized. For example,
``8, 1, 1`` are equivalent to ``0.8, 0.1, 0.1``.

Returns:
list[TransformableDataset]: A list of subsets.
, The actual file loading logic.

Args:
filepath: The path to a dataset.
",
https://github.com/hankcs/HanLP,keras_component.py,0,0.0,200,61.35,97,29.75,10,3.07,19,5.83,0,0.0,39,1,72,4,,"KerasComponent, KeyboardInterrupt, RuntimeError, ValueError, X, X_to_inputs, Y, Y_to_outputs, __init__, __version__, _capture_config, _keras_mask, abc, anneal_factor, append, batch, batch_size, best, best_epoch_ago, bool, build, build_callbacks, build_config, build_loss, build_metrics, build_model, build_optimizer, build_train_dataset, build_transform, build_valid_dataset, build_vocab, built, by_type, checkpoint, cleanup, cls, cmd, compile, compile_model, config, copy_from, custom_objects, data_is_list, debug, delta_seconds, delta_time, dev_data, dev_steps, dry_run, early_stopping_patience, enumerate, epoch, epochs, eval_outputs, evaluate, evaluate_dataset, evaluate_output, evaluate_output_to_file, exclude, export_dir, export_model_for_serving, ext, extra_report, f1, file_prefix, file_to_dataset, filename, finetune, fit, flat, from_meta, get, get_config, grpc_port, hanlp, hanlp_common, hasattr, history, idx, index, info, input_path, input_shape, input_to_inputs, input_truth_output_to_str, inputs, inputs_to_dataset, isinstance, items, keras, key, kwargs, learning_rate, len, list, load, load_config, load_json, load_meta, load_transform, load_vocabs, load_weights, locals, lock_vocabs, log_util, logger, logging, loss, lr_decay_per_epoch, makedirs, math, meta, metapath, metrics, model, model_path, monitor, monitor_history, name, num_batches, num_examples, num_samples, num_samples_in, numpy, on_train_begin, open, optimizer, os, out, output, output_shapes, outputs, overall, overwrite, path_join, pop, predict, predict_batch, predict_on_batch, print, property, repr, reset_states, rest_api_port, result, results, run_eagerly, sample_data, sample_inputs, samples, samples_in_batch, save, save_config, save_dir, save_json, save_meta, save_vocabs, save_weights, self, serve, setattr, shape, show_hint, speed, state, staticmethod, stop, summarize_vocabs, super, sys, tensorboard_callback, tensorflow, timer, to_dict, to_json, train_examples, train_loop, train_steps, train_steps_per_epoch, trained_epoch, transform, trn_data, tst_data, tuple, typing, update, utils, value, vars, verbose, version, vocabs, warm_up, warning, weights, write, x_shape, y_gold, y_pred, zip","abc.ABC, abc.abstractmethod, hanlp.callbacks.fine_csv_logger.FineCSVLogger, hanlp.common.component.Component, hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.metrics.chunking.iobes_tf.IOBES_F1_TF, hanlp.optimizers.adamw.AdamWeightDecay, hanlp.utils.io_util, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.tempdir_human, hanlp.utils.log_util.init_logger, hanlp.utils.log_util.logger, hanlp.utils.string_util.format_scores, hanlp.utils.tf_util.NumpyEncoder, hanlp.utils.tf_util.format_metrics, hanlp.utils.tf_util.get_callback_by_class, hanlp.utils.tf_util.size_of_dataset, hanlp.utils.tf_util.summary_of_model, hanlp.utils.time_util.Timer, hanlp.utils.time_util.now_datetime, hanlp_common.io.load_json, hanlp_common.io.save_json, hanlp_common.reflection.classpath_of, hanlp_common.reflection.str_to_type, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_dict, keras.saving.object_registration.CustomObjectScope, logging.INFO, logging.Logger, logging.WARN, math.ceil, np.Inf, os.path.basename, os.path.isdir, os.path.isfile, os.path.join, os.path.splitext, os.system, sys.version_info, sys.version_info.major, sys.version_info.minor, tf.Tensor, tf.__version__, tf.keras.Model, tf.keras.callbacks.Callback, tf.keras.callbacks.EarlyStopping, tf.keras.callbacks.History, tf.keras.callbacks.LearningRateScheduler, tf.keras.callbacks.ModelCheckpoint, tf.keras.callbacks.ReduceLROnPlateau, tf.keras.callbacks.TensorBoard, tf.keras.losses, tf.keras.losses.Loss, tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.metrics.Metric, tf.keras.metrics.SparseCategoricalAccuracy, tf.keras.optimizers, tf.keras.optimizers.Optimizer, tf.keras.utils.deserialize_keras_object, tf.keras.utils.serialize_keras_object, tf.saved_model.save, tf.shape, typing.Any, typing.Dict, typing.List, typing.Optional","__init__, _capture_config, build, build_callbacks, build_loss, build_metrics, build_model, build_optimizer, build_train_dataset, build_transform, build_valid_dataset, build_vocab, compile_model, evaluate, evaluate_dataset, evaluate_output, evaluate_output_to_file, export_model_for_serving, fit, from_meta, input_shape, load, load_config, load_meta, load_transform, load_vocabs, load_weights, num_samples_in, on_train_begin, predict, predict_batch, sample_data, save, save_config, save_meta, save_vocabs, save_weights, serve, train_loop",KerasComponent,"X, Y, anneal_factor, batch, batch_size, best_epoch_ago, by_type, callbacks, checkpoint, cls, cmd, config, csv_logger, custom_objects, data_is_list, dataset, delta_time, dev_data, dev_steps, early_stopping_patience, eval_outputs, export_dir, ext, extra_report, f1, file_prefix, finetune, flat, history, idx, input_path, inputs, key, learning_rate, logger, loss, lr_decay_per_epoch, metapath, metric, metrics, model_path, monitor, monitor_history, name, num_batches, num_examples, num_samples, optimizer, out, output, outputs, overall, results, sample_inputs, samples, samples_in_batch, save_dir, shape, speed, tensorboard_callback, timer, train_examples, train_steps_per_epoch, trained_epoch, trn_data, tst_data, value, vocab, vocabs, x_shape, y_gold, y_pred","Callback before the training starts, Parameters
----------
meta
kwargs

Returns
-------
KerasComponent, Save arguments to config

Parameters
----------
config
    `locals()`
exclude, Try to load transform only. This method might fail due to the fact it avoids building the model.
If it do fail, then you have to use `load` which might be too heavy but that's the best we can do.
:param save_dir: The path to load.",", 

        Parameters
        ----------
        meta
        kwargs

        Returns
        -------
        KerasComponent

        , 
        Callback before the training starts
        , 
        Save arguments to config

        Parameters
        ----------
        config
            `locals()`
        exclude
        , 
        Try to load transform only. This method might fail due to the fact it avoids building the model.
        If it do fail, then you have to use `load` which might be too heavy but that's the best we can do.
        :param save_dir: The path to load.
        , {}/{} {},  
,  --model_base_path=,  --port=,  --rest_api_port=,  --rest_api_port=8888,  ...,  = ,  >serve.log 2>&1 &,  doesn't contain load_path field,  epochs ago,  exists, skip since overwrite = ,  for checkpoint,  for finetuning,  has not been tested on Python ,  saved ,  | , ., . Please downgrade to Python<=3.9 in case any compatibility issues arise., .4f, .predict, /1, Aborted with model saved, Aborted with model saved with best , AdamWeightDecay, Building..., Created LearningRateScheduler with lr_decay_per_epoch=, Evaluation results for {} - loss: {:.4f} - {} - speed: {:.2f} sample/sec{}, Exporting to , Hyperparameter:
, Learning rate decay not supported for optimizer={}, Loaded pretrained weights from , Model built:
, Monitor , Must pass save_dir in order to output, Please call fit or load before predict, Restored the best model saved with best , Running ...
, Saving output to {}, Successfully exported model to , The inputs of exported model is shown below., Trained {} epochs in {}, each epoch takes {}, Training with TensorFlow , When not specifying save_dir, load_path has to present, You can serve it through 
tensorflow_model_server --model_name=, You have to fit or load a model before exporting it, You must call self.model.built() in build_model() in order to load it, __class__, _keras_mask, accuracy, adamweightdecay, anneal_factor, anneal_patience, class_path, config, config.json, create_time, decay, dev_batch_size, dev_data, early_stopping_patience, epoch, evaluate, get_config, gold, hanlp_version, history.json, kwargs, learning_rate, load_path, logger, logs, loss, lr_decay_per_epoch, max, meta.json, metrics, model.h5, nohup tensorflow_model_server --model_name=, output ({}) must be of type bool or str, save_dir, saved_model_cli show --all --dir , self, train, train.log, training, trn_data, utf-8, val_, verbose, vocabs.json, w","0, 1, 10, 128, 2, 3, 8500, False, None, True","

Parameters
----------
meta
kwargs

Returns
-------
KerasComponent

, 
Callback before the training starts
, 
Save arguments to config

Parameters
----------
config
`locals()`
exclude
, 
Try to load transform only. This method might fail due to the fact it avoids building the model.
If it do fail, then you have to use `load` which might be too heavy but that's the best we can do.
:param save_dir: The path to load.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 14:45, # allow for different, # assert num_examples, 'You forgot to return the number of training examples in your build_vocab', # batch + X.shape, # free memory, # if data is a generator, it's usually one-time, not able to transform into a list, # need to know #batches, otherwise progbar crashes, # noinspection PyTypeChecker, # out.write('x\ty_true\ty_pred\n'), # restore best model, # share config with transform for convenience, so we don't need to pass args around, # type:tf.keras.callbacks.History, # verbose=1,",
https://github.com/hankcs/HanLP,structure.py,0,0.0,22,52.38,10,23.81,3,7.14,7,16.67,0,0.0,3,2,2,4,,"ConfigTracker, History, __init__, config, exclude, gradient_accumulation, hanlp_common, hasattr, items, k, len, locals_, num_batches, num_mini_batches, num_training_steps, object, range, self, step, typing, update, v","hanlp_common.configurable.Configurable, hanlp_common.reflection.classpath_of, hanlp_common.structure.SerializableDict, typing.Dict","__init__, num_training_steps, step","ConfigTracker, History","k, v","A history of training context. It records how many steps have passed and provides methods to decide whether
an update should be performed, and to caculate number of training steps given dataloader size and
``gradient_accumulation``., Caculate number of training steps.

Args:
    num_batches: Size of dataloader.
    gradient_accumulation: Number of batches per update.

Returns:, This base class helps sub-classes to capture their arguments passed to ``__init__``, and also their types so
that they can be deserialized from a config in dict form.

Args:
    locals_: Obtained by :meth:`locals`.
    exclude: Arguments to be excluded.

Examples:
    >>> class MyClass(ConfigTracker):
    >>>     def __init__(self, i_need_this='yes') -> None:
    >>>         super().__init__(locals())
    >>> obj = MyClass()
    >>> print(obj.config)
    {'i_need_this': 'yes', 'classpath': 'test_config_tracker.MyClass'}, Whether the training procedure should perform an update.

Args:
    gradient_accumulation: Number of batches per update.

Returns:
    bool: ``True`` to update."," A history of training context. It records how many steps have passed and provides methods to decide whether
        an update should be performed, and to caculate number of training steps given dataloader size and
        ``gradient_accumulation``.
        ,  Caculate number of training steps.

        Args:
            num_batches: Size of dataloader.
            gradient_accumulation: Number of batches per update.

        Returns:

        ,  Whether the training procedure should perform an update.

        Args:
            gradient_accumulation: Number of batches per update.

        Returns:
            bool: ``True`` to update.
        , This base class helps sub-classes to capture their arguments passed to ``__init__``, and also their types so
        that they can be deserialized from a config in dict form.

        Args:
            locals_: Obtained by :meth:`locals`.
            exclude: Arguments to be excluded.

        Examples:
            >>> class MyClass(ConfigTracker):
            >>>     def __init__(self, i_need_this='yes') -> None:
            >>>         super().__init__(locals())
            >>> obj = MyClass()
            >>> print(obj.config)
            {'i_need_this': 'yes', 'classpath': 'test_config_tracker.MyClass'}

        , __class__, classpath, config, kwargs, locals_, self","0, 1, None"," A history of training context. It records how many steps have passed and provides methods to decide whether
an update should be performed, and to caculate number of training steps given dataloader size and
``gradient_accumulation``.
,  Caculate number of training steps.

Args:
num_batches: Size of dataloader.
gradient_accumulation: Number of batches per update.

Returns:

,  Whether the training procedure should perform an update.

Args:
gradient_accumulation: Number of batches per update.

Returns:
bool: ``True`` to update.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 14:58, This base class helps sub-classes to capture their arguments passed to ``__init__``, and also their types so
that they can be deserialized from a config in dict form.

Args:
locals_: Obtained by :meth:`locals`.
exclude: Arguments to be excluded.

Examples:
>>> class MyClass(ConfigTracker):
>>>     def __init__(self, i_need_this='yes') -> None:
>>>         super().__init__(locals())
>>> obj = MyClass()
>>> print(obj.config)
{'i_need_this': 'yes', 'classpath': 'test_config_tracker.MyClass'}

",
https://github.com/hankcs/HanLP,torch_component.py,0,0.0,142,49.65,89,31.12,9,3.15,46,16.08,0,0.0,35,1,40,29,,"DataParallel, Module, NotImplementedError, RuntimeError, StopIteration, TorchComponent, TypeError, ValueError, __call__, __init__, _capture_config, _create_dummy_placeholder_on, _description, _device_placeholder, _dummy_placeholder, _k, _logger_name, _savable_config, _v, abc, add_idx, args, batch_size, bool, build_criterion, build_dataloader, build_logger, build_metric, build_model, build_optimizer, build_vocabs, cache, config, convert, criterion, data, dataset, dev_data, device, device_ids, devices, elapsed, enumerate, epochs, eval, eval_trn, evaluate, evaluate_dataloader, exclude, execute_training_loop, filename, finetune, first_device, fit, fit_dataloader, float, from_config, generate_prediction_filename, get, hanlp, hanlp_common, hasattr, idx, index, info, int, isinstance, items, iter, k, kwargs, len, list, load, load_config, load_json, load_state_dict, load_vocabs, load_weights, locals, locals_, logging, metric, model, model_, module, named_modules, named_parameters, next, num_samples, numel, on_config_ready, on_device, optimizer, os, output, outputs, parallelize, parameters, pop, predict, property, ratio_width, re, regex, repr, requires_grad, sample, samples, save, save_config, save_dir, save_json, save_vocabs, save_weights, seed, self, sorted, split, start, startswith, state_dict, staticmethod, sum, super, time, to, to_json, torch, trainable_names, trainable_only, trn, trn_data, trn_size, tst_data, tuple, typing, unlock, update, values, verbose, vocabs","abc.ABC, abc.abstractmethod, hanlp.__version__, hanlp.common.component.Component, hanlp.common.dataset.TransformableDataset, hanlp.common.transform.VocabDict, hanlp.utils.io_util.basename_no_ext, hanlp.utils.io_util.get_resource, hanlp.utils.log_util.flash, hanlp.utils.log_util.init_logger, hanlp.utils.torch_util.cuda_devices, hanlp.utils.torch_util.set_seed, hanlp_common.configurable.Configurable, hanlp_common.constant.HANLP_VERBOSE, hanlp_common.constant.IDX, hanlp_common.reflection.classpath_of, hanlp_common.structure.SerializableDict, hanlp_common.util.isdebugging, hanlp_common.util.merge_dict, logging.INFO, logging.Logger, os.path.basename, os.path.join, os.path.splitext, re.match, time.time, torch.device, torch.load, torch.nn, torch.nn.Module, torch.no_grad, torch.save, torch.utils.data.DataLoader, torch.utils.data.Dataset, torch.zeros, typing.Callable, typing.Dict, typing.List, typing.Optional, typing.Union","__call__, __init__, _capture_config, _create_dummy_placeholder_on, _savable_config, add_idx, build_criterion, build_dataloader, build_logger, build_metric, build_model, build_optimizer, build_vocabs, convert, device, devices, evaluate, evaluate_dataloader, execute_training_loop, fit, fit_dataloader, generate_prediction_filename, load, load_config, load_vocabs, load_weights, model_, on_config_ready, parallelize, predict, save, save_config, save_vocabs, save_weights, to",TorchComponent,"_description, _dummy_placeholder, _k, _logger_name, _v, batch_size, config, criterion, data, dataset, dev, device, devices, elapsed, filename, first_device, idx, k, locals_, logger, metric, model, module, name, num_samples, on_device, optimizer, output, outputs, ratio_width, regex, sample, save_dir, start, state_dict, trainable_names, trn, trn_size, tst_data, v","Build a :class:`logging.Logger`.

Args:
    name: The name of this logger.
    save_dir: The directory this logger should save logs into.

Returns:
    logging.Logger: A logger., Build dataloader for training, dev and test sets. It's suggested to build vocabs in this method if they are
not built yet.

Args:
    data: Data representing samples, which can be a path or a list of samples.
    batch_size: Number of samples per batch.
    shuffle: Whether to shuffle this dataloader.
    device: Device tensors should be loaded onto.
    logger: Logger for reporting some message if dataloader takes a long time or if vocabs has to be built.
    **kwargs: Arguments from ``**self.config``., Build model.

Args:
    training: ``True`` if called during training.
    **kwargs: ``**self.config``., Called when config is ready, either during ``fit`` or ``load``. Subclass can perform extra initialization
tasks in this callback.

Args:
    **kwargs: Not used., Evaluate on a dataloader.

Args:
    data: Dataloader which can build from any data source.
    criterion: Loss function.
    metric: Metric(s).
    output: Whether to save outputs into some file.
    **kwargs: Not used., Evaluate test set.

Args:
    tst_data: Test set, which is usually a file path.
    save_dir: The directory to save evaluation scores or predictions.
    logger: Logger for reporting progress.
    batch_size: Batch size for test dataloader.
    output: Whether to save outputs into some file.
    **kwargs: Not used.

Returns:
    (metric, outputs) where outputs are the return values of ``evaluate_dataloader``., Fit onto a dataloader.

Args:
    trn: Training set.
    criterion: Loss function.
    optimizer: Optimizer.
    metric: Metric(s).
    logger: Logger for reporting progress.
    **kwargs: Other hyper-parameters passed from sub-class., Fit to data, triggers the training procedure. For training set and dev set, they shall be local or remote
files.

Args:
    trn_data: Training set.
    dev_data: Development set.
    save_dir: The directory to save trained component.
    batch_size: The number of samples in a batch.
    epochs: Number of epochs.
    devices: Devices this component will live on.
    logger: Any :class:`logging.Logger` instance.
    seed: Random seed to reproduce this training.
    finetune: ``True`` to load from ``save_dir`` instead of creating a randomly initialized component. ``str``
        to specify a different ``save_dir`` to load from.
    eval_trn: Evaluate training set after each update. This can slow down the training but provides a quick
        diagnostic for debugging.
    _device_placeholder: ``True`` to create a placeholder tensor which triggers PyTorch to occupy devices so
        other components won't take these devices as first choices.
    **kwargs: Hyperparameters used by sub-classes.

Returns:
    Any results sub-classes would like to return. Usually the best metrics on training set., Implement this method to build an optimizer.

Args:
    **kwargs: The subclass decides the method signature., Implement this method to build criterion (loss function).

Args:
    **kwargs: The subclass decides the method signature., Implement this to build metric(s).

Args:
    **kwargs: The subclass decides the method signature., Implement this to run training loop.

Args:
    trn: Training set.
    dev: Development set.
    epochs: Number of epochs.
    criterion: Loss function.
    optimizer: Optimizer(s).
    metric: Metric(s)
    save_dir: The directory to save this component.
    logger: Logger for reporting progress.
    devices: Devices this component and dataloader will live on.
    ratio_width: The width of dataset size measured in number of characters. Used for logger to align messages.
    **kwargs: Other hyper-parameters passed from sub-class., Load config from a directory.

Args:
    save_dir: The directory to load config.
    filename: A file name for config.
    **kwargs: K-V pairs to override config., Load from a local/remote component.

Args:
    save_dir: An identifier which can be a local path or a remote URL or a pre-defined string.
    devices: The devices this component will be moved onto.
    verbose: ``True`` to log loading progress.
    **kwargs: To override some configs., Load vocabularies from a directory.

Args:
    save_dir: The directory to load vocabularies.
    filename:  The name for vocabularies., Load weights from a directory.

Args:
    save_dir: The directory to load weights from.
    filename: A file name for weights.
    **kwargs: Not used., Move this component to devices.

Args:
    devices: Target devices.
    logger: Logger for printing progress report, as copying a model from CPU to GPU can takes several seconds.
    verbose: ``True`` to print progress when logger is None., Override this method to build vocabs.

Args:
    trn: Training set.
    logger: Logger for reporting progress., Predict on data fed by user. This method calls :meth:`~hanlp.common.torch_component.predict` but decorates
it with ``torch.no_grad``.

Args:
    *args: Sentences or tokens.
    **kwargs: Used in sub-classes., Predict on data fed by user. Users shall avoid directly call this method since it is not guarded with
``torch.no_grad`` and will introduces unnecessary gradient computation. Use ``__call__`` instead.

Args:
    *args: Sentences or tokens.
    **kwargs: Used in sub-classes., Save arguments to config

Args:
  locals_: Dict: 
  exclude:  (Default value = ('trn_data')
  'dev_data': 
  'save_dir': 
  'kwargs': 
  'self': 
  'logger': 
  'verbose': 
  'dev_batch_size': 
  '__class__': 
  'devices'): 

Returns:, Save config into a directory.

Args:
    save_dir: The directory to save config.
    filename: A file name for config., Save model weights to a directory.

Args:
    save_dir: The directory to save weights into.
    filename: A file name for weights.
    trainable_only: ``True`` to only save trainable weights. Useful when the model contains lots of static
        embeddings.
    **kwargs: Not used for now., Save this component to a directory.

Args:
    save_dir: The directory to save this component.
    **kwargs: Not used., Save vocabularies to a directory.

Args:
    save_dir: The directory to save vocabularies.
    filename:  The name for vocabularies., The actual model when it's wrapped by a `DataParallel`

Returns: The ""real"" model, The base class for all components using PyTorch as backend. It provides common workflows of building vocabs,
datasets, dataloaders and models. These workflows are more of a conventional guideline than en-forced
protocols, which means subclass has the freedom to override or completely skip some steps.

Args:
    **kwargs: Addtional arguments to be stored in the ``config`` property., The devices this component lives on.
        , The first device this component lives on.
        ",", 
, 
        The actual model when it's wrapped by a `DataParallel`

        Returns: The ""real"" model

        ,  [blink][yellow]...[/yellow][/blink],  batches/second,  samples in trn/dev set.,  samples/second,  trainable/total parameters., %(message)s, *, .0f, .pred, /, Build a :class:`logging.Logger`.

        Args:
            name: The name of this logger.
            save_dir: The directory this logger should save logs into.

        Returns:
            logging.Logger: A logger.
        , Build dataloader for training, dev and test sets. It's suggested to build vocabs in this method if they are
        not built yet.

        Args:
            data: Data representing samples, which can be a path or a list of samples.
            batch_size: Number of samples per batch.
            shuffle: Whether to shuffle this dataloader.
            device: Device tensors should be loaded onto.
            logger: Logger for reporting some message if dataloader takes a long time or if vocabs has to be built.
            **kwargs: Arguments from ``**self.config``.
        , Build model.

        Args:
            training: ``True`` if called during training.
            **kwargs: ``**self.config``.
        , Building model [blink][yellow]...[/yellow][/blink], Call fit or load before evaluate., Called when config is ready, either during ``fit`` or ``load``. Subclass can perform extra initialization
        tasks in this callback.

        Args:
            **kwargs: Not used.
        , Evaluate on a dataloader.

        Args:
            data: Dataloader which can build from any data source.
            criterion: Loss function.
            metric: Metric(s).
            output: Whether to save outputs into some file.
            **kwargs: Not used.
        , Evaluate test set.

        Args:
            tst_data: Test set, which is usually a file path.
            save_dir: The directory to save evaluation scores or predictions.
            logger: Logger for reporting progress.
            batch_size: Batch size for test dataloader.
            output: Whether to save outputs into some file.
            **kwargs: Not used.

        Returns:
            (metric, outputs) where outputs are the return values of ``evaluate_dataloader``.
        , Finetune model loaded with , Fit onto a dataloader.

        Args:
            trn: Training set.
            criterion: Loss function.
            optimizer: Optimizer.
            metric: Metric(s).
            logger: Logger for reporting progress.
            **kwargs: Other hyper-parameters passed from sub-class.
        , Fit to data, triggers the training procedure. For training set and dev set, they shall be local or remote
        files.

        Args:
            trn_data: Training set.
            dev_data: Development set.
            save_dir: The directory to save trained component.
            batch_size: The number of samples in a batch.
            epochs: Number of epochs.
            devices: Devices this component will live on.
            logger: Any :class:`logging.Logger` instance.
            seed: Random seed to reproduce this training.
            finetune: ``True`` to load from ``save_dir`` instead of creating a randomly initialized component. ``str``
                to specify a different ``save_dir`` to load from.
            eval_trn: Evaluate training set after each update. This can slow down the training but provides a quick
                diagnostic for debugging.
            _device_placeholder: ``True`` to create a placeholder tensor which triggers PyTorch to occupy devices so
                other components won't take these devices as first choices.
            **kwargs: Hyperparameters used by sub-classes.

        Returns:
            Any results sub-classes would like to return. Usually the best metrics on training set.

        , Implement this method to build an optimizer.

        Args:
            **kwargs: The subclass decides the method signature.
        , Implement this method to build criterion (loss function).

        Args:
            **kwargs: The subclass decides the method signature.
        , Implement this to build metric(s).

        Args:
            **kwargs: The subclass decides the method signature.
        , Implement this to run training loop.

        Args:
            trn: Training set.
            dev: Development set.
            epochs: Number of epochs.
            criterion: Loss function.
            optimizer: Optimizer(s).
            metric: Metric(s)
            save_dir: The directory to save this component.
            logger: Logger for reporting progress.
            devices: Devices this component and dataloader will live on.
            ratio_width: The width of dataset size measured in number of characters. Used for logger to align messages.
            **kwargs: Other hyper-parameters passed from sub-class.
        , Load config from a directory.

        Args:
            save_dir: The directory to load config.
            filename: A file name for config.
            **kwargs: K-V pairs to override config.
        , Load from a local/remote component.

        Args:
            save_dir: An identifier which can be a local path or a remote URL or a pre-defined string.
            devices: The devices this component will be moved onto.
            verbose: ``True`` to log loading progress.
            **kwargs: To override some configs.
        , Load vocabularies from a directory.

        Args:
            save_dir: The directory to load vocabularies.
            filename:  The name for vocabularies.
        , Load weights from a directory.

        Args:
            save_dir: The directory to load weights from.
            filename: A file name for weights.
            **kwargs: Not used.
        , Model built with , Move this component to devices.

        Args:
            devices: Target devices.
            logger: Logger for printing progress report, as copying a model from CPU to GPU can takes several seconds.
            verbose: ``True`` to print progress when logger is None.
        , Moving model to , Moving model to GPUs , Moving module [yellow], Override this method to build vocabs.

        Args:
            trn: Training set.
            logger: Logger for reporting progress.
        , Predict on data fed by user. This method calls :meth:`~hanlp.common.torch_component.predict` but decorates
        it with ``torch.no_grad``.

        Args:
            *args: Sentences or tokens.
            **kwargs: Used in sub-classes.
        , Predict on data fed by user. Users shall avoid directly call this method since it is not guarded with
        ``torch.no_grad`` and will introduces unnecessary gradient computation. Use ``__call__`` instead.

        Args:
            *args: Sentences or tokens.
            **kwargs: Used in sub-classes.
        , Save arguments to config

        Args:
          locals_: Dict: 
          exclude:  (Default value = ('trn_data')
          'dev_data': 
          'save_dir': 
          'kwargs': 
          'self': 
          'logger': 
          'verbose': 
          'dev_batch_size': 
          '__class__': 
          'devices'): 

        Returns:

        
        , Save config into a directory.

        Args:
            save_dir: The directory to save config.
            filename: A file name for config.
        , Save model weights to a directory.

        Args:
            save_dir: The directory to save weights into.
            filename: A file name for weights.
            trainable_only: ``True`` to only save trainable weights. Useful when the model contains lots of static
                embeddings.
            **kwargs: Not used for now.
        , Save this component to a directory.

        Args:
            save_dir: The directory to save this component.
            **kwargs: Not used.
        , Save vocabularies to a directory.

        Args:
            save_dir: The directory to save vocabularies.
            filename:  The name for vocabularies.
        , The base class for all components using PyTorch as backend. It provides common workflows of building vocabs,
        datasets, dataloaders and models. These workflows are more of a conventional guideline than en-forced
        protocols, which means subclass has the freedom to override or completely skip some steps.

        Args:
            **kwargs: Addtional arguments to be stored in the ``config`` property.
        , The devices this component lives on.
        , The first device this component lives on.
        , Unrecognized devices , Using GPUs: [on_blue][cyan][bold], Using [red]CPU[/red], [/bold][/cyan][/on_blue], [/bold][/magenta][/on_yellow]: [red], [/red]
, [/yellow] to [on_yellow][magenta][bold], [yellow]Building model [blink]...[/blink][/yellow], [yellow]Querying CUDA devices [blink]...[/blink][/yellow], _, __class__, __len__, batch_size, build_model is not properly implemented., classpath, config, config.json, cpu, cpu:0, dataset, dev_batch_size, dev_data, device_ids, devices, eval_trn, gradient_accumulation, hanlp_version, kwargs, logger, model.pt, save_dir, self, speed: , test.txt, train, training, trn_data, tst_data has be a str in order to infer the output name, verbose, vocabs, vocabs.json","0, 1, 10, 16, 233, 32, False, None, True","
The actual model when it's wrapped by a `DataParallel`

Returns: The ""real"" model

, #     devices = torch.device('mps:0'), # 'create_time': now_datetime(),, # -*- coding:utf-8 -*-, # Author: hankcs, # Common initialization steps, # Date: 2020-05-08 21:20, # For extending vocabs, # Some legacy versions accidentally put training into config file, # else:, # flash(''), # flash('Loading config and vocabs [blink][yellow]...[/yellow][/blink]'), # flash(f'Available GPUs: {devices}'), # flash(f'Loading model: {filename} [blink]...[/blink][/yellow]'), # if getattr(torch, 'has_mps', None):  # mac M1 chips, # next(parser.model.parameters()).device, # noinspection PyMethodOverriding, # overwrite config loaded from disk, Build a :class:`logging.Logger`.

Args:
name: The name of this logger.
save_dir: The directory this logger should save logs into.

Returns:
logging.Logger: A logger.
, Build dataloader for training, dev and test sets. It's suggested to build vocabs in this method if they are
not built yet.

Args:
data: Data representing samples, which can be a path or a list of samples.
batch_size: Number of samples per batch.
shuffle: Whether to shuffle this dataloader.
device: Device tensors should be loaded onto.
logger: Logger for reporting some message if dataloader takes a long time or if vocabs has to be built.
**kwargs: Arguments from ``**self.config``.
, Build model.

Args:
training: ``True`` if called during training.
**kwargs: ``**self.config``.
, Called when config is ready, either during ``fit`` or ``load``. Subclass can perform extra initialization
tasks in this callback.

Args:
**kwargs: Not used.
, Evaluate on a dataloader.

Args:
data: Dataloader which can build from any data source.
criterion: Loss function.
metric: Metric(s).
output: Whether to save outputs into some file.
**kwargs: Not used.
, Evaluate test set.

Args:
tst_data: Test set, which is usually a file path.
save_dir: The directory to save evaluation scores or predictions.
logger: Logger for reporting progress.
batch_size: Batch size for test dataloader.
output: Whether to save outputs into some file.
**kwargs: Not used.

Returns:
(metric, outputs) where outputs are the return values of ``evaluate_dataloader``.
, Fit onto a dataloader.

Args:
trn: Training set.
criterion: Loss function.
optimizer: Optimizer.
metric: Metric(s).
logger: Logger for reporting progress.
**kwargs: Other hyper-parameters passed from sub-class.
, Fit to data, triggers the training procedure. For training set and dev set, they shall be local or remote
files.

Args:
trn_data: Training set.
dev_data: Development set.
save_dir: The directory to save trained component.
batch_size: The number of samples in a batch.
epochs: Number of epochs.
devices: Devices this component will live on.
logger: Any :class:`logging.Logger` instance.
seed: Random seed to reproduce this training.
finetune: ``True`` to load from ``save_dir`` instead of creating a randomly initialized component. ``str``
to specify a different ``save_dir`` to load from.
eval_trn: Evaluate training set after each update. This can slow down the training but provides a quick
diagnostic for debugging.
_device_placeholder: ``True`` to create a placeholder tensor which triggers PyTorch to occupy devices so
other components won't take these devices as first choices.
**kwargs: Hyperparameters used by sub-classes.

Returns:
Any results sub-classes would like to return. Usually the best metrics on training set.

, Implement this method to build an optimizer.

Args:
**kwargs: The subclass decides the method signature.
, Implement this method to build criterion (loss function).

Args:
**kwargs: The subclass decides the method signature.
, Implement this to build metric(s).

Args:
**kwargs: The subclass decides the method signature.
, Implement this to run training loop.

Args:
trn: Training set.
dev: Development set.
epochs: Number of epochs.
criterion: Loss function.
optimizer: Optimizer(s).
metric: Metric(s)
save_dir: The directory to save this component.
logger: Logger for reporting progress.
devices: Devices this component and dataloader will live on.
ratio_width: The width of dataset size measured in number of characters. Used for logger to align messages.
**kwargs: Other hyper-parameters passed from sub-class.
, Load config from a directory.

Args:
save_dir: The directory to load config.
filename: A file name for config.
**kwargs: K-V pairs to override config.
, Load from a local/remote component.

Args:
save_dir: An identifier which can be a local path or a remote URL or a pre-defined string.
devices: The devices this component will be moved onto.
verbose: ``True`` to log loading progress.
**kwargs: To override some configs.
, Load vocabularies from a directory.

Args:
save_dir: The directory to load vocabularies.
filename:  The name for vocabularies.
, Load weights from a directory.

Args:
save_dir: The directory to load weights from.
filename: A file name for weights.
**kwargs: Not used.
, Move this component to devices.

Args:
devices: Target devices.
logger: Logger for printing progress report, as copying a model from CPU to GPU can takes several seconds.
verbose: ``True`` to print progress when logger is None.
, Override this method to build vocabs.

Args:
trn: Training set.
logger: Logger for reporting progress.
, Predict on data fed by user. This method calls :meth:`~hanlp.common.torch_component.predict` but decorates
it with ``torch.no_grad``.

Args:
*args: Sentences or tokens.
**kwargs: Used in sub-classes.
, Predict on data fed by user. Users shall avoid directly call this method since it is not guarded with
``torch.no_grad`` and will introduces unnecessary gradient computation. Use ``__call__`` instead.

Args:
*args: Sentences or tokens.
**kwargs: Used in sub-classes.
, Save arguments to config

Args:
locals_: Dict:
exclude:  (Default value = ('trn_data')
'dev_data':
'save_dir':
'kwargs':
'self':
'logger':
'verbose':
'dev_batch_size':
'__class__':
'devices'):

Returns:


, Save config into a directory.

Args:
save_dir: The directory to save config.
filename: A file name for config.
, Save model weights to a directory.

Args:
save_dir: The directory to save weights into.
filename: A file name for weights.
trainable_only: ``True`` to only save trainable weights. Useful when the model contains lots of static
embeddings.
**kwargs: Not used for now.
, Save this component to a directory.

Args:
save_dir: The directory to save this component.
**kwargs: Not used.
, Save vocabularies to a directory.

Args:
save_dir: The directory to save vocabularies.
filename:  The name for vocabularies.
, The base class for all components using PyTorch as backend. It provides common workflows of building vocabs,
datasets, dataloaders and models. These workflows are more of a conventional guideline than en-forced
protocols, which means subclass has the freedom to override or completely skip some steps.

Args:
**kwargs: Addtional arguments to be stored in the ``config`` property.
, The devices this component lives on.
, The first device this component lives on.
",
https://github.com/hankcs/HanLP,transform.py,0,0.0,128,69.95,33,18.03,6,3.28,16,8.74,0,0.0,24,24,24,11,,"AppendEOS, BMESOtoIOBES, Bigram, ConfigurableNamedTransform, ConfigurableTransform, CopyField, EmbeddingNamedTransform, FieldLength, FieldToIndex, FilterField, LowerCase, LowerCase3D, NamedTransform, NormalizeCharacter, NormalizeDigit, NormalizeToken, PunctuationMask, RenameField, ToChar, ToIndex, TransformList, ValueError, VocabDict, VocabList, WhitespaceTokenizer, __call__, __dict__, __getattr__, __getitem__, __init__, __setattr__, __setitem__, _load_vocabs, _table, abc, any, append, args, char, chars, classmethod, cls, config, convert, copy_from, delta, dict, dst, each, enumerate, eos, extend, field, fields, filename, from_config, get, hanlp, hanlp_common, index_by_type, info, int, isdigit, isinstance, item, items, k, key, keys, kwargs, len, list, load_json, load_vocab, load_vocabs, lock, logger, logging, lower, mapper, max_word_length, min_word_length, mutable, nested, new_word, object, os, output_dim, pad, pop, print, property, put, range, report, sample, save_dir, save_json, save_vocab, save_vocabs, self, split, src, startswith, staticmethod, status, summary, super, text, to_chars, to_dict, token, tokenize, trans, transform, transforms, tuple, typing, unlock, update, value, values, vd, vocab, vocab_cls, vocabs, word, x","abc.ABC, abc.abstractmethod, hanlp.common.vocab.Vocab, hanlp.utils.io_util.get_resource, hanlp.utils.string_util.ispunct, hanlp_common.configurable.Configurable, hanlp_common.constant.EOS, hanlp_common.constant.PAD, hanlp_common.io.load_json, hanlp_common.reflection.classpath_of, hanlp_common.reflection.str_to_type, hanlp_common.structure.SerializableDict, logging.Logger, os.path.join, typing.List, typing.Tuple, typing.Union","__call__, __getattr__, __getitem__, __init__, __setattr__, __setitem__, _load_vocabs, append, config, convert, from_config, index_by_type, load_vocab, load_vocabs, lock, mutable, put, save_vocab, save_vocabs, summary, to_chars, tokenize, transform, unlock","AppendEOS, BMESOtoIOBES, Bigram, ConfigurableNamedTransform, ConfigurableTransform, CopyField, EmbeddingNamedTransform, FieldLength, FieldToIndex, FilterField, LowerCase, LowerCase3D, NamedTransform, NormalizeCharacter, NormalizeDigit, NormalizeToken, PunctuationMask, RenameField, ToChar, ToIndex, TransformList, VocabDict, VocabList, WhitespaceTokenizer","char, chars, cls, config, dst, each, field, filename, item, k, key, mapper, nested, new_word, report, sample, save_dir, src, status, trans, value, vocab, vocabs, x","A dict holding :class:`hanlp.common.vocab.Vocab` instances. When used as a transform, it transforms the field
corresponding to each :class:`hanlp.common.vocab.Vocab` into indices.

Args:
    *args: A list of vocab names.
    **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances., Args:
    vd:
    vocabs:
    vocab_cls: Default class for the new vocab, Args:
  config: 
  kwargs: 
  config: dict: 

Returns:, Composes several transforms together.

Args:
  transforms(list of ``Transform`` objects): list of transforms to compose.
Example:

Returns:

>>> transforms.TransformList(
    >>>     transforms.CenterCrop(10),
    >>>     transforms.ToTensor(),
    >>> ), Load vocabularies from a directory.

Args:
    save_dir: The directory to load vocabularies.
    filename:  The name for vocabularies., Lock each vocab., Log a summary of vocabs using a given logger.

Args:
    logger: The logger to use., Mask out all punctuations (set mask of punctuations to False)

Args:
  src:
  dst:

Returns:, Put names and corresponding :class:`hanlp.common.vocab.Vocab` instances into self.

Args:
    **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances., Save vocabularies to a directory.

Args:
    save_dir: The directory to save vocabularies.
    filename:  The name for vocabularies., Unlock each vocab.",", 

        Args:
            vd:
            vocabs:
            vocab_cls: Default class for the new vocab
        , 

        Args:
          config: 
          kwargs: 
          config: dict: 

        Returns:

        
        , 
        Lock each vocab.
        , 
        Unlock each vocab.
        ,  doesn't contain classpath field, 0, A dict holding :class:`hanlp.common.vocab.Vocab` instances. When used as a transform, it transforms the field
        corresponding to each :class:`hanlp.common.vocab.Vocab` into indices.

        Args:
            *args: A list of vocab names.
            **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.
        , Composes several transforms together.

    Args:
      transforms(list of ``Transform`` objects): list of transforms to compose.
    Example:

    Returns:

    >>> transforms.TransformList(
        >>>     transforms.CenterCrop(10),
        >>>     transforms.ToTensor(),
        >>> )
    , I-, Load vocabularies from a directory.

        Args:
            save_dir: The directory to load vocabularies.
            filename:  The name for vocabularies.
        , Log a summary of vocabs using a given logger.

        Args:
            logger: The logger to use.
        , M-, Mask out all punctuations (set mask of punctuations to False)

        Args:
          src:
          dst:

        Returns:

        , Put names and corresponding :class:`hanlp.common.vocab.Vocab` instances into self.

        Args:
            **kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.
        , Save vocabularies to a directory.

        Args:
            save_dir: The directory to save vocabularies.
            filename:  The name for vocabularies.
        , Unrecognized mapper type , Unsupported argument length: , Unsupported argument type: , _, __, _bigram, _id, _length, _punct_mask, _vocab.json, char, classpath, idx_to_token, tag, type, vocab.json, vocabs.json","0, 1, 2, 3, False, None","

Args:
config:
kwargs:
config: dict:

Returns:


, 

Args:
vd:
vocabs:
vocab_cls: Default class for the new vocab
, 
Lock each vocab.
, 
Unlock each vocab.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-03 14:44, # nested Vocab, # noinspection PyTypeChecker, A dict holding :class:`hanlp.common.vocab.Vocab` instances. When used as a transform, it transforms the field
corresponding to each :class:`hanlp.common.vocab.Vocab` into indices.

Args:
*args: A list of vocab names.
**kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.
, Composes several transforms together.

Args:
transforms(list of ``Transform`` objects): list of transforms to compose.
Example:

Returns:

>>> transforms.TransformList(
>>>     transforms.CenterCrop(10),
>>>     transforms.ToTensor(),
>>> )
, Load vocabularies from a directory.

Args:
save_dir: The directory to load vocabularies.
filename:  The name for vocabularies.
, Log a summary of vocabs using a given logger.

Args:
logger: The logger to use.
, Mask out all punctuations (set mask of punctuations to False)

Args:
src:
dst:

Returns:

, Put names and corresponding :class:`hanlp.common.vocab.Vocab` instances into self.

Args:
**kwargs: Names and corresponding :class:`hanlp.common.vocab.Vocab` instances.
, Save vocabularies to a directory.

Args:
save_dir: The directory to save vocabularies.
filename:  The name for vocabularies.
",
https://github.com/hankcs/HanLP,transform_tf.py,0,0.0,94,63.09,17,11.41,8,5.37,30,20.13,0,0.0,25,1,24,9,,"NotImplementedError, Transform, X, XY_to_inputs_outputs, X_to_inputs, Y, Y_to_outputs, __class__, __init__, __name__, abc, all, batch_size, bool, build_config, cache, cleanup, config, create_types_shapes_values, dataset, debug, drop_remainder, file_to_dataset, file_to_inputs, file_to_samples, filepath, fit, flat, generator, generator_to_callable, gold, hanlp, hanlp_common, hasattr, header, info, input, input_is_single_sample, input_to_inputs, input_truth_output_to_str, inputs, inputs_to_dataset, inputs_to_samples, inspect, int, isinstance, items, k, key, kv, kwargs, len, list, lock, lock_vocabs, map, map_x, map_y, mapper, new_py_funcs, output, output_shapes, output_types, padded_batch, padding_values, prefetch, print, py_func_set_after, py_func_set_before, py_func_set_to_cleanup, repeat, repr, samples, samples_to_dataset, self, set, shuffle, sorted, staticmethod, str_to_idx, strip, summarize_vocabs, summary, super, tensorflow, truth, typing, value, vars, vocabs, x, x_to_idx, y_to_idx, zip","abc.ABC, abc.abstractmethod, hanlp.common.vocab_tf.VocabTF, hanlp.utils.io_util.get_resource, hanlp.utils.log_util.logger, hanlp_common.structure.SerializableDict, inspect.stack, tf.Tensor, tf.compat.v1.get_default_graph, tf.data.Dataset, tf.data.Dataset.from_generator, tf.data.experimental.AUTOTUNE, typing.Any, typing.Generator, typing.Iterable, typing.Tuple, typing.Union","XY_to_inputs_outputs, X_to_inputs, Y_to_outputs, __init__, build_config, cleanup, create_types_shapes_values, file_to_dataset, file_to_inputs, file_to_samples, fit, generator, generator_to_callable, input_is_single_sample, input_to_inputs, input_truth_output_to_str, inputs_to_dataset, inputs_to_samples, lock_vocabs, mapper, samples_to_dataset, str_to_idx, summarize_vocabs, x_to_idx, y_to_idx",Transform,"X, Y, config, dataset, filepath, flat, input, inputs, k, key, map_x, map_y, new_py_funcs, output, output_shapes, output_types, padding_values, py_func_set_after, py_func_set_before, samples, shuffle, value, vocabs, x","Build the vocabulary from training file

Parameters
----------
trn_path : path to training set
kwargs

Returns
-------
int
    How many samples in the training set, By default, call build_types_shapes_values, usually called in component's build method.
You can perform other building task here. Remember to call super().build_config, Convert input truth output to string representation, usually for writing to file during evaluation

Parameters
----------
input
truth
output

Returns
-------, Convert predicted tensors to outputs

Parameters
----------
X : Union[tf.Tensor, Tuple[tf.Tensor]]
    The inputs of model
Y : Union[tf.Tensor, Tuple[tf.Tensor]]
    The outputs of model

Returns
-------, Create dataset related values,, If input is one sample, convert it to a list which contains this unique sample

Parameters
----------
input :
    sample or samples

Returns
-------
(inputs, converted) : Tuple[Any, bool], Transform file to dataset

Parameters
----------
filepath
gold : bool
    Whether it's processing gold data or not. Example: there is usually a column for gold answer
    when gold = True.
map_x : bool
    Whether call map_x or not. Default to self.map_x
map_y : bool
    Whether call map_y or not. Default to self.map_y
batch_size
shuffle
repeat
prefetch
kwargs

Returns
-------, Transform file to inputs. The inputs are defined as raw features (e.g. words) to be processed into more
features (e.g. forms and characters)

Parameters
----------
filepath
gold, Transform file to samples
Parameters
----------
filepath
gold",", 	, 
, 
        Build the vocabulary from training file

        Parameters
        ----------
        trn_path : path to training set
        kwargs

        Returns
        -------
        int
            How many samples in the training set
        , 
        By default, call build_types_shapes_values, usually called in component's build method.
        You can perform other building task here. Remember to call super().build_config
        , 
        Convert input truth output to string representation, usually for writing to file during evaluation

        Parameters
        ----------
        input
        truth
        output

        Returns
        -------

        , 
        Convert predicted tensors to outputs

        Parameters
        ----------
        X : Union[tf.Tensor, Tuple[tf.Tensor]]
            The inputs of model
        Y : Union[tf.Tensor, Tuple[tf.Tensor]]
            The outputs of model

        Returns
        -------

        , 
        Create dataset related values,
        , 
        If input is one sample, convert it to a list which contains this unique sample

        Parameters
        ----------
        input :
            sample or samples

        Returns
        -------
        (inputs, converted) : Tuple[Any, bool]

        , 
        Transform file to dataset

        Parameters
        ----------
        filepath
        gold : bool
            Whether it's processing gold data or not. Example: there is usually a column for gold answer
            when gold = True.
        map_x : bool
            Whether call map_x or not. Default to self.map_x
        map_y : bool
            Whether call map_y or not. Default to self.map_y
        batch_size
        shuffle
        repeat
        prefetch
        kwargs

        Returns
        -------

        , 
        Transform file to inputs. The inputs are defined as raw features (e.g. words) to be processed into more
        features (e.g. forms and characters)

        Parameters
        ----------
        filepath
        gold
        , 
        Transform file to samples
        Parameters
        ----------
        filepath
        gold
        , %s.%s(), Dataset cache enabled, Vocab summary:, Your create_types_shapes_values returns None, which is not allowed, _py_funcs_used_in_graph","0, 1, 1024, 3, 32, False, None, True","
Build the vocabulary from training file

Parameters
----------
trn_path : path to training set
kwargs

Returns
-------
int
How many samples in the training set
, 
By default, call build_types_shapes_values, usually called in component's build method.
You can perform other building task here. Remember to call super().build_config
, 
Convert input truth output to string representation, usually for writing to file during evaluation

Parameters
----------
input
truth
output

Returns
-------

, 
Convert predicted tensors to outputs

Parameters
----------
X : Union[tf.Tensor, Tuple[tf.Tensor]]
The inputs of model
Y : Union[tf.Tensor, Tuple[tf.Tensor]]
The outputs of model

Returns
-------

, 
Create dataset related values,
, 
If input is one sample, convert it to a list which contains this unique sample

Parameters
----------
input :
sample or samples

Returns
-------
(inputs, converted) : Tuple[Any, bool]

, 
Transform file to dataset

Parameters
----------
filepath
gold : bool
Whether it's processing gold data or not. Example: there is usually a column for gold answer
when gold = True.
map_x : bool
Whether call map_x or not. Default to self.map_x
map_y : bool
Whether call map_y or not. Default to self.map_y
batch_size
shuffle
repeat
prefetch
kwargs

Returns
-------

, 
Transform file to inputs. The inputs are defined as raw features (e.g. words) to be processed into more
features (e.g. forms and characters)

Parameters
----------
filepath
gold
, 
Transform file to samples
Parameters
----------
filepath
gold
, #             shapes[j] = list(shape), #         if isinstance(shape, tuple):, #         self.output_shapes[i] = list(shapes), #     for j, shape in enumerate(shapes):, #     if isinstance(shapes, tuple):, #     pass, #     samples = Transform.generator_to_callable(samples), #     self.output_shapes = list(self.output_shapes), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-27 14:22, # Fix tf memory leak: https://github.com/tensorflow/tensorflow/issues/37653#issuecomment-1000517720, # We prefer list over shape here, as it's easier to type [] than (), # debug, # for i, shapes in enumerate(self.output_shapes):, # for sample in self.file_to_samples(filepath):, # for sample in self.inputs_to_samples(inputs):, # if isinstance(self.output_shapes, tuple):, # if not callable(samples):, # print('Did you forget to call build_config() on your transform?'), # tag vocab comes last usually",
https://github.com/hankcs/HanLP,vocab.py,0,0.0,105,59.66,35,19.89,6,3.41,30,17.05,0,0.0,37,7,25,23,,"CustomVocab, LowercaseVocab, Vocab, Vocab3D, VocabCounter, VocabWithFrequency, VocabWithNone, __call__, __contains__, __getitem__, __init__, __len__, __repr__, __setitem__, __str__, add, append, build_idx_to_token, clear, collections, copy_from, counter, create_label_vocab, d, dict, each, enumerate, extend, first_token, freq, frequencies, get, get_frequency, get_idx, get_idx_without_add, get_token, hanlp_common, has_key, idx, idx_to_token, indices, inside, int, isinstance, item, items, iter, k, key, keys, len, list, locals, lock, locked, lower, max, max_idx, merge, min, min_frequency, min_occur_cnt, most_common, mutable, next, other, pad_idx, pad_token, pop, print, property, reload_idx_to_token, report, safe_pad_token, safe_pad_token_idx, safe_unk_token, self, sent, set, set_unk_as_safe_unk, setattr, some_token, specials, summary, super, survivors, t2i, to_dict, token, token_to_idx, tokens, trim, tuple, type, typing, unk_idx, unk_token, unlock, update, v, value, values, verbose, word, x","collections.Counter, hanlp_common.constant.PAD, hanlp_common.constant.UNK, hanlp_common.reflection.classpath_of, hanlp_common.structure.Serializable, typing.Dict, typing.Iterable, typing.List, typing.Union","__call__, __contains__, __getitem__, __init__, __len__, __repr__, __setitem__, __str__, add, build_idx_to_token, clear, copy_from, create_label_vocab, extend, first_token, get_frequency, get_idx, get_idx_without_add, get_token, has_key, lock, locked, lower, merge, pad_idx, reload_idx_to_token, safe_pad_token, safe_pad_token_idx, safe_unk_token, set_unk_as_safe_unk, summary, to_dict, tokens, trim, unk_idx, unlock, update","CustomVocab, LowercaseVocab, Vocab, Vocab3D, VocabCounter, VocabWithFrequency, VocabWithNone","d, each, freq, idx, idx_to_token, indices, inside, items, k, key, max_idx, mutable, pad_token, report, sent, specials, survivors, t2i, token, token_to_idx, unk_token, v, value, word, x","A set of all tokens in this vocab., Convert all tokens to lower case.

Returns:
    Itself., Convert this vocab to a dict so that it can be json serialized.

Returns:
    A dict., Copy properties from a dict so that it can json de-serialized.

Args:
    item: A dict holding ``token_to_idx``

Returns:
    Itself., Get or print a summary of this vocab.

Args:
    verbose: ``True`` to print the summary to stdout.

Returns:
    Summary in text form., Get the idx of a token. If it's not there, it will be added to the vocab when the vocab is locked otherwise
the id of UNK will be returned.

Args:
    token: A token.

Returns:
    The id of that token., Get the idx to the pad token safely. It always returns an index, which corresponds to the pad token or the
first token if pad does not present in the vocab., Get the index/indices associated with a token or a list of tokens or vice versa.

Args:
    key: ``str`` for token(s) and ``int`` for index/indices.

Returns: Associated indices or tokens., Get the pad token safely. It always returns a pad token, which is the pad token or the first token
if pad does not present in the vocab., Get the token using its index.

Args:
    idx: The index to a token.

Returns:, Get the unk token safely. It always returns a unk token, which is the unk token or the first token if unk
does not presented in the vocab., It supports 3D arrays of tokens.

Args:
    some_token: Tokens of 1D to 3D

Returns:
    A list of indices., Lock this vocab up so that it won't accept new tokens.

Returns:
    Itself., Merge this with another vocab inplace.

Args:
    other (Vocab): Another vocab., Set ``self.unk_token = self.safe_unk_token``. It's useful when the dev/test set contains OOV labels.
        , The first token in this vocab.
        , The index of ``PAD`` token., The index of ``UNK`` token., Tries to add a token into a vocab and returns its id. If it has already been there, its id will be returned
and the vocab won't be updated. If the vocab is locked, an assertion failure will occur.

Args:
    token: A new or existing token.

Returns:
    Its associated id., Unlock this vocab so that new tokens can be added in.

Returns:
    Itself., Update the vocab with these tokens by adding them to vocab one by one.

Args:
  tokens (Iterable[str]): A list of tokens., Vocabulary base class which converts tokens to indices and vice versa.

Args:
    idx_to_token: id to token mapping.
    token_to_idx: token to id mapping.
    mutable: ``True`` to allow adding new tokens, ``False`` to map OOV to ``unk``.
    pad_token: The token representing padding.
    unk_token: The token representing OOV., ``True`` indicates this vocab is locked.","
        A set of all tokens in this vocab.
        , 
        The index of ``PAD`` token.
        , 
        The index of ``UNK`` token.
        , 
        ``True`` indicates this vocab is locked.
        ,  Get the index/indices associated with a token or a list of tokens or vice versa.

        Args:
            key: ``str`` for token(s) and ``int`` for index/indices.

        Returns: Associated indices or tokens.

        ,  Tries to add a token into a vocab and returns its id. If it has already been there, its id will be returned
        and the vocab won't be updated. If the vocab is locked, an assertion failure will occur.

        Args:
            token: A new or existing token.

        Returns:
            Its associated id.

        ,  from , Convert all tokens to lower case.

        Returns:
            Itself.

        , Convert this vocab to a dict so that it can be json serialized.

        Returns:
            A dict.

        , Copy properties from a dict so that it can json de-serialized.

        Args:
            item: A dict holding ``token_to_idx``

        Returns:
            Itself.

        , Get or print a summary of this vocab.

        Args:
            verbose: ``True`` to print the summary to stdout.

        Returns:
            Summary in text form.

        , Get the idx of a token. If it's not there, it will be added to the vocab when the vocab is locked otherwise
        the id of UNK will be returned.

        Args:
            token: A token.

        Returns:
            The id of that token.

        , Get the idx to the pad token safely. It always returns an index, which corresponds to the pad token or the
        first token if pad does not present in the vocab.
        , Get the pad token safely. It always returns a pad token, which is the pad token or the first token
        if pad does not present in the vocab.
        , Get the token using its index.

        Args:
            idx: The index to a token.

        Returns:

        , Get the unk token safely. It always returns a unk token, which is the unk token or the first token if unk
        does not presented in the vocab.
        , It is not allowed to call add on an immutable Vocab, It is not allowed to update an immutable Vocab, It supports 3D arrays of tokens.

        Args:
            some_token: Tokens of 1D to 3D

        Returns:
            A list of indices.

        , Lock this vocab up so that it won't accept new tokens.

        Returns:
            Itself.

        , Merge this with another vocab inplace.

        Args:
            other (Vocab): Another vocab.
        , Set ``self.unk_token = self.safe_unk_token``. It's useful when the dev/test set contains OOV labels.
        , The first token in this vocab.
        , Token must not be None, Token type must be str but got , Unlock this vocab so that new tokens can be added in.

        Returns:
            Itself.

        , Update an immutable Vocab object is not allowed, Update the vocab with these tokens by adding them to vocab one by one.

        Args:
          tokens (Iterable[str]): A list of tokens.
        , Vocabulary base class which converts tokens to indices and vice versa.

        Args:
            idx_to_token: id to token mapping.
            token_to_idx: token to id mapping.
            mutable: ``True`` to allow adding new tokens, ``False`` to map OOV to ``unk``.
            pad_token: The token representing padding.
            unk_token: The token representing OOV.
        , [{}] = , counter, frequencies, self, token has to be `str`, type","0, 1, 50, False, None, True","
A set of all tokens in this vocab.
, 
The index of ``PAD`` token.
, 
The index of ``UNK`` token.
, 
``True`` indicates this vocab is locked.
,  Get the index/indices associated with a token or a list of tokens or vice versa.

Args:
key: ``str`` for token(s) and ``int`` for index/indices.

Returns: Associated indices or tokens.

,  Tries to add a token into a vocab and returns its id. If it has already been there, its id will be returned
and the vocab won't be updated. If the vocab is locked, an assertion failure will occur.

Args:
token: A new or existing token.

Returns:
Its associated id.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 22:42, # report += 'Mutable: {}'.format(self.mutable), # report += 'Samples: {}\n'.format(str(list(self.token_to_idx.keys())[:min(50, len(self))])), # report = 'Length: {}\n'.format(len(self)), # report = report.strip(), Convert all tokens to lower case.

Returns:
Itself.

, Convert this vocab to a dict so that it can be json serialized.

Returns:
A dict.

, Copy properties from a dict so that it can json de-serialized.

Args:
item: A dict holding ``token_to_idx``

Returns:
Itself.

, Get or print a summary of this vocab.

Args:
verbose: ``True`` to print the summary to stdout.

Returns:
Summary in text form.

, Get the idx of a token. If it's not there, it will be added to the vocab when the vocab is locked otherwise
the id of UNK will be returned.

Args:
token: A token.

Returns:
The id of that token.

, Get the idx to the pad token safely. It always returns an index, which corresponds to the pad token or the
first token if pad does not present in the vocab.
, Get the pad token safely. It always returns a pad token, which is the pad token or the first token
if pad does not present in the vocab.
, Get the token using its index.

Args:
idx: The index to a token.

Returns:

, Get the unk token safely. It always returns a unk token, which is the unk token or the first token if unk
does not presented in the vocab.
, It supports 3D arrays of tokens.

Args:
some_token: Tokens of 1D to 3D

Returns:
A list of indices.

, Lock this vocab up so that it won't accept new tokens.

Returns:
Itself.

, Merge this with another vocab inplace.

Args:
other (Vocab): Another vocab.
, Set ``self.unk_token = self.safe_unk_token``. It's useful when the dev/test set contains OOV labels.
, The first token in this vocab.
, Unlock this vocab so that new tokens can be added in.

Returns:
Itself.

, Update the vocab with these tokens by adding them to vocab one by one.

Args:
tokens (Iterable[str]): A list of tokens.
, Vocabulary base class which converts tokens to indices and vice versa.

Args:
idx_to_token: id to token mapping.
token_to_idx: token to id mapping.
mutable: ``True`` to allow adding new tokens, ``False`` to map OOV to ``unk``.
pad_token: The token representing padding.
unk_token: The token representing OOV.
",
https://github.com/hankcs/HanLP,vocab_tf.py,0,0.0,81,74.31,11,10.09,6,5.5,11,10.09,0,0.0,32,1,17,3,,"VocabTF, __call__, __contains__, __getitem__, __init__, __len__, __setitem__, __str__, add, append, build_idx_to_token, build_lookup_table, copy_from, create_label_vocab, dict, enumerate, first_token, get, get_idx, get_idx_without_add, get_token, hanlp_common, has_key, idx, idx_to_token, idx_to_token_table, indices, int, isinstance, item, items, iter, key, keys, len, list, locals, lock, locked, lookup, lower, max, max_idx, merge, min, mutable, next, other, pad_idx, pad_token, pop, print, property, report, safe_pad_token, safe_pad_token_idx, safe_unk_token, self, setattr, some_token, summary, super, t2i, tensorflow, to_dict, token, token_tensor, token_to_idx, token_to_idx_table, tokens, type, typing, unk_idx, unk_token, unlock, update, v, value, values, verbose, word","hanlp_common.constant.PAD, hanlp_common.constant.UNK, hanlp_common.structure.Serializable, tensorflow.python.ops.lookup_ops.index_table_from_tensor, tf.Tensor, tf.constant, tf.lookup.StaticHashTable, tf.string, typing.Dict, typing.Iterable, typing.List, typing.Union","__call__, __contains__, __getitem__, __init__, __len__, __setitem__, __str__, add, build_idx_to_token, build_lookup_table, copy_from, create_label_vocab, first_token, get_idx, get_idx_without_add, get_token, has_key, lock, locked, lookup, lower, merge, pad_idx, safe_pad_token, safe_pad_token_idx, safe_unk_token, summary, to_dict, tokens, unk_idx, unlock, update",VocabTF,"idx, idx_to_token, indices, items, key, max_idx, mutable, pad_token, report, t2i, tensor, token, token_to_idx, unk_token, v, value, word","Get the pad token safely. It always returns a pad token, which is the token
closest to pad if not presented in the vocab.

Args:

Returns:, Get the unk token safely. It always returns a unk token, which is the token
closest to unk if not presented in the vocab.

Args:

Returns:, Update the vocab with these tokens by adding them to vocab one by one.

Args:
  tokens: Iterable[str]: 

Returns:"," from , Get the pad token safely. It always returns a pad token, which is the token
        closest to pad if not presented in the vocab.

        Args:

        Returns:

        
        , Get the unk token safely. It always returns a unk token, which is the token
        closest to unk if not presented in the vocab.

        Args:

        Returns:

        
        , It is not allowed to call add on an immutable Vocab, It is not allowed to update an immutable Vocab, Token must not be None or length 0, Token type must be str but got , Update an immutable Vocab object is not allowed, Update the vocab with these tokens by adding them to vocab one by one.

        Args:
          tokens: Iterable[str]: 

        Returns:

        
        , [{}] = , self","0, 1, 50, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 22:42, # report += 'Mutable: {}'.format(self.mutable), # report += 'Samples: {}\n'.format(str(list(self.token_to_idx.keys())[:min(50, len(self))])), # report = 'Length: {}\n'.format(len(self)), # report = report.strip(), # self.idx_to_token_table = index_to_string_table_from_tensor(self.idx_to_token, self.safe_unk_token), Get the pad token safely. It always returns a pad token, which is the token
closest to pad if not presented in the vocab.

Args:

Returns:


, Get the unk token safely. It always returns a unk token, which is the token
closest to unk if not presented in the vocab.

Args:

Returns:


, Update the vocab with these tokens by adding them to vocab one by one.

Args:
tokens: Iterable[str]:

Returns:


",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 14:45",
https://github.com/hankcs/HanLP,lambda_wrapper.py,0,0.0,19,73.08,3,11.54,1,3.85,3,11.54,0,0.0,3,1,3,0,,"LambdaComponent, __init__, cls, config, data, dict, from_config, function, hanlp, hanlp_common, kwargs, meta, pop, predict, self, staticmethod, super, typing, unpack","hanlp.common.component.Component, hanlp_common.reflection.classpath_of, hanlp_common.reflection.object_from_classpath, hanlp_common.reflection.str_to_type, typing.Any, typing.Callable","__init__, from_config, predict",LambdaComponent,"cls, function, unpack",,"_hanlp_unpack, classpath, function",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 18:36",
https://github.com/hankcs/HanLP,lemmatizer.py,0,0.0,27,72.97,5,13.51,1,2.7,4,10.81,0,0.0,4,1,8,1,,"TransformerLemmatizer, __init__, add_lemma_rules_to_sample, append, batch, build_dataset, data, dict, hanlp, isinstance, kwargs, lemma_per_sent, lemma_rules, list, pred, prediction_to_human, rule_per_sent, rules, sample, str, super, token, token_per_sent, typing, vocab, word, zip","hanlp.common.transform.TransformList, hanlp.components.parsers.ud.lemma_edit.apply_lemma_rule, hanlp.components.parsers.ud.lemma_edit.gen_lemma_rule, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, typing.List","__init__, add_lemma_rules_to_sample, build_dataset, prediction_to_human",TransformerLemmatizer,"lemma_per_sent, lemma_rules, rule_per_sent, rules, token, token_per_sent, transform, word","A transition based lemmatizer using transformer as encoder.

Args:
    **kwargs: Predefined config.","A transition based lemmatizer using transformer as encoder.

        Args:
            **kwargs: Predefined config.
        , _, lemma, tag, token",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-08 18:35, A transition based lemmatizer using transformer as encoder.

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,pipeline.py,0,0.0,54,67.5,14,17.5,4,5.0,8,10.0,0,0.0,12,2,14,3,,"Pipe, Pipeline, __call__, __class__, __copy__, __init__, __name__, __repr__, append, clear, cls, component, config, copy, dict, extend, filepath, from_config, function, hanlp, hanlp_common, hasattr, index, input, input_key, insert, int, isinstance, key, kwargs, len, list, load, meta, name, output, output_key, pipe, pipes, predict, property, save, self, sent, setter, staticmethod, super, tuple, types, typing, unpack, update, value, zip","hanlp.common.component.Component, hanlp.components.lambda_wrapper.LambdaComponent, hanlp.utils.component_util.load_from_meta, hanlp.version.__version__, hanlp_common.document.Document, hanlp_common.io.load_json, hanlp_common.io.save_json, hanlp_common.reflection.classpath_of, hanlp_common.reflection.str_to_type, types.GeneratorType, typing.Any, typing.Callable, typing.Iterable, typing.Union","__call__, __copy__, __init__, __repr__, append, copy, from_config, insert, load, meta, predict, save","Pipe, Pipeline","cls, component, config, doc, input, input_key, key, meta, name, output, pipe, sent, unpack, value","Append a pipe to the tail of this pipeline.

Args:
    component: A callable function.
    input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
        previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
        :class:`~hanlp_common.document.Document`.
    output_key: The output key indicating where to store the outputs
    **kwargs: Extra arguments passed to the ``Pipe`` constructor.

Returns:

    Pipeline: A pipeline., Args:
    index: The index of the new pipe.
    input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
        previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
        :class:`~hanlp_common.document.Document`.
    output_key: The output key indicating where to store the outputs
    **kwargs: Extra arguments passed to the ``Pipe`` constructor.

Returns:

    Pipeline: A pipeline., Run the pipeline as a function.

Args:
    doc: A :class:`~hanlp_common.document.Document` or other data types.
    **kwargs: If `doc` is set to None then create a :class:`~hanlp_common.document.Document` as the
        input to the first pipe using all the parameters in ``kwargs``.

Returns:
    A :class:`~hanlp_common.document.Document`.","

        Args:
            index: The index of the new pipe.
            input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
                previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
                :class:`~hanlp_common.document.Document`.
            output_key: The output key indicating where to store the outputs
            **kwargs: Extra arguments passed to the ``Pipe`` constructor.

        Returns:

            Pipeline: A pipeline.
        , 
        Append a pipe to the tail of this pipeline.

        Args:
            component: A callable function.
            input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
                previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
                :class:`~hanlp_common.document.Document`.
            output_key: The output key indicating where to store the outputs
            **kwargs: Extra arguments passed to the ``Pipe`` constructor.

        Returns:

            Pipeline: A pipeline.
        , *, ->, Run the pipeline as a function.

        Args:
            doc: A :class:`~hanlp_common.document.Document` or other data types.
            **kwargs: If `doc` is set to None then create a :class:`~hanlp_common.document.Document` as the
                input to the first pipe using all the parameters in ``kwargs``.

        Returns:
            A :class:`~hanlp_common.document.Document`.
        , _hanlp_unpack, classpath, component, config, hanlp_version, input_key, kwargs, output_key, pipes","1, False, None, True","

Args:
index: The index of the new pipe.
input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
:class:`~hanlp_common.document.Document`.
output_key: The output key indicating where to store the outputs
**kwargs: Extra arguments passed to the ``Pipe`` constructor.

Returns:

Pipeline: A pipeline.
, 
Append a pipe to the tail of this pipeline.

Args:
component: A callable function.
input_key: The input key indicating which fields will be inputted to the pipe. ``None``: inherit from
previous pipe; ``*``: use all the outputs from previous pipes wrapped in a
:class:`~hanlp_common.document.Document`.
output_key: The output key indicating where to store the outputs
**kwargs: Extra arguments passed to the ``Pipe`` constructor.

Returns:

Pipeline: A pipeline.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 00:22, # assume functions take multiple arguments, # noinspection PyShadowingBuiltins, Run the pipeline as a function.

Args:
doc: A :class:`~hanlp_common.document.Document` or other data types.
**kwargs: If `doc` is set to None then create a :class:`~hanlp_common.document.Document` as the
input to the first pipe using all the parameters in ``kwargs``.

Returns:
A :class:`~hanlp_common.document.Document`.
",
https://github.com/hankcs/HanLP,rnn_language_model_tf.py,0,0.0,49,62.03,11,13.92,13,16.46,6,7.59,0,0.0,7,1,14,0,,"RNNLanguageModel, __init__, add, batch_size, build_model, build_optimizer, build_train_dataset, build_valid_dataset, char_mode, clipnorm, config, dev_data, dict, embedding, extra_args, file_to_dataset, first_or_last_token, fit, float, forward, generate_text, hanlp, int, isinstance, kwargs, learning_rate, len, list, locals, model, num_steps, optimizer, output, predict, range, rnn_input_dropout, rnn_output_dropout, rnn_units, self, seq_len, step, super, tensorflow, training, transform, trn_data, typing, v, vocab","hanlp.common.keras_component.KerasComponent, hanlp.transform.text_tf.TextTransform, tf.keras.Model, tf.keras.Sequential, tf.keras.layers.Dense, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.layers.LSTM, tf.keras.layers.TimeDistributed, tf.keras.optimizers.SGD, typing.List, typing.Union","__init__, build_model, build_optimizer, build_train_dataset, build_valid_dataset, fit, generate_text",RNNLanguageModel,"char_mode, dev_data, embedding, extra_args, first_or_last_token, forward, model, optimizer, output, step, text, transform, trn_data, v",,", 
, batch_input_shape, decoder, encoder, forward, kwargs, rnn_input_dropout, rnn_output_dropout, self, sgd","0.1, 0.25, 1, 10, 100, 1000, 1024, 20, 250, 50, False, None, True","# -*- coding:utf-8 -*-, # A slow implementation. Might better to let LSTM return states., # Author: hankcs, # But anyway, this interface is for fun so let's take it easy, # Date: 2019-12-04 17:28, # noinspection PyMethodOverriding",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,1,25.0,0,0.0,0,0.0,3,75.0,0,0.0,0,0,0,0,,pipeline,pipeline.Pipeline,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 16:10",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 18:15",
https://github.com/hankcs/HanLP,cnn_encoder.py,0,0.0,39,60.0,4,6.15,7,10.77,13,20.0,2,3.08,4,1,8,1,,"CnnEncoder, __init__, _activation, _convolution_layers, _embedding_dim, _ngram_filter_sizes, _num_filters, _output_dim, add_module, append, conv_layer, conv_layer_activation, convolution_layer, embedding_dim, enumerate, filter_outputs, forward, get_input_dim, get_output_dim, getattr, int, len, mask, maxpool_output, maxpool_output_dim, ngram_filter_sizes, ngram_size, num_filters, output_dim, projection_layer, range, result, self, str, super, tokens, torch, typing, unsqueeze","torch.BoolTensor, torch.Tensor, torch.cat, torch.nn, torch.nn.Conv1d, torch.nn.Linear, torch.nn.Module, torch.transpose, typing.Optional, typing.Tuple","__init__, forward, get_input_dim, get_output_dim",CnnEncoder,"conv_layer, convolution_layer, filter_outputs, maxpool_output, maxpool_output_dim, ngram_size, result, tokens","A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a
[`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

Registered as a `Seq2VecEncoder` with name ""cnn"".

# Parameters

embedding_dim : `int`, required
    This is the input dimension to the encoder.  We need this because we can't do shape
    inference in pytorch, and we need to know what size filters to construct in the CNN.
num_filters : `int`, required
    This is the output dim for each convolutional layer, which is the number of ""filters""
    learned by that layer.
ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)
    This specifies both the number of convolutional layers we will create and their sizes.  The
    default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
    ngrams of size 2 to 5 with some number of filters.
conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)
    Activation to use after the convolution layers.
output_dim : `Optional[int]`, optional (default=`None`)
    After doing convolutions and pooling, we'll project the collected features into a vector of
    this size.  If this value is `None`, we will just return the result of the max pooling,
    giving an output of shape `len(ngram_filter_sizes) * num_filters`.","
    A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a
    [`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,
    input_dim)`, and the output is of shape `(batch_size, output_dim)`.

    The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
    out a vector of size num_filters. The number of times a convolution layer will be used
    is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
    outputs from the convolution layer and outputs the max.

    This operation is repeated for every ngram size passed, and consequently the dimensionality of
    the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
    (optionally) projected down to a lower dimensional output, specified by `output_dim`.

    We then use a fully connected layer to project in back to the desired output_dim.  For more
    details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
    Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

    Registered as a `Seq2VecEncoder` with name ""cnn"".

    # Parameters

    embedding_dim : `int`, required
        This is the input dimension to the encoder.  We need this because we can't do shape
        inference in pytorch, and we need to know what size filters to construct in the CNN.
    num_filters : `int`, required
        This is the output dim for each convolutional layer, which is the number of ""filters""
        learned by that layer.
    ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)
        This specifies both the number of convolutional layers we will create and their sizes.  The
        default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
        ngrams of size 2 to 5 with some number of filters.
    conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)
        Activation to use after the convolution layers.
    output_dim : `Optional[int]`, optional (default=`None`)
        After doing convolutions and pooling, we'll project the collected features into a vector of
        this size.  If this value is `None`, we will just return the result of the max pooling,
        giving an output of shape `len(ngram_filter_sizes) * num_filters`.
    , ReLU, conv_layer_%d, conv_layer_{}","0, 1, 2, 3, 4, 5, None","
A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a
[`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

Registered as a `Seq2VecEncoder` with name ""cnn"".

# Parameters

embedding_dim : `int`, required
This is the input dimension to the encoder.  We need this because we can't do shape
inference in pytorch, and we need to know what size filters to construct in the CNN.
num_filters : `int`, required
This is the output dim for each convolutional layer, which is the number of ""filters""
learned by that layer.
ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)
This specifies both the number of convolutional layers we will create and their sizes.  The
default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
ngrams of size 2 to 5 with some number of filters.
conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)
Activation to use after the convolution layers.
output_dim : `Optional[int]`, optional (default=`None`)
After doing convolutions and pooling, we'll project the collected features into a vector of
this size.  If this value is `None`, we will just return the result of the max pooling,
giving an output of shape `len(ngram_filter_sizes) * num_filters`.
, # Concatenating them gives us a tensor of shape `(batch_size, num_filters * num_conv_layers)`., # Each convolution layer returns output of size `(batch_size, num_filters, pool_length)`,, # Now we have a list of `num_conv_layers` tensors of shape `(batch_size, num_filters)`., # Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The, # `(batch_size, num_conv_layers * num_filters)`, which then gets projected using the, # convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,, # pooling is simple, we just use `torch.max`.  The resultant tensor of has shape, # projection layer, if requested., # tensor first., # then do max pooling over each filter for the whole input sequence.  Because our max, # where `pool_length = num_tokens - ngram_size + 1`.  We then do an activation function,, # where the conv layer `in_channels` is our `embedding_dim`.  We thus need to transpose the","
A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.  As a
[`Seq2VecEncoder`](./seq2vec_encoder.md), the input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

Registered as a `Seq2VecEncoder` with name ""cnn"".

# Parameters

embedding_dim : `int`, required
This is the input dimension to the encoder.  We need this because we can't do shape
inference in pytorch, and we need to know what size filters to construct in the CNN.
num_filters : `int`, required
This is the output dim for each convolutional layer, which is the number of ""filters""
learned by that layer.
ngram_filter_sizes : `Tuple[int]`, optional (default=`(2, 3, 4, 5)`)
This specifies both the number of convolutional layers we will create and their sizes.  The
default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
ngrams of size 2 to 5 with some number of filters.
conv_layer_activation : `Activation`, optional (default=`torch.nn.ReLU`)
Activation to use after the convolution layers.
output_dim : `Optional[int]`, optional (default=`None`)
After doing convolutions and pooling, we'll project the collected features into a vector of
this size.  If this value is `None`, we will just return the result of the max pooling,
giving an output of shape `len(ngram_filter_sizes) * num_filters`.
, Practitioners’"
https://github.com/hankcs/HanLP,dropout.py,0,0.0,50,72.46,6,8.7,8,11.59,5,7.25,0,0.0,5,4,15,2,,"IndependentDropout, LockedDropout, SharedDropout, ValueError, WordDropout, __init__, batch_first, dim, dropout_mask, dropout_rate, exclude, exclude_tokens, expand_as, extra_repr, float, forward, get_mask, int, item, items, len, mask, masks, max, new, new_empty, new_ones, oov_fill, oov_mask, oov_token, p, pad, padding_mask, result, scale, self, shape, size, staticmethod, sum, super, token_dropout, tokens, torch, total, training, typing, unsqueeze, x, zip","nn.Module, torch.LongTensor, torch.bool, torch.empty_like, torch.float, torch.long, torch.ones_like, torch.where, typing.List","__init__, extra_repr, forward, get_mask, token_dropout","IndependentDropout, LockedDropout, SharedDropout, WordDropout","dropout_mask, exclude_tokens, item, items, mask, masks, oov_fill, oov_mask, pad, padding_mask, result, s, scale, total, x","During training, randomly replaces some of the non-padding tokens to a mask token with probability ``p``

Adopted from https://github.com/Hyperparticle/udify

Args:
  tokens: The current batch of padded sentences with word ids
  oov_token: The mask token
  exclude_tokens: The tokens for padding the input batch
  p: The probability a word gets mapped to the unknown token
  training: Applies the dropout if set to ``True``
  tokens: torch.LongTensor: 
  oov_token: int: 
  exclude_tokens: List[int]: 
  p: float:  (Default value = 0.2)
  training: float:  (Default value = True)

Returns:
  A copy of the input batch with token dropout applied, For :math:`N` tensors, they use different dropout masks respectively.
When :math:`N-M` of them are dropped, the remaining :math:`M` ones are scaled by a factor of :math:`N/M` to compensate,
and when all of them are dropped together, zeros are returned.
Copied from https://github.com/yzhangcs/parser/master/supar/modules/dropout.py.

Args:
    p (float):
        The probability of an element to be zeroed. Default: 0.5.

Examples:
    >>> x, y = torch.ones(1, 3, 5), torch.ones(1, 3, 5)
    >>> x, y = IndependentDropout()(x, y)
    >>> x
    tensor([[[1., 1., 1., 1., 1.],
             [0., 0., 0., 0., 0.],
             [2., 2., 2., 2., 2.]]])
    >>> y
    tensor([[[1., 1., 1., 1., 1.],
             [2., 2., 2., 2., 2.],
             [0., 0., 0., 0., 0.]]])","
        For :math:`N` tensors, they use different dropout masks respectively.
        When :math:`N-M` of them are dropped, the remaining :math:`M` ones are scaled by a factor of :math:`N/M` to compensate,
        and when all of them are dropped together, zeros are returned.
        Copied from https://github.com/yzhangcs/parser/master/supar/modules/dropout.py.

        Args:
            p (float):
                The probability of an element to be zeroed. Default: 0.5.

        Examples:
            >>> x, y = torch.ones(1, 3, 5), torch.ones(1, 3, 5)
            >>> x, y = IndependentDropout()(x, y)
            >>> x
            tensor([[[1., 1., 1., 1., 1.],
                     [0., 0., 0., 0., 0.],
                     [2., 2., 2., 2., 2.]]])
            >>> y
            tensor([[[1., 1., 1., 1., 1.],
                     [2., 2., 2., 2., 2.],
                     [0., 0., 0., 0., 0.]]])
        , , batch_first=, . Only 2d (T,C) or 3d (B,T,C) is supported, During training, randomly replaces some of the non-padding tokens to a mask token with probability ``p``
        
        Adopted from https://github.com/Hyperparticle/udify

        Args:
          tokens: The current batch of padded sentences with word ids
          oov_token: The mask token
          exclude_tokens: The tokens for padding the input batch
          p: The probability a word gets mapped to the unknown token
          training: Applies the dropout if set to ``True``
          tokens: torch.LongTensor: 
          oov_token: int: 
          exclude_tokens: List[int]: 
          p: float:  (Default value = 0.2)
          training: float:  (Default value = True)

        Returns:
          A copy of the input batch with token dropout applied

        , Unsupported dim: , p=","0, 0.2, 0.5, 1, 2, 3, None, True","# -*- coding:utf-8 -*-, # Create a uniformly random mask selecting either the original words or OOV tokens, # Date: 2020-06-05 17:47, # This creates a mask that only considers unpadded tokens for mapping to oov, During training, randomly replaces some of the non-padding tokens to a mask token with probability ``p``

Adopted from https://github.com/Hyperparticle/udify

Args:
tokens: The current batch of padded sentences with word ids
oov_token: The mask token
exclude_tokens: The tokens for padding the input batch
p: The probability a word gets mapped to the unknown token
training: Applies the dropout if set to ``True``
tokens: torch.LongTensor:
oov_token: int:
exclude_tokens: List[int]:
p: float:  (Default value = 0.2)
training: float:  (Default value = True)

Returns:
A copy of the input batch with token dropout applied

",
https://github.com/hankcs/HanLP,feedforward.py,0,0.0,37,77.08,5,10.42,3,6.25,3,6.25,0,0.0,4,1,11,2,,"FeedForward, ValueError, __init__, _activations, _dropout, _linear_layers, _output_dim, activations, append, dropout, dropout_layers, float, forward, get_input_dim, get_output_dim, hanlp, hidden_dims, input_dim, input_dims, inputs, int, isinstance, layer, layer_input_dim, layer_output_dim, len, linear_layers, list, num_layers, output, self, str, super, torch, typing, value, zip","hanlp.utils.torch_util.activation_from_name, torch.Tensor, torch.nn.Dropout, torch.nn.Linear, torch.nn.Module, torch.nn.ModuleList, typing.List, typing.Union","__init__, forward, get_input_dim, get_output_dim",FeedForward,"activations, dropout, dropout_layers, hidden_dims, input_dims, layer, layer_input_dim, layer_output_dim, linear_layers, output, value","A feed-forward neural network., This `Module` is a feed-forward neural network, just a sequence of `Linear` layers with
activation functions in between.

# Parameters

input_dim : `int`, required
    The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.
num_layers : `int`, required
    The number of `Linear` layers to apply to the input.
hidden_dims : `Union[int, List[int]]`, required
    The output dimension of each of the `Linear` layers.  If this is a single `int`, we use
    it for all `Linear` layers.  If it is a `List[int]`, `len(hidden_dims)` must be
    `num_layers`.
activations : `Union[Activation, List[Activation]]`, required
    The activation function to use after each `Linear` layer.  If this is a single function,
    we use it after all `Linear` layers.  If it is a `List[Activation]`,
    `len(activations)` must be `num_layers`. Activation must have torch.nn.Module type.
dropout : `Union[float, List[float]]`, optional (default = `0.0`)
    If given, we will apply this amount of dropout after each layer.  Semantics of `float`
    versus `List[float]` is the same as with other parameters.

# Examples

```python
FeedForward(124, 2, [64, 32], torch.nn.ReLU(), 0.2)
#> FeedForward(
#>   (_activations): ModuleList(
#>     (0): ReLU()
#>     (1): ReLU()
#>   )
#>   (_linear_layers): ModuleList(
#>     (0): Linear(in_features=124, out_features=64, bias=True)
#>     (1): Linear(in_features=64, out_features=32, bias=True)
#>   )
#>   (_dropout): ModuleList(
#>     (0): Dropout(p=0.2, inplace=False)
#>     (1): Dropout(p=0.2, inplace=False)
#>   )
#> )
```","
    This `Module` is a feed-forward neural network, just a sequence of `Linear` layers with
    activation functions in between.

    # Parameters

    input_dim : `int`, required
        The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.
    num_layers : `int`, required
        The number of `Linear` layers to apply to the input.
    hidden_dims : `Union[int, List[int]]`, required
        The output dimension of each of the `Linear` layers.  If this is a single `int`, we use
        it for all `Linear` layers.  If it is a `List[int]`, `len(hidden_dims)` must be
        `num_layers`.
    activations : `Union[Activation, List[Activation]]`, required
        The activation function to use after each `Linear` layer.  If this is a single function,
        we use it after all `Linear` layers.  If it is a `List[Activation]`,
        `len(activations)` must be `num_layers`. Activation must have torch.nn.Module type.
    dropout : `Union[float, List[float]]`, optional (default = `0.0`)
        If given, we will apply this amount of dropout after each layer.  Semantics of `float`
        versus `List[float]` is the same as with other parameters.

    # Examples

    ```python
    FeedForward(124, 2, [64, 32], torch.nn.ReLU(), 0.2)
    #> FeedForward(
    #>   (_activations): ModuleList(
    #>     (0): ReLU()
    #>     (1): ReLU()
    #>   )
    #>   (_linear_layers): ModuleList(
    #>     (0): Linear(in_features=124, out_features=64, bias=True)
    #>     (1): Linear(in_features=64, out_features=32, bias=True)
    #>   )
    #>   (_dropout): ModuleList(
    #>     (0): Dropout(p=0.2, inplace=False)
    #>     (1): Dropout(p=0.2, inplace=False)
    #>   )
    #> )
    ```
    , 
A feed-forward neural network.
, len(activations) (%d) != num_layers (%d), len(dropout) (%d) != num_layers (%d), len(hidden_dims) (%d) != num_layers (%d)","0.0, 1, None","
A feed-forward neural network.
, 
This `Module` is a feed-forward neural network, just a sequence of `Linear` layers with
activation functions in between.

# Parameters

input_dim : `int`, required
The dimensionality of the input.  We assume the input has shape `(batch_size, input_dim)`.
num_layers : `int`, required
The number of `Linear` layers to apply to the input.
hidden_dims : `Union[int, List[int]]`, required
The output dimension of each of the `Linear` layers.  If this is a single `int`, we use
it for all `Linear` layers.  If it is a `List[int]`, `len(hidden_dims)` must be
`num_layers`.
activations : `Union[Activation, List[Activation]]`, required
The activation function to use after each `Linear` layer.  If this is a single function,
we use it after all `Linear` layers.  If it is a `List[Activation]`,
`len(activations)` must be `num_layers`. Activation must have torch.nn.Module type.
dropout : `Union[float, List[float]]`, optional (default = `0.0`)
If given, we will apply this amount of dropout after each layer.  Semantics of `float`
versus `List[float]` is the same as with other parameters.

# Examples

```python
FeedForward(124, 2, [64, 32], torch.nn.ReLU(), 0.2)
#> FeedForward(
#>   (_activations): ModuleList(
#>     (0): ReLU()
#>     (1): ReLU()
#>   )
#>   (_linear_layers): ModuleList(
#>     (0): Linear(in_features=124, out_features=64, bias=True)
#>     (1): Linear(in_features=64, out_features=32, bias=True)
#>   )
#>   (_dropout): ModuleList(
#>     (0): Dropout(p=0.2, inplace=False)
#>     (1): Dropout(p=0.2, inplace=False)
#>   )
#> )
```
, # type: ignore",
https://github.com/hankcs/HanLP,feed_forward.py,0,0.0,14,73.68,0,0.0,2,10.53,3,15.79,0,0.0,1,1,0,0,,"FeedForward, __init__, activations, dropout, float, hanlp, hidden_dims, input_dim, int, locals, num_layers, self, super, typing","hanlp.common.structure.ConfigTracker, hanlp.layers.feedforward, typing.List, typing.Union",__init__,FeedForward,,,,"0.0, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-06 14:37",
https://github.com/hankcs/HanLP,scalar_mix.py,0,0.0,48,49.48,8,8.25,10,10.31,31,31.96,0,0.0,4,2,17,3,,"ScalarMixWithDropout, ScalarMixWithDropoutBuilder, ValueError, __init__, _do_layer_norm, append, bool, broadcast_mask, build, config, do_layer_norm, dropout, dropout_fill, dropout_mask, dropout_value, float, forward, gamma, hanlp, initial_scalar_parameters, input_dim, int, len, locals, mask, mask_float, mean, mixture_range, mixture_size, normed_weights, num_elements_not_masked, pieces, register_buffer, scalar_parameters, self, super, tensor, tensor_masked, tensors, torch, trainable, typing, uniform_, unsqueeze, variance, weight, weights, zip","hanlp.common.structure.ConfigTracker, torch.FloatTensor, torch.Tensor, torch.einsum, torch.empty, torch.nn.Module, torch.nn.Parameter, torch.nn.ParameterList, torch.nn.functional.softmax, torch.split, torch.sqrt, torch.sum, torch.where, torch.zeros, typing.List, typing.Tuple","__init__, _do_layer_norm, build, forward","ScalarMixWithDropout, ScalarMixWithDropoutBuilder","broadcast_mask, dropout_fill, dropout_mask, initial_scalar_parameters, input_dim, mask_float, mean, mixture_size, normed_weights, num_elements_not_masked, pieces, tensor, tensor_masked, tensors, variance, weight, weights","Compute a weighted average of the ``tensors``.  The input tensors an be any shape
with at least two dimensions, but must all be the same shape.

When ``do_layer_norm=True``, the ``mask`` is required input.  If the ``tensors`` are
dimensioned  ``(dim_0, ..., dim_{n-1}, dim_n)``, then the ``mask`` is dimensioned
``(dim_0, ..., dim_{n-1})``, as in the typical case with ``tensors`` of shape
``(batch_size, timesteps, dim)`` and ``mask`` of shape ``(batch_size, timesteps)``.

When ``do_layer_norm=False`` the ``mask`` is ignored.

Args:
  tensors: List[torch.Tensor]: 
  # pylint: disable:  (Default value = arguments-differmask: torch.Tensor = None)

Returns:, Computes a parameterised scalar mixture of N tensors, ``mixture = gamma * sum(s_k * tensor_k)``
where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.

If ``do_layer_norm=True`` then apply layer normalization to each tensor before weighting.

If ``dropout > 0``, then for each scalar weight, adjust its softmax weight mass to 0 with
the dropout probability (i.e., setting the unnormalized weight to -inf). This effectively
should redistribute dropped probability mass to all other weights.

Args:

Returns:, The dot-product ""Layer Attention"" that is applied to the layers of BERT, along with layer dropout to reduce overfitting","
The dot-product ""Layer Attention"" that is applied to the layers of BERT, along with layer dropout to reduce overfitting
, Compute a weighted average of the ``tensors``.  The input tensors an be any shape
        with at least two dimensions, but must all be the same shape.
        
        When ``do_layer_norm=True``, the ``mask`` is required input.  If the ``tensors`` are
        dimensioned  ``(dim_0, ..., dim_{n-1}, dim_n)``, then the ``mask`` is dimensioned
        ``(dim_0, ..., dim_{n-1})``, as in the typical case with ``tensors`` of shape
        ``(batch_size, timesteps, dim)`` and ``mask`` of shape ``(batch_size, timesteps)``.
        
        When ``do_layer_norm=False`` the ``mask`` is ignored.

        Args:
          tensors: List[torch.Tensor]: 
          # pylint: disable:  (Default value = arguments-differmask: torch.Tensor = None)

        Returns:

        , Computes a parameterised scalar mixture of N tensors, ``mixture = gamma * sum(s_k * tensor_k)``
    where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.
    
    If ``do_layer_norm=True`` then apply layer normalization to each tensor before weighting.
    
    If ``dropout > 0``, then for each scalar weight, adjust its softmax weight mass to 0 with
    the dropout probability (i.e., setting the unnormalized weight to -inf). This effectively
    should redistribute dropped probability mass to all other weights.

    Args:

    Returns:

    , Length of initial_scalar_parameters {} differs from mixture_size {}, dropout_fill, dropout_mask, i,ijkl->jkl, {} tensors were passed, but the module was initialized to mix {} tensors.","0, 0.0, 1, 1.0, 1e+20, 1e-12, 2, False, None, True","
The dot-product ""Layer Attention"" that is applied to the layers of BERT, along with layer dropout to reduce overfitting
, #, #                requires_grad=trainable) for i, #      in range(mixture_size)]), #     [Parameter(torch.FloatTensor([initial_scalar_parameters[i]]),, #     pieces.append(weight * tensor), # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2019 Dan Kondratyuk, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # This file is modified from udify, which is licensed under the MIT license:, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # for weight, tensor in zip(normed_weights, tensors):, # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # pieces = [], # pylint: disable=arguments-differ, # return self.gamma * sum(pieces), # self.scalar_parameters = ParameterList(, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell, Compute a weighted average of the ``tensors``.  The input tensors an be any shape
with at least two dimensions, but must all be the same shape.

When ``do_layer_norm=True``, the ``mask`` is required input.  If the ``tensors`` are
dimensioned  ``(dim_0, ..., dim_{n-1}, dim_n)``, then the ``mask`` is dimensioned
``(dim_0, ..., dim_{n-1})``, as in the typical case with ``tensors`` of shape
``(batch_size, timesteps, dim)`` and ``mask`` of shape ``(batch_size, timesteps)``.

When ``do_layer_norm=False`` the ``mask`` is ignored.

Args:
tensors: List[torch.Tensor]:
# pylint: disable:  (Default value = arguments-differmask: torch.Tensor = None)

Returns:

, Computes a parameterised scalar mixture of N tensors, ``mixture = gamma * sum(s_k * tensor_k)``
where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.

If ``do_layer_norm=True`` then apply layer normalization to each tensor before weighting.

If ``dropout > 0``, then for each scalar weight, adjust its softmax weight mass to 0 with
the dropout probability (i.e., setting the unnormalized weight to -inf). This effectively
should redistribute dropped probability mass to all other weights.

Args:

Returns:

",
https://github.com/hankcs/HanLP,time_distributed.py,0,0.0,33,70.21,4,8.51,3,6.38,7,14.89,0,0.0,3,1,12,2,,"RuntimeError, TimeDistributed, __init__, _module, _reshape_tensor, contiguous, forward, input_size, input_tensor, inputs, isinstance, items, key, kwargs, len, list, module, new_size, outputs, pass_through, reshaped_inputs, reshaped_kwargs, reshaped_outputs, self, size, some_input, squashed_shape, staticmethod, str, super, torch, typing, value","torch.Tensor, torch.nn.Module, typing.List","__init__, _reshape_tensor, forward",TimeDistributed,"input_size, input_tensor, key, new_size, outputs, pass_through, reshaped_inputs, reshaped_kwargs, reshaped_outputs, some_input, squashed_shape, value","A wrapper that unrolls the second (time) dimension of a tensor
into the first (batch) dimension, applies some other `Module`,
and then rolls the time dimension back up., Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes
inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be
`(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.

Note that while the above gives shapes with `batch_size` first, this `Module` also works if
`batch_size` is second - we always just combine the first two dimensions, then split them.

It also reshapes keyword arguments unless they are not tensors or their name is specified in
the optional `pass_through` iterable.","
    Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes
    inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be
    `(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.

    Note that while the above gives shapes with `batch_size` first, this `Module` also works if
    `batch_size` is second - we always just combine the first two dimensions, then split them.

    It also reshapes keyword arguments unless they are not tensors or their name is specified in
    the optional `pass_through` iterable.
    , 
A wrapper that unrolls the second (time) dimension of a tensor
into the first (batch) dimension, applies some other `Module`,
and then rolls the time dimension back up.
, No dimension to distribute: , No input tensor to time-distribute","1, 2, None","
A wrapper that unrolls the second (time) dimension of a tensor
into the first (batch) dimension, applies some other `Module`,
and then rolls the time dimension back up.
, 
Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes
inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be
`(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.

Note that while the above gives shapes with `batch_size` first, this `Module` also works if
`batch_size` is second - we always just combine the first two dimensions, then split them.

It also reshapes keyword arguments unless they are not tensors or their name is specified in
the optional `pass_through` iterable.
, # (batch_size * time_steps, **input_size)., # (batch_size, time_steps, **output_size), # Need some input to then get the batch_size and time_steps., # Now get the output back into the right shape., # Squash batch_size and time_steps into a single axis; result has shape",
https://github.com/hankcs/HanLP,weight_normalization.py,0,0.0,68,56.67,22,18.33,7,5.83,23,19.17,0,0.0,10,1,18,6,,"ValueError, WeightNormalization, __future__, __init__, _data_dep_init, _do_nothing, _init_critical_section, _init_norm, _initialize_weights, _initialized, _naked_clone_layer, _track_trackable, _update_weights, activation, add_weight, append, assign, assign_tensors, base_config, bias, bias_tensor, build, built, call, cell, compute_output_shape, config, data_init, data_norm_axes, dict, dtype, execute, g, g_tensor, get_config, get_weights, hanlp, hasattr, input_shape, input_spec, inputs, is_rnn, isinstance, items, kernel, kernel_layer, kernel_norm_axes, kwargs, layer, layer_config, layer_depth, list, m_init, outputs, range, rank, scale_init, self, set_weights, shape, super, tensorflow, update_kernel, v, v_flat, v_init, v_norm, x_init","__future__.absolute_import, __future__.division, __future__.print_function, hanlp.utils.tf_util.hanlp_register, tf.CriticalSection, tf.TensorShape, tf.cond, tf.control_dependencies, tf.debugging.assert_equal, tf.dtypes.bool, tf.identity, tf.keras.layers.InputSpec, tf.keras.layers.RNN, tf.keras.layers.Wrapper, tf.keras.layers.deserialize, tf.keras.layers.serialize, tf.linalg.norm, tf.math.sqrt, tf.name_scope, tf.nn.l2_normalize, tf.nn.moments, tf.reshape","__init__, _data_dep_init, _do_nothing, _init_norm, _initialize_weights, _update_weights, build, call, compute_output_shape, get_config",WeightNormalization,"assign_tensors, base_config, bias_tensor, config, data_norm_axes, g, g_tensor, input_shape, kernel_layer, layer_config, m_init, outputs, scale_init, update_kernel, v_flat, v_init, v_norm, x_init","Build `Layer`

Args:
  input_shape: 

Returns:, Call `Layer`

Args:
  inputs: 

Returns:, Data dependent initialization.

Args:
  inputs: 

Returns:, Initialize weight g.

The initial value of g could either from the initial value in v,
or by the input value if self.data_init is True.

Args:
  inputs: 

Returns:, Set the weight g with the norm of the weight vector., This wrapper reparameterizes a layer by decoupling the weight's
magnitude and direction.

This speeds up convergence by improving the
conditioning of the optimization problem.
Weight Normalization: A Simple Reparameterization to Accelerate
Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868
Tim Salimans, Diederik P. Kingma (2016)
WeightNormalization wrapper works for keras and tf layers.
```python
  net = WeightNormalization(
      tf.keras.layers.Conv2D(2, 2, activation='relu'),
      input_shape=(32, 32, 3),
      data_init=True)(x)
  net = WeightNormalization(
      tf.keras.layers.Conv2D(16, 5, activation='relu'),
      data_init=True)(net)
  net = WeightNormalization(
      tf.keras.layers.Dense(120, activation='relu'),
      data_init=True)(net)
  net = WeightNormalization(
      tf.keras.layers.Dense(n_classes),
      data_init=True)(net)
```

Args:
  layer: a layer instance
  data_init: If

Returns:

Raises:
  ValueError: If not initialized with a
  ValueError: If
  NotImplementedError: If","Build `Layer`

        Args:
          input_shape: 

        Returns:

        , Call `Layer`

        Args:
          inputs: 

        Returns:

        , Data dependent initialization.

        Args:
          inputs: 

        Returns:

        , Initialize weight g.
        
        The initial value of g could either from the initial value in v,
        or by the input value if self.data_init is True.

        Args:
          inputs: 

        Returns:

        , Set the weight g with the norm of the weight vector., The layer has been initialized., This wrapper reparameterizes a layer by decoupling the weight's
    magnitude and direction.
    
    This speeds up convergence by improving the
    conditioning of the optimization problem.
    Weight Normalization: A Simple Reparameterization to Accelerate
    Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868
    Tim Salimans, Diederik P. Kingma (2016)
    WeightNormalization wrapper works for keras and tf layers.
    ```python
      net = WeightNormalization(
          tf.keras.layers.Conv2D(2, 2, activation='relu'),
          input_shape=(32, 32, 3),
          data_init=True)(x)
      net = WeightNormalization(
          tf.keras.layers.Conv2D(16, 5, activation='relu'),
          data_init=True)(net)
      net = WeightNormalization(
          tf.keras.layers.Dense(120, activation='relu'),
          data_init=True)(net)
      net = WeightNormalization(
          tf.keras.layers.Dense(n_classes),
          data_init=True)(net)
    ```

    Args:
      layer: a layer instance
      data_init: If

    Returns:

    Raises:
      ValueError: If not initialized with a
      ValueError: If
      NotImplementedError: If

    , `WeightNormalization` must wrap a layer that contains a `kernel` for weights, bias, compute_weights, config, data_dep_init, data_init, g, init_mutex, init_norm, initialized, kernel, layer, ones, trainable, zeros","0, 1, 1.0, 1e-10, False, None, True","#, #     http://www.apache.org/licenses/LICENSE-2.0, # =============================================================================, # Copyright 2019 The TensorFlow Authors. All Rights Reserved., # Ensure we calculate result after updating kernel., # Ensure we read `self.g` after _update_weights., # Licensed under the Apache License, Version 2.0 (the ""License"");, # Replace kernel by normalized weight variable., # See the License for the specific language governing permissions and, # The kernel's filter or unit dimension is -1, # Unless required by applicable law or agreed to in writing, software, # Used for data initialization in self._data_dep_init., # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # distributed under the License is distributed on an ""AS IS"" BASIS,, # limitations under the License., # pylint: disable=bad-continuation, # you may not use this file except in compliance with the License., Build `Layer`

Args:
input_shape:

Returns:

, Call `Layer`

Args:
inputs:

Returns:

, Initialize weight g.

The initial value of g could either from the initial value in v,
or by the input value if self.data_init is True.

Args:
inputs:

Returns:

, Set the weight g with the norm of the weight vector.
with tf.name_scope('init_norm'):
v_flat = tf.reshape(self.v, [-1, self.layer_depth])
v_norm = tf.linalg.norm(v_flat, axis=0)
g_tensor = self.g.assign(tf.reshape(v_norm, (self.layer_depth,)))
return [g_tensor]

def _data_dep_init(self, inputs):
Data dependent initialization., This wrapper reparameterizes a layer by decoupling the weight's
magnitude and direction.

This speeds up convergence by improving the
conditioning of the optimization problem.
Weight Normalization: A Simple Reparameterization to Accelerate
Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868
Tim Salimans, Diederik P. Kingma (2016)
WeightNormalization wrapper works for keras and tf layers.
```python
net = WeightNormalization(
tf.keras.layers.Conv2D(2, 2, activation='relu'),
input_shape=(32, 32, 3),
data_init=True)(x)
net = WeightNormalization(
tf.keras.layers.Conv2D(16, 5, activation='relu'),
data_init=True)(net)
net = WeightNormalization(
tf.keras.layers.Dense(120, activation='relu'),
data_init=True)(net)
net = WeightNormalization(
tf.keras.layers.Dense(n_classes),
data_init=True)(net)
```

Args:
layer: a layer instance
data_init: If

Returns:

Raises:
ValueError: If not initialized with a
ValueError: If
NotImplementedError: If

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-26 00:50",
https://github.com/hankcs/HanLP,sparse_categorical_crossentropy.py,0,0.0,21,65.62,1,3.12,3,9.38,7,21.88,0,0.0,3,3,4,0,,"MaskedSparseCategoricalCrossentropyOverBatchFirstDim, SparseCategoricalCrossentropyOverBatchFirstDim, SparseCategoricalCrossentropyOverNonzeroWeights, __call__, __init__, __name__, active_labels, active_logits, active_loss, get_config, hanlp, loss, mask_value, object, sample_weight, self, super, tensorflow, type, y_pred, y_true","hanlp.utils.tf_util.hanlp_register, tf.boolean_mask, tf.cast, tf.float32, tf.keras.losses.sparse_categorical_crossentropy, tf.not_equal, tf.reduce_sum, tf.shape","__call__, __init__, get_config","MaskedSparseCategoricalCrossentropyOverBatchFirstDim, SparseCategoricalCrossentropyOverBatchFirstDim, SparseCategoricalCrossentropyOverNonzeroWeights","active_labels, active_logits, active_loss, loss",,"the mask will be computed via y_true != mask_value, it might conflict with sample_weight","0, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-20 01:29, # This is equivalent to SUM_OVER_BATCH_SIZE, # This one is SUM_BY_NONZERO_WEIGHTS, # could use sum of sample_weight[:,0] too, # loss /= tf.reduce_sum(tf.ones_like(sample_weight, dtype=loss.dtype))",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-20 01:28",
https://github.com/hankcs/HanLP,accuracy.py,0,0.0,56,53.85,14,13.46,6,5.77,28,26.92,0,0.0,7,2,13,6,,"BooleanAccuracy, CategoricalAccuracy, ValueError, __call__, __init__, __repr__, _correct_count, _tie_break, _top_k, _total_count, accuracy, batch_size, bool, correct, correct_count, detach, detach_tensors, device, dim, eq, float, get_metric, gold_labels, hanlp, int, isinstance, keep, mask, max, max_predictions, max_predictions_mask, min, num_classes, numel, predictions, property, reset, score, self, shape, size, staticmethod, str, sum, tensors, tie_break, tie_counts, top_k, topk, torch, total_count, typing, unsqueeze, unsqueeze_, view, x","hanlp.metrics.metric.Metric, torch.BoolTensor, torch.Tensor, torch.arange, torch.ones, typing.Iterable, typing.Optional","__call__, __init__, __repr__, detach_tensors, get_metric, reset, score","BooleanAccuracy, CategoricalAccuracy","accuracy, batch_size, correct, gold_labels, keep, mask, max_predictions, max_predictions_mask, num_classes, predictions, tie_counts, top_k, x","# Parameters

predictions : `torch.Tensor`, required.
    A tensor of predictions of shape (batch_size, ...).
gold_labels : `torch.Tensor`, required.
    A tensor of the same shape as `predictions`.
mask : `torch.BoolTensor`, optional (default = `None`).
    A tensor of the same shape as `predictions`., # Parameters

predictions : `torch.Tensor`, required.
    A tensor of predictions of shape (batch_size, ..., num_classes).
gold_labels : `torch.Tensor`, required.
    A tensor of integer class label of shape (batch_size, ...). It must be the same
    shape as the `predictions` tensor without the `num_classes` dimension.
mask : `torch.BoolTensor`, optional (default = `None`).
    A masking tensor the same size as `gold_labels`., # Returns

The accumulated accuracy., Categorical Top-K accuracy. Assumes integer labels, with
each item to be classified having a single correct class.
Tie break enables equal distribution of scores among the
classes with same maximum predicted scores.
Copied from AllenNLP and added several methods., If you actually passed gradient-tracking Tensors to a Metric, there will be
a huge memory leak, because it will prevent garbage collection for the computation
graph. This method ensures the tensors are detached., Just checks batch-equality of two tensors and computes an accuracy metric based on that.
That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that
as a set of `batch_size` predictions and checks that each is *entirely* correct across the remaining dims.
This means the denominator in the accuracy computation is `batch_size`, with the caveat that predictions
that are totally masked are ignored (in which case the denominator is the number of predictions that have
at least one unmasked element).

This is similar to [`CategoricalAccuracy`](./categorical_accuracy.md), if you've already done a `.max()`
on your predictions.  If you have categorical output, though, you should typically just use
`CategoricalAccuracy`.  The reason you might want to use this instead is if you've done
some kind of constrained inference and don't have a prediction tensor that matches the API of
`CategoricalAccuracy`, which assumes a final dimension of size `num_classes`.","
        # Parameters

        predictions : `torch.Tensor`, required.
            A tensor of predictions of shape (batch_size, ...).
        gold_labels : `torch.Tensor`, required.
            A tensor of the same shape as `predictions`.
        mask : `torch.BoolTensor`, optional (default = `None`).
            A tensor of the same shape as `predictions`.
        , 
        # Parameters

        predictions : `torch.Tensor`, required.
            A tensor of predictions of shape (batch_size, ..., num_classes).
        gold_labels : `torch.Tensor`, required.
            A tensor of integer class label of shape (batch_size, ...). It must be the same
            shape as the `predictions` tensor without the `num_classes` dimension.
        mask : `torch.BoolTensor`, optional (default = `None`).
            A masking tensor the same size as `gold_labels`.
        , 
        # Returns

        The accumulated accuracy.
        , 
        If you actually passed gradient-tracking Tensors to a Metric, there will be
        a huge memory leak, because it will prevent garbage collection for the computation
        graph. This method ensures the tensors are detached.
        , 
    Categorical Top-K accuracy. Assumes integer labels, with
    each item to be classified having a single correct class.
    Tie break enables equal distribution of scores among the
    classes with same maximum predicted scores.
    Copied from AllenNLP and added several methods.
    , 
    Just checks batch-equality of two tensors and computes an accuracy metric based on that.
    That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that
    as a set of `batch_size` predictions and checks that each is *entirely* correct across the remaining dims.
    This means the denominator in the accuracy computation is `batch_size`, with the caveat that predictions
    that are totally masked are ignored (in which case the denominator is the number of predictions that have
    at least one unmasked element).

    This is similar to [`CategoricalAccuracy`](./categorical_accuracy.md), if you've already done a `.max()`
    on your predictions.  If you have categorical output, though, you should typically just use
    `CategoricalAccuracy`.  The reason you might want to use this instead is if you've done
    some kind of constrained inference and don't have a prediction tensor that matches the API of
    `CategoricalAccuracy`, which assumes a final dimension of size `num_classes`.
    , .2%, A gold label passed to Categorical Accuracy contains an id >= {}, the number of classes., Accuracy:, Tie break in Categorical Accuracy can be done only for maximum (top_k = 1), gold_labels must have dimension == predictions.size() - 1 but found tensor of shape: {}, gold_labels must have shape == predictions.size() but found tensor of shape: , mask must have shape == predictions.size() but found tensor of shape: , top_k passed to Categorical Accuracy must be > 0","0, 0.0, 1, 1e-12, False, None","
# Parameters

predictions : `torch.Tensor`, required.
A tensor of predictions of shape (batch_size, ...).
gold_labels : `torch.Tensor`, required.
A tensor of the same shape as `predictions`.
mask : `torch.BoolTensor`, optional (default = `None`).
A tensor of the same shape as `predictions`.
, 
# Parameters

predictions : `torch.Tensor`, required.
A tensor of predictions of shape (batch_size, ..., num_classes).
gold_labels : `torch.Tensor`, required.
A tensor of integer class label of shape (batch_size, ...). It must be the same
shape as the `predictions` tensor without the `num_classes` dimension.
mask : `torch.BoolTensor`, optional (default = `None`).
A masking tensor the same size as `gold_labels`.
, 
# Returns

The accumulated accuracy.
, 
Categorical Top-K accuracy. Assumes integer labels, with
each item to be classified having a single correct class.
Tie break enables equal distribution of scores among the
classes with same maximum predicted scores.
Copied from AllenNLP and added several methods.
, 
If you actually passed gradient-tracking Tensors to a Metric, there will be
a huge memory leak, because it will prevent garbage collection for the computation
graph. This method ensures the tensors are detached.
, 
Just checks batch-equality of two tensors and computes an accuracy metric based on that.
That is, if your prediction has shape (batch_size, dim_1, ..., dim_n), this metric considers that
as a set of `batch_size` predictions and checks that each is *entirely* correct across the remaining dims.
This means the denominator in the accuracy computation is `batch_size`, with the caveat that predictions
that are totally masked are ignored (in which case the denominator is the number of predictions that have
at least one unmasked element).

This is similar to [`CategoricalAccuracy`](./categorical_accuracy.md), if you've already done a `.max()`
on your predictions.  If you have categorical output, though, you should typically just use
`CategoricalAccuracy`.  The reason you might want to use this instead is if you've done
some kind of constrained inference and don't have a prediction tensor that matches the API of
`CategoricalAccuracy`, which assumes a final dimension of size `num_classes`.
, # -*- coding:utf-8 -*-, # At this point, predictions is (batch_size, rest_of_dims_combined),, # Author: hankcs, # Because of how we're handling masking, masked positions are automatically ""correct""., # Check if it's actually a tensor in case something else was passed., # Date: 2020-06-12 17:56, # For each row check if index pointed by gold_label is was 1 or not (among max scored classes), # Since masked positions are correct, we need to explicitly exclude instance predictions, # Some sanity checks., # Special case topk == 1, because it's common and .max() is much faster than .topk()., # This is of shape (batch_size, ..., top_k)., # Top K indexes of the predictions (or fewer, if there aren't K of them)., # We can multiply by the mask up front, because we're just checking equality below, and, # We want to skip predictions that are completely masked;, # and 0 if at least one element of the instance prediction is wrong., # ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions, # max_predictions_mask is (rows X num_classes) and gold_labels is (batch_size), # prediction is correct if gold label falls on any of the max scores. distribute score by tie_counts, # so .eq -> .prod will be 1 if every element of the instance prediction is correct, # so we'll keep predictions that aren't., # this way everything that's masked will be equal., # where the entire prediction is masked (because they look ""correct"").",
https://github.com/hankcs/HanLP,f1.py,0,0.0,25,67.57,4,10.81,5,13.51,3,8.11,0,0.0,6,2,6,0,,"F1, F1_, NotImplementedError, __call__, __init__, __repr__, abc, f, gold, hanlp, len, nb_correct, nb_pred, nb_true, p, pred, prf, property, r, reset, score, self, set, str, super","abc.ABC, hanlp.metrics.metric.Metric","__call__, __init__, __repr__, prf, reset, score","F1, F1_","f, nb_correct, nb_pred, nb_true, p, r",," F1: ,  R: , .2%, P: ","0, 0.0, 1, 2, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-10 14:55",
https://github.com/hankcs/HanLP,metric.py,0,0.0,16,76.19,1,4.76,1,4.76,3,14.29,0,0.0,11,1,0,0,,"Metric, __call__, __eq__, __float__, __ge__, __gt__, __le__, __lt__, __ne__, __repr__, abc, other, property, reset, score, self","abc.ABC, abc.abstractmethod","__call__, __eq__, __float__, __ge__, __gt__, __le__, __lt__, __ne__, __repr__, reset, score",Metric,,,:.4f,None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-03 11:35",
https://github.com/hankcs/HanLP,mtl.py,0,0.0,34,57.63,16,27.12,4,6.78,5,8.47,0,0.0,5,1,12,0,,"MetricDict, _, _COLORS, __call__, __repr__, _level, child_is_dict, color, cstr, dict, enumerate, float, gold, hanlp, idx, isinstance, items, k, lb, len, level, min, pred, prefix, property, rb, reset, score, self, str, sum, v, values, x",hanlp.metrics.metric.Metric,"__call__, __repr__, cstr, reset, score",MetricDict,"_, _COLORS, _level, child_is_dict, color, idx, k, lb, prefix, rb, v, x",,",  ,  [/, (, ), [, [/, [/underline][/bold], [bold][underline], ], cyan, green, magenta, yellow, {[(, }])","0, 1, 2, None","#     _level = 2, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-03 00:16, # if level != 0 and not child_is_dict:",
https://github.com/hankcs/HanLP,spearman_correlation.py,0,0.0,33,60.0,6,10.91,7,12.73,9,16.36,0,0.0,7,1,9,3,,"NotImplemented, SpearmanCorrelation, __call__, __init__, __str__, _get_ranks, argsort, device, down, gold_labels, hanlp, len, mask, predictions, property, ranks, reset, reshape, score, self, size, spearman_correlation, str, super, to, torch, total_gold_labels, total_predictions, upper, x, x_rank, y, y_rank","hanlp.metrics.metric.Metric, torch.Tensor, torch.arange, torch.cat, torch.sum, torch.zeros, torch.zeros_like","__call__, __init__, __str__, _get_ranks, reset, score, spearman_correlation",SpearmanCorrelation,"argsort, down, gold_labels, n, predictions, ranks, upper, x_rank, y_rank","# Parameters

predictions : `torch.Tensor`, required.
    A tensor of predictions of shape (batch_size, ...).
gold_labels : `torch.Tensor`, required.
    A tensor of the same shape as `predictions`., Compute correlation between 2 1-D vectors. Adopted from
https://discuss.pytorch.org/t/spearmans-correlation/91931/5

Args:
    x: Shape (N, )
    y: Shape (N, ), This `Metric` calculates the sample Spearman correlation coefficient (r)
between two tensors. Each element in the two tensors is assumed to be
a different observation of the variable (i.e., the input tensors are
implicitly flattened into vectors and the correlation is calculated
between the vectors).

<https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>","
        # Parameters

        predictions : `torch.Tensor`, required.
            A tensor of predictions of shape (batch_size, ...).
        gold_labels : `torch.Tensor`, required.
            A tensor of the same shape as `predictions`.
        , 
    This `Metric` calculates the sample Spearman correlation coefficient (r)
    between two tensors. Each element in the two tensors is assumed to be
    a different observation of the variable (i.e., the input tensors are
    implicitly flattened into vectors and the correlation is calculated
    between the vectors).

    <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>
    , .2f, Compute correlation between 2 1-D vectors. Adopted from
    https://discuss.pytorch.org/t/spearmans-correlation/91931/5

    Args:
        x: Shape (N, )
        y: Shape (N, )

    , mask not supported in SpearmanCorrelation for now., spearman: ","0, 1, 1.0, 100, 2, 6, None","
# Parameters

predictions : `torch.Tensor`, required.
A tensor of predictions of shape (batch_size, ...).
gold_labels : `torch.Tensor`, required.
A tensor of the same shape as `predictions`.
, 
This `Metric` calculates the sample Spearman correlation coefficient (r)
between two tensors. Each element in the two tensors is assumed to be
a different observation of the variable (i.e., the input tensors are
implicitly flattened into vectors and the correlation is calculated
between the vectors).

<https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-23 16:12, # Flatten predictions, gold_labels, and mask. We calculate the Spearman correlation between, # the vectors, since each element in the predictions and gold_labels tensor is assumed, # to be a separate observation., Compute correlation between 2 1-D vectors. Adopted from
https://discuss.pytorch.org/t/spearmans-correlation/91931/5

Args:
x: Shape (N, )
y: Shape (N, )

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-09-14 21:55",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-11 18:44",
https://github.com/hankcs/HanLP,amr.py,0,0.0,6,27.27,8,36.36,0,0.0,8,36.36,0,0.0,0,0,5,0,,"ALL, AMR3_GRAPH_PRETRAIN_PARSER, AMR3_SEQ2SEQ_BART_LARGE, MRP2020_AMR_ENG_ZHO_XLM_BASE, MRP2020_AMR_ZHO_MENGZI_BASE, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, AMR3_GRAPH_PRETRAIN_PARSER, AMR3_SEQ2SEQ_BART_LARGE, MRP2020_AMR_ENG_ZHO_XLM_BASE, MRP2020_AMR_ZHO_MENGZI_BASE",,"A Chinese Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020  
Chinese AMR corpus using Mengzi BERT base (:cite:`zhang2021mengzi`). Its performance on dev set is 
``{amr-zho [tops F1: 85.43%][anchors F1: 93.41%][labels F1: 87.68%][properties F1: 82.02%][edges F1: 73.17%]
[attributes F1: 0.00%][all F1: 84.11%]}``. Test set performance is unknown since the test set is not released to the 
public. 
, A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract 
Meaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`). 
Its performance is ``84.3`` according to their official repository. Using ``amr-evaluation-enhanced``, the performance is
slightly lower:

 =================== ========= ========= ========= 
  Metric              P         R         F1       
 =================== ========= ========= ========= 
  Smatch             84.4       83.6        84.0       
  Unlabeled          86.7       85.8        86.2       
  No WSD             84.9       84.1        84.5       
  Non_sense_frames   91.8       91.6        91.7       
  Wikification       83.6       81.7        82.6       
  Named Ent.         89.3       87.4        88.4       
  Negations          71.6       72.2        71.9       
  IgnoreVars         74.6       74.2        74.4       
  Concepts           90.7       90.0        90.3       
  Frames             88.8       88.5        88.7       
  Reentrancies       72.1       72.9        72.5       
  SRL                80.1       80.7        80.4      
 =================== ========= ========= ========= 
    
Note this parser does NOT perform wikification.
, A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract 
Meaning Representation 3.0 (:cite:`knight2014abstract`). Its performance is

 =================== ========= ========= ========= 
  Metric              P         R         F1       
 =================== ========= ========= ========= 
  Smatch              84.00     82.60     83.30    
  Unlabeled           86.40     84.90     85.70    
  No WSD              84.50     83.10     83.80    
  Non_sense_frames    91.90     91.30     91.60    
  Wikification        81.70     80.80     81.20    
  Named Ent.          89.20     87.00     88.10    
  Negations           71.70     70.90     71.30    
  IgnoreVars          73.80     73.10     73.50    
  Concepts            90.70     89.60     90.10    
  Frames              88.50     87.90     88.20    
  Reentrancies        70.40     71.80     71.10    
  SRL                 79.00     79.60     79.30    
 =================== ========= ========= ========= 
    
Note this parser does NOT perform wikification.
, A wrapper for the Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020 English 
and Chinese AMR corpus. It was ranked the top in the MRP2020 competition, while this release is a base version. 
See the original paper for the detailed performance. Note this model requires tokens and lemmas (for English) to be 
provided as inputs. 
, amr/amr3_graph_pretrain_parser_20221207_153759.zip, amr/amr3_seq2seq_bart_large_83.30_20220125_114450.zip, http://download.hanlp.com/amr/extra/amr-eng-zho-xlm-roberta-base_20220412_223756.zip, http://download.hanlp.com/amr/extra/amr-zho-mengzi-base_20220415_101941.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-01-25 11:47, # Will be filled up during runtime, A Chinese Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020
Chinese AMR corpus using Mengzi BERT base (:cite:`zhang2021mengzi`). Its performance on dev set is
``{amr-zho [tops F1: 85.43%][anchors F1: 93.41%][labels F1: 87.68%][properties F1: 82.02%][edges F1: 73.17%]
[attributes F1: 0.00%][all F1: 84.11%]}``. Test set performance is unknown since the test set is not released to the
public.
, A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract
Meaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`).
Its performance is ``84.3`` according to their official repository. Using ``amr-evaluation-enhanced``, the performance is
slightly lower:

=================== ========= ========= =========
Metric              P         R         F1
=================== ========= ========= =========
Smatch             84.4       83.6        84.0
Unlabeled          86.7       85.8        86.2
No WSD             84.9       84.1        84.5
Non_sense_frames   91.8       91.6        91.7
Wikification       83.6       81.7        82.6
Named Ent.         89.3       87.4        88.4
Negations          71.6       72.2        71.9
IgnoreVars         74.6       74.2        74.4
Concepts           90.7       90.0        90.3
Frames             88.8       88.5        88.7
Reentrancies       72.1       72.9        72.5
SRL                80.1       80.7        80.4
=================== ========= ========= =========

Note this parser does NOT perform wikification.
, A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large parser trained on Abstract
Meaning Representation 3.0 (:cite:`knight2014abstract`). Its performance is

=================== ========= ========= =========
Metric              P         R         F1
=================== ========= ========= =========
Smatch              84.00     82.60     83.30
Unlabeled           86.40     84.90     85.70
No WSD              84.50     83.10     83.80
Non_sense_frames    91.90     91.30     91.60
Wikification        81.70     80.80     81.20
Named Ent.          89.20     87.00     88.10
Negations           71.70     70.90     71.30
IgnoreVars          73.80     73.10     73.50
Concepts            90.70     89.60     90.10
Frames              88.50     87.90     88.20
Reentrancies        70.40     71.80     71.10
SRL                 79.00     79.60     79.30
=================== ========= ========= =========

Note this parser does NOT perform wikification.
, A wrapper for the Permutation-invariant Semantic Parser (:cite:`samuel-straka-2020-ufal`) trained on MRP2020 English
and Chinese AMR corpus. It was ranked the top in the MRP2020 competition, while this release is a base version.
See the original paper for the detailed performance. Note this model requires tokens and lemmas (for English) to be
provided as inputs.
",
https://github.com/hankcs/HanLP,amr2text.py,0,0.0,3,30.0,2,20.0,0,0.0,5,50.0,0,0.0,0,0,2,0,,"ALL, AMR3_GRAPH_PRETRAIN_GENERATION, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, AMR3_GRAPH_PRETRAIN_GENERATION",,"A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large AMR2Text generator trained on 
Abstract Meaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`). 
Its Sacre-BLEU is ``50.38`` according to their official repository.
, amr2text/amr3_graph_pretrain_generation_20221207_153535.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-07 15:19, # Will be filled up during runtime, A seq2seq (:cite:`bevilacqua-etal-2021-one`) BART (:cite:`lewis-etal-2020-bart`) large AMR2Text generator trained on
Abstract Meaning Representation 3.0 (:cite:`knight2014abstract`) with graph pre-training (:cite:`bai-etal-2022-graph`).
Its Sacre-BLEU is ``50.38`` according to their official repository.
",
https://github.com/hankcs/HanLP,classifiers.py,0,0.0,6,35.29,6,35.29,0,0.0,5,29.41,0,0.0,0,0,5,0,,"ALL, CHNSENTICORP_BERT_BASE_ZH, LID_176_FASTTEXT_BASE, LID_176_FASTTEXT_SMALL, SST2_ALBERT_BASE_EN, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CHNSENTICORP_BERT_BASE_ZH, LID_176_FASTTEXT_BASE, LID_176_FASTTEXT_SMALL, SST2_ALBERT_BASE_EN",,"
126MB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.
, 
917kB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.
, classification/chnsenticorp_bert_base_20211228_163210.zip, classification/sst2_albert_base_20211228_164917.zip, https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin, https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz",,"
126MB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.
, 
917kB FastText model for language identification trained on data from Wikipedia, Tatoeba and SETimes.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-01 03:51",
https://github.com/hankcs/HanLP,constituency.py,0,0.0,5,33.33,6,40.0,0,0.0,4,26.67,0,0.0,0,0,4,0,,"ALL, CTB9_CON_ELECTRA_SMALL, CTB9_CON_FULL_TAG_ELECTRA_SMALL, CTB9_CON_FULL_TAG_ERNIE_GRAM, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CTB9_CON_ELECTRA_SMALL, CTB9_CON_FULL_TAG_ELECTRA_SMALL, CTB9_CON_FULL_TAG_ERNIE_GRAM",,"ERNIE-GRAM (:cite:`xiao-etal-2021-ernie`) base tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with full subcategories. Its performance is UCM=42.04% LCM=31.72% UP=91.33% UR=91.53% UF=91.43% LP=85.31% LR=85.49% LF=85.40%., Electra (:cite:`clark2020electra`) small tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with full subcategories. Its performance is UCM=38.29% LCM=28.95% UP=90.16% UR=90.13% UF=90.15% LP=83.46% LR=83.43% LF=83.45%., Electra (:cite:`clark2020electra`) small tree CRF model (:cite:`ijcai2020-560`) trained on CTB9 with major categories. Its performance is UCM=39.06% LCM=34.99% UP=90.05% UR=90.01% UF=90.03% LP=87.02% LR=86.98% LF=87.00%., constituency/ctb9_con_electra_small_20220215_230116.zip, constituency/ctb9_full_tag_con_electra_small_20220118_103119.zip, http://download.hanlp.com/constituency/extra/ctb9_full_tag_con_ernie_20220331_121430.zip",,"# -*- coding:utf-8 -*-, # Author=hankcs, # Date=2022-01-18 10:34, # Will be filled up during runtime",
https://github.com/hankcs/HanLP,dep.py,0,0.0,8,34.78,12,52.17,0,0.0,3,13.04,0,0.0,0,0,7,0,,"ALL, CTB5_BIAFFINE_DEP_ZH, CTB7_BIAFFINE_DEP_ZH, CTB9_DEP_ELECTRA_SMALL, CTB9_UDC_ELECTRA_SMALL, PMT1_DEP_ELECTRA_SMALL, PTB_BIAFFINE_DEP_EN, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CTB5_BIAFFINE_DEP_ZH, CTB7_BIAFFINE_DEP_ZH, CTB9_DEP_ELECTRA_SMALL, CTB9_UDC_ELECTRA_SMALL, PMT1_DEP_ELECTRA_SMALL, PTB_BIAFFINE_DEP_EN",,"Biaffine LSTM model (:cite:`dozat:17a`) trained on CTB5., Biaffine LSTM model (:cite:`dozat:17a`) trained on CTB7., Biaffine LSTM model (:cite:`dozat:17a`) trained on PTB., Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on CTB9-SD330. Performance is UAS=87.68% LAS=83.54%., Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on CTB9-UD420. Performance is UAS=85.92% LAS=81.13% ., Electra small encoder (:cite:`clark2020electra`) with Biaffine decoder (:cite:`dozat:17a`) trained on PKU Multi-view Chinese Treebank (PMT) 1.0 (:cite:`qiu-etal-2014-multi`). Performance is UAS=91.21% LAS=88.65%., dep/biaffine_ctb5_20191229_025833.zip, dep/biaffine_ctb7_20200109_022431.zip, dep/ctb9_dep_electra_small_20220216_100306.zip, dep/pmt_dep_electra_small_20220218_134518.zip, dep/ptb_dep_biaffine_20200101_174624.zip, dep/udc_dep_electra_small_20220218_095452.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 02:55",
https://github.com/hankcs/HanLP,eos.py,0,0.0,3,33.33,2,22.22,0,0.0,4,44.44,0,0.0,0,0,2,0,,"ALL, UD_CTB_EOS_MUL, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, UD_CTB_EOS_MUL",,"EOS model (:cite:`Schweter:Ahmed:2019`) trained on concatenated UD2.3 and CTB9., eos/eos_ud_ctb_mul_20201222_133543.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-22 13:22, # Will be filled up during runtime",
https://github.com/hankcs/HanLP,fasttext.py,0,0.0,6,30.0,9,45.0,0,0.0,5,25.0,0,0.0,0,0,6,0,,"ALL, FASTTEXT_CC_300_EN, FASTTEXT_DEBUG_EMBEDDING_EN, FASTTEXT_WIKI_300_ZH, FASTTEXT_WIKI_300_ZH_CLASSICAL, FASTTEXT_WIKI_NYT_AMAZON_FRIENDS_200_EN",,,,"ALL, FASTTEXT_CC_300_EN, FASTTEXT_DEBUG_EMBEDDING_EN, FASTTEXT_WIKI_300_ZH, FASTTEXT_WIKI_300_ZH_CLASSICAL, FASTTEXT_WIKI_NYT_AMAZON_FRIENDS_200_EN",,"FastText (:cite:`bojanowski2017enriching`) embeddings trained on Chinese Wikipedia., FastText (:cite:`bojanowski2017enriching`) embeddings trained on Common Crawl., FastText (:cite:`bojanowski2017enriching`) embeddings trained on traditional Chinese wikipedia., FastText (:cite:`bojanowski2017enriching`) embeddings trained on wikipedia, nytimes and friends., https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz, https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin, https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh_classical.zip#wiki.zh_classical.bin, https://elit-models.s3-us-west-2.amazonaws.com/fasttext-200-wikipedia-nytimes-amazon-friends-20191107.bin, https://elit-models.s3-us-west-2.amazonaws.com/fasttext.debug.bin.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 18:57, #wiki.zh.bin', #wiki.zh_classical.bin'",
https://github.com/hankcs/HanLP,glove.py,0,0.0,7,26.92,12,46.15,0,0.0,7,26.92,0,0.0,0,0,7,0,,"ALL, GLOVE_6B_100D, GLOVE_6B_200D, GLOVE_6B_300D, GLOVE_6B_50D, GLOVE_840B_300D, _GLOVE_6B_ROOT",,,,"ALL, GLOVE_6B_100D, GLOVE_6B_200D, GLOVE_6B_300D, GLOVE_6B_50D, GLOVE_840B_300D, _GLOVE_6B_ROOT",,"#, Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 100d trained on 6B tokens., Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 200d trained on 6B tokens., Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 300d trained on 6B tokens., Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 300d trained on 840B tokens., Global Vectors for Word Representation (:cite:`pennington-etal-2014-glove`) 50d trained on 6B tokens., glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt, glove.6B.50d.txt, http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip, http://nlp.stanford.edu/data/glove.840B.300d.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-27 20:42, #' + 'glove.6B.100d.txt', #' + 'glove.6B.200d.txt', #' + 'glove.6B.300d.txt', #' + 'glove.6B.50d.txt'",
https://github.com/hankcs/HanLP,mtl.py,0,0.0,12,24.0,20,40.0,0,0.0,8,16.0,10,20.0,0,0,11,0,,"ALL, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH, CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH, NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA, OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH, OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ERNIE_GRAM_ZH, CLOSE_TOK_POS_NER_SRL_UDEP_SDP_CON_ELECTRA_SMALL_ZH, NPCMJ_UD_KYOTO_TOK_POS_CON_BERT_BASE_CHAR_JA, OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH, OPEN_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L12, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_MMINILMV2L6, UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE",,"
Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep (UD Standard), sdp and con model trained on close-source Chinese corpus.
Performance: ``{con UCM: 39.33% LCM: 35.69% UP: 90.24% UR: 90.28% UF: 90.26% LP: 87.55% LR: 87.59% LF: 87.57%}{dep UAS: 86.80% LAS: 82.82%}{ner/msra P: 95.45% R: 96.65% F1: 96.05%}{ner/ontonotes P: 75.98% R: 79.09% F1: 77.50%}{ner/pku P: 95.77% R: 96.75% F1: 96.26%}{pos/863 Accuracy:94.83%}{pos/ctb Accuracy:96.57%}{pos/pku Accuracy:97.54%}{sdp UF: 85.55% LF: 73.67%}{srl P: 75.71% R: 74.25% F1: 74.97%}{tok/coarse P: 97.77% R: 97.70% F1: 97.74%}{tok/fine P: 97.44% R: 97.32% F1: 97.38%}``.
, 
XLM-R (:cite:`conneau-etal-2020-unsupervised`) base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K'iche', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 20.31% LCM: 16.82% UP: 77.50% UR: 76.63% UF: 77.06% LP: 71.25% LR: 70.46% LF: 70.85%}{ner P: 79.93% R: 80.76% F1: 80.34%}{sdp/dm UF: 93.71% LF: 93.00%}{sdp/pas UF: 97.63% LF: 96.37%}{sdp/psd UF: 93.08% LF: 80.95%}{srl [predicate P: 90.95% R: 84.25% F1: 87.47%][e2e P: 78.89% R: 67.32% F1: 72.65%]}{tok P: 98.50% R: 98.70% F1: 98.60%}{ud [lemmas Accuracy:85.95%][upos Accuracy:89.95%][deps UAS: 85.78% LAS: 78.51%][feats Accuracy:82.18%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K'iche', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 17.32% LCM: 13.28% UP: 70.53% UR: 68.73% UF: 69.62% LP: 63.03% LR: 61.42% LF: 62.22%}{ner P: 76.91% R: 78.72% F1: 77.80%}{sdp/dm UF: 92.78% LF: 92.02%}{sdp/pas UF: 96.43% LF: 95.02%}{sdp/psd UF: 92.75% LF: 81.86%}{srl [predicate P: 91.82% R: 77.57% F1: 84.10%][e2e P: 78.33% R: 59.14% F1: 67.40%]}{tok P: 93.69% R: 94.34% F1: 94.02%}{ud [lemmas Accuracy:82.48%][upos Accuracy:87.09%][deps UAS: 82.41% LAS: 73.69%][feats Accuracy:78.58%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K'iche', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 15.48% LCM: 11.45% UP: 68.92% UR: 66.88% UF: 67.88% LP: 61.19% LR: 59.38% LF: 60.27%}{ner P: 76.06% R: 77.83% F1: 76.93%}{sdp/dm UF: 91.84% LF: 91.00%}{sdp/pas UF: 95.46% LF: 93.90%}{sdp/psd UF: 91.94% LF: 81.26%}{srl [predicate P: 91.71% R: 74.51% F1: 82.22%][e2e P: 77.48% R: 55.28% F1: 64.52%]}{tok P: 93.17% R: 93.53% F1: 93.35%}{ud [lemmas Accuracy:81.74%][upos Accuracy:85.94%][deps UAS: 80.60% LAS: 71.21%][feats Accuracy:77.17%]}``.
, BERT (:cite:`devlin-etal-2019-bert`) base char encoder trained on NPCMJ/UD/Kyoto corpora with decoders including tok, pos, ner, dep, con, srl., ERNIE (:cite:`xiao-etal-2021-ernie`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on close-source Chinese corpus., Electra (:cite:`clark2020electra`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on close-source Chinese corpus., Electra (:cite:`clark2020electra`) base version of joint tok, pos, ner, srl, dep, sdp and con model trained on open-source Chinese corpus., Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep (SD Standard), sdp and con model trained on close-source Chinese corpus., Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep, sdp and con model trained on open-source Chinese corpus., mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_base_20210111_124519.zip, mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip, mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20220626_175100.zip, mtl/close_tok_pos_ner_srl_dep_sdp_con_ernie_gram_base_aug_20210904_145403.zip, mtl/npcmj_ud_kyoto_tok_pos_ner_dep_con_srl_bert_base_char_ja_20210914_133742.zip, mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_base_20201223_201906.zip, mtl/open_tok_pos_ner_srl_dep_sdp_con_electra_small_20201223_035557.zip, mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L12_no_space_20220807_133143.zip, mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_mMiniLMv2L6_no_space_20220731_161526.zip, mtl/ud_ontonotes_tok_pos_lem_fea_ner_srl_dep_sdp_con_xlm_base_20220608_003435.zip",,"
Electra (:cite:`clark2020electra`) small version of joint tok, pos, ner, srl, dep (UD Standard), sdp and con model trained on close-source Chinese corpus.
Performance: ``{con UCM: 39.33% LCM: 35.69% UP: 90.24% UR: 90.28% UF: 90.26% LP: 87.55% LR: 87.59% LF: 87.57%}{dep UAS: 86.80% LAS: 82.82%}{ner/msra P: 95.45% R: 96.65% F1: 96.05%}{ner/ontonotes P: 75.98% R: 79.09% F1: 77.50%}{ner/pku P: 95.77% R: 96.75% F1: 96.26%}{pos/863 Accuracy:94.83%}{pos/ctb Accuracy:96.57%}{pos/pku Accuracy:97.54%}{sdp UF: 85.55% LF: 73.67%}{srl P: 75.71% R: 74.25% F1: 74.97%}{tok/coarse P: 97.77% R: 97.70% F1: 97.74%}{tok/fine P: 97.44% R: 97.32% F1: 97.38%}``.
, 
XLM-R (:cite:`conneau-etal-2020-unsupervised`) base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 20.31% LCM: 16.82% UP: 77.50% UR: 76.63% UF: 77.06% LP: 71.25% LR: 70.46% LF: 70.85%}{ner P: 79.93% R: 80.76% F1: 80.34%}{sdp/dm UF: 93.71% LF: 93.00%}{sdp/pas UF: 97.63% LF: 96.37%}{sdp/psd UF: 93.08% LF: 80.95%}{srl [predicate P: 90.95% R: 84.25% F1: 87.47%][e2e P: 78.89% R: 67.32% F1: 72.65%]}{tok P: 98.50% R: 98.70% F1: 98.60%}{ud [lemmas Accuracy:85.95%][upos Accuracy:89.95%][deps UAS: 85.78% LAS: 78.51%][feats Accuracy:82.18%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 17.32% LCM: 13.28% UP: 70.53% UR: 68.73% UF: 69.62% LP: 63.03% LR: 61.42% LF: 62.22%}{ner P: 76.91% R: 78.72% F1: 77.80%}{sdp/dm UF: 92.78% LF: 92.02%}{sdp/pas UF: 96.43% LF: 95.02%}{sdp/psd UF: 92.75% LF: 81.86%}{srl [predicate P: 91.82% R: 77.57% F1: 84.10%][e2e P: 78.33% R: 59.14% F1: 67.40%]}{tok P: 93.69% R: 94.34% F1: 94.02%}{ud [lemmas Accuracy:82.48%][upos Accuracy:87.09%][deps UAS: 82.41% LAS: 73.69%][feats Accuracy:78.58%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 15.48% LCM: 11.45% UP: 68.92% UR: 66.88% UF: 67.88% LP: 61.19% LR: 59.38% LF: 60.27%}{ner P: 76.06% R: 77.83% F1: 76.93%}{sdp/dm UF: 91.84% LF: 91.00%}{sdp/pas UF: 95.46% LF: 93.90%}{sdp/psd UF: 91.94% LF: 81.26%}{srl [predicate P: 91.71% R: 74.51% F1: 82.22%][e2e P: 77.48% R: 55.28% F1: 64.52%]}{tok P: 93.17% R: 93.53% F1: 93.35%}{ud [lemmas Accuracy:81.74%][upos Accuracy:85.94%][deps UAS: 80.60% LAS: 71.21%][feats Accuracy:77.17%]}``.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-22 13:16, # Will be filled up during runtime","
XLM-R (:cite:`conneau-etal-2020-unsupervised`) base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 20.31% LCM: 16.82% UP: 77.50% UR: 76.63% UF: 77.06% LP: 71.25% LR: 70.46% LF: 70.85%}{ner P: 79.93% R: 80.76% F1: 80.34%}{sdp/dm UF: 93.71% LF: 93.00%}{sdp/pas UF: 97.63% LF: 96.37%}{sdp/psd UF: 93.08% LF: 80.95%}{srl [predicate P: 90.95% R: 84.25% F1: 87.47%][e2e P: 78.89% R: 67.32% F1: 72.65%]}{tok P: 98.50% R: 98.70% F1: 98.60%}{ud [lemmas Accuracy:85.95%][upos Accuracy:89.95%][deps UAS: 85.78% LAS: 78.51%][feats Accuracy:82.18%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 base version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 17.32% LCM: 13.28% UP: 70.53% UR: 68.73% UF: 69.62% LP: 63.03% LR: 61.42% LF: 62.22%}{ner P: 76.91% R: 78.72% F1: 77.80%}{sdp/dm UF: 92.78% LF: 92.02%}{sdp/pas UF: 96.43% LF: 95.02%}{sdp/psd UF: 92.75% LF: 81.86%}{srl [predicate P: 91.82% R: 77.57% F1: 84.10%][e2e P: 78.33% R: 59.14% F1: 67.40%]}{tok P: 93.69% R: 94.34% F1: 94.02%}{ud [lemmas Accuracy:82.48%][upos Accuracy:87.09%][deps UAS: 82.41% LAS: 73.69%][feats Accuracy:78.58%]}``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 small version of joint tok, pos, lem, fea, ner, srl, dep, sdp and con model trained on UD 2.10 and OntoNotes5 corpora.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``{con UCM: 15.48% LCM: 11.45% UP: 68.92% UR: 66.88% UF: 67.88% LP: 61.19% LR: 59.38% LF: 60.27%}{ner P: 76.06% R: 77.83% F1: 76.93%}{sdp/dm UF: 91.84% LF: 91.00%}{sdp/pas UF: 95.46% LF: 93.90%}{sdp/psd UF: 91.94% LF: 81.26%}{srl [predicate P: 91.71% R: 74.51% F1: 82.22%][e2e P: 77.48% R: 55.28% F1: 64.52%]}{tok P: 93.17% R: 93.53% F1: 93.35%}{ud [lemmas Accuracy:81.74%][upos Accuracy:85.94%][deps UAS: 80.60% LAS: 71.21%][feats Accuracy:77.17%]}``.
, Apurinã, Guajajára, Makuráp, MbyáGuaraní, Mundurukú, Tupinambá, Urubú-Kaapor"
https://github.com/hankcs/HanLP,ner.py,0,0.0,6,35.29,8,47.06,0,0.0,3,17.65,0,0.0,0,0,5,0,,"ALL, CONLL03_NER_BERT_BASE_CASED_EN, MSRA_NER_ALBERT_BASE_ZH, MSRA_NER_BERT_BASE_ZH, MSRA_NER_ELECTRA_SMALL_ZH, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CONLL03_NER_BERT_BASE_CASED_EN, MSRA_NER_ALBERT_BASE_ZH, MSRA_NER_BERT_BASE_ZH, MSRA_NER_ELECTRA_SMALL_ZH",,"ALBERT model (:cite:`Lan2020ALBERT:`) trained on MSRA with 3 entity types., BERT model (:cite:`devlin-etal-2019-bert`) trained on CoNLL03., BERT model (:cite:`devlin-etal-2019-bert`) trained on MSRA with 3 entity types., Electra small model (:cite:`clark2020electra`) trained on MSRA with 26 entity types. F1 = `95.16`, ner/msra_ner_albert_base_20211228_173323.zip, ner/msra_ner_electra_small_20220215_205503.zip, ner/ner_bert_base_msra_20211227_114712.zip, ner/ner_conll03_bert_base_cased_en_20211227_121443.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 20:07",
https://github.com/hankcs/HanLP,pos.py,0,0.0,11,34.38,18,56.25,0,0.0,3,9.38,0,0.0,0,0,10,0,,"ALL, C863_POS_ELECTRA_SMALL, CTB5_POS_RNN, CTB5_POS_RNN_FASTTEXT_ZH, CTB9_POS_ALBERT_BASE, CTB9_POS_ELECTRA_SMALL, CTB9_POS_ELECTRA_SMALL_TF, CTB9_POS_RADICAL_ELECTRA_SMALL, PKU_POS_ELECTRA_SMALL, PTB_POS_RNN_FASTTEXT_EN, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, C863_POS_ELECTRA_SMALL, CTB5_POS_RNN, CTB5_POS_RNN_FASTTEXT_ZH, CTB9_POS_ALBERT_BASE, CTB9_POS_ELECTRA_SMALL, CTB9_POS_ELECTRA_SMALL_TF, CTB9_POS_RADICAL_ELECTRA_SMALL, PKU_POS_ELECTRA_SMALL, PTB_POS_RNN_FASTTEXT_EN",,"ALBERT model (:cite:`Lan2020ALBERT:`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). This is a TF component., An old school BiLSTM tagging model trained on CTB5., An old school BiLSTM tagging model with FastText (:cite:`bojanowski2017enriching`) embeddings trained on CTB5., An old school BiLSTM tagging model with FastText (:cite:`bojanowski2017enriching`) embeddings trained on PTB., Electra small model (:cite:`clark2020electra`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.26`., Electra small model (:cite:`clark2020electra`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.75`. This is a TF component., Electra small model (:cite:`clark2020electra`) trained on Chinese 863 corpus. Accuracy = `95.19`., Electra small model (:cite:`clark2020electra`) trained on Chinese PKU corpus. Accuracy = `97.55`., Electra small model (:cite:`clark2020electra`) with radical embeddings (:cite:`he2018dual`) trained on CTB9 (:cite:`https://doi.org/10.35111/gvd0-xk91`). Accuracy = `96.14`., pos/ctb5_pos_rnn_20200113_235925.zip, pos/ctb5_pos_rnn_fasttext_20191230_202639.zip, pos/ctb9_albert_base_20211228_163935.zip, pos/pos_863_electra_small_20220217_101958.zip, pos/pos_ctb_electra_small_20211227_121341.zip, pos/pos_ctb_electra_small_20220215_111944.zip, pos/pos_ctb_radical_electra_small_20220215_111932.zip, pos/pos_pku_electra_small_20220217_142436.zip, pos/ptb_pos_rnn_fasttext_20220418_101708.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 01:57",
https://github.com/hankcs/HanLP,rnnlm.py,0,0.0,5,31.25,6,37.5,0,0.0,5,31.25,0,0.0,0,0,4,0,,"ALL, FLAIR_LM_BW_WMT11_EN_TF, FLAIR_LM_FW_WMT11_EN_TF, FLAIR_LM_WMT11_EN, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, FLAIR_LM_BW_WMT11_EN_TF, FLAIR_LM_FW_WMT11_EN_TF, FLAIR_LM_WMT11_EN",,"The BiLSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`)., The backward LSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`)., The forward LSTM of Contextual String Embedding (:cite:`akbik-etal-2018-contextual`)., lm/flair_lm_wmt11_en_20200211_091932.zip#flair_lm_bw_wmt11_en, lm/flair_lm_wmt11_en_20200211_091932.zip#flair_lm_fw_wmt11_en, lm/flair_lm_wmt11_en_20200601_205350.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-19 03:47, #flair_lm_bw_wmt11_en', #flair_lm_fw_wmt11_en'",
https://github.com/hankcs/HanLP,sdp.py,0,0.0,8,34.78,12,52.17,0,0.0,3,13.04,0,0.0,0,0,7,0,,"ALL, SEMEVAL15_DM_BIAFFINE_EN, SEMEVAL15_PAS_BIAFFINE_EN, SEMEVAL15_PSD_BIAFFINE_EN, SEMEVAL16_ALL_ELECTRA_SMALL_ZH, SEMEVAL16_NEWS_BIAFFINE_ZH, SEMEVAL16_TEXT_BIAFFINE_ZH, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, SEMEVAL15_DM_BIAFFINE_EN, SEMEVAL15_PAS_BIAFFINE_EN, SEMEVAL15_PSD_BIAFFINE_EN, SEMEVAL16_ALL_ELECTRA_SMALL_ZH, SEMEVAL16_NEWS_BIAFFINE_ZH, SEMEVAL16_TEXT_BIAFFINE_ZH",,"Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 DM data., Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 PAS data., Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval15 PSD data., Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 news data., Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 text and news data. Performance: ``UF: 83.03% LF: 72.58%``, Biaffine SDP (:cite:`he-choi-2019`) trained on SemEval16 text data., sdp/semeval15_biaffine_dm_20200106_122808.zip, sdp/semeval15_biaffine_pas_20200103_152405.zip, sdp/semeval15_biaffine_psd_20200106_123009.zip, sdp/semeval16-news-biaffine_20191231_235407.zip, sdp/semeval16-text-biaffine_20200101_002257.zip, sdp/semeval16_sdp_electra_small_20220719_171433.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 23:54",
https://github.com/hankcs/HanLP,srl.py,0,0.0,3,37.5,2,25.0,0,0.0,3,37.5,0,0.0,0,0,2,0,,"ALL, CPB3_SRL_ELECTRA_SMALL, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CPB3_SRL_ELECTRA_SMALL",,"Electra small model (:cite:`clark2020electra`) trained on CPB3. P=75.87% R=76.24% F1=76.05%., srl/cpb3_electra_small_crf_has_transform_20220218_135910.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-08-07 19:07",
https://github.com/hankcs/HanLP,sts.py,0,0.0,3,33.33,2,22.22,0,0.0,4,44.44,0,0.0,0,0,2,0,,"ALL, STS_ELECTRA_BASE_ZH, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, STS_ELECTRA_BASE_ZH",,"A naive regression model trained on concatenated STS corpora., sts/sts_electra_base_zh_20210530_200109.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-24 12:51, # Will be filled up during runtime",
https://github.com/hankcs/HanLP,tok.py,0,0.0,16,27.12,28,47.46,0,0.0,6,10.17,9,15.25,0,0,15,0,,"ALL, COARSE_ELECTRA_SMALL_ZH, CTB6_CONVSEG, CTB9_TOK_ELECTRA_BASE, CTB9_TOK_ELECTRA_BASE_CRF, CTB9_TOK_ELECTRA_SMALL, FINE_ELECTRA_SMALL_ZH, LARGE_ALBERT_BASE, MSR_TOK_ELECTRA_BASE_CRF, PKU_NAME_MERGED_SIX_MONTHS_CONVSEG, SIGHAN2005_MSR_CONVSEG, SIGHAN2005_PKU_BERT_BASE_ZH, SIGHAN2005_PKU_CONVSEG, UD_TOK_MMINILMV2L12, UD_TOK_MMINILMV2L6, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, COARSE_ELECTRA_SMALL_ZH, CTB6_CONVSEG, CTB9_TOK_ELECTRA_BASE, CTB9_TOK_ELECTRA_BASE_CRF, CTB9_TOK_ELECTRA_SMALL, FINE_ELECTRA_SMALL_ZH, LARGE_ALBERT_BASE, MSR_TOK_ELECTRA_BASE_CRF, PKU_NAME_MERGED_SIX_MONTHS_CONVSEG, SIGHAN2005_MSR_CONVSEG, SIGHAN2005_PKU_BERT_BASE_ZH, SIGHAN2005_PKU_CONVSEG, UD_TOK_MMINILMV2L12, UD_TOK_MMINILMV2L6",,"
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L12xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K'iche', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 95.41% R: 95.25% F1: 95.33%``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K'iche', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 94.99% R: 94.74% F1: 94.86%``.
, ALBERT model (:cite:`Lan2020ALBERT:`) trained on the largest CWS dataset in the world., BERT model (:cite:`devlin-etal-2019-bert`) trained on sighan2005 pku dataset., Conv model (:cite:`wang-xu-2017-convolutional`) trained on CTB6 dataset., Conv model (:cite:`wang-xu-2017-convolutional`) trained on pku98 six months dataset with familiy name and given name merged into one unit., Conv model (:cite:`wang-xu-2017-convolutional`) trained on sighan2005 msr dataset., Conv model (:cite:`wang-xu-2017-convolutional`) trained on sighan2005 pku dataset., Electra (:cite:`clark2020electra`) base model trained on CTB9. Its performance is ``P: 97.62% R: 97.67% F1: 97.65%`` which is much higher than that of MTL model , Electra (:cite:`clark2020electra`) base model trained on CTB9. Its performance is ``P: 97.68% R: 97.71% F1: 97.69%`` which is much higher than that of MTL model , Electra (:cite:`clark2020electra`) base model trained on MSR CWS dataset. Its performance is ``P: 98.71% R: 98.64% F1: 98.68%`` which is much higher than that of MTL model , Electra (:cite:`clark2020electra`) small model trained on CTB9. Its performance is P=97.15% R=97.36% F1=97.26% which is much higher than that of MTL model , Electra (:cite:`clark2020electra`) small model trained on coarse-grained CWS corpora. Its performance is ``P: 98.34% R: 98.38% F1: 98.36%`` which is much higher than that of MTL model , Electra (:cite:`clark2020electra`) small model trained on fine-grained CWS corpora. Its performance is ``P: 98.14% R: 98.07% F1: 98.11%`` which is much higher than that of MTL model , http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_20220426_111949.zip, http://download.hanlp.com/tok/extra/ctb9_tok_electra_base_crf_20220426_161255.zip, http://download.hanlp.com/tok/extra/msra_crf_electra_base_20220507_113936.zip, tok/coarse_electra_small_20220616_012050.zip, tok/convseg-msr-nocrf-noembed_20200110_153524.zip, tok/ctb6_convseg_nowe_nocrf_20200110_004046.zip, tok/ctb9_electra_small_20220215_205427.zip, tok/fine_electra_small_20220615_231803.zip, tok/large_corpus_cws_albert_base_20211228_160926.zip, tok/pku98_6m_conv_ngram_20200110_134736.zip, tok/sighan2005-pku-convseg_20200110_153722.zip, tok/sighan2005_pku_bert_base_zh_20201231_141130.zip, tok/ud_tok_mMiniLMv2L12_no_space_mul_20220619_091159.zip, tok/ud_tok_mMiniLMv2L6_no_space_mul_20220619_091824.zip",,"
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L12xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 95.41% R: 95.25% F1: 95.33%``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 94.99% R: 94.74% F1: 94.86%``.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 21:12, # Will be filled up during runtime","
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L12xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 95.41% R: 95.25% F1: 95.33%``.
, 
mMiniLMv2 (:cite:`wang-etal-2021-minilmv2`) L6xH384 based tokenizer trained on UD 2.10.
The following 130 languages are supported: ``Afrikaans, Akkadian, Akuntsu, Albanian, Amharic, AncientGreek (to 1453), Ancient Hebrew, Apurinã, Arabic, Armenian, AssyrianNeo-Aramaic, Bambara, Basque, Beja, Belarusian, Bengali, Bhojpuri, Breton, Bulgarian, Catalan, Cebuano, Central Siberian Yupik, Chinese, Chukot, ChurchSlavic, Coptic, Croatian, Czech, Danish, Dutch, Emerillon, English, Erzya, Estonian, Faroese, Finnish, French, Galician, German, Gothic, Guajajára, Guarani, Hebrew, Hindi, Hittite, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, K\'iche\', Kangri, Karelian, Karo(Brazil), Kazakh, Khunsari, Komi-Permyak, Komi-Zyrian, Korean, Latin, Latvian, Ligurian, LiteraryChinese, Lithuanian, Livvi, LowGerman, Madi, Makuráp, Maltese, Manx, Marathi, MbyáGuaraní, Modern Greek (1453-), Moksha, Mundurukú, Nayini, Neapolitan, Nigerian Pidgin, NorthernKurdish, Northern Sami, Norwegian, OldFrench (842-ca. 1400), OldRussian, Old Turkish, Persian, Polish, Portuguese, Romanian, Russia Buriat, Russian, Sanskrit, ScottishGaelic, Serbian, SkoltSami, Slovak, Slovenian, Soi, South Levantine Arabic, Spanish, Swedish, SwedishSign Language, SwissGerman, Tagalog, Tamil, Tatar, Telugu, Thai, Tupinambá, Turkish, Uighur, Ukrainian, Umbrian, UpperSorbian, Urdu, Urubú-Kaapor, Vietnamese, Warlpiri, Welsh, Western Armenian, WesternFrisian, Wolof, Xibe, Yakut, Yoruba, YueChinese``.
Performance: ``P: 94.99% R: 94.74% F1: 94.86%``.
, Apurinã, Guajajára, Makuráp, MbyáGuaraní, Mundurukú, Tupinambá, Urubú-Kaapor"
https://github.com/hankcs/HanLP,word2vec.py,0,0.0,21,33.33,25,39.68,0,0.0,17,26.98,0,0.0,0,0,20,0,,"ALL, CONVSEG_W2V_NEWS_TENSITE, CONVSEG_W2V_NEWS_TENSITE_CHAR, CONVSEG_W2V_NEWS_TENSITE_WORD_MSR, CONVSEG_W2V_NEWS_TENSITE_WORD_PKU, CTB5_FASTTEXT_300_CN, MERGE_SGNS_BIGRAM_CHAR_300_ZH, RADICAL_CHAR_EMBEDDING_100, SEMEVAL16_EMBEDDINGS_300_NEWS_CN, SEMEVAL16_EMBEDDINGS_300_TEXT_CN, SEMEVAL16_EMBEDDINGS_CN, SUBWORD_ENCODING_CWS_CTB_GAZETTEER_50, SUBWORD_ENCODING_CWS_GIGAWORD_BI, SUBWORD_ENCODING_CWS_GIGAWORD_UNI, SUBWORD_ENCODING_CWS_ZH_WIKI_BPE_50, TENCENT_AILAB_EMBEDDING_LARGE_100, TENCENT_AILAB_EMBEDDING_LARGE_200, TENCENT_AILAB_EMBEDDING_SMALL_100, TENCENT_AILAB_EMBEDDING_SMALL_200, _SUBWORD_ENCODING_CWS, hanlp_common",hanlp_common.constant.HANLP_URL,,,"ALL, CONVSEG_W2V_NEWS_TENSITE, CONVSEG_W2V_NEWS_TENSITE_CHAR, CONVSEG_W2V_NEWS_TENSITE_WORD_MSR, CONVSEG_W2V_NEWS_TENSITE_WORD_PKU, CTB5_FASTTEXT_300_CN, MERGE_SGNS_BIGRAM_CHAR_300_ZH, RADICAL_CHAR_EMBEDDING_100, SEMEVAL16_EMBEDDINGS_300_NEWS_CN, SEMEVAL16_EMBEDDINGS_300_TEXT_CN, SEMEVAL16_EMBEDDINGS_CN, SUBWORD_ENCODING_CWS_CTB_GAZETTEER_50, SUBWORD_ENCODING_CWS_GIGAWORD_BI, SUBWORD_ENCODING_CWS_GIGAWORD_UNI, SUBWORD_ENCODING_CWS_ZH_WIKI_BPE_50, TENCENT_AILAB_EMBEDDING_LARGE_100, TENCENT_AILAB_EMBEDDING_LARGE_200, TENCENT_AILAB_EMBEDDING_SMALL_100, TENCENT_AILAB_EMBEDDING_SMALL_200, _SUBWORD_ENCODING_CWS",,"#ctb.50d.vec, #gigaword_chn.all.a2b.bi.ite50.vec, #gigaword_chn.all.a2b.uni.ite50.vec, #news.fasttext.300.txt, #news_tensite.msr.words.w2v50, #news_tensite.pku.words.w2v50, #news_tensite.w2v200, #text.fasttext.300.txt, #zh.wiki.bpe.vs200000.d50.w2v.txt, Chinese character embedding enhanced with rich radical information (:cite:`he2018dual`)., Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with large vocabulary size and 100 dimension provided by Tencent AI lab., Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with large vocabulary size and 200 dimension provided by Tencent AI lab., Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with small vocabulary size and 100 dimension provided by Tencent AI lab., Chinese word embeddings (:cite:`NIPS2013_9aa42b31`) with small vocabulary size and 200 dimension provided by Tencent AI lab., Chinese word embeddings trained with context features (word, ngram, character, and more) using Skip-Gram with Negative Sampling (SGNS) (:cite:`li-etal-2018-analogical`)., embeddings/convseg_embeddings.zip, embeddings/ctb.fasttext.300.txt.zip, embeddings/radical_char_vec_20191229_013849.zip#character.vec.txt, embeddings/semeval16_embeddings.zip, http://download.hanlp.com/embeddings/extra/merge_sgns_bigram_char300_20220130_214613.txt.zip, http://download.hanlp.com/embeddings/extra/subword_encoding_cws_20200524_190636.zip, https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d100-v0.2.0-s.tar.gz#tencent-ailab-embedding-zh-d100-v0.2.0-s.txt, https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d100-v0.2.0.tar.gz#tencent-ailab-embedding-zh-d100-v0.2.0.txt, https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d200-v0.2.0-s.tar.gz#tencent-ailab-embedding-zh-d200-v0.2.0-s.txt, https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d200-v0.2.0.tar.gz#tencent-ailab-embedding-zh-d200-v0.2.0.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 18:25, #character.vec.txt', #ctb.50d.vec', #gigaword_chn.all.a2b.bi.ite50.vec', #gigaword_chn.all.a2b.uni.ite50.vec', #news.fasttext.300.txt', #news_tensite.msr.words.w2v50', #news_tensite.pku.words.w2v50', #news_tensite.w2v200', #tencent-ailab-embedding-zh-d100-v0.2.0-s.txt', #tencent-ailab-embedding-zh-d100-v0.2.0.txt', #tencent-ailab-embedding-zh-d200-v0.2.0-s.txt', #tencent-ailab-embedding-zh-d200-v0.2.0.txt', #text.fasttext.300.txt', #zh.wiki.bpe.vs200000.d50.w2v.txt'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,2,33.33,0,0.0,0,0.0,4,66.67,0,0.0,0,0,1,0,,"ALL, hanlp","hanlp.pretrained.amr, hanlp.pretrained.amr2text, hanlp.pretrained.classifiers, hanlp.pretrained.constituency, hanlp.pretrained.dep, hanlp.pretrained.eos, hanlp.pretrained.fasttext, hanlp.pretrained.glove, hanlp.pretrained.mtl, hanlp.pretrained.ner, hanlp.pretrained.pos, hanlp.pretrained.rnnlm, hanlp.pretrained.sdp, hanlp.pretrained.srl, hanlp.pretrained.sts, hanlp.pretrained.tok, hanlp.pretrained.word2vec",,,ALL,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 19:10, # Will be filled up during runtime",
https://github.com/hankcs/HanLP,conll_tf.py,0,0.0,195,68.66,25,8.8,14,4.93,50,17.61,0,0.0,22,4,101,1,,"CoNLLTransform, CoNLL_DEP_Transform, CoNLL_SDP_Transform, CoNLL_Transformer_Transform, ID, ValueError, X, XY_to_inputs_outputs, X_to_inputs, Y, Y_to_outputs, __init__, _find_orphan_relation, abc, add, append, arc, arc_preds, arc_sent, arcs, batch, batch_forms, batch_heads, batch_indices, batch_input_ids, batch_input_mask, batch_masked_offsets, batch_pos, batch_prefix_offset, batch_rels, batch_size, batched_inputs_to_batches, bool, bos, bucket, buckets, cache, callable, cell, cells, chunks, cls_token, collections, config, conllu, convert_tokens_to_ids, corpus, counter, cpos, cpos_vocab, cposes, cposes_batch, cposes_sent, create_types_shapes_values, deprel, deps, dict, drop_remainder, endswith, enhanced_only, enumerate, feats, file_to_inputs, file_to_samples, filepath, fit, float, form_batch, form_sent, form_vocab, forms, freq, generator, get, get_idx_without_add, gold, graph, hanlp, hanlp_common, head, heads, idx, idx_to_token, index, indices, input, input_ids, input_is_single_sample, input_mask, inputs, inputs_to_samples, insert, isinstance, items, len, len_of_sent, length, lengths, lock_vocabs, lookup, lower, map_x, map_y, mask, mask_offsets, mask_p, mask_token_id, max, max_len, max_samples_per_batch, max_seq_length, min, min_freq, most_common, n_buckets, n_tokens, num, num_masks, num_samples, numpy, offset, orphan_relation, pad_idx, pad_label_idx, pad_token, pad_token_id, parsed_sent, pop, pred, prefetch, prefix_mask, prefix_offset, prev_cells, property, puncts, range_fn, raw_batch, rel, rel_preds, rel_sent, rel_vocab, rels, repeat, repr, rid, roberta, root, root_rel_idx, round, safe_pad_token, safe_pad_token_idx, sample, samples, samples_to_dataset, segment_ids, self, sent, sent_idx, sent_len, sents, sep_token, shapes, shuffle, size, sizes, sorted, split, split_sizes, sum, super, tensorflow, token_mapping, token_to_idx, token_to_idx_table, tokenizer, topk, topk_vocab, transformer_config, transformers, trn_path, tuple, types, types_shapes_values, typing, update, use_pos, values, warning, words, x_to_idx, xlnet, xs, y_to_idx, ys, zip","abc.abstractmethod, collections.Counter, hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.components.parsers.alg_tf.arange, hanlp.components.parsers.alg_tf.kmeans, hanlp.components.parsers.alg_tf.randperm, hanlp.components.parsers.alg_tf.tolist, hanlp.components.parsers.conll.read_conll, hanlp.layers.transformers.utils_tf.adjust_tokens_for_transformers, hanlp.layers.transformers.utils_tf.config_is, hanlp.layers.transformers.utils_tf.convert_examples_to_features, hanlp.utils.log_util.logger, hanlp.utils.string_util.ispunct, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLUWord, hanlp_common.conll.CoNLLWord, hanlp_common.constant.ROOT, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_locals_kwargs, np.arange, np.bool, np.ceil, np.int64, np.random.choice, np.zeros, tf.Tensor, tf.bool, tf.constant, tf.data.Dataset, tf.int64, tf.keras.preprocessing.sequence.pad_sequences, tf.math.count_nonzero, tf.not_equal, tf.split, transformers.PreTrainedTokenizer, transformers.PretrainedConfig, typing.Any, typing.Generator, typing.Iterable, typing.Tuple, typing.Union","XY_to_inputs_outputs, X_to_inputs, Y_to_outputs, __init__, _find_orphan_relation, batched_inputs_to_batches, bos, create_types_shapes_values, file_to_inputs, fit, generator, graph, input_is_single_sample, inputs_to_samples, len_of_sent, lock_vocabs, mask_p, root_rel_idx, samples_to_dataset, use_pos, x_to_idx, y_to_idx","CoNLLTransform, CoNLL_DEP_Transform, CoNLL_SDP_Transform, CoNLL_Transformer_Transform","ID, ar, arc, arc_preds, arc_sent, arcs, b, batch, batch_forms, batch_heads, batch_indices, batch_input_ids, batch_input_mask, batch_masked_offsets, batch_pos, batch_prefix_offset, batch_rels, bucket, buckets, cell, cells, chunks, cls_token, config, conllu, corpus, counter, cpos, cposes, cposes_batch, cposes_sent, deprel, deps, enhanced_only, feats, form, form_batch, form_sent, forms, freq, head, heads, idx, indices, input_ids, input_mask, inputs, length, lengths, mask, mask_offsets, mask_p, mask_token_id, max_len, max_samples_per_batch, max_seq_length, n_buckets, n_tokens, num, num_masks, num_samples, offset, pad_label_idx, pad_token, pad_token_id, parsed_sent, pred, prefix_mask, prefix_offset, prev_cells, range_fn, raw_batch, rel, rel_preds, rel_sent, rels, rid, roberta, root, root_rel_idx, sample, segment_ids, sent, sent_idx, sent_len, sents, sep_token, shapes, size, sizes, split_sizes, token_mapping, tokenizer, types, types_shapes_values, use_pos, values, words, xlnet, xs, ys","Convert batched inputs to batches of samples

Args:
  corpus(list): A list of inputs
  indices(list): A list of indices, each list belongs to a batch
  shuffle:

Returns:"," not exists in train, .conll, .conllu, :, Convert batched inputs to batches of samples

        Args:
          corpus(list): A list of inputs
          indices(list): A list of indices, each list belongs to a batch
          shuffle:

        Returns:


        , Expect X to be 2 or 3 elements but got , Relation OOV: , Unknown data arrangement, enhanced_only, graph, int64, joint_pos, lower, mask_p, max_samples_per_batch, only support gold file for now, post, roberta, root, root_rel_idx, token_mapping, topk, use_pos, xlnet, |","0, 1, 2, 256, 3, 32, 4, 5000, 6, 7, 8, False, None, True","#             deprel = rels[offset], #         if not self.rel_vocab or rels[offset] in self.rel_vocab:, #         offset = heads.index(head), #     deps = [x.split(':', 1) for x in deps.split('|')], #     deps = cell[8], #     f'word {mask_word} prefix {mask_prefix} not match'  # could vs couldn, #     heads = [int(x[0]) for x in deps if '_' not in x[0] and '.' not in x[0]], #     if head in heads:, #     mask_word) or mask_prefix == ""'"", \, #     rels = [x[1] for x in deps if '_' not in x[0] and '.' not in x[0]], # -*- coding:utf-8 -*-, # <root> is now [CLS], # Author: hankcs, # Date: 2020-05-08 15:30, # No masking in prediction, # Transformer tokenizing, # assert len(sent) == num_masks  # each token has a True subtoken, # assert mask_word.startswith(mask_prefix) or mask_prefix.startswith(, # custom bucketing, load corpus into memory, # debug for transformer, # form, # form, cpos, head, deprel = sample[0], # found end of token, # head, # heuristic to find the orphan relation, # how many sentences in each batch, # if conllu:, # insert root word with arbitrary fields, anyway it will be masked, # long sent gets truncated, +1 for root, # make root the 2ed elements while 0th is pad, 1st is unk, # mask prefix, # mask_offsets.append(input_ids[offset]) # subword token, # mask_offsets.append(offset)  # form token, # mask_prefix = tokenizer.convert_ids_to_tokens([input_ids[prefix_offset[offset]]])[0], # mask_word = raw_batch[0][sent_idx][offset], # never mask [CLS], # next(generator()), # noinspection PyCallByClass, # orphan, # pad on the left for xlnet, # pos, # range [1, len(bucket)], # rel, # remove <root> use [CLS] instead, # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805, # take ROOT into account, # the number of chunks in each bucket, which is clipped by, # whole word masking, mask the rest of the word, # xlnet has a cls token at the end, Convert batched inputs to batches of samples

Args:
corpus(list): A list of inputs
indices(list): A list of indices, each list belongs to a batch
shuffle:

Returns:


",
https://github.com/hankcs/HanLP,glue_tf.py,0,0.0,26,68.42,1,2.63,7,18.42,4,10.53,0,0.0,4,2,3,0,,"MicrosoftResearchParaphraseCorpus, StanfordSentimentTreebank2Transorm, __init__, _test_mrpc, _test_sst2, batch, build_config, config, delimiter, file_to_dataset, fit, hanlp, hanlp_common, kwargs, label_vocab, lock_vocabs, main, map_x, map_y, print, skip_header, summary, super, take, x_columns, y_column","hanlp.datasets.glu.glue.MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV, hanlp.datasets.glu.glue.STANFORD_SENTIMENT_TREEBANK_2_TRAIN, hanlp.transform.table_tf.TableTransform, hanlp_common.structure.SerializableDict","__init__, _test_mrpc, _test_sst2, main","MicrosoftResearchParaphraseCorpus, StanfordSentimentTreebank2Transorm","batch, dataset, transform",,auto,"0, 1, 3, 4, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-08 16:34, # _test_sst2()",
https://github.com/hankcs/HanLP,table_tf.py,0,0.0,49,67.12,9,12.33,5,6.85,10,13.7,0,0.0,7,1,10,0,,"TableTransform, __init__, abc, add, config, create_types_shapes_values, delimiter, enumerate, eval, file_to_inputs, file_to_samples, filepath, fit, get, gold, hanlp, hanlp_common, inputs, inputs_to_samples, kwargs, label_vocab, labels, len, lookup, map_x, map_y, multi_label, num_features, numpy, pad, safe_pad_token, samples, self, shapes, skip_header, super, tensorflow, trn_path, tuple, type, types, typing, values, warning, x, x_columns, x_to_idx, y_column, y_to_idx","abc.ABC, hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.create_label_vocab, hanlp.utils.io_util.read_cells, hanlp.utils.log_util.logger, hanlp_common.constant.PAD, hanlp_common.structure.SerializableDict, tf.Tensor, tf.string, typing.Tuple, typing.Union","__init__, create_types_shapes_values, file_to_inputs, fit, inputs_to_samples, x_to_idx, y_to_idx",TableTransform,"inputs, labels, num_features, pad, samples, shapes, types, values, x_columns, y_column",," inconsistent with current , ,, Numbers of columns , TableTransform can not map x to idx. Please override x_to_idx, Y value has to be string, [, auto, multi_label, num_features","0, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 21:00, # It's crucial to use tuple instead of list for all the three, # cases just one column being the doc content. y_column being the single label, which shall be modified, # multi-label is in literal form of a list, # multi-label support, # the second one regardless of t is pair or triple, # to load a list of labels., #multi-label: Dataset in .tsv format: x_columns: at most 2 columns being a sentence pair while in most",
https://github.com/hankcs/HanLP,tacred_tf.py,0,0.0,58,69.88,13,15.66,5,6.02,7,8.43,0,0.0,8,1,22,1,,"TACREDTransform, __init__, add, any, count, create_types_shapes_values, data, deprel, deprel_vocab, end_idx, file_to_inputs, file_to_samples, filepath, fit, get_positions, gold, hanlp, hanlp_common, head, input, inputs, inputs_to_samples, int, len, length, list, lookup, ner, ner_vocab, obj_positions, obj_type, oe, os, pads, pos, pos_vocab, range, rel_vocab, relation, safe_pad_token, se, self, shapes, ss, start_idx, subj_positions, subj_type, super, tensorflow, token_vocab, tokens, trn_path, types, typing, update, x, x_to_idx, y_to_idx","hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp_common.io.load_json, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_locals_kwargs, tf.Tensor, tf.int32, tf.string, typing.Tuple, typing.Union","__init__, create_types_shapes_values, file_to_inputs, fit, get_positions, inputs_to_samples, x_to_idx, y_to_idx",TACREDTransform,"count, data, deprel, head, input, l, ner, obj_positions, obj_type, oe, os, pads, pos, relation, se, shapes, ss, subj_positions, subj_type, tokens, types, x","Get subj/obj position sequence.

Args:
  start_idx: 
  end_idx: 
  length: 

Returns:","Get subj/obj position sequence.

    Args:
      start_idx: 
      end_idx: 
      length: 

    Returns:

    , OBJ-, SUBJ-, obj_end, obj_start, relation, stanford_deprel, stanford_head, stanford_ner, stanford_pos, subj_end, subj_start, token","0, 1, False, None, True","# (tokens, pos, ner, head, deprel, subj_positions, obj_positions, subj_type, obj_type), relation, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-14 17:06, # anonymize tokens, # min head is 0, but root is not included in tokens, so take 1 off from each head, Get subj/obj position sequence.

Args:
start_idx:
end_idx:
length:

Returns:

",
https://github.com/hankcs/HanLP,text_tf.py,0,0.0,72,80.9,8,8.99,6,6.74,3,3.37,0,0.0,11,1,26,0,,"TextTransform, Y, Y_to_outputs, __init__, append, bmes_to_flat, bool, buffer, cells, chars, chunks, config, create_types_shapes_values, defaults, end, file_to_inputs, filepath, fit, forward, gold, hanlp, hanlp_common, idx_to_token, inpath, input, input_is_single_sample, inputs, inputs_to_samples, int, isinstance, kwargs, len, line, list, lookup, map_x, map_y, ms, num_samples, open, out, outpath, pad_token, pop, pred, ret, reversed, self, seq_len, shapes, split, src, start, super, tag, tensorflow, tokenize_func, tokenizer, tokens, trn_path, types, typing, update, vocab, word, words, write, x, x_to_idx, y_to_idx, ys, zip","hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.metrics.chunking.sequence_labeling.get_entities, hanlp.utils.file_read_backwards.FileReadBackwards, hanlp.utils.io_util.read_tsv_as_sents, hanlp_common.structure.SerializableDict, tf.Tensor, tf.argmax, tf.string, typing.Any, typing.Iterable, typing.Tuple, typing.Union","Y_to_outputs, __init__, bmes_to_flat, create_types_shapes_values, file_to_inputs, fit, input_is_single_sample, inputs_to_samples, tokenize_func, x_to_idx, y_to_idx",TextTransform,"buffer, cells, chars, chunks, defaults, end, forward, line, ms, num_samples, out, pred, ret, seq_len, shapes, src, start, tag, tokenizer, tokens, types, word, words, x, y, ys",,", 
,  , /, char, utf-8, w, whitespace","0, 1, 10, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-04 11:46",
https://github.com/hankcs/HanLP,transformer_tokenizer.py,0,0.0,183,55.62,39,11.85,9,2.74,92,27.96,6,1.82,10,3,98,2,,"TransformerSequenceTokenizer, TransformerTextTokenizer, TransformerTokenizer, _KEY, __call__, __index, __init__, __name__, _ids, _init_prefix_mask, _input_ids, _input_tokens, _o, _offset, _offsets, _subtoken_offsets, _subtokens, _test_sequence_transform, _test_text_transform, _tokenizer, _tokens, add_special_tokens, append, args, attention_mask, batch_encode_plus, begin, bes, char_offset, char_per_subtoken, check_space_before, chunk, chunks, cls_is_bos, cls_token, cls_token_at_end, cls_token_id, cls_token_segment_id, config, config_is, convert_examples_to_features, convert_tokens_to_ids, custom_words, data, delta, dict, dict_force, do_basic_tokenize, do_padding, each, encode_plus, encoding, encodings, end, end_piece_ids, enumerate, eos_token, eos_token_id, extend, fixed_ids, fixed_offsets, fixed_tokens, flat_wordpiece_ids, from_pretrained, get, hanlp, hanlp_common, hanlp_trie, has_cls, hasattr, id, ids, input_ids, input_is_str, input_key, input_mask, input_str, input_tokens, insert, int, is_fast, isinstance, isspace, key, label, label_ids, label_map, labels, last_window, len, list, locals, main, mapping, mask, mask_padding_with_zero, max, max_seq_length, missing_token, model, name_or_path, no_padding, no_truncation, non_blank_offsets, object, offset, offsets, offsets_mapping, output_key, outputs, pad_on_left, pad_token, pad_token_id, pad_token_label_id, pad_token_segment_id, padding_length, penultimate_window, pop, prefix_mask, print, range, results, ret_mask_and_type, ret_prefix_mask, ret_subtokens, ret_subtokens_group, ret_token_span, return_offsets_mapping, same_tail, sample, segment_ids, self, sep_is_eos, sep_token, sep_token_extra, sep_token_id, sequence, sequence_a_segment_id, set, sliding_window, span, special_tokens_count, split, start_piece_ids, stride, strip_cls_sep, subtoken_ids, subtoken_ids_per_token, subtoken_offsets, subtokens, suffixes, sum, super, text_a, text_a_key, text_b, text_b_key, token, token_span, token_type_ids, tokenize, tokenize_str, tokenizer, tokenizer_name, tokens, truncate_long_sequences, type, typing, unk_token, unk_token_id, use_fast, v, warnings, window_length, word, word_tokens, wordpiece, wordpiece_ids, wordpiece_windows, words, x, xlnet, zip","hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.layers.transformers.pt_imports.PretrainedConfig, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.structure.SerializableDict, hanlp_trie.DictInterface, typing.Optional, typing.Union, warnings.warn","__call__, __init__, _init_prefix_mask, _test_sequence_transform, _test_text_transform, config_is, convert_examples_to_features, main, sliding_window, tokenize_str","TransformerSequenceTokenizer, TransformerTextTokenizer, TransformerTokenizer","_KEY, __index, _ids, _input_ids, _input_tokens, _o, _offset, _offsets, _subtoken_offsets, _subtokens, _t, _tokens, args, attention_mask, begin, bes, char_offset, char_per_subtoken, check_space_before, chunk, chunks, cls_is_bos, cls_token_at_end, cls_token_segment_id, custom_words, delta, each, encoding, encodings, end, end_piece_ids, fixed_ids, fixed_offsets, fixed_tokens, flat_wordpiece_ids, id, ids, input_ids, input_is_str, input_mask, input_tokens, key, label, label_ids, labels, last_window, mapping, mask, max_seq_length, missing_token, non_blank_offsets, offset, offsets, offsets_mapping, output_key, outputs, pad_on_left, pad_token, pad_token_label_id, pad_token_segment_id, padding_length, penultimate_window, prefix_mask, results, ret_token_span, return_offsets_mapping, sample, segment_ids, sep_is_eos, sequence, span, special_tokens_count, start_piece_ids, stride, subtoken_ids, subtoken_ids_per_token, subtoken_offsets, subtokens, suffixes, text_a, text_b, token, token_span, token_type_ids, tokenizer, tokenizer_name, tokens, transform, transformer, v, window_length, word, word_tokens, wordpiece, wordpiece_ids, wordpiece_windows, x, xlnet","A transformer tokenizer for token-level tasks. It honors the boundary of tokens and tokenize each token into
several subtokens then merge them. The information about each subtoken belongs to which token are kept and
returned as a new field in the sample. It also provides out-of-box sliding window trick on long sequences.

Args:
    tokenizer: The identifier of a pre-trained tokenizer or a ``PreTrainedTokenizer``.
    input_key: The token key in samples.
    output_key: The output keys to store results.
        max_seq_length: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    truncate_long_sequences: ``True`` to truncate exceeded parts of long sequences. ``False`` to  enable
        sliding window.
    config: The ``PretrainedConfig`` to determine the model structure of the transformer, so that special
        tokenization can be applied.
    cls_token_at_end: ``True`` to put ``[CLS]`` at the end of input tokens.
    cls_token_segment_id: The id of ``[CLS]``.
    pad_token_segment_id: The id of ``[SEP]``.
    pad_on_left: ``True`` to put ``[PAD]`` at the left side of input tokens.
    do_padding: ``True`` to pad sequence to the left.
    sep_token_extra: ``True`` to have two ``[SEP]``.
    ret_mask_and_type: ``True`` to return masks and type ids.
    ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
    ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
    ret_subtokens: ``True`` to return list of subtokens belonging to each token for tokenization purpose.
        When enabled, the prefix mask for each subtoken is set to True as each subtoken is a token unit in
        tokenization task. Similarity, the token span for each token will be a continuous integer sequence.
    ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
    cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
                ``False`` (default) means the first token is not [CLS], it will have its own embedding other than
                the embedding of [CLS].
    sep_is_eos: ``True`` means the last token of input is [SEP].
                ``False`` means it's not but [SEP] will be appended,
                ``None`` means it dependents on `input[-1] == [EOS]`.
    do_basic_tokenize: Whether to do basic tokenization before wordpiece.
    use_fast: Whether or not to try to load the fast version of the tokenizer.
    dict_force: A dictionary doing longest-prefix-match on input text so that the head and tail of each keyword
        won't be concatenated to other tokens by transformer tokenizers.
    strip_cls_sep: ``True`` to strip [CLS] and [SEP] off the input tokens.
    check_space_before: ``True`` to detect the space before each token to handle underline in sentence piece
        tokenization.

Examples:

.. highlight:: python
.. code-block:: python

    transform = TransformerSequenceTokenizer('bert-base-uncased', 'token')
    sample = {'token': 'HanLP good'.split()}
    print(transform(sample)), Loads a data file into a list of `InputBatch`s
    `cls_token_at_end` define the location of the CLS token:
        - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
        - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
    `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

Args:
  words: 
  max_seq_length: 
  tokenizer: 
  labels:  (Default value = None)
  label_map:  (Default value = None)
  cls_token_at_end:  (Default value = False)
  cls_token:  (Default value = ""[CLS]"")
  cls_token_segment_id:  (Default value = 1)
  sep_token:  (Default value = ""[SEP]"")
  sep_token_extra:  (Default value = False)
  pad_on_left:  (Default value = False)
  pad_token_id:  (Default value = 0)
  pad_token_segment_id:  (Default value = 0)
  pad_token_label_id:  (Default value = 0)
  sequence_a_segment_id:  (Default value = 0)
  mask_padding_with_zero:  (Default value = True)
  unk_token:  (Default value = '[UNK]')
  do_padding:  (Default value = True)

Returns:",",  ,  exceed the max sequence length of ,  tokens beforehand.,  tokens beforehand.Or simply set truncate_long_sequences = False to enable sliding window., . The exceeded part will be truncated and ignored. You are recommended to split your long text into several sentences within , A transformer tokenizer for token-level tasks. It honors the boundary of tokens and tokenize each token into
        several subtokens then merge them. The information about each subtoken belongs to which token are kept and
        returned as a new field in the sample. It also provides out-of-box sliding window trick on long sequences.

        Args:
            tokenizer: The identifier of a pre-trained tokenizer or a ``PreTrainedTokenizer``.
            input_key: The token key in samples.
            output_key: The output keys to store results.
                max_seq_length: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            truncate_long_sequences: ``True`` to truncate exceeded parts of long sequences. ``False`` to  enable
                sliding window.
            config: The ``PretrainedConfig`` to determine the model structure of the transformer, so that special
                tokenization can be applied.
            cls_token_at_end: ``True`` to put ``[CLS]`` at the end of input tokens.
            cls_token_segment_id: The id of ``[CLS]``.
            pad_token_segment_id: The id of ``[SEP]``.
            pad_on_left: ``True`` to put ``[PAD]`` at the left side of input tokens.
            do_padding: ``True`` to pad sequence to the left.
            sep_token_extra: ``True`` to have two ``[SEP]``.
            ret_mask_and_type: ``True`` to return masks and type ids.
            ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
            ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
            ret_subtokens: ``True`` to return list of subtokens belonging to each token for tokenization purpose.
                When enabled, the prefix mask for each subtoken is set to True as each subtoken is a token unit in
                tokenization task. Similarity, the token span for each token will be a continuous integer sequence.
            ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
            cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
                        ``False`` (default) means the first token is not [CLS], it will have its own embedding other than
                        the embedding of [CLS].
            sep_is_eos: ``True`` means the last token of input is [SEP].
                        ``False`` means it's not but [SEP] will be appended,
                        ``None`` means it dependents on `input[-1] == [EOS]`.
            do_basic_tokenize: Whether to do basic tokenization before wordpiece.
            use_fast: Whether or not to try to load the fast version of the tokenizer.
            dict_force: A dictionary doing longest-prefix-match on input text so that the head and tail of each keyword
                won't be concatenated to other tokens by transformer tokenizers.
            strip_cls_sep: ``True`` to strip [CLS] and [SEP] off the input tokens.
            check_space_before: ``True`` to detect the space before each token to handle underline in sentence piece
                tokenization.

        Examples:

        .. highlight:: python
        .. code-block:: python

            transform = TransformerSequenceTokenizer('bert-base-uncased', 'token')
            sample = {'token': 'HanLP good'.split()}
            print(transform(sample))

        , HanLP good, Input tokens , Loads a data file into a list of `InputBatch`s
        `cls_token_at_end` define the location of the CLS token:
            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

    Args:
      words: 
      max_seq_length: 
      tokenizer: 
      labels:  (Default value = None)
      label_map:  (Default value = None)
      cls_token_at_end:  (Default value = False)
      cls_token:  (Default value = ""[CLS]"")
      cls_token_segment_id:  (Default value = 1)
      sep_token:  (Default value = ""[SEP]"")
      sep_token_extra:  (Default value = False)
      pad_on_left:  (Default value = False)
      pad_token_id:  (Default value = 0)
      pad_token_segment_id:  (Default value = 0)
      pad_token_label_id:  (Default value = 0)
      sequence_a_segment_id:  (Default value = 0)
      mask_padding_with_zero:  (Default value = True)
      unk_token:  (Default value = '[UNK]')
      do_padding:  (Default value = True)

    Returns:

    , [CLS], [SEP], [UNK], _, __main__, _subtoken_offsets, _subtoken_offsets_group, _tokenizer, attention_mask, bert, bert-base-uncased, custom_words, failed for:
 , google/mt5-base, google/mt5-small, input_ids, mMiniLMv2L12-no-space, mMiniLMv2L6-no-space, no_truncation, prefix_mask, text, token, token_span, token_type_ids, xlm-roberta-base, xlm-roberta-base-no-space, xlm-roberta-large, xlnet, ▁","0, 1, 2, 3, 4, 512, False, None, True","#, #             'To use this feature, set use_fast = True.'), #             'ret_subtokens is not available when using Python tokenizers. ', #         input_ids = [tokenizer.unk_token_id], #         input_ids.insert(1, tokenizer.unk_token_id), #         input_tokens = [input_str], #         input_tokens.insert(1, input_str), #         prefix_mask[1] = False, #         raise NotImplementedError(, #         subtoken_offsets = [(0, len(input_str))], #         subtoken_offsets.append((0, len(input_str))), #     f'For sequence tasks, truncate_long_sequences = True is not supported.', #     f'Input tokens {input_tokens} exceed the max sequence length of {self.max_seq_length - 2}. ', #     f'Or simply set truncate_long_sequences = False to enable sliding window.'), #     f'You are recommended to split your long text into several sentences within ', #     f'{self.max_seq_length - 2} tokens beforehand. ', #     if len(input_tokens) == 2:  # bos and eos, meaning that the text contains only some spaces, #     if not input_ids:  # This chunk might be some control chars getting removed by tokenizer, #     if not use_fast:, #     if offsets_mapping[0] and not input_tokens[0].startswith(' '):, #  token_type_ids:   0   0   0   0  0     0   0, #  token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1, #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP], #  tokens:   [CLS] the dog is hairy . [SEP], # (a) For sequence pairs:, # (b) For single sequences:, # -*- coding:utf-8 -*-, # 1. During decoding, mostly no ▁ will be created unless blanks are placed between tokens (which, # 2. For T5, '▁' is used as CLS, # Account for [CLS] and [SEP] with ""- 2"" and with ""- 3"" for RoBERTa., # Author: hankcs, # Check for overlap in the last window. Throw it away if it is redundant., # Date: 2020-05-03 16:23, # Dirty fix upstream bug: https://github.com/hankcs/HanLP/issues/1602, # Edge case that the input_str is swallowed in whole, # Fill up missing non-blank characters swallowed by HF tokenizer, # For classification tasks, the first vector (corresponding to [CLS]) is, # For tokenization of raw text, # In the future, we may want space back, # LI™ -> LIT + M, # MT5 generates tokens like ▁of, which is bad for the tokenizer. So we want to remove the prefix., # Many tokenizers do not offer fast version, # Match original text directly, # None means to check whether sep is at the tail or between tokens, # Remove ▁ generated by spm for 2 reasons:, # Since non-fast tok generates no mapping, we have to guess, # Some tokens get stripped out, # TODO: other fields should be properly handled too, # The convention in BERT is:, # The following block will tokenize each empty string (space) into an unk token, # The mask has 1 for real tokens and 0 for padding tokens. Only real, # These tokenizer is BPE-based which appends a space before each token and tokenizes loving into, # This happens in a tokenizer component where the raw sentence is fed., # Use the real label id for the first token of the word, and padding ids for the remaining tokens, # Use transformed text as it's what models are trained on, # Where ""token_type_ids"" are used to indicate whether this is the first, # Zero-pad up to the sequence length., # ['▁lo', 'ving'], tokenize 商品 into ['▁', '商品']. For the later case, the prefix '▁' has to be removed, # _test_text_transform(tokenizer), # `type=1` were learned during pre-training and are added to the wordpiece, # as there is no space between some languages like Chinese, # chunk offset is on char level, at this moment, there is no concept of tokens, just subtokens, # else self.sep_is_eos == False means sep is between tokens and don't bother to check, # else:, # embedding vector (and position vector). This is not *strictly* necessary, # if add_special_tokens:, # if self.check_space_before:, # if self.ret_subtokens:, # is true for English but in English it will likely be concatenated to the token following it), # it easier for the model to learn the concept of sequences., # mt5 doesn't have cls or sep, but we can use something similar, # noinspection PyShadowingNames, # noinspection PyUnboundLocalVariable, # offsets = [(offsets[0][0], offsets[-1][-1])], # only need input_ids and token_span, use a light version, # pad on the left for xlnet, # raise SequenceTooLong(, # roberta uses an extra separator b/w pairs of sentences, # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805, # sequence or the second sequence. The embedding vectors for `type=0` and, # since the [SEP] token unambiguously separates the sequences, but it makes, # skip [CLS] and [SEP], # some wired chars cause the tagger to return empty list, # the entire model is fine-tuned., # the token is an empty string, # tokens are attended to., # track the subword offset of this chunk, -1 for [CLS], # used as as the ""sentence vector"". Note that this only makes sense because, # xlnet has a cls token at the end, # … --> ..., A transformer tokenizer for token-level tasks. It honors the boundary of tokens and tokenize each token into
several subtokens then merge them. The information about each subtoken belongs to which token are kept and
returned as a new field in the sample. It also provides out-of-box sliding window trick on long sequences.

Args:
tokenizer: The identifier of a pre-trained tokenizer or a ``PreTrainedTokenizer``.
input_key: The token key in samples.
output_key: The output keys to store results.
max_seq_length: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
truncate_long_sequences: ``True`` to truncate exceeded parts of long sequences. ``False`` to  enable
sliding window.
config: The ``PretrainedConfig`` to determine the model structure of the transformer, so that special
tokenization can be applied.
cls_token_at_end: ``True`` to put ``[CLS]`` at the end of input tokens.
cls_token_segment_id: The id of ``[CLS]``.
pad_token_segment_id: The id of ``[SEP]``.
pad_on_left: ``True`` to put ``[PAD]`` at the left side of input tokens.
do_padding: ``True`` to pad sequence to the left.
sep_token_extra: ``True`` to have two ``[SEP]``.
ret_mask_and_type: ``True`` to return masks and type ids.
ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
ret_subtokens: ``True`` to return list of subtokens belonging to each token for tokenization purpose.
When enabled, the prefix mask for each subtoken is set to True as each subtoken is a token unit in
tokenization task. Similarity, the token span for each token will be a continuous integer sequence.
ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
``False`` (default) means the first token is not [CLS], it will have its own embedding other than
the embedding of [CLS].
sep_is_eos: ``True`` means the last token of input is [SEP].
``False`` means it's not but [SEP] will be appended,
``None`` means it dependents on `input[-1] == [EOS]`.
do_basic_tokenize: Whether to do basic tokenization before wordpiece.
use_fast: Whether or not to try to load the fast version of the tokenizer.
dict_force: A dictionary doing longest-prefix-match on input text so that the head and tail of each keyword
won't be concatenated to other tokens by transformer tokenizers.
strip_cls_sep: ``True`` to strip [CLS] and [SEP] off the input tokens.
check_space_before: ``True`` to detect the space before each token to handle underline in sentence piece
tokenization.

Examples:

.. highlight:: python
.. code-block:: python

transform = TransformerSequenceTokenizer('bert-base-uncased', 'token')
sample = {'token': 'HanLP good'.split()}
print(transform(sample))

, Loads a data file into a list of `InputBatch`s
`cls_token_at_end` define the location of the CLS token:
- False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
- True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
`cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

Args:
words:
max_seq_length:
tokenizer:
labels:  (Default value = None)
label_map:  (Default value = None)
cls_token_at_end:  (Default value = False)
cls_token:  (Default value = ""[CLS]"")
cls_token_segment_id:  (Default value = 1)
sep_token:  (Default value = ""[SEP]"")
sep_token_extra:  (Default value = False)
pad_on_left:  (Default value = False)
pad_token_id:  (Default value = 0)
pad_token_segment_id:  (Default value = 0)
pad_token_label_id:  (Default value = 0)
sequence_a_segment_id:  (Default value = 0)
mask_padding_with_zero:  (Default value = True)
unk_token:  (Default value = '[UNK]')
do_padding:  (Default value = True)

Returns:

","1. During decoding, mostly no ▁ will be created unless blanks are placed between tokens (which, 2. For T5, '▁' is used as CLS, LI™ -> LIT + M, MT5 generates tokens like ▁of, which is bad for the tokenizer. So we want to remove the prefix., Remove ▁ generated by spm for 2 reasons:, ['▁lo', 'ving'], tokenize 商品 into ['▁', '商品']. For the later case, the prefix '▁' has to be removed"
https://github.com/hankcs/HanLP,tsv_tf.py,0,0.0,86,82.69,7,6.73,8,7.69,3,2.88,0,0.0,15,2,24,0,,"TSVTaggingTransform, TsvTaggingFormat, X, X_to_inputs, Y, Y_to_outputs, __init__, abc, add, append, batch_size, bool, cells, char_vocab, config, create_types_shapes_values, dataset, dataset_from_generator, dataset_from_tsv, defaults, file_to_inputs, filepath, first_token, fit, functools, generator, get, gold, gold_tag, hanlp, hanlp_common, idx_to_token, input, input_is_single_sample, input_truth_output_to_str, inputs, inputs_to_samples, int, isinstance, keys, len, line, list, lock, lock_char_vocab, lock_tag_vocab, lock_word_vocab, lookup, lower, max_seq_length, num_samples, open, output, pad_token, padded_batch, padding_values, pred_tag, prefetch, property, repeat, self, shapes, shuffle, strip, super, tag_vocab, tensorflow, text, token_to_idx, trn_path, truth, tsv_file, tsv_file_path, types, typing, unk_token, update, values, vocab_from_tsv, word_vocab, x, x_to_idx, xs, y_to_idx, ys, zip","abc.ABC, functools.partial, hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.utils.io_util.generate_words_tags_from_tsv, hanlp.utils.tf_util.str_tensor_to_str, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_locals_kwargs, tf.Tensor, tf.argmax, tf.data.Dataset.from_generator, tf.string, typing.Iterable, typing.List, typing.Optional, typing.Tuple, typing.Union","X_to_inputs, Y_to_outputs, __init__, create_types_shapes_values, dataset_from_generator, dataset_from_tsv, file_to_inputs, fit, input_is_single_sample, input_truth_output_to_str, inputs_to_samples, max_seq_length, vocab_from_tsv, x_to_idx, y_to_idx","TSVTaggingTransform, TsvTaggingFormat","Y, cells, char_vocab, dataset, defaults, generator, gold_tag, line, lower, num_samples, pred_tag, shapes, shuffle, tag_vocab, tags, text, tsv_file, types, values, word_vocab, words, x, xs, ys",,", 
,  , TsvTaggingFormat does not support reading non-gold files, lower, max_seq_length, utf-8","0, 1, 1024, 2, 32, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-06-13 21:15",
https://github.com/hankcs/HanLP,txt_tf.py,0,0.0,85,71.43,11,9.24,12,10.08,11,9.24,0,0.0,13,2,29,1,,"TxtBMESFormat, TxtFormat, ValueError, X, Y, Y_to_outputs, Y_to_tokens, abc, append, batch_size, bigram_only, bool, cells, char_vocab, chars, config, dataset, dataset_from_generator, dataset_from_txt, defaults, delimiter, end, extend, extract_ngram_features, extract_ngram_features_and_tags, file_path, file_to_inputs, filepath, filter, first_token, functools, generate_ngram_bmes, generate_words_per_line, generator, get, gold, hanlp, idx_to_token, input, input_is_single_sample, inputs, inputs_to_samples, int, isinstance, len, line, list, max_seq_length, ngram, ngram_size, ngram_vocab, normalize_chars, open, pad_token, padded_batch, prefetch, repeat, ret, safe_pad_token, segmented, self, sentence, shapes, short_chars, shuffle, src, start, strip, super, tag_vocab, tags, tensorflow, text, tuple, txt_file_path, types, typing, update, vec_dim, vocab_from_txt, window_size, words_to_bmes, x, ys, zip","abc.ABC, functools.partial, hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.utils.io_util.get_resource, hanlp.utils.lang.zh.char_table.CharTable, hanlp.utils.span_util.bmes_of, hanlp.utils.span_util.bmes_to_words, hanlp.utils.string_util.split_long_sent, tf.Tensor, tf.argmax, tf.data.Dataset.from_generator, tf.string, typing.Iterable, typing.List, typing.Tuple, typing.Union","Y_to_outputs, Y_to_tokens, dataset_from_generator, dataset_from_txt, extract_ngram_features, extract_ngram_features_and_tags, file_to_inputs, generate_ngram_bmes, generate_words_per_line, input_is_single_sample, inputs_to_samples, vocab_from_txt, words_to_bmes","TxtBMESFormat, TxtFormat","X, Y, cells, char_vocab, chars, dataset, defaults, delimiter, end, filepath, generator, line, max_seq_length, ngram, ngram_size, ngram_vocab, ret, sentence, shapes, short_chars, shuffle, src, start, tag_vocab, tags, text, types, vec_dim, ys","Feature extraction for windowed approaches
See Also https://github.com/chqiwang/convseg/
Parameters
----------
sentence
bigram_only
window_size
segmented

Returns
-------",", 
    Feature extraction for windowed approaches
    See Also https://github.com/chqiwang/convseg/
    Parameters
    ----------
    sentence
    bigram_only
    window_size
    segmented

    Returns
    -------

    , B, E, M, S, max_seq_length, transformer, utf-8, {} contains None or zero-length word {}, 。！？：；、，,;!?、,","0, 1, 1024, 2, 3, 32, 4, 5, 6, False, None, True","
Feature extraction for windowed approaches
See Also https://github.com/chqiwang/convseg/
Parameters
----------
sentence
bigram_only
window_size
segmented

Returns
-------

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-24 15:07, # TODO: optimize ngram generation using https://www.tensorflow.org/api_docs/python/tf/strings/ngrams, # allow for [CLS] and [SEP], # bi chars, # four chars, # single char, # tri chars, # x, y",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 22:24",
https://github.com/hankcs/HanLP,component_util.py,0,0.0,65,40.37,79,49.07,5,3.11,11,6.83,1,0.62,2,0,17,1,,"ALL, ConnectionError, Exception, FileNotFoundError, LID_176_FASTTEXT_BASE, LID_176_FASTTEXT_SMALL, ModuleNotFoundError, NotCompatible, ValueError, __class__, __name__, __version__, args, classifiers, cls, config, dict, difflib, endswith, extras, fasttext, from_config, get, hanlp, hanlp_common, hasattr, identifier, installed_version, isinstance, isupper, keys, kwargs, latest_version, len, load, load_from_meta, load_from_meta_file, load_path, load_transform, meta, meta_filename, metapath, model_version, name, os, pkg_resources, platform, range, save_dir, similar_keys, sorted, sys, tensorflow, tf_e, tf_model, tf_version, tips, torch, transform_only, tuple, upgrade, values, verbose, word2vec, you_installed_wrong_versions","difflib.SequenceMatcher, hanlp.common.component.Component, hanlp.pretrained, hanlp.utils.io_util.check_version_conflicts, hanlp.utils.io_util.get_latest_info_from_pypi, hanlp.utils.io_util.get_resource, hanlp.version, hanlp_common.constant.HANLP_VERBOSE, hanlp_common.io.eprint, hanlp_common.io.load_json, hanlp_common.io.save_json, hanlp_common.reflection.object_from_classpath, hanlp_common.reflection.str_to_type, hanlp_common.util.isdebugging, hanlp_common.util.set_tuple_with, os.path.basename, os.path.dirname, os.path.isfile, os.path.join, pkg_resources.parse_version, platform.platform, platform.python_version, sys.stderr.flush, tensorflow.__version__, torch.__version__","load_from_meta, load_from_meta_file",,"cls, extras, identifier, installed_version, latest_version, load_path, meta, meta_filename, metapath, model_version, save_dir, similar_keys, tf_model, tf_version, tips, upgrade, you_installed_wrong_versions","Load a component from a ``meta.json`` (legacy TensorFlow component) or a ``config.json`` file.

Args:
    save_dir: The identifier.
    meta_filename (str): The meta file of that saved component, which stores the classpath and version.
    transform_only: Load and return only the transform.
    **kwargs: Extra parameters passed to ``component.load()``.

Returns:

    A component.",", 
, 

	pip install --upgrade hanlp, 
    Load a component from a ``meta.json`` (legacy TensorFlow component) or a ``config.json`` file.

    Args:
        save_dir: The identifier.
        meta_filename (str): The meta file of that saved component, which stores the classpath and version.
        transform_only: Load and return only the transform.
        **kwargs: Extra parameters passed to ``component.load()``.

    Returns:

        A component.
    , 
Please reinstall HanLP in the proper way:,  doesn't contain classpath field,  etc.) required by this model are missing. Please install the full version:

	pip install hanlp[full] -U,  resolves to a nonexistent meta file ,  was created with hanlp-, ,, , while you are running a lower version: , . , . Note this is not a bug of HanLP, but rather a compatability issue caused by TensorFlow., .json, 2.0.0, 2.0.0-alpha.0, : , =^80, Check its spelling based on the available keys:
, ERROR LOG BEGINS, ERROR LOG ENDS, Failed to load , HanLP: , Hugging Face 🤗 Transformers failed to download because your Internet connection is either off or bad.
See https://hanlp.hankcs.com/docs/install.html#server-without-internet for solutions., If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues
When reporting an issue, make sure to paste the FULL ERROR LOG below., Internet connection, OS: , Please upgrade HanLP with:

	pip install --upgrade hanlp
, PyTorch: , Python: , Some modules (, TensorFlow cannot be imported due to , TensorFlow: , The identifier , Tips: it might be one of , [, ], class_path, classpath, config.json, embed, field, filepath, full, hanlp.components.classifiers.fasttext_classifier.FastTextClassifier, hanlp.components.classifiers.transformer_classifier.TransformerClassifier, hanlp.components.classifiers.transformer_classifier_tf.TransformerClassifierTF, hanlp.components.ner.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.components.ner_tf.TransformerNamedEntityRecognizerTF, hanlp.components.parsers.biaffine_parser.BiaffineDependencyParser, hanlp.components.parsers.biaffine_parser.BiaffineSemanticDependencyParser, hanlp.components.parsers.biaffine_parser_tf.BiaffineDependencyParserTF, hanlp.components.parsers.biaffine_parser_tf.BiaffineSemanticDependencyParserTF, hanlp.components.pos.RNNPartOfSpeechTagger, hanlp.components.pos_tf.CNNPartOfSpeechTaggerTF, hanlp.components.pos_tf.RNNPartOfSpeechTaggerTF, hanlp.components.taggers.pos_tf.CNNPartOfSpeechTaggerTF, hanlp.components.taggers.pos_tf.RNNPartOfSpeechTaggerTF, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF, hanlp.components.tok.NgramConvTokenizer, hanlp.components.tok_tf.NgramConvTokenizerTF, hanlp.components.tok_tf.TransformerTokenizerTF, hanlp.components.tokenizers.tok_tf.NgramConvTokenizerTF, hanlp.components.tokenizers.tok_tf.TransformerTokenizerTF, hanlp.layers.embeddings.fast_text.FastTextEmbedding, hanlp.layers.embeddings.fast_text.FastTextEmbeddingComponent, hanlp.layers.embeddings.word2vec.Word2VecEmbedding, hanlp.layers.embeddings.word2vec.Word2VecEmbeddingComponent, hanlp_version, l2, load, load_path, meta.json, model_path, normalize, not installed, src, token","0, 5, False, None, True","
Load a component from a ``meta.json`` (legacy TensorFlow component) or a ``config.json`` file.

Args:
save_dir: The identifier.
meta_filename (str): The meta file of that saved component, which stores the classpath and version.
transform_only: Load and return only the transform.
**kwargs: Extra parameters passed to ``component.load()``.

Returns:

A component.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-31 19:24, # For older version, # Quick fix: the first version used a wrong string, # Some users often install an incompatible tf and put the blame on HanLP. Teach them the basics., # These components are not intended to be loaded in this way, but I'm tired of explaining it again and again, # noinspection PyUnresolvedReferences, # tf models are trained with version < 2.1. To migrate them to 2.1, map their classpath to new locations, #server-without-internet for solutions.') \",🤗
https://github.com/hankcs/HanLP,init_util.py,0,0.0,11,64.71,0,0.0,3,17.65,3,17.65,0,0.0,1,0,3,0,,"bound, embedding_uniform, fan_out, functools, gen, math, seed, size, tensor, torch, uniform_","math.sqrt, torch.Generator, torch.Tensor, torch.nn, torch.no_grad",embedding_uniform,,"bound, fan_out, gen",,,"1, 233, 3.0","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-27 13:25",
https://github.com/hankcs/HanLP,io_util.py,0,0.0,234,67.05,91,26.07,12,3.44,12,3.44,0,0.0,42,1,93,9,,"BaseException, Exception, FileNotFoundError, RuntimeError, TimingFileIterator, ValueError, __init__, __iter__, _path, accumulated, anchor, append, append_location, archive, args, basename_no_ext, bool, cache_name, cache_time, cache_valid, cells, char_level, check_outdated, check_version_conflicts, child, close, cmd, communicate, compressed, compressed_ext, contextlib, copied, current, debug, decode, delimiter, dest, dev, dict, domain, download, dst, encode, endswith, enumerate, erase, err, error, exitcode, extract, extractall, extras, f_in, f_out, fd, file_cache, file_exist, file_extension, file_is_zip, file_or_fd, file_prefix, file_size, file_time, filelock, filenames, fileno, filepath, files, filter, flush, folder_name, fp, generate_words_tags_from_tsv, get, get_exitcode_stdout_stderr, get_latest_info_from_pypi, get_resource, getattr, getnames, glob, gold, gzip, hanlp, hanlp_common, hanlp_downloader, hanlp_home, hanlp_home_default, hard_constraint, hasattr, hashlib, hints, human_bytes, idx, ignore_prefix, info, installed_version, interval, isinstance, items, iter, join, json, keys, latest_version, line, load_jsonl, log, logging, lower, lstrip, make_debug_corpus, makedirs, max_samples, max_seq_length, merge_files, message, middle, min, msg, namelist, names, netloc, new_dir, newline, next, no_ext, num_samples, offset, os, out, outs, package, packaging, parent_dir, parse_url_path, parsed, parts, path, path_from_url, path_join, pathlib, paths, pattern, percentage, pkg_resources, platform, prefix, previous_dir, property, purge, pushd, random, range, ratio, ratio_percentage, ratio_width, read, read_cells, read_tsv_as_sents, readline, realpath, relative_path, remove_file, renamed_realpath, replace_ext, repository_url, repr, requirements, requires, response, returncode, root, root_of_folder, roots, run_cmd, sample, samples, save_dir, save_path, self, sent_delimiter, set, shlex, shorter_tags, shorter_words, shuffle, shutil, sid, skip_header, sorted, split, split_file, split_if_compressed, splits, src, start_sync, startswith, stdout, stdout_fd, stdout_redirected, step, strip, subprocess, subscribe, sum, super, sys, system, tags, tar_gz, tarfile, tell, temp_lock, tempdir, tempdir_human, tempfile, test, tmp_path, to_file, total, tsv_file, tsv_file_path, tuple, typing, uncompress, urllib, values, verbose, windows, with_context, word, words, write, zipfile","contextlib.contextmanager, filelock.FileLock, glob.glob, gzip.open, hanlp.pretrained.ALL.get, hanlp.utils.log_util.cprint, hanlp.utils.log_util.logger, hanlp.utils.log_util.remove_color_tag, hanlp.utils.string_util.split_long_sentence_into, hanlp.utils.time_util.CountdownTimer, hanlp.utils.time_util.now_filename, hanlp.version.__version__, hanlp_common.constant.HANLP_URL, hanlp_common.constant.HANLP_VERBOSE, hanlp_common.io.eprint, hanlp_downloader.Downloader, hanlp_downloader.log.DownloadCallback, hashlib.md5, json.loads, logging.Logger, os.chdir, os.devnull, os.dup, os.dup2, os.environ.get, os.fdopen, os.getcwd, os.getenv, os.listdir, os.makedirs, os.pardir, os.path.basename, os.path.dirname, os.path.exists, os.path.expanduser, os.path.getmtime, os.path.getsize, os.path.isdir, os.path.isfile, os.path.join, os.path.normpath, os.path.splitext, os.remove, os.rename, packaging.version.Version, pathlib.Path, pkg_resources.DistributionNotFound, pkg_resources.Requirement, pkg_resources.VersionConflict, pkg_resources.WorkingSet, pkg_resources.get_distribution, platform.platform, platform.system, random.shuffle, shlex.split, shutil.copyfileobj, shutil.rmtree, subprocess.PIPE, subprocess.Popen, sys.stdout, tarfile.open, tempfile.gettempdir, typing.List, typing.Optional, typing.Tuple, typing.Union, urllib.parse.ParseResult, urllib.parse.urlparse, urllib.request.urlopen, urllib.request.urlretrieve, zipfile.ZipFile","__init__, __iter__, basename_no_ext, check_outdated, check_version_conflicts, close, download, file_cache, file_exist, fileno, generate_words_tags_from_tsv, get_exitcode_stdout_stderr, get_latest_info_from_pypi, get_resource, hanlp_home, hanlp_home_default, human_bytes, load_jsonl, log, make_debug_corpus, makedirs, merge_files, parent_dir, parse_url_path, path_from_url, path_join, pushd, ratio, ratio_width, read_cells, read_tsv_as_sents, remove_file, replace_ext, run_cmd, split_file, split_if_compressed, stdout_redirected, temp_lock, tempdir, tempdir_human, uncompress, windows",TimingFileIterator,"_path, accumulated, anchor, archive, args, basename, cache_name, cache_time, cache_valid, cells, child, compressed, copied, delimiter, dest, domain, downloader, err, error, exitcode, extras, f_in, f_out, fd, file_extension, file_is_zip, file_prefix, file_size, file_time, filenames, filepath, files, folder_name, fp, hints, idx, installed_version, latest_version, line, lock, max_samples, message, middle, namelist, names, no_ext, num_samples, offset, out, outs, parsed, parts, path, pattern, pkg, prefix, previous_dir, proc, r, ratio, read, realpath, relative_path, renamed_realpath, requirements, response, root, root_of_folder, roots, sample, samples, save_dir, save_path, sent, shorter_tags, shorter_words, shuffle, sid, splits, src, stdout, stdout_fd, system, tags, tar_gz, tmp_path, to_file, tsv_file, tsv_file_path, url, word, words, write","Default data directory depending on the platform and environment variables, Execute the external command and get its exitcode, stdout and stderr.
See https://stackoverflow.com/a/21000308/3730690

Args:
  cmd: Command.

Returns:
    Exit code, stdout, stderr., Fetch real (local) path for a resource (model, corpus, whatever) to ``save_dir``.

Args:
  path: A local path (which will returned as is) or a remote URL (which will be downloaded, decompressed then
    returned).
  save_dir: Where to store the resource (Default value = :meth:`hanlp.utils.io_util.hanlp_home`)
  extract: Whether to unzip it if it's a zip file (Default value = True)
  prefix: A prefix when matched with an URL (path), then that URL is considered to be official. For official
    resources, they will not go to a folder called ``thirdparty`` under :const:`~hanlp_common.constants.HANLP_HOME`.
  append_location: Whether to put unofficial files in a ``thirdparty`` folder.
  verbose: Whether to print log messages.

Returns:
  The real path to the resource., Given the name of a package on PyPI and a version (both strings), checks
if the given version is the latest version of the package available.
Returns a 2-tuple (installed_version, latest_version)
`repository_url` is a `%` style format string
to use a different repository PyPI repository URL,
e.g. test.pypi.org or a private repository.
The string is formatted with the package name.
Adopted from https://github.com/alexmojaki/outdated/blob/master/outdated/__init__.py

Args:
    package: Package name.
    version: Installed version string.
    repository_url: URL on pypi.

Returns:
    Parsed installed version and latest version., Home directory for HanLP resources.

Returns:
    Data directory in the filesystem for storage, for example when downloading models.

This home directory can be customized with the following shell command or equivalent environment variable on Windows
systems.

.. highlight:: bash
.. code-block:: bash

    $ export HANLP_HOME=/data/hanlp, Map a URL to a local path.

Args:
    url: Remote URL.
    save_dir: The root folder to save this file.
    prefix: The prefix of official website. Any URLs starting with this prefix will be considered official.
    append_location: Whether to put unofficial files in a ``thirdparty`` folder.

Returns:
    The real path that this URL is mapped to., Redirect stdout to else where.
Copied from https://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262

Args:
  to:  Target device.
  stdout:  Source device., Replace the extension of filepath to ext.

Args:
    filepath: Filepath to be replaced.
    ext: Extension to replace.

Returns:
    A new path., Uncompress a file and clean up uncompressed files once an error is triggered.

Args:
  path: The path to a compressed file
  dest: The dest folder.
  remove: Remove archive file after decompression.
  verbose: ``True`` to print log message.

Returns:
    Destination path.",", 	, 
, 

, 
The command is:
,  (,  -O ,  Home directory for HanLP resources.

    Returns:
        Data directory in the filesystem for storage, for example when downloading models.

    This home directory can be customized with the following shell command or equivalent environment variable on Windows
    systems.

    .. highlight:: bash
    .. code-block:: bash

        $ export HANLP_HOME=/data/hanlp

    ,  Replace the extension of filepath to ext.

    Args:
        filepath: Filepath to be replaced.
        ext: Extension to replace.

    Returns:
        A new path.
    ,  by yourself , #, %.1f GB, %.1f MB, %d KB, ), ) with:[/green]

	[yellow]pip install -U hanlp[/yellow]
, ,, , will use the first one., ., .*, ./, .cache, .conll, .conllu, .conllx, .csv, .debug, .downloading, .gz, .hanlp, .lock, .tar.gz, .tgz, .tsv, .xz, .zip, /, /., : , APPDATA, Decompressing {} to {}, Default data directory depending on the platform and environment variables, Download failed due to [red], Downloading {} to {}, Execute the external command and get its exitcode, stdout and stderr.
    See https://stackoverflow.com/a/21000308/3730690

    Args:
      cmd: Command.

    Returns:
        Exit code, stdout, stderr.
    , Expected a file (`.fileno()`) or a file descriptor, Failed to load , Fetch real (local) path for a resource (model, corpus, whatever) to ``save_dir``.

    Args:
      path: A local path (which will returned as is) or a remote URL (which will be downloaded, decompressed then
        returned).
      save_dir: Where to store the resource (Default value = :meth:`hanlp.utils.io_util.hanlp_home`)
      extract: Whether to unzip it if it's a zip file (Default value = True)
      prefix: A prefix when matched with an URL (path), then that URL is considered to be official. For official
        resources, they will not go to a folder called ``thirdparty`` under :const:`~hanlp_common.constants.HANLP_HOME`.
      append_location: Whether to put unofficial files in a ``thirdparty`` folder.
      verbose: Whether to print log messages.

    Returns:
      The real path to the resource.

    , For third party data, unrestricted connectivity to the global network may be required., Found multiple files with , Given the name of a package on PyPI and a version (both strings), checks
    if the given version is the latest version of the package available.
    Returns a 2-tuple (installed_version, latest_version)
    `repository_url` is a `%` style format string
    to use a different repository PyPI repository URL,
    e.g. test.pypi.org or a private repository.
    The string is formatted with the package name.
    Adopted from https://github.com/alexmojaki/outdated/blob/master/outdated/__init__.py

    Args:
        package: Package name.
        version: Installed version string.
        repository_url: URL on pypi.

    Returns:
        Parsed installed version and latest version.
    , HANLP_HOME, HanLP/, Map a URL to a local path.

    Args:
        url: Remote URL.
        save_dir: The root folder to save this file.
        prefix: The prefix of official website. Any URLs starting with this prefix will be considered official.
        append_location: Whether to put unofficial files in a ``thirdparty`` folder.

    Returns:
        The real path that this URL is mapped to.
    , Redirect stdout to else where.
    Copied from https://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262

    Args:
      to:  Target device.
      stdout:  Source device.

    , See also https://hanlp.hankcs.com/docs/install.html#install-models for instructions., Uncompress a file and clean up uncompressed files once an error is triggered.

    Args:
      path: The path to a compressed file
      dest: The dest folder.
      remove: Remove archive file after decompression.
      verbose: ``True`` to print log message.

    Returns:
        Destination path.
    
    , User-agent, Using local {}, ignore {}, Windows, [/red].
, [/yellow]

, [green]Please re-try or download it to , [green]Please upgrade to the latest version (, args, auto, bz2, dev, fileno, hanlp, http:, https:, https://pypi.python.org/pypi/%s/json, ignore, info, msg, r, r:*, rb, test, thirdparty, train, using some decent downloading tools.[/green]
, utf-8, utf8, version, w, wb, with:[/green]

	[yellow]wget , {}.downloading, ~","0, 0.1, 0.5, 0.8, 1, 100, 1024, 2, 4, False, None, True","
# assert path.endswith('.zip')
prefix, ext = split_if_compressed(path)
folder_name = os.path.basename(prefix)
file_is_zip = ext == '.zip'
root_of_folder = None
if ext == '.gz':
try:
with gzip.open(path, 'rb') as f_in, open(prefix, 'wb') as f_out:
shutil.copyfileobj(f_in, f_out)
except Exception as e:
remove_file(prefix)
remove_file(path)
raise e
else:
try:
with zipfile.ZipFile(path, ""r"") if ext == '.zip' else tarfile.open(path, 'r:*') as archive:
if not dest:
namelist = sorted(archive.namelist() if file_is_zip else archive.getnames())
if namelist[0] == '.':
namelist = namelist[1:]
namelist = [p[len('./'):] if p.startswith('./') else p for p in namelist]
if ext == '.tgz':
roots = set(x.split('/')[0] for x in namelist)
if len(roots) == 1:
root_of_folder = next(iter(roots))
else:
# only one file, root_of_folder = ''
root_of_folder = namelist[0].strip('/') if len(namelist) > 1 else ''
if all(f.split('/')[0] == root_of_folder for f in namelist[1:]) or not root_of_folder:
dest = os.path.dirname(path)  # only one folder, unzip to the same dir
else:
root_of_folder = None
dest = prefix  # assume zip contains more than one file or folder
if verbose:
eprint('Decompressing {} to {}'.format(path, dest))
archive.extractall(dest)
if root_of_folder:
if root_of_folder != folder_name:
# move root to match folder name
os.rename(path_join(dest, root_of_folder), path_join(dest, folder_name))
dest = path_join(dest, folder_name)
elif len(namelist) == 1:
dest = path_join(dest, namelist[0])
except Exception as e:
remove_file(path)
if os.path.exists(prefix):
if os.path.isfile(prefix):
os.remove(prefix)
elif os.path.isdir(prefix):
shutil.rmtree(prefix)
raise e
if remove:
remove_file(path)
return dest


def split_if_compressed(path: str, compressed_ext=('.zip', '.tgz', '.gz', 'bz2', '.xz')) -> Tuple[str, Optional[str]]:
tar_gz = '.tar.gz'
if path.endswith(tar_gz):
root, ext = path[:-len(tar_gz)], tar_gz
else:
root, ext = os.path.splitext(path)
if ext in compressed_ext or ext == tar_gz:
return root, ext
return path, None


def get_resource(path: str, save_dir=hanlp_home(), extract=True, prefix=HANLP_URL, append_location=True,
verbose=HANLP_VERBOSE):
Fetch real (local) path for a resource (model, corpus, whatever) to ``save_dir``., 
_path = path
path = hanlp.pretrained.ALL.get(path, path)
anchor: str = None
compressed = None
if os.path.isdir(path):
return path
elif os.path.isfile(path):
pass
elif path.startswith('http:') or path.startswith('https:'):
url = path
if '#' in url:
url, anchor = url.split('#', maxsplit=1)
realpath = path_from_url(path, save_dir, prefix, append_location)
realpath, compressed = split_if_compressed(realpath)
# check if resource is there
if anchor:
if anchor.startswith('/'):
# indicates the folder name has to be polished
anchor = anchor.lstrip('/')
parts = anchor.split('/')
renamed_realpath = str(Path(realpath).parent.joinpath(parts[0]))
if os.path.isfile(realpath + compressed):
os.rename(realpath + compressed, renamed_realpath + compressed)
realpath = renamed_realpath
anchor = '/'.join(parts[1:])
child = path_join(realpath, anchor)
if os.path.exists(child):
return child
elif os.path.isdir(realpath) or (os.path.isfile(realpath) and (compressed and extract)):
return realpath
else:
if compressed:
pattern = realpath + '.*'
files = glob.glob(pattern)
files = list(filter(lambda x: not x.endswith('.downloading') and not x.endswith(compressed), files))
if files:
if len(files) > 1:
logger.debug(f'Found multiple files with {pattern}, will use the first one.')
return files[0]
# realpath is where its path after exaction
if compressed:
realpath += compressed
with temp_lock(path):
if not os.path.isfile(realpath):
path = download(url=path, save_path=realpath, verbose=verbose)
else:
path = realpath
if extract and compressed:
with temp_lock(path):
if os.path.isfile(path):
path = uncompress(path, verbose=verbose)
else:  # other process must have already decompressed it and deleted it
return get_resource(_path, save_dir, extract, prefix, append_location, verbose)
if anchor:
path = path_join(path, anchor)

return path


def path_from_url(url, save_dir=hanlp_home(), prefix=HANLP_URL, append_location=True):
Map a URL to a local path., 
args = shlex.split(cmd)
proc = Popen(args, stdout=PIPE, stderr=PIPE)
out, err = proc.communicate()
exitcode = proc.returncode
return exitcode, out.decode('utf-8'), err.decode('utf-8')


def run_cmd(cmd: str) -> str:
exitcode, out, err = get_exitcode_stdout_stderr(cmd)
if exitcode:
raise RuntimeError(err + '\nThe command is:\n' + cmd)
return out


@contextlib.contextmanager
def pushd(new_dir):
previous_dir = os.getcwd()
os.chdir(new_dir)
try:
yield
finally:
os.chdir(previous_dir)


def basename_no_ext(path):
basename = os.path.basename(path)
no_ext, ext = os.path.splitext(basename)
return no_ext


def file_cache(path: str, purge=False):
cache_name = path + '.cache'
cache_time = os.path.getmtime(cache_name) if os.path.isfile(cache_name) and not purge else 0
file_time = os.path.getmtime(path)
cache_valid = cache_time > file_time
return cache_name, cache_valid


def merge_files(files: List[str], dst: str):
with open(dst, 'wb') as write:
for f in files:
with open(f, 'rb') as read:
shutil.copyfileobj(read, write)


class TimingFileIterator(CountdownTimer):

def __init__(self, filepath) -> None:
super().__init__(os.path.getsize(filepath))
self.filepath = filepath

def __iter__(self):
if not os.path.isfile(self.filepath):
raise FileNotFoundError(self.filepath)
fp = open(self.filepath, encoding='utf-8', errors='ignore')
line = fp.readline()
while line:
yield line
self.current = fp.tell()
line = fp.readline()
fp.close()

def log(self, info=None, ratio_percentage=True, ratio=True, step=0, interval=0.5, erase=True,
logger: Union[logging.Logger, bool] = None, newline=False, ratio_width=None):
assert step == 0
super().log(info, ratio_percentage, ratio, step, interval, erase, logger, newline, ratio_width)

@property
def ratio(self) -> str:
return f'{human_bytes(self.current)}/{human_bytes(self.total)}'

@property
def ratio_width(self) -> int:
return len(f'{human_bytes(self.total)}') * 2 + 1

def close(self):
pass


def check_outdated(package='hanlp', version=__version__, repository_url='https://pypi.python.org/pypi/%s/json'):
Given the name of a package on PyPI and a version (both strings), checks, 
file_prefix, _ = os.path.splitext(filepath)
return file_prefix + ext


def read_tsv_as_sents(tsv_file_path, ignore_prefix=None, delimiter=None):
sent = []
tsv_file_path = get_resource(tsv_file_path)
with open(tsv_file_path, encoding='utf-8') as tsv_file:
for line in tsv_file:
if ignore_prefix and line.startswith(ignore_prefix):
continue
line = line.strip()
cells = line.split(delimiter)
if line and cells:
sent.append(cells)
elif sent:
yield sent
sent = []
if sent:
yield sent


def generate_words_tags_from_tsv(tsv_file_path, lower=False, gold=True, max_seq_length=None, sent_delimiter=None,
char_level=False, hard_constraint=False):
for sent in read_tsv_as_sents(tsv_file_path):
words = [cells[0] for cells in sent]
if max_seq_length:
offset = 0
# try to split the sequence to make it fit into max_seq_length
for shorter_words in split_long_sentence_into(words, max_seq_length, sent_delimiter, char_level,
hard_constraint):
if gold:
shorter_tags = [cells[1] for cells in sent[offset:offset + len(shorter_words)]]
offset += len(shorter_words)
else:
shorter_tags = None
if lower:
shorter_words = [word.lower() for word in shorter_words]
yield shorter_words, shorter_tags
else:
if gold:
try:
tags = [cells[1] for cells in sent]
except:
raise ValueError(f'Failed to load {tsv_file_path}: {sent}')
else:
tags = None
if lower:
words = [word.lower() for word in words]
yield words, tags


def split_file(filepath, train=0.8, dev=0.1, test=0.1, names=None, shuffle=False):
num_samples = 0
if filepath.endswith('.tsv'):
for sent in read_tsv_as_sents(filepath):
num_samples += 1
else:
with open(filepath, encoding='utf-8') as src:
for sample in src:
num_samples += 1
splits = {'train': train, 'dev': dev, 'test': test}
splits = dict((k, v) for k, v in splits.items() if v)
splits = dict((k, v / sum(splits.values())) for k, v in splits.items())
accumulated = 0
r = []
for k, v in splits.items():
r.append(accumulated)
accumulated += v
r.append(accumulated)
splits[k] = accumulated
if names is None:
names = {}
name, ext = os.path.splitext(filepath)
filenames = [names.get(split, name + '.' + split + ext) for split in splits.keys()]
outs = [open(f, 'w', encoding='utf-8') for f in filenames]
if shuffle:
shuffle = list(range(num_samples))
random.shuffle(shuffle)
if filepath.endswith('.tsv'):
src = read_tsv_as_sents(filepath)
else:
src = open(filepath, encoding='utf-8')
for idx, sample in enumerate(src):
if shuffle:
idx = shuffle[idx]
ratio = idx / num_samples
for sid, out in enumerate(outs):
if r[2 * sid] <= ratio < r[2 * sid + 1]:
if isinstance(sample, list):
sample = '\n'.join('\t'.join(x) for x in sample) + '\n\n'
out.write(sample)
break
if not filepath.endswith('.tsv'):
src.close()
for out in outs:
out.close()
return filenames


def fileno(file_or_fd):
try:
fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()
except:
return None
if not isinstance(fd, int):
raise ValueError(""Expected a file (`.fileno()`) or a file descriptor"")
return fd


@contextmanager
def stdout_redirected(to=os.devnull, stdout=None):
Redirect stdout to else where., 
if not save_dir:
save_dir = hanlp_home()
domain, relative_path = parse_url_path(url)
if append_location:
if not url.startswith(prefix):
save_dir = os.path.join(save_dir, 'thirdparty', domain)
else:
# remove the relative path in prefix
middle = prefix.split(domain)[-1].lstrip('/')
if relative_path.startswith(middle):
relative_path = relative_path[len(middle):]
realpath = os.path.join(save_dir, relative_path)
else:
realpath = os.path.join(save_dir, os.path.basename(relative_path))
return realpath


def human_bytes(file_size: int) -> str:
file_size /= 1024  # KB
if file_size > 1024:
file_size /= 1024  # MB
if file_size > 1024:
file_size /= 1024  # GB
return '%.1f GB' % file_size
return '%.1f MB' % file_size
return '%d KB' % file_size


def read_cells(filepath: str, delimiter='auto', strip=True, skip_header=False):
filepath = get_resource(filepath)
if delimiter == 'auto':
if filepath.endswith('.tsv'):
delimiter = '\t'
elif filepath.endswith('.csv'):
delimiter = ','
else:
delimiter = None
with open(filepath, encoding='utf-8') as src:
if skip_header:
next(src)
for line in src:
line = line.strip()
if not line:
continue
cells = line.split(delimiter)
if strip:
cells = [c.strip() for c in cells]
yield cells


def replace_ext(filepath, ext) -> str:
 Replace the extension of filepath to ext., 
if windows():  # This doesn't play well with windows
yield None
return
if stdout is None:
stdout = sys.stdout
stdout_fd = fileno(stdout)
if not stdout_fd:
yield None
return
# copy stdout_fd before it is overwritten
# NOTE: `copied` is inheritable on Windows when duplicating a standard stream
with os.fdopen(os.dup(stdout_fd), 'wb') as copied:
stdout.flush()  # flush library buffers that dup2 knows nothing about
try:
os.dup2(fileno(to), stdout_fd)  # $ exec >&to
except ValueError:  # filename
with open(to, 'wb') as to_file:
os.dup2(to_file.fileno(), stdout_fd)  # $ exec > to
try:
yield stdout  # allow code to be run with the redirected stdout
finally:
# restore stdout to its previous value
# NOTE: dup2 makes stdout_fd inheritable unconditionally
try:
stdout.flush()
os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied
except:
# This is the best we can do
pass


def get_exitcode_stdout_stderr(cmd):
Execute the external command and get its exitcode, stdout and stderr., 
return os.getenv('HANLP_HOME', hanlp_home_default())


def file_exist(filename) -> bool:
return os.path.isfile(filename)


def remove_file(filename):
if file_exist(filename):
os.remove(filename)


def parent_dir(path):
return os.path.normpath(os.path.join(path, os.pardir))


def download(url, save_path=None, save_dir=hanlp_home(), prefix=HANLP_URL, append_location=True, verbose=HANLP_VERBOSE):
if not save_path:
save_path = path_from_url(url, save_dir, prefix, append_location)
if os.path.isfile(save_path):
if verbose:
eprint('Using local {}, ignore {}'.format(save_path, url))
return save_path
else:
makedirs(parent_dir(save_path))
if verbose:
eprint('Downloading {} to {}'.format(url, save_path))
tmp_path = '{}.downloading'.format(save_path)
remove_file(tmp_path)
try:
downloader = Downloader(url, tmp_path, 4, headers={
'User-agent': f'HanLP/{__version__} ({platform.platform()})'})
if verbose:
downloader.subscribe(DownloadCallback(show_header=False))
downloader.start_sync()
except BaseException as e:
remove_file(tmp_path)
url = url.split('#')[0]
try:
installed_version, latest_version = check_outdated()
except:
installed_version, latest_version = None, None  # No Internet
if installed_version != latest_version:
# Always prompt user to upgrade whenever a new version is available
hints = f'[green]Please upgrade to the latest version ({latest_version}) with:[/green]' \
f'\n\n\t[yellow]pip install -U hanlp[/yellow]\n'
else:  # Otherwise, prompt user to re-try
hints = f'[green]Please re-try or download it to {save_path} by yourself '
if not windows():
hints += f'with:[/green]\n\n\t[yellow]wget {url} -O {save_path}[/yellow]\n\n'
else:
hints += 'using some decent downloading tools.[/green]\n'
if not url.startswith(HANLP_URL):
hints += 'For third party data, unrestricted connectivity to the global network may be required.'
else:
hints += 'See also https://hanlp.hankcs.com/docs/install.html#install-models for instructions.'
message = f'Download failed due to [red]{repr(e)}[/red].\n' \
f'{hints}'
if verbose:
cprint(message)
if hasattr(e, 'msg'):
e.msg += '\n' + remove_color_tag(message)
elif hasattr(e, 'args') and e.args and isinstance(e.args, tuple) and isinstance(e.args[0], str):
e.args = (e.args[0] + '\n' + remove_color_tag(message),) + e.args[1:]
raise e from None
remove_file(save_path)
os.rename(tmp_path, save_path)
return save_path


def parse_url_path(url):
parsed: urllib.parse.ParseResult = urlparse(url)
path = parsed.path.strip('/')
return parsed.netloc, path


def uncompress(path, dest=None, remove=True, verbose=HANLP_VERBOSE):
Uncompress a file and clean up uncompressed files once an error is triggered., # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-26 15:02, #22434262, Default data directory depending on the platform and environment variables
if windows():
return os.path.join(os.environ.get('APPDATA'), 'hanlp')
else:
return os.path.join(os.path.expanduser(""~""), '.hanlp')


def windows():
system = platform.system()
return system == 'Windows'


def hanlp_home():
 Home directory for HanLP resources.",
https://github.com/hankcs/HanLP,log_util.py,0,0.0,82,65.08,25,19.84,5,3.97,14,11.11,0,0.0,15,2,20,0,,"ColoredFormatter, ErasablePrinter, __init__, __name__, _last_print_width, _len, _printer, _replace_color_offset, addHandler, args, attached_to_std, c_text, chunks, close, color, color_format, color_format_len, consoleHandler, cprint, ctrl, datefmt, datetime, debug, delta, enable, enable_debug, end, erase, file, fileHandler, flash, flush, fmt, formatMessage, getvalue, handler, handlers, hanlp_common, info, init_logger, io, isinstance, items, join, kwargs, len, level, line, log_path, logger, logging, main, message, mode, msg, name, object, os, out, print, propagate, record, remove_color_tag, replace, rootLogger, root_dir, self, setFormatter, setLevel, show_colors, show_colors_and_formats, split, start, str, stream, style, super, sys, tag, termcolor, text, write","datetime.datetime.now, hanlp_common.constant.IPYTHON, io.StringIO, logging.DEBUG, logging.ERROR, logging.FileHandler, logging.Formatter, logging.INFO, logging.LogRecord, logging.Logger, logging.StreamHandler, logging.getLogger, os.environ.get, os.makedirs, os.path.join, sys.stderr, sys.stdout, termcolor.ATTRIBUTES, termcolor.ATTRIBUTES.keys, termcolor.COLORS, termcolor.COLORS.keys, termcolor.HIGHLIGHTS, termcolor.HIGHLIGHTS.keys, termcolor.RESET","__init__, _replace_color_offset, color_format, color_format_len, cprint, enable_debug, erase, flash, formatMessage, init_logger, main, print, remove_color_tag, show_colors, show_colors_and_formats","ColoredFormatter, ErasablePrinter","_len, _printer, attached_to_std, c_text, chunks, consoleHandler, delta, end, fileHandler, handler, log_path, logger, message, msg, name, out, rootLogger, start, tag, text",,", , , [%dm,  ,  [/, "",[/, %, %(asctime)s %(levelname)s %(message)s, %Y-%m-%d %H:%M:%S, %Y-%m-%d_%H.%M.%S, +, HANLP_LOG_LEVEL, INFO, [, [/, ], ] , ]"", ][, ][/, __main__, hanlp, w, {0}/{1}.log","0, 1, False, None, True","#     flash(f'[red]{i}[/red]'), #   http://www.doxygen.nl/manual/markdown.html#md_tables, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-24 22:12, # Generates tables for Doxygen flavored Markdown.  See the Doxygen, # Translation dictionaries for table alignment, # \r is essential when multi-lines were printed, # cprint('[blink][yellow]...[/yellow][/blink]'), # documentation for details:, # for i in range(10):, # print('previous', end=''), # show_colors_and_formats(), # stderr will be rendered as red which is bad",
https://github.com/hankcs/HanLP,rules.py,0,0.0,26,60.47,16,37.21,1,2.33,0,0.0,0,0.0,2,0,14,0,,"_AB_ACRONYM, _AB_SENIOR, _RE_SENTENCE, _SEPARATOR, _UNDO_AB_ACRONYM, _UNDO_AB_SENIOR, _replace_with_separator, best, chunk, finditer, group, list, processed, re, regex, regexs, replacement, result, sentence, sents, separator, split, split_sentence, strip, sub, text","re.UNICODE, re.compile, re.sub","_replace_with_separator, split_sentence",,"_AB_ACRONYM, _AB_SENIOR, _RE_SENTENCE, _SEPARATOR, _UNDO_AB_ACRONYM, _UNDO_AB_SENIOR, chunk, processed, regex, replacement, result, sentence, sents, text",,"
,  , ([A-Z][a-z]{1,2}\.), ([A-Z][a-z]{1,2}\.)\s(\w), ([。！？?])([^”’]), ([。！？?][”’])([^，。！？?]), (\.[a-zA-Z]\.), (\.[a-zA-Z]\.)\s(\w), (\.{6})([^”’]), (\S.+?[.!?])(?=\s+|$)|(\S.+?)(?=[\n]|$), (\w), (…{2})([^”’]), @, \1, \1\n\2, \2",True,,
https://github.com/hankcs/HanLP,span_util.py,0,0.0,130,53.5,38,15.64,6,2.47,69,28.4,0,0.0,24,1,61,8,,"Exception, FutureWarning, InvalidTagSequence, TypedSpan, TypedStringSpan, ValueError, __init__, __str__, _iob1_start_of_chunk, active_conll_tag, add, allowed, allowed_transitions, any, append, bio_tag, bio_tags_to_spans, bioul_sequence, bioul_tags_to_spans, bmes_of, bmes_tag, bmes_tags_to_spans, bmes_to_spans, bmes_to_words, bool, cells, chars, classes_to_ignore, conll_tag, constraint_type, curr_bio_tag, curr_conll_tag, dst, encoding, end, end_index, end_tag, enumerate, enumerate_spans, extend, extract_bmes_tag_label, file_path, filter_function, first_end_index, from_entity, from_label, from_label_index, from_tag, full_label, generate_words_per_line, in_stack, index, int, iob1_tags_to_spans, iob1_to_bioul, iobes_tags_to_spans, iobes_to_bilou, is_transition_allowed, items, label, labels, labels_with_boundaries, last_end_index, len, line, list, max_span_width, min, min_span_width, new_label, new_tag, num_labels, offset, open, out, out_stack, partition, parts, pop, pop_replace_append, pre_offset, prev_bio_tag, prev_bmes_tag, prev_conll_tag, prev_type, process_stack, range, recoded_stack, replace_label, result, reverse, segmented, self, sentence, set, slice, span, span_end, span_start, spans, split, src, stack, start, start_index, start_tag, startswith, str, string_tag, strip, super, tag, tag_sequence, tags, text, this_type, to_bioul, to_entity, to_label, to_label_index, to_tag, typing, w, warnings, word, words, words_to_bi, words_to_bmes, write, zip","typing.Callable, typing.Dict, typing.List, typing.Optional, typing.Set, typing.Tuple, warnings.warn","__init__, __str__, _iob1_start_of_chunk, allowed_transitions, bio_tags_to_spans, bioul_tags_to_spans, bmes_of, bmes_tags_to_spans, bmes_to_spans, bmes_to_words, enumerate_spans, extract_bmes_tag_label, generate_words_per_line, iob1_tags_to_spans, iob1_to_bioul, iobes_tags_to_spans, iobes_to_bilou, is_transition_allowed, pop_replace_append, process_stack, replace_label, to_bioul, words_to_bi, words_to_bmes",InvalidTagSequence,"T, TypedSpan, TypedStringSpan, active_conll_tag, allowed, bio_tag, bioul_sequence, bmes_tag, cells, chars, classes_to_ignore, conll_tag, curr_bio_tag, curr_conll_tag, end, end_index, end_tag, filter_function, first_end_index, from_entity, from_label, from_label_index, from_tag, index, label, labels_with_boundaries, last_end_index, line, max_span_width, new_tag, num_labels, offset, out, parts, pre_offset, prev_bio_tag, prev_bmes_tag, prev_conll_tag, prev_type, recoded_stack, result, span, span_end, span_start, spans, src, stack, start, start_index, start_tag, string_tag, tag, tags, this_type, to_entity, to_label, to_label_index, to_tag, w, word, words","Given a constraint type and strings `from_tag` and `to_tag` that
represent the origin and destination of the transition, return whether
the transition is allowed under the given constraint type.

# Parameters

constraint_type : `str`, required
    Indicates which constraint to apply. Current choices are
    ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
from_tag : `str`, required
    The tag that the transition originates from. For example, if the
    label is `I-PER`, the `from_tag` is `I`.
from_entity : `str`, required
    The entity corresponding to the `from_tag`. For example, if the
    label is `I-PER`, the `from_entity` is `PER`.
to_tag : `str`, required
    The tag that the transition leads to. For example, if the
    label is `I-PER`, the `to_tag` is `I`.
to_entity : `str`, required
    The entity corresponding to the `to_tag`. For example, if the
    label is `I-PER`, the `to_entity` is `PER`.

# Returns

`bool`
    Whether the transition is allowed under the given `constraint_type`., Given a sentence, return all token spans within the sentence. Spans are `inclusive`.
Additionally, you can provide a maximum and minimum span width, which will be used
to exclude spans outside of this range.

Finally, you can provide a function mapping `List[T] -> bool`, which will
be applied to every span to decide whether that span should be included. This
allows filtering by length, regex matches, pos tags or any Spacy `Token`
attributes, for example.

# Parameters

sentence : `List[T]`, required.
    The sentence to generate spans for. The type is generic, as this function
    can be used with strings, or Spacy `Tokens` or other sequences.
offset : `int`, optional (default = `0`)
    A numeric offset to add to all span start and end indices. This is helpful
    if the sentence is part of a larger structure, such as a document, which
    the indices need to respect.
max_span_width : `int`, optional (default = `None`)
    The maximum length of spans which should be included. Defaults to len(sentence).
min_span_width : `int`, optional (default = `1`)
    The minimum length of spans which should be included. Defaults to 1.
filter_function : `Callable[[List[T]], bool]`, optional (default = `None`)
    A function mapping sequences of the passed type T to a boolean value.
    If `True`, the span is included in the returned spans from the
    sentence, otherwise it is excluded.., Given a sequence corresponding to BIO tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
as otherwise it is possible to get a perfect precision score whilst still predicting
ill-formed spans in addition to the correct spans. This function works properly when
the spans are unlabeled (i.e., your labels are simply ""B"", ""I"", and ""O"").

# Parameters

tag_sequence : `List[str]`, required.
    The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
    A list of string class labels `excluding` the bio tag
    which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
    The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
    Note that the label `does not` contain any BIO tag prefixes., Given a sequence corresponding to BIOUL tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are not allowed and will raise `InvalidTagSequence`.
This function works properly when the spans are unlabeled (i.e., your labels are
simply ""B"", ""I"", ""O"", ""U"", and ""L"").

# Parameters

tag_sequence : `List[str]`, required.
    The tag sequence encoded in BIOUL, e.g. [""B-PER"", ""L-PER"", ""O""].
classes_to_ignore : `List[str]`, optional (default = `None`).
    A list of string class labels `excluding` the bio tag
    which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
    The typed, extracted spans from the sequence, in the format (label, (span_start, span_end))., Given a sequence corresponding to BMES tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
as otherwise it is possible to get a perfect precision score whilst still predicting
ill-formed spans in addition to the correct spans.
This function works properly when the spans are unlabeled (i.e., your labels are
simply ""B"", ""M"", ""E"" and ""S"").

# Parameters

tag_sequence : `List[str]`, required.
    The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
    A list of string class labels `excluding` the bio tag
    which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
    The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
    Note that the label `does not` contain any BIO tag prefixes., Given a sequence corresponding to IOB1 tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e., those where ""B-LABEL"" is not preceded
by ""I-LABEL"" or ""B-LABEL"").

# Parameters

tag_sequence : `List[str]`, required.
    The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
    A list of string class labels `excluding` the bio tag
    which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
    The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
    Note that the label `does not` contain any BIO tag prefixes., Given a tag sequence encoded with IOB1 labels, recode to BIOUL.

In the IOB1 scheme, I is a token inside a span, O is a token outside
a span and B is the beginning of span immediately following another
span of the same type.

In the BIO scheme, I is a token inside a span, O is a token outside
a span and B is the beginning of a span.

# Parameters

tag_sequence : `List[str]`, required.
    The tag sequence encoded in IOB1, e.g. [""I-PER"", ""I-PER"", ""O""].
encoding : `str`, optional, (default = `""IOB1""`).
    The encoding type to convert from. Must be either ""IOB1"" or ""BIO"".

# Returns

bioul_sequence : `List[str]`
    The tag sequence encoded in IOB1, e.g. [""B-PER"", ""L-PER"", ""O""]., Given labels and a constraint type, returns the allowed transitions. It will
additionally include transitions for the start and end states, which are used
by the conditional random field.

# Parameters

constraint_type : `str`, required
    Indicates which constraint to apply. Current choices are
    ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
labels : `Dict[int, str]`, required
    A mapping {label_id -> label}. Most commonly this would be the value from
    Vocabulary.get_index_to_token_vocabulary()

# Returns

`List[Tuple[int, int]]`
    The allowed transitions (from_label_id, to_label_id).",", 	, 
, 
    Given a constraint type and strings `from_tag` and `to_tag` that
    represent the origin and destination of the transition, return whether
    the transition is allowed under the given constraint type.

    # Parameters

    constraint_type : `str`, required
        Indicates which constraint to apply. Current choices are
        ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
    from_tag : `str`, required
        The tag that the transition originates from. For example, if the
        label is `I-PER`, the `from_tag` is `I`.
    from_entity : `str`, required
        The entity corresponding to the `from_tag`. For example, if the
        label is `I-PER`, the `from_entity` is `PER`.
    to_tag : `str`, required
        The tag that the transition leads to. For example, if the
        label is `I-PER`, the `to_tag` is `I`.
    to_entity : `str`, required
        The entity corresponding to the `to_tag`. For example, if the
        label is `I-PER`, the `to_entity` is `PER`.

    # Returns

    `bool`
        Whether the transition is allowed under the given `constraint_type`.
    , 
    Given a sentence, return all token spans within the sentence. Spans are `inclusive`.
    Additionally, you can provide a maximum and minimum span width, which will be used
    to exclude spans outside of this range.

    Finally, you can provide a function mapping `List[T] -> bool`, which will
    be applied to every span to decide whether that span should be included. This
    allows filtering by length, regex matches, pos tags or any Spacy `Token`
    attributes, for example.

    # Parameters

    sentence : `List[T]`, required.
        The sentence to generate spans for. The type is generic, as this function
        can be used with strings, or Spacy `Tokens` or other sequences.
    offset : `int`, optional (default = `0`)
        A numeric offset to add to all span start and end indices. This is helpful
        if the sentence is part of a larger structure, such as a document, which
        the indices need to respect.
    max_span_width : `int`, optional (default = `None`)
        The maximum length of spans which should be included. Defaults to len(sentence).
    min_span_width : `int`, optional (default = `1`)
        The minimum length of spans which should be included. Defaults to 1.
    filter_function : `Callable[[List[T]], bool]`, optional (default = `None`)
        A function mapping sequences of the passed type T to a boolean value.
        If `True`, the span is included in the returned spans from the
        sentence, otherwise it is excluded..
    , 
    Given a sequence corresponding to BIO tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
    as otherwise it is possible to get a perfect precision score whilst still predicting
    ill-formed spans in addition to the correct spans. This function works properly when
    the spans are unlabeled (i.e., your labels are simply ""B"", ""I"", and ""O"").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    , 
    Given a sequence corresponding to BIOUL tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.
    This function works properly when the spans are unlabeled (i.e., your labels are
    simply ""B"", ""I"", ""O"", ""U"", and ""L"").

    # Parameters

    tag_sequence : `List[str]`, required.
        The tag sequence encoded in BIOUL, e.g. [""B-PER"", ""L-PER"", ""O""].
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
    , 
    Given a sequence corresponding to BMES tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
    as otherwise it is possible to get a perfect precision score whilst still predicting
    ill-formed spans in addition to the correct spans.
    This function works properly when the spans are unlabeled (i.e., your labels are
    simply ""B"", ""M"", ""E"" and ""S"").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    , 
    Given a sequence corresponding to IOB1 tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e., those where ""B-LABEL"" is not preceded
    by ""I-LABEL"" or ""B-LABEL"").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    , 
    Given a tag sequence encoded with IOB1 labels, recode to BIOUL.

    In the IOB1 scheme, I is a token inside a span, O is a token outside
    a span and B is the beginning of span immediately following another
    span of the same type.

    In the BIO scheme, I is a token inside a span, O is a token outside
    a span and B is the beginning of a span.

    # Parameters

    tag_sequence : `List[str]`, required.
        The tag sequence encoded in IOB1, e.g. [""I-PER"", ""I-PER"", ""O""].
    encoding : `str`, optional, (default = `""IOB1""`).
        The encoding type to convert from. Must be either ""IOB1"" or ""BIO"".

    # Returns

    bioul_sequence : `List[str]`
        The tag sequence encoded in IOB1, e.g. [""B-PER"", ""L-PER"", ""O""].
    , 
    Given labels and a constraint type, returns the allowed transitions. It will
    additionally include transitions for the start and end states, which are used
    by the conditional random field.

    # Parameters

    constraint_type : `str`, required
        Indicates which constraint to apply. Current choices are
        ""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
    labels : `Dict[int, str]`, required
        A mapping {label_id -> label}. Most commonly this would be the value from
        Vocabulary.get_index_to_token_vocabulary()

    # Returns

    `List[Tuple[int, int]]`
        The allowed transitions (from_label_id, to_label_id).
    ,  ,  passed to 'to_bioul'., -, B, BIO, BIOUL, BMES, E, E-, END, I, IOB1, Invalid encoding , L, L-, M, O, S, S-, START, U, U-, Unknown constraint type: , iob1_to_bioul has been replaced with 'to_bioul' to allow more encoding options., utf-8, w, {} contains None or zero-length word {}","0, 1, 2, False, None, True","
Given a constraint type and strings `from_tag` and `to_tag` that
represent the origin and destination of the transition, return whether
the transition is allowed under the given constraint type.

# Parameters

constraint_type : `str`, required
Indicates which constraint to apply. Current choices are
""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
from_tag : `str`, required
The tag that the transition originates from. For example, if the
label is `I-PER`, the `from_tag` is `I`.
from_entity : `str`, required
The entity corresponding to the `from_tag`. For example, if the
label is `I-PER`, the `from_entity` is `PER`.
to_tag : `str`, required
The tag that the transition leads to. For example, if the
label is `I-PER`, the `to_tag` is `I`.
to_entity : `str`, required
The entity corresponding to the `to_tag`. For example, if the
label is `I-PER`, the `to_entity` is `PER`.

# Returns

`bool`
Whether the transition is allowed under the given `constraint_type`.
, 
Given a sentence, return all token spans within the sentence. Spans are `inclusive`.
Additionally, you can provide a maximum and minimum span width, which will be used
to exclude spans outside of this range.

Finally, you can provide a function mapping `List[T] -> bool`, which will
be applied to every span to decide whether that span should be included. This
allows filtering by length, regex matches, pos tags or any Spacy `Token`
attributes, for example.

# Parameters

sentence : `List[T]`, required.
The sentence to generate spans for. The type is generic, as this function
can be used with strings, or Spacy `Tokens` or other sequences.
offset : `int`, optional (default = `0`)
A numeric offset to add to all span start and end indices. This is helpful
if the sentence is part of a larger structure, such as a document, which
the indices need to respect.
max_span_width : `int`, optional (default = `None`)
The maximum length of spans which should be included. Defaults to len(sentence).
min_span_width : `int`, optional (default = `1`)
The minimum length of spans which should be included. Defaults to 1.
filter_function : `Callable[[List[T]], bool]`, optional (default = `None`)
A function mapping sequences of the passed type T to a boolean value.
If `True`, the span is included in the returned spans from the
sentence, otherwise it is excluded..
, 
Given a sequence corresponding to BIO tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
as otherwise it is possible to get a perfect precision score whilst still predicting
ill-formed spans in addition to the correct spans. This function works properly when
the spans are unlabeled (i.e., your labels are simply ""B"", ""I"", and ""O"").

# Parameters

tag_sequence : `List[str]`, required.
The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
A list of string class labels `excluding` the bio tag
which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
Note that the label `does not` contain any BIO tag prefixes.
, 
Given a sequence corresponding to BIOUL tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are not allowed and will raise `InvalidTagSequence`.
This function works properly when the spans are unlabeled (i.e., your labels are
simply ""B"", ""I"", ""O"", ""U"", and ""L"").

# Parameters

tag_sequence : `List[str]`, required.
The tag sequence encoded in BIOUL, e.g. [""B-PER"", ""L-PER"", ""O""].
classes_to_ignore : `List[str]`, optional (default = `None`).
A list of string class labels `excluding` the bio tag
which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
, 
Given a sequence corresponding to BMES tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e those which do not start with a ""B-LABEL""),
as otherwise it is possible to get a perfect precision score whilst still predicting
ill-formed spans in addition to the correct spans.
This function works properly when the spans are unlabeled (i.e., your labels are
simply ""B"", ""M"", ""E"" and ""S"").

# Parameters

tag_sequence : `List[str]`, required.
The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
A list of string class labels `excluding` the bio tag
which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
Note that the label `does not` contain any BIO tag prefixes.
, 
Given a sequence corresponding to IOB1 tags, extracts spans.
Spans are inclusive and can be of zero length, representing a single word span.
Ill-formed spans are also included (i.e., those where ""B-LABEL"" is not preceded
by ""I-LABEL"" or ""B-LABEL"").

# Parameters

tag_sequence : `List[str]`, required.
The integer class labels for a sequence.
classes_to_ignore : `List[str]`, optional (default = `None`).
A list of string class labels `excluding` the bio tag
which should be ignored when extracting spans.

# Returns

spans : `List[TypedStringSpan]`
The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
Note that the label `does not` contain any BIO tag prefixes.
, 
Given a tag sequence encoded with IOB1 labels, recode to BIOUL.

In the IOB1 scheme, I is a token inside a span, O is a token outside
a span and B is the beginning of span immediately following another
span of the same type.

In the BIO scheme, I is a token inside a span, O is a token outside
a span and B is the beginning of a span.

# Parameters

tag_sequence : `List[str]`, required.
The tag sequence encoded in IOB1, e.g. [""I-PER"", ""I-PER"", ""O""].
encoding : `str`, optional, (default = `""IOB1""`).
The encoding type to convert from. Must be either ""IOB1"" or ""BIO"".

# Returns

bioul_sequence : `List[str]`
The tag sequence encoded in IOB1, e.g. [""B-PER"", ""L-PER"", ""O""].
, 
Given labels and a constraint type, returns the allowed transitions. It will
additionally include transitions for the start and end states, which are used
by the conditional random field.

# Parameters

constraint_type : `str`, required
Indicates which constraint to apply. Current choices are
""BIO"", ""IOB1"", ""BIOUL"", and ""BMES"".
labels : `Dict[int, str]`, required
A mapping {label_id -> label}. Most commonly this would be the value from
Vocabulary.get_index_to_token_vocabulary()

# Returns

`List[Tuple[int, int]]`
The allowed transitions (from_label_id, to_label_id).
, #                   'gold': 'I-PER'}, # -*- coding:utf-8 -*-, # 1) the span hasn't started - i.e. an ill formed span., # 1. Valid transition: B/M -> M/E., # 2) The span is an I tag for a different conll annotation., # 2. Matched label., # Actual BIO tag., # Author: hankcs, # B-x can only transition to I-x or L-x, # Best effort split for invalid span., # Can always transition to O or B-x, # Can always transition to O or I-x, # Can only transition to B or S from E or S., # Can only transition to B-x from B-x or I-x, where, # Can only transition to E-x from B-x or M-x, where, # Can only transition to I-x from B-x or I-x, # Can only transition to M-x from B-x, where, # Cannot transition into START or from END, # Date: 2020-06-12 20:34, # I-x can only transition to I-x or L-x, # L-x can transition to O, B-*, or U-*, # Last token might have been a part of a valid span., # O can transition to O, B-* or U-*, # Only expand the span if, # Process the tag_sequence one tag at a time, adding spans to a stack,, # Regardless of tag, we start a new span when reaching B & S., # The span has ended., # This is the case the bio label is an ""I"", but either:, # U-x can transition to O, B-*, or U-*, # We are entering a new span; reset indices, # We don't care about tags we are, # We'll process the previous span if it exists, but also, # We're continuing a span., # We're inside a span., # a model may get a perfect F1 score whilst still including, # a new entity, # add 1 to end index because span indices are inclusive., # and active tag to new span., # bio_tag == ""I"" and curr_conll_tag == active_conll_tag, # check if the previous type is the same as this one, # example: full_label = 'I-PER', new_label = 'U', returns 'U-PER', # false positive ill-formed spans., # if it is then append to stack, # include this span. This is important, because otherwise,, # is different, # just a U token, # need to code as BIL, # need to make a dict like, # need to process the entries on the stack plus this one, # otherwise this start a new entity if the type, # pop the last element from in_stack, replace the label, append, # process a stack of labels, add them to out_stack, # process the stack, # then recode them., # to out_stack, # to tuple., # token = {'token': 'Matt', ""labels"": {'conll2003': ""B-PER""}, # told to ignore, so we do nothing., # update previous BMES tag., # where 'gold' is the raw value from the CoNLL data set, # x is the same tag.",
https://github.com/hankcs/HanLP,string_util.py,0,0.0,48,71.64,7,10.45,5,7.46,7,10.45,0,0.0,8,0,20,1,,"_gen_short_sent, _len, all, append, b, char, char_level, delimiter_in_entity, delimiters, end, enumerate, float, format_scores, guess_delimiter, hard_constraint, idx, ispunct, items, j, k, len, length_at_next_punct, max_seq_length, new_states, offset, ord, part, parts, possible_tokenization, punct_offset, range, results, sent, sent_delimiter, short, split_long_sent, split_long_sentence_into, start, states, str, text, token, token_to_char_offset, tokens, typing, unicodedata, v, x","typing.Dict, typing.List, typing.Tuple, unicodedata.category","_gen_short_sent, _len, format_scores, guess_delimiter, ispunct, possible_tokenization, split_long_sent, split_long_sentence_into",,"b, char, delimiter_in_entity, end, idx, j, k, length_at_next_punct, new_states, offset, part, parts, punct_offset, short, start, states, token, token_to_char_offset, v, x","Enumerate all possible tokenizations of a text.

Args:
    text: A text.

Returns: All possible tokenizations.",",  ,  - , .4f, : , Enumerate all possible tokenizations of a text.

    Args:
        text: A text.

    Returns: All possible tokenizations.

    , P","0, 1, 128, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-25 00:19, # not to split, # to split, # treat every token as punct, Enumerate all possible tokenizations of a text.

Args:
text: A text.

Returns: All possible tokenizations.

",
https://github.com/hankcs/HanLP,tf_util.py,0,0.0,87,68.5,23,18.11,6,4.72,11,8.66,0,0.0,21,1,15,5,,"NumpyEncoder, RuntimeError, __name__, _absl_handler, _keras_mask, _warn_preinit_stderr, absl, append, arg, built, class_name, cls, config, copy_mask, count, custom_cls, dataset, default, dst, dtype, e, element, experimental, format_metrics, get_callback_by_class, get_visible_gpus, getattr, gpu, gpus, growth, hanlp_common, hanlp_register, hasattr, idx, int, isinstance, json, len, level, line_list, list_logical_devices, list_physical_devices, logging, logical_devices, mask, model, name, nice, nice_gpu, numpy, os, pad, pred, predict, random, register_custom_cls, registered_name, result, self, sent, set_gpu, set_gpu_memory_growth, set_memory_growth, set_seed_tf, set_tf_loglevel, set_visible_devices, shape, shut_up_python_logging, size_of_dataset, src, str, str_tensor, str_tensor_2d_to_list, str_tensor_to_list, str_tensor_to_str, summary, summary_of_model, tag, tensor, tensor_is_eager, tensorflow, tf, tf_bernoulli, tolist, typing, unbatch, x","hanlp_common.constant.PAD, json.JSONEncoder, json.JSONEncoder.default, logging.ERROR, logging.FATAL, logging.WARNING, logging.getLogger, logging.root.removeHandler, np.float16, np.float32, np.float64, np.float_, np.int16, np.int32, np.int64, np.int8, np.int_, np.intc, np.intp, np.ndarray, np.random.seed, np.uint16, np.uint32, np.uint64, np.uint8, os.environ, random.seed, tf.Tensor, tf.data.Dataset, tf.keras.Model, tf.keras.backend.random_binomial, tf.keras.callbacks.Callback, tf.keras.metrics.Metric, tf.keras.utils.get_custom_objects, tf.random.set_seed, typing.List","copy_mask, default, format_metrics, get_callback_by_class, get_visible_gpus, hanlp_register, nice, nice_gpu, register_custom_cls, set_gpu, set_gpu_memory_growth, set_seed_tf, set_tf_loglevel, shut_up_python_logging, size_of_dataset, str_tensor_2d_to_list, str_tensor_to_list, str_tensor_to_str, summary_of_model, tensor_is_eager, tf_bernoulli",NumpyEncoder,"class_name, count, element, gpu, gpus, j, l, line_list, logical_devices, mask, name, registered_name, sent, summary, tag","Registers a class with the Keras serialization framework.

Args:
  arg: 

Returns:, Restrict TensorFlow to only use the GPU of idx

Args:
  idx:  (Default value = 0)

Returns:, Special json encoder for numpy types
See https://interviewbubble.com/typeerror-object-of-type-float32-is-not-json-serializable/

Args:
    obj: Object to be json encoded.

Returns:
    Json string., Use GPU nicely., https://stackoverflow.com/a/53668338/3730690

Args:
  model: tf.keras.Model: 

Returns:","
,  - , .4f, 0, 1, 2, 3, : , >, GPU, HanLP, Registers a class with the Keras serialization framework.

    Args:
      arg: 

    Returns:

    , Restrict TensorFlow to only use the GPU of idx

    Args:
      idx:  (Default value = 0)

    Returns:

    
    , Special json encoder for numpy types
        See https://interviewbubble.com/typeerror-object-of-type-float32-is-not-json-serializable/

        Args:
            obj: Object to be json encoded.

        Returns:
            Json string.
        , TF_CPP_MIN_LOG_LEVEL, TF_CPP_MIN_VLOG_LEVEL, Use GPU nicely., _keras_mask, https://stackoverflow.com/a/53668338/3730690

    Args:
      model: tf.keras.Model: 

    Returns:

    
    , model structure unknown until calling fit() with some data, numpy, tensorflow, utf-8","0, 1, 233, False, None, True","
class_name = arg.__name__
registered_name = 'HanLP' + '>' + class_name

# if tf_inspect.isclass(arg) and not hasattr(arg, 'get_config'):
#     raise ValueError(
#         'Cannot register a class that does not have a get_config() method.')

tf.keras.utils.get_custom_objects()[registered_name] = arg

return arg


def tensor_is_eager(tensor: tf.Tensor):
return hasattr(tensor, 'numpy')


def copy_mask(src: tf.Tensor, dst: tf.Tensor):
mask = getattr(src, '_keras_mask', None)
if mask is not None:
dst._keras_mask = mask
return mask


def get_callback_by_class(callbacks: List[tf.keras.callbacks.Callback], cls) -> tf.keras.callbacks.Callback:
for callback in callbacks:
if isinstance(callback, cls):
return callback


def tf_bernoulli(shape, p, dtype=None):
return tf.keras.backend.random_binomial(shape, p, dtype)


def str_tensor_to_str(str_tensor: tf.Tensor) -> str:
return str_tensor.numpy().decode('utf-8')


def str_tensor_2d_to_list(str_tensor: tf.Tensor, pad=PAD) -> List[List[str]]:
l = []
for i in str_tensor:
sent = []
for j in i:
j = str_tensor_to_str(j)
if j == pad:
break
sent.append(j)
l.append(sent)
return l


def str_tensor_to_list(pred):
return [tag.predict('utf-8') for tag in pred]


def format_metrics(metrics: List[tf.keras.metrics.Metric]):
return ' - '.join(f'{m.name}: {m.result():.4f}' for m in metrics)


class NumpyEncoder(json.JSONEncoder):
def default(self, obj):
Special json encoder for numpy types, 
if not model.built:
return 'model structure unknown until calling fit() with some data'
line_list = []
model.summary(print_fn=lambda x: line_list.append(x))
summary = ""\n"".join(line_list)
return summary


def register_custom_cls(custom_cls, name=None):
if not name:
name = custom_cls.__name__
tf.keras.utils.get_custom_objects()[name] = custom_cls


def set_seed_tf(seed=233):
tf.random.set_seed(seed)
np.random.seed(seed)
random.seed(seed)


def nice():
nice_gpu()
set_seed_tf()


def hanlp_register(arg):
Registers a class with the Keras serialization framework., # -*- coding:utf-8 -*-, # Author: hankcs, # Currently, memory growth needs to be the same across GPUs, # Date: 2019-08-27 01:27, # Memory growth must be set before GPUs have been initialized, # Virtual devices must be set before GPUs have been initialized, # print(e), Restrict TensorFlow to only use the GPU of idx

Args:
idx:  (Default value = 0)

Returns:


, Use GPU nicely.
set_gpu_memory_growth()
set_gpu()


def shut_up_python_logging():
logging.getLogger('tensorflow').setLevel(logging.ERROR)
import absl.logging
logging.root.removeHandler(absl.logging._absl_handler)
absl.logging._warn_preinit_stderr = False


def set_tf_loglevel(level=logging.ERROR):
if level >= logging.FATAL:
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'
if level >= logging.ERROR:
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2'
if level >= logging.WARNING:
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '1'
else:
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'
shut_up_python_logging()
logging.getLogger('tensorflow').setLevel(level)


set_tf_loglevel()

shut_up_python_logging()
import tensorflow as tf

nice_gpu()


def size_of_dataset(dataset: tf.data.Dataset) -> int:
count = 0
for element in dataset.unbatch().batch(1):
count += 1
return count


def summary_of_model(model: tf.keras.Model):
https://stackoverflow.com/a/53668338/3730690",
https://github.com/hankcs/HanLP,time_util.py,0,0.0,90,74.38,15,12.4,12,9.92,4,3.31,0,0.0,29,3,22,1,,"CountdownTimer, HumanTimeDelta, Timer, __init__, __str__, __truediv__, _last_print_width, append, bool, cells, current, datetime, days, delimiter, delta_seconds, elapsed, elapsed_average, elapsed_average_human, elapsed_human, erase, et_eta, et_eta_human, eta, eta_human, finished, finished_in, float, fmt, hanlp, hours, human, human_time_delta, info, interval, isinstance, items, join, key, last, last_log_time, locals, log, logger, logging, max, min, minutes, msg, msg_len, newline, non_zero, now, now_datetime, now_filename, now_human, object, pop, print, property, ratio, ratio_percentage, ratio_width, report, report_time_delta, result, rjust, round, scalar, seconds, seconds_to_time_delta, self, sorted, start, step, stop, str, strftime, super, sys, text, time, total, total_time, total_time_human, typing, units, update, val, x, year","datetime.datetime.now, hanlp.utils.log_util.ErasablePrinter, hanlp.utils.log_util.color_format, hanlp.utils.log_util.color_format_len, logging.Logger, sys.stdout, sys.stdout.flush, sys.stdout.write, time.time, typing.Union","__init__, __str__, __truediv__, elapsed, elapsed_average, elapsed_average_human, elapsed_human, et_eta, et_eta_human, eta, eta_human, finished, human_time_delta, log, now_datetime, now_filename, now_human, print, ratio, ratio_percentage, ratio_width, report, report_time_delta, seconds_to_time_delta, start, stop, total_time, total_time_human, update","CountdownTimer, HumanTimeDelta, Timer","_, append, cells, days, elapsed, eta, hours, key, minutes, msg, msg_len, non_zero, now, ratio, ratio_width, result, seconds, t, text, units, val, x","Generate filename using current datetime, in 20180102_030405 format

Args:
  fmt:  (Default value = ""%y%m%d_%H%M%S"")

Returns:","
,  , %, %y%m%d_%H%M%S, -%m-%d %H:%M:%S, .2%, /, 0 s, ET: , ETA: , Generate filename using current datetime, in 20180102_030405 format

    Args:
      fmt:  (Default value = ""%y%m%d_%H%M%S"")

    Returns:

    
    , Y, delimiter, y, {} {}","0, 0.1, 0.5, 1, 2, 24, 3600, 60, 86400, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-27 00:01, Generate filename using current datetime, in 20180102_030405 format

Args:
fmt:  (Default value = ""%y%m%d_%H%M%S"")

Returns:


",
https://github.com/hankcs/HanLP,torch_util.py,0,0.0,131,70.43,26,13.98,9,4.84,19,10.22,1,0.54,16,0,41,4,,"Exception, IOError, Module, ValueError, __name__, activation_from_name, append, batch_size, batched_index_select, binpath, bool, broad_cast_seq_len, cache, clip_grad_norm, clip_grad_norm_, copy_, cuda_devices, data, debug, delimiter, dict, dim, dont_care_speed, dtype, dtype_of, enumerate, expanse, filepath, filter, filter_state_dict_safely, float, free, gather, get, getattr, gpus, gpus_available, gpus_with_same_size, grad_norm, hanlp, hanlp_common, idx, ind, index, info, input, int, is_transformer, isinstance, items, join, keys, len, lengths_to_mask, line, list, load_state, load_word2vec, load_word2vec_as_vocab_tensor, log, lt, main, mask, matrix, matrix_path, max, max_len, mean, mean_model, model, model_state, model_v, mul_, name, new_empty, non_transformer, numpy, open, os, out, pad_lists, padding_value, parameters, path, print, pynvml, query, random, range, ratio, real_id, realpath, repr, requires_grad, rstrip, safe_state, save_word2vec, seq_len, sequences, set, set_seed, shape, size, sorted, split, start, std, str, strip, time, tmp, torch, total, transformer, transformer_grad_norm, truncated_normal_, typing, unsqueeze, utils, valid, values, vec, view, views, visible_devices, vocab, vocab_path, warning, word, word2vec, write","hanlp.utils.io_util.TimingFileIterator, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.replace_ext, hanlp.utils.log_util.flash, hanlp.utils.log_util.logger, hanlp_common.constant.HANLP_VERBOSE, hanlp_common.io.load_pickle, hanlp_common.io.save_pickle, np.array, np.float32, np.ndarray, np.random.seed, np.stack, os.environ.get, pynvml.nvmlDeviceGetCount, pynvml.nvmlDeviceGetHandleByIndex, pynvml.nvmlDeviceGetMemoryInfo, pynvml.nvmlInit, pynvml.nvmlShutdown, random.choice, random.seed, time.time, torch.Tensor, torch.arange, torch.backends.cudnn.benchmark, torch.backends.cudnn.deterministic, torch.bool, torch.cuda.device_count, torch.cuda.is_available, torch.cuda.manual_seed_all, torch.float, torch.gather, torch.load, torch.long, torch.manual_seed, torch.mean, torch.nn, torch.nn.Module, torch.nn.utils.rnn.pad_sequence, torch.save, torch.stack, torch.sum, torch.tensor, typing.Dict, typing.List, typing.Tuple, typing.Union","activation_from_name, batched_index_select, clip_grad_norm, cuda_devices, dtype_of, filter_state_dict_safely, gpus_available, lengths_to_mask, load_word2vec, load_word2vec_as_vocab_tensor, main, mean_model, pad_lists, save_word2vec, set_seed, truncated_normal_",,"batch_size, binpath, broad_cast_seq_len, dim, expanse, f, free, gpus, gpus_with_same_size, h, idx, ind, index, info, is_transformer, line, mask, matrix, matrix_path, max_len, model_v, non_transformer, out, query, ratio, real_id, realpath, safe_state, size, start, tmp, total, transformer, valid, vec, views, visible_devices, vocab, vocab_path, word, word2vec",".. code-block::

    >>> seq_len = torch.arange(2, 16)
    >>> mask = lengths_to_mask(seq_len)
    >>> print(mask.size())
    torch.Size([14, 15])
    >>> seq_len = np.arange(2, 16)
    >>> mask = lengths_to_mask(seq_len)
    >>> print(mask.shape)
    (14, 15)
    >>> seq_len = torch.arange(2, 16)
    >>> mask = lengths_to_mask(seq_len, max_len=100)
    >>>print(mask.size())
    torch.Size([14, 100])

:param torch.LongTensor seq_len: (B,)
:param int max_len: max sequence length。
:return:  torch.Tensor  (B, max_len), Args:
  input: B x * x ... x *
  index: B x M
  dim:  (Default value = 1)

Returns:, Copied from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L76

Args:
  seed:  (Default value = 233)
  dont_care_speed: True may have a negative single-run performance impact, but ensures deterministic

Returns:, Decide which GPUs to use

Args:
  query:  (Default value = None)

Returns:",", 
, 

    Args:
      input: B x * x ... x *
      index: B x M
      dim:  (Default value = 1)

    Returns:

    
    , 
    .. code-block::

        >>> seq_len = torch.arange(2, 16)
        >>> mask = lengths_to_mask(seq_len)
        >>> print(mask.size())
        torch.Size([14, 15])
        >>> seq_len = np.arange(2, 16)
        >>> mask = lengths_to_mask(seq_len)
        >>> print(mask.shape)
        (14, 15)
        >>> seq_len = torch.arange(2, 16)
        >>> mask = lengths_to_mask(seq_len, max_len=100)
        >>>print(mask.size())
        torch.Size([14, 100])

    :param torch.LongTensor seq_len: (B,)
    :param int max_len: max sequence length。
    :return:  torch.Tensor  (B, max_len)
    ,  , ,, ., .pkl, .pt, .vocab, CUDA_VISIBLE_DEVICES, Caching vocab and matrix [blink][yellow]...[/yellow][/blink], Caching word2vec [blink][yellow]...[/yellow][/blink], Copied from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L76

    Args:
      seed:  (Default value = 233)
      dont_care_speed: True may have a negative single-run performance impact, but ensures deterministic

    Returns:

    
    , Decide which GPUs to use

    Args:
      query:  (Default value = None)

    Returns:

    
    , Failed to get gpu info due to , Loading vocab and matrix from cache [blink][yellow]...[/yellow][/blink], Loading word2vec from cache [blink][yellow]...[/yellow][/blink], Loading word2vec from text file [blink][yellow]...[/yellow][/blink], Unsupported type of , __main__, cpu, seq_len can only have one dimension, got , utf-8, w, {}#{} length mismatches with {}","0, 1, 1.0, 2, 233, 4, False, None, True","

Args:
input: B x * x ... x *
index: B x M
dim:  (Default value = 1)

Returns:


, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-09 15:52, # When multiple GPUs have the same size, randomly pick one to avoid conflicting, # ^^ safe to call this function even if cuda is not available, # a = torch.cuda.memory_allocated(0), # c = torch.cuda.memory_cached(0), # print(cuda_devices()), # print(cuda_devices(0.1)), # print(f'free     : {info.free}'), # print(f'total    : {info.total}'), # print(f'used     : {info.used}'), # print(gpus_available()), # print(t, c, a), # t = torch.cuda.get_device_properties(0).total_memory, #{} length mismatches with {}'.format(path, idx + 1, dim)), Copied from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L76

Args:
seed:  (Default value = 233)
dont_care_speed: True may have a negative single-run performance impact, but ensures deterministic

Returns:


, Decide which GPUs to use

Args:
query:  (Default value = None)

Returns:


",length。
https://github.com/hankcs/HanLP,__init__.py,0,0.0,14,58.33,6,25.0,0,0.0,4,16.67,0,0.0,1,0,3,0,,"__dict__, dict, endswith, isinstance, items, k, ls_resource_in_module, res, root, startswith, str, type, update, v",rules,ls_resource_in_module,,"k, res, v",,"#, /, ALL, _, http, module",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-24 22:12, #') and not v.startswith('_'):",
https://github.com/hankcs/HanLP,buffer_work_space.py,0,0.0,49,59.04,16,19.28,5,6.02,13,15.66,0,0.0,12,1,15,11,,"BufferWorkSpace, __init__, _find_furthest_new_line, _get_file_size, _get_next_chunk, _get_what_to_read_next, _is_partially_read_new_line, _remove_trailing_new_line, add_to_buffer, after_new_line, b, chunk_size, content, encode, endswith, fileno, find, fp, has_returned_every_line, i, l, len, max, min, n, new_line_positions, new_lines, new_lines_bytes, os, previously_read_position, r, read, read_buffer, read_content, read_position, read_size, read_until_yieldable, remove_new_line, return_line, rfind, seek, seek_position, self, slice, sorted, t, up_to_include_new_line, x, yieldable",os.fstat,"__init__, _find_furthest_new_line, _get_file_size, _get_next_chunk, _get_what_to_read_next, _is_partially_read_new_line, _remove_trailing_new_line, add_to_buffer, has_returned_every_line, read_until_yieldable, return_line, yieldable",BufferWorkSpace,"after_new_line, i, l, n, new_line_positions, new_lines, new_lines_bytes, r, read_content, read_position, read_size, remove_new_line, seek_position, t, up_to_include_new_line","Add additional bytes content as read from the read_position.

Args:
  content(bytes): data to be added to buffer working BufferWorkSpac.
  read_position(int): where in the file pointer the data was read from.

Returns:, Args:

Returns:
  Precondition: self.yieldable() must be True, BufferWorkSpace module., Convention for the data.

When read_buffer is not None, it represents contents of the file from `read_position` onwards
    that has not been processed/returned.
read_position represents the file pointer position that has been read into read_buffer
    initialized to be just past the end of file., It is a helper module for FileReadBackwards., Read in additional chunks until it is yieldable., Remove a single instance of new line at the end of l if it exists.

Args:
  l: 

Returns:
  : bytestring, Return -1 if read_buffer does not contain new line otherwise the position of the rightmost newline.

Args:
  read_buffer: bytestring

Returns:
  int: The right most position of new line character in read_buffer if found, else -1, Return True when b is part of a new line separator found at index >= 1, False otherwise.

Args:
  b: bytestring

Returns:
  bool, Return information on which file pointer position to read from and how many bytes.

Args:
  fp: 
  past_read_positon: int
  chunk_size: int
  previously_read_position: 

Returns:
  (int, int): The next seek position, how many bytes to read next, Return next chunk of data that we would from the file pointer.

Args:
  fp: file
  previously_read_position: file pointer position that we have read from
  chunk_size: desired read chunk_size

Returns:
  (bytestring, int): data that has been read in, the file pointer position where the data has been read from","
, 

        Args:

        Returns:
          Precondition: self.yieldable() must be True

        , , 
,  , Add additional bytes content as read from the read_position.

        Args:
          content(bytes): data to be added to buffer working BufferWorkSpac.
          read_position(int): where in the file pointer the data was read from.

        Returns:

        , BufferWorkSpace module., Convention for the data.

        When read_buffer is not None, it represents contents of the file from `read_position` onwards
            that has not been processed/returned.
        read_position represents the file pointer position that has been read into read_buffer
            initialized to be just past the end of file.
        , It is a helper module for FileReadBackwards., Read in additional chunks until it is yieldable., Remove a single instance of new line at the end of l if it exists.

    Args:
      l: 

    Returns:
      : bytestring

    , Return -1 if read_buffer does not contain new line otherwise the position of the rightmost newline.

    Args:
      read_buffer: bytestring

    Returns:
      int: The right most position of new line character in read_buffer if found, else -1

    , Return True when b is part of a new line separator found at index >= 1, False otherwise.

    Args:
      b: bytestring

    Returns:
      bool

    , Return information on which file pointer position to read from and how many bytes.

    Args:
      fp: 
      past_read_positon: int
      chunk_size: int
      previously_read_position: 

    Returns:
      (int, int): The next seek position, how many bytes to read next

    , Return next chunk of data that we would from the file pointer.

    Args:
      fp: file
      previously_read_position: file pointer position that we have read from
      chunk_size: desired read chunk_size

    Returns:
      (bytestring, int): data that has been read in, the file pointer position where the data has been read from

    , ascii","0, 1, False, None, True","
# replace only 1 instance of newline
# match longest line first (hence the reverse=True), we want to match ""\r\n"" rather than ""\n"" if we can
for n in sorted(new_lines_bytes, key=lambda x: len(x), reverse=True):
if l.endswith(n):
remove_new_line = slice(None, -len(n))
return l[remove_new_line]
return l


def _find_furthest_new_line(read_buffer):
Return -1 if read_buffer does not contain new line otherwise the position of the rightmost newline., 
assert(self.yieldable())

t = _remove_trailing_new_line(self.read_buffer)
i = _find_furthest_new_line(t)

if i >= 0:
l = i + 1
after_new_line = slice(l, None)
up_to_include_new_line = slice(0, l)
r = t[after_new_line]
self.read_buffer = t[up_to_include_new_line]
else:  # the case where we have read in entire file and at the ""last"" line
r = t
self.read_buffer = None
return r

def read_until_yieldable(self):
Read in additional chunks until it is yieldable., 
new_line_positions = [read_buffer.rfind(n) for n in new_lines_bytes]
return max(new_line_positions)


def _is_partially_read_new_line(b):
Return True when b is part of a new line separator found at index >= 1, False otherwise., 
seek_position = max(previously_read_position - chunk_size, 0)
read_size = chunk_size

# examples: say, our new_lines are potentially ""\r\n"", ""\n"", ""\r""
# find a reading point where it is not ""\n"", rewind further if necessary
# if we have ""\r\n"" and we read in ""\n"",
# the next iteration would treat ""\r"" as a different new line.
# Q: why don't I just check if it is b""\n"", but use a function ?
# A: so that we can potentially expand this into generic sets of separators, later on.
while seek_position > 0:
fp.seek(seek_position)
if _is_partially_read_new_line(fp.read(1)):
seek_position -= 1
read_size += 1  # as we rewind further, let's make sure we read more to compensate
else:
break

# take care of special case when we are back to the beginnin of the file
read_size = min(previously_read_position - seek_position, read_size)
return seek_position, read_size


def _remove_trailing_new_line(l):
Remove a single instance of new line at the end of l if it exists., 
seek_position, read_size = _get_what_to_read_next(fp, previously_read_position, chunk_size)
fp.seek(seek_position)
read_content = fp.read(read_size)
read_position = seek_position
return read_content, read_position


def _get_what_to_read_next(fp, previously_read_position, chunk_size):
Return information on which file pointer position to read from and how many bytes.,  
if self.read_buffer is None:
return False

t = _remove_trailing_new_line(self.read_buffer)
n = _find_furthest_new_line(t)
if n >= 0:
return True

# we have read in entire file and have some unprocessed lines
if self.read_position == 0 and self.read_buffer is not None:
return True
return False

def return_line(self):
,  
if self.read_position == 0 and self.read_buffer is None:
return True
return False


def _get_file_size(fp):
return os.fstat(fp.fileno()).st_size


def _get_next_chunk(fp, previously_read_position, chunk_size):
Return next chunk of data that we would from the file pointer., # -*- coding: utf-8 -*-, # set the previously read position to the, #!/usr/bin/env python, Add additional bytes content as read from the read_position.

Args:
content(bytes): data to be added to buffer working BufferWorkSpac.
read_position(int): where in the file pointer the data was read from.

Returns:

, BufferWorkSpace module.

import os

new_lines = [""\r\n"", ""\n"", ""\r""]
new_lines_bytes = [n.encode(""ascii"") for n in new_lines]  # we only support encodings that's backward compat with ascii


class BufferWorkSpace:

It is a helper module for FileReadBackwards., Convention for the data.

When read_buffer is not None, it represents contents of the file from `read_position` onwards
that has not been processed/returned.
read_position represents the file pointer position that has been read into read_buffer
initialized to be just past the end of file.
",
https://github.com/hankcs/HanLP,file_read_backwards.py,0,0.0,33,50.77,20,30.77,1,1.54,11,16.92,0,0.0,8,2,4,11,,"FileReadBackwards, FileReadBackwardsIterator, NotImplementedError, StopIteration, __buf, __enter__, __exit__, __fp, __init__, __iter__, __next__, buffer_work_space, chunk_size, close, closed, decode, encoding, error_message, fp, has_returned_every_line, io, iterator, lower, name, next, os, path, property, read_until_yieldable, readline, return_line, self, supported_encodings","buffer_work_space.BufferWorkSpace, io.DEFAULT_BUFFER_SIZE, io.open, os.linesep","__enter__, __exit__, __init__, __iter__, close, closed, next, readline","FileReadBackwards, FileReadBackwardsIterator","__next__, error_message, r, supported_encodings","Class definition for `FileReadBackwards`.

A `FileReadBackwards` will spawn a `FileReadBackwardsIterator` and keep an opened file handler.

It can be used as a Context Manager. If done so, when exited, it will close its file handler.

In any mode, `close()` can be called to close the file handler..

Args:

Returns:, Closes all opened it s file handler., Closes all opened its file handler and propagates all exceptions on exit., Closes the file handler., Constructor for FileReadBackwards.

Args:
    path: Path to the file to be read
    encoding (str): Encoding
    chunk_size (int): How many bytes to read at a time, Constructor for FileReadBackwardsIterator

Args:
    fp (File): A file that we wish to start reading backwards from
    encoding (str): Encoding of the file
    chunk_size (int): How many bytes to read at a time, FileReadBackwards module., Iterator for `FileReadBackwards`.

This will read backwards line by line a file. It holds an opened file handler.

Args:

Returns:, Return its iterator., Returns unicode string from the last line until the beginning of file.

Gets exhausted if::

    * already reached the beginning of the file on previous iteration
    * the file got closed

When it gets exhausted, it closes the file handler.

Args:

Returns:, The status of the file handler.

:return: True if the file handler is still opened. False otherwise.

Args:

Returns:",",  , ,, Class definition for `FileReadBackwards`.
    
    A `FileReadBackwards` will spawn a `FileReadBackwardsIterator` and keep an opened file handler.
    
    It can be used as a Context Manager. If done so, when exited, it will close its file handler.
    
    In any mode, `close()` can be called to close the file handler..

    Args:

    Returns:

    , Closes all opened it s file handler., Closes all opened its file handler and propagates all exceptions on exit., Closes the file handler., Constructor for FileReadBackwards.

        Args:
            path: Path to the file to be read
            encoding (str): Encoding
            chunk_size (int): How many bytes to read at a time
        , Constructor for FileReadBackwardsIterator

        Args:
            fp (File): A file that we wish to start reading backwards from
            encoding (str): Encoding of the file
            chunk_size (int): How many bytes to read at a time
        , FileReadBackwards module., Iterator for `FileReadBackwards`.
    
    This will read backwards line by line a file. It holds an opened file handler.

    Args:

    Returns:

    , Return its iterator., Returns unicode string from the last line until the beginning of file.
        
        Gets exhausted if::
        
            * already reached the beginning of the file on previous iteration
            * the file got closed
        
        When it gets exhausted, it closes the file handler.

        Args:

        Returns:

        , Supported encodings are '{0}', The status of the file handler.
        
        :return: True if the file handler is still opened. False otherwise.

        Args:

        Returns:

        , ascii, latin-1, rb, utf-8, {0} encoding was not supported/tested.",False,"

def __init__(self, path, encoding=""utf-8"", chunk_size=io.DEFAULT_BUFFER_SIZE):
Constructor for FileReadBackwards., 
# Using binary mode, because some encodings such as ""utf-8"" use variable number of
# bytes to encode different Unicode points.
# Without using binary mode, we would probably need to understand each encoding more
# and do the seek operations to find the proper boundary before issuing read
if self.closed:
raise StopIteration
if self.__buf.has_returned_every_line():
self.close()
raise StopIteration
self.__buf.read_until_yieldable()
r = self.__buf.return_line()
return r.decode(self.encoding)

__next__ = next

@property
def closed(self):
The status of the file handler., 
def __init__(self, fp, encoding, chunk_size):
Constructor for FileReadBackwardsIterator, 
if encoding.lower() not in supported_encodings:
error_message = ""{0} encoding was not supported/tested."".format(encoding)
error_message += ""Supported encodings are '{0}'"".format("","".join(supported_encodings))
raise NotImplementedError(error_message)

self.path = path
self.encoding = encoding.lower()
self.chunk_size = chunk_size
self.iterator = FileReadBackwardsIterator(io.open(self.path, mode=""rb""), self.encoding, self.chunk_size)

def __iter__(self):
Return its iterator., 
return self.__fp.closed

def close(self):
Closes the file handler., 
self.path = fp.name
self.encoding = encoding
self.chunk_size = chunk_size
self.__fp = fp
self.__buf = BufferWorkSpace(self.__fp, self.chunk_size)

def __iter__(self):
return self

def next(self):
Returns unicode string from the last line until the beginning of file.,  

try:
r = next(self.iterator) + os.linesep
return r
except StopIteration:
return """"


class FileReadBackwardsIterator:
Iterator for `FileReadBackwards`., # -*- coding: utf-8 -*-, #!/usr/bin/env python, Closes all opened its file handler and propagates all exceptions on exit.
self.close()
return False

def close(self):
Closes all opened it s file handler., FileReadBackwards module.

import io
import os

from .buffer_work_space import BufferWorkSpace

supported_encodings = [""utf-8"", ""ascii"", ""latin-1""]  # any encodings that are backward compatible with ascii should work


class FileReadBackwards:

Class definition for `FileReadBackwards`.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,4,44.44,3,33.33,0,0.0,2,22.22,0,0.0,0,0,3,0,,"__author__, __email__, __version__, file_read_backwards",file_read_backwards.FileReadBackwards,,,"__author__, __email__, __version__",,"2.0.0, Robin Robin, robinsquare42@gmail.com",,"# -*- coding: utf-8 -*-, # noqa: F401",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,1,20.0,1,20.0,0,0.0,3,60.0,0,0.0,0,0,1,0,,__doc__,,,,__doc__,,"
This package holds misc utils for specific languages.
",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-09 18:46",
https://github.com/hankcs/HanLP,english_tokenizer.py,0,0.0,67,30.59,116,52.97,12,5.48,16,7.31,8,3.65,7,0,40,7,,"ALNUM, APOSTROPHE, APOSTROPHES, APO_MATCHER, HYPHEN, HYPHENATED_LINEBREAK, HYPHENS, IS_CONTRACTION, IS_POSSESSIVE, LETTER, LINEBREAK, MAP_CONCAT_WORD, NUMBER, POWER, RE_APOSTROPHE, SENTENCE_TERMINALS, SPACE, SUBDIGIT, __author__, _matches, any, append, automaton, char, chunks, dirty, end, enumerate, find, flat, fn, get, idx, insert, isinstance, j, last, len, length, list, lower, match, match_decorator, pos, pruned, range, re, regex, results, reversed, rfind, search, sentence, sents, space_tokenizer, span, split, split_contractions, split_possessive_markers, start, sub, symbol_tokenizer, t, token, tokenize_english, tokens, word","re.UNICODE, re.VERBOSE, re.compile","_matches, match_decorator, space_tokenizer, split_contractions, split_possessive_markers, symbol_tokenizer, tokenize_english",,"ALNUM, APOSTROPHE, APOSTROPHES, APO_MATCHER, HYPHEN, HYPHENATED_LINEBREAK, HYPHENS, IS_CONTRACTION, IS_POSSESSIVE, LETTER, LINEBREAK, MAP_CONCAT_WORD, NUMBER, POWER, RE_APOSTROPHE, SENTENCE_TERMINALS, SPACE, SUBDIGIT, __author__, automaton, char, chunks, dirty, flat, i, idx, j, last, length, m, pos, pruned, results, sentence, sents, span, t, token, tokens, word","A function to split apostrophe contractions at the end of alphanumeric (and hyphenated) tokens.

Takes the output of any of the tagger functions and produces and updated list.

:param tokens: a list of tokens
:returns: an updated list if a split was made or the original list otherwise, A function to split possessive markers at the end of alphanumeric (and hyphenated) tokens.

Takes the output of any of the tagger functions and produces and updated list.
To use it, simply wrap the tagger function, for example::

>>> my_sentence = ""This is Fred's latest book.""
>>> split_possessive_markers(tokenize_english(my_sentence))
['This', 'is', 'Fred', ""'s"", 'latest', 'book', '.']

:param tokens: a list of tokens
:returns: an updated list if a split was made or the original list otherwise, A modified version of the segtok tagger: https://github.com/fnl/segtok
This tagger extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:

1. Dots appearing after a letter are maintained as part of the word, except for the last word
   in a sentence if that dot is the sentence terminal. Therefore, abbreviation marks (words
   containing or ending in a ``.``, like ""i.e."") remain intact and URL or ID segments remain
   complete (""www.ex-ample.com"", ""EC1.2.3.4.5"", etc.). The only dots that never are attached
   are triple dots (``...``; ellipsis).
2. Commas surrounded by alphanumeric characters are maintained in the word, too, e.g. ``a,b``.
   Colons surrounded by digits are maintained, e.g., 'at 12:30pm' or 'Isaiah 12:3'.
   Commas, semi-colons, and colons dangling at the end of a token are always spliced off.
3. Any two alphanumeric letters that are separated by a single hyphen are joined together;
   Those ""inner"" hyphens may optionally be followed by a linebreak surrounded by spaces;
   The spaces will be removed, however. For example, ``Hel- \r\n     lo`` contains a (Windows)
   linebreak and will be returned as ``Hel-lo``.
4. Apostrophes are always allowed in words as long as they are not repeated; The single quote
   ASCII letter ``'`` is only allowed as a terminal apostrophe after the letter ``s``,
   otherwise it must be surrounded by letters. To support DNA and chemicals, a apostrophe
   (prime) may be located before the hyphen, as in the single token ""5'-ACGT-3'"" (if any
   non-ASCII hyphens are used instead of the shown single quote).
5. Superscript 1, 2, and 3, optionally prefixed with a superscript minus, are attached to a
   word if it is no longer than 3 letters (optionally 4 if the first letter is a power prefix
   in the range from yocto, y (10^-24) to yotta, Y (10^+24)).
6. Subscript digits are attached if prefixed with letters that look like a chemical formula., For a given input `sentence`, return a list of its tokens.

Split on Unicode spaces ``\s+`` (i.e., any kind of **Unicode** space character).
The separating space characters are not included in the resulting token list., Regex-based word tokenizers.

Note that small/full/half-width character variants are *not* covered.
If a text were to contains such characters, normalize it first.
A modified version of https://github.com/fnl/segtok

- dropped dependency on regex
- dropped web_tokenize
- supported concat word, Regular expression compiling function decorator., The symbol tagger extends the :func:`space_tokenizer` by separating alphanumerics.

Separates alphanumeric Unicode character sequences in already space-split tokens.","
    A function to split apostrophe contractions at the end of alphanumeric (and hyphenated) tokens.

    Takes the output of any of the tagger functions and produces and updated list.

    :param tokens: a list of tokens
    :returns: an updated list if a split was made or the original list otherwise
    , 
    A function to split possessive markers at the end of alphanumeric (and hyphenated) tokens.

    Takes the output of any of the tagger functions and produces and updated list.
    To use it, simply wrap the tagger function, for example::

    >>> my_sentence = ""This is Fred's latest book.""
    >>> split_possessive_markers(tokenize_english(my_sentence))
    ['This', 'is', 'Fred', ""'s"", 'latest', 'book', '.']

    :param tokens: a list of tokens
    :returns: an updated list if a split was made or the original list otherwise
    , 
    A modified version of the segtok tagger: https://github.com/fnl/segtok
    This tagger extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:

    1. Dots appearing after a letter are maintained as part of the word, except for the last word
       in a sentence if that dot is the sentence terminal. Therefore, abbreviation marks (words
       containing or ending in a ``.``, like ""i.e."") remain intact and URL or ID segments remain
       complete (""www.ex-ample.com"", ""EC1.2.3.4.5"", etc.). The only dots that never are attached
       are triple dots (``...``; ellipsis).
    2. Commas surrounded by alphanumeric characters are maintained in the word, too, e.g. ``a,b``.
       Colons surrounded by digits are maintained, e.g., 'at 12:30pm' or 'Isaiah 12:3'.
       Commas, semi-colons, and colons dangling at the end of a token are always spliced off.
    3. Any two alphanumeric letters that are separated by a single hyphen are joined together;
       Those ""inner"" hyphens may optionally be followed by a linebreak surrounded by spaces;
       The spaces will be removed, however. For example, ``Hel- \r\n 	 lo`` contains a (Windows)
       linebreak and will be returned as ``Hel-lo``.
    4. Apostrophes are always allowed in words as long as they are not repeated; The single quote
       ASCII letter ``'`` is only allowed as a terminal apostrophe after the letter ``s``,
       otherwise it must be surrounded by letters. To support DNA and chemicals, a apostrophe
       (prime) may be located before the hyphen, as in the single token ""5'-ACGT-3'"" (if any
       non-ASCII hyphens are used instead of the shown single quote).
    5. Superscript 1, 2, and 3, optionally prefixed with a superscript minus, are attached to a
       word if it is no longer than 3 letters (optionally 4 if the first letter is a power prefix
       in the range from yocto, y (10^-24) to yotta, Y (10^+24)).
    6. Subscript digits are attached if prefixed with letters that look like a chemical formula.
    , 
    For a given input `sentence`, return a list of its tokens.

    Split on Unicode spaces ``\s+`` (i.e., any kind of **Unicode** space character).
    The separating space characters are not included in the resulting token list.
    , 
    The symbol tagger extends the :func:`space_tokenizer` by separating alphanumerics.

    Separates alphanumeric Unicode character sequences in already space-split tokens.
    , 
Regex-based word tokenizers.

Note that small/full/half-width character variants are *not* covered.
If a text were to contains such characters, normalize it first.
A modified version of https://github.com/fnl/segtok

- dropped dependency on regex
- dropped web_tokenize
- supported concat word

, 
The pattern matches any alphanumeric Unicode character, followed by a hyphen,
a single line-break surrounded by optional (non-breaking) spaces,
and terminates with a alphanumeric character on this next line.
The opening char and hyphen as well as the terminating char are captured in two groups.
, 'tis, 'twas, '´ʹʼ’′, (%s+), ((?:
    # Dots, except ellipsis
    {alnum} \. (?!\.\.)
    | # Comma, surrounded by digits (e.g., chemicals) or letters
    {alnum} , (?={alnum})
    | # Colon, surrounded by digits (e.g., time, references)
    {number} : (?={number})
    | # Hyphen, surrounded by digits (e.g., DNA endings: ""5'-ACGT-3'"") or letters
    {alnum} {apo}? {hyphen} (?={alnum})  # incl. optional apostrophe for DNA segments
    | # Apostophes, non-consecutive
    {apo} (?!{apo})
    | # ASCII single quote, surrounded by digits or letters (no dangling allowed)
    {alnum} ' (?={alnum})
    | # ASCII single quote after an s and at the token's end
    s ' $
    | # Terminal dimensions (superscript minus, 1, 2, and 3) attached to physical units
    #  size-prefix                 unit-acronym    dimension
    \b [yzafpn\u00B5mcdhkMGTPEZY]? {letter}{{1,3}} {power} $
    | # Atom counts (subscript numbers) and ionization states (optional superscript
    #   2 or 3 followed by a + or -) are attached to valid fragments of a chemical formula
    \b (?:[A-Z][a-z]?|[\)\]])+ {subdigit}+ (?:[\u00B2\u00B3]?[\u207A\u207B])?
    | # Any (Unicode) letter, digit, or the underscore
    {alnum}
    )+), (?:\r\n|\n|\r|\u2028), (?i)[a-z](n[\'\u2019]t|[\'\u2019](ll|nt|re|ve|[dmstz]))(\W|$), ({alnum}{hyphen}){space}*?{linebreak}{space}*?({alnum}), ,;:, .!?‼‽⁇⁈⁉。﹒﹗！．？｡, ..., A pattern that matches English words with a possessive s terminal form., A pattern that matches tokens with valid English contractions ``'(d|ll|m|re|s|t|ve)``., All apostrophe-like marks, including the ASCII ""single quote""., Any Unicode letter character that can form part of a word: Ll, Lm, Lt, Lu., Any Unicode number character: Nd or Nl., Any alphanumeric Unicode character: letter or number., Any apostrophe-like marks, including ""prime"" but not the ASCII ""single quote""., Any unicode space character plus the (horizontal) tab., Any valid linebreak sequence (Windows, Unix, Mac, or U+2028)., Any valid word-breaking hyphen, including ASCII hyphen minus., Florian Leitner <florian.leitner@gmail.com>, Matcher for any apostrophe., Regular expression compiling function decorator., Subscript digits., Superscript 1, 2, and 3, optionally prefixed with a minus sign., The list of valid Unicode sentence terminal characters., [%s], [', [\u00B4\u02B9\u02BC\u2019\u2032], [\u2080-\u2089], [^\W\d_], \1\2, \d, \s, \s+, \u207B?[\u00B9\u00B2\u00B3], ], aint, arent, cannot, cant, coulda, couldnt, d'ye, didnt, doesnt, don'cha, doncha, dont, don’cha, dunno, d’ye, finna, gimme, gonna, gotta, hadnt, hasnt, havent, i'mma, i'mmm, isnt, itd, itll, i’mma, i’mmm, lemme, lets, mightnt, more'n, more’n, mustnt, n, s, shant, shoulda, shouldnt, t, thatd, thatll, thats, theyd, theyre, theyve, wanna, wasnt, werent, weve, whadya, whatcha, whatre, whats, whatve, whatz, whod, wholl, woncha, wont, woulda, wouldnt, youd, youll, youve, {alnum}+(?:{hyphen}{alnum}+)*(?:{apo}[sS]|[sS]{apo})$, {alnum}+(?:{hyphen}{alnum}+)*{apo}(?:d|ll|m|re|s|t|ve)$, ­֊־༌᐀᠆‐-‒⸗゠-, ’tis, ’twas","0, 1, 2, 3, 4, 5, 6, 7, 8, False, None, True","
A function to split apostrophe contractions at the end of alphanumeric (and hyphenated) tokens.

Takes the output of any of the tagger functions and produces and updated list.

:param tokens: a list of tokens
:returns: an updated list if a split was made or the original list otherwise
, 
A function to split possessive markers at the end of alphanumeric (and hyphenated) tokens.

Takes the output of any of the tagger functions and produces and updated list.
To use it, simply wrap the tagger function, for example::

>>> my_sentence = ""This is Fred's latest book.""
>>> split_possessive_markers(tokenize_english(my_sentence))
['This', 'is', 'Fred', ""'s"", 'latest', 'book', '.']

:param tokens: a list of tokens
:returns: an updated list if a split was made or the original list otherwise
, 
Regex-based word tokenizers.

Note that small/full/half-width character variants are *not* covered.
If a text were to contains such characters, normalize it first.
A modified version of https://github.com/fnl/segtok

- dropped dependency on regex
- dropped web_tokenize
- supported concat word

, 
The pattern matches any alphanumeric Unicode character, followed by a hyphen,
a single line-break surrounded by optional (non-breaking) spaces,
and terminates with a alphanumeric character on this next line.
The opening char and hyphen as well as the terminating char are captured in two groups.
, 
return [token for span in space_tokenizer(sentence) for
token in symbol_tokenizer.split(span) if token]


@_matches(r""""""((?:
# Dots, except ellipsis
{alnum} \. (?!\.\.)
| # Comma, surrounded by digits (e.g., chemicals) or letters
{alnum} , (?={alnum})
| # Colon, surrounded by digits (e.g., time, references)
{number} : (?={number})
| # Hyphen, surrounded by digits (e.g., DNA endings: ""5'-ACGT-3'"") or letters
{alnum} {apo}? {hyphen} (?={alnum})  # incl. optional apostrophe for DNA segments
| # Apostophes, non-consecutive
{apo} (?!{apo})
| # ASCII single quote, surrounded by digits or letters (no dangling allowed)
{alnum} ' (?={alnum})
| # ASCII single quote after an s and at the token's end
s ' $
| # Terminal dimensions (superscript minus, 1, 2, and 3) attached to physical units
#  size-prefix                 unit-acronym    dimension
\b [yzafpn\u00B5mcdhkMGTPEZY]? {letter}{{1,3}} {power} $
| # Atom counts (subscript numbers) and ionization states (optional superscript
#   2 or 3 followed by a + or -) are attached to valid fragments of a chemical formula
\b (?:[A-Z][a-z]?|[\)\]])+ {subdigit}+ (?:[\u00B2\u00B3]?[\u207A\u207B])?
| # Any (Unicode) letter, digit, or the underscore
{alnum}
)+)"""""".format(alnum=ALNUM, apo=APOSTROPHE, power=POWER, subdigit=SUBDIGIT,
hyphen=HYPHEN, letter=LETTER, number=NUMBER))
def tokenize_english(sentence):
, 
return [token for token in space_tokenizer.split(sentence) if token]


@_matches(r'(%s+)' % ALNUM)
def symbol_tokenizer(sentence):
, # Note that Unicode the category Pd is NOT a good set for valid word-breaking hyphens,, # because it contains many dashes that should not be considered part of a word., #!/usr/bin/env python, A pattern that matches English words with a possessive s terminal form.

IS_CONTRACTION = compile(r""{alnum}+(?:{hyphen}{alnum}+)*{apo}(?:d|ll|m|re|s|t|ve)$"".format(
alnum=ALNUM, hyphen=HYPHEN, apo=""['"" + APOSTROPHE[1:]
), UNICODE
)
A pattern that matches tokens with valid English contractions ``'(d|ll|m|re|s|t|ve)``., All apostrophe-like marks, including the ASCII ""single quote"".

APOSTROPHE = r""[\u00B4\u02B9\u02BC\u2019\u2032]""
Any apostrophe-like marks, including ""prime"" but not the ASCII ""single quote""., Any Unicode number character: Nd or Nl.

POWER = r'\u207B?[\u00B9\u00B2\u00B3]'
Superscript 1, 2, and 3, optionally prefixed with a minus sign., Any unicode space character plus the (horizontal) tab.

APO_MATCHER = compile(APOSTROPHE, UNICODE)
Matcher for any apostrophe., Any valid linebreak sequence (Windows, Unix, Mac, or U+2028).

LETTER = r'[^\W\d_]'
Any Unicode letter character that can form part of a word: Ll, Lm, Lt, Lu., Regular expression compiling function decorator.

def match_decorator(fn):
automaton = compile(regex, UNICODE | VERBOSE)
fn.split = automaton.split
fn.match = automaton.match
return fn

return match_decorator


@_matches(r'\s+')
def space_tokenizer(sentence):
, Subscript digits.

ALNUM = LETTER[:-1] + NUMBER + ']'
Any alphanumeric Unicode character: letter or number.","'´ʹʼ’′, don’cha, d’ye, i’mma, i’mmm, more’n, ’tis, ’twas"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:28",
https://github.com/hankcs/HanLP,bert_tok.py,0,0.0,34,75.56,1,2.22,4,8.89,6,13.33,0,0.0,1,2,5,1,,"BertJapaneseTokenizer, BertJapaneseTokenizerFast, _BertJapaneseTokenizer, add_special_tokens, bool, encode_plus, encoding, encodings, enumerate, fixed_offsets, int, is_split_into_words, kwargs, list, max_length, offsets, pad_to_multiple_of, padding, return_attention_mask, return_length, return_offsets_mapping, return_overflowing_tokens, return_special_tokens_mask, return_tensors, return_token_type_ids, self, str, stride, text, text_pair, transformers, truncation, typing, verbose","transformers.BatchEncoding, transformers.BertJapaneseTokenizer, transformers.BertTokenizerFast, transformers.TensorType, transformers.file_utils.PaddingStrategy, transformers.tokenization_utils_base.EncodedInput, transformers.tokenization_utils_base.PreTokenizedInput, transformers.tokenization_utils_base.TextInput, transformers.tokenization_utils_base.TruncationStrategy, typing.Optional, typing.Union",encode_plus,"BertJapaneseTokenizer, BertJapaneseTokenizerFast","encoding, fixed_offsets, is_split_into_words, offsets, text","Tokenize and prepare for the model a sequence or a pair of sequences.

.. warning::
    This method is deprecated, ``__call__`` should be used instead.

Args:
    text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):
        The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
        ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``
        method).
    text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):
        Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
        the ``tokenize`` method) or a list of integers (tokenized string ids using the
        ``convert_tokens_to_ids`` method).","
        Tokenize and prepare for the model a sequence or a pair of sequences.

        .. warning::
            This method is deprecated, ``__call__`` should be used instead.

        Args:
            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):
                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``
                method).
            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):
                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
                the ``tokenize`` method) or a list of integers (tokenized string ids using the
                ``convert_tokens_to_ids`` method).
        ","0, False, None, True","
Tokenize and prepare for the model a sequence or a pair of sequences.

.. warning::
This method is deprecated, ``__call__`` should be used instead.

Args:
text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):
The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``
method).
text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):
Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the ``tokenize`` method) or a list of integers (tokenized string ids using the
``convert_tokens_to_ids`` method).
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-13 13:24, # TODO: This doesn't work with rust tokenizers, # We may need to customize character level tokenization to handle English words and URLs",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-13 13:24",
https://github.com/hankcs/HanLP,char_table.py,0,0.0,26,72.22,5,13.89,1,2.78,4,11.11,0,0.0,5,2,8,0,,"CharTable, HANLP_CHAR_TABLE_JSON, HANLP_CHAR_TABLE_TXT, JsonCharTable, _init, b, cells, chars, convert, convert_char, get, hanlp, hanlp_common, len, line, load, mapper, normalize_chars, normalize_text, open, rstrip, src, staticmethod, str, text, typing","hanlp.utils.io_util.get_resource, hanlp_common.io.load_json, typing.List","_init, convert_char, load, normalize_chars, normalize_text","CharTable, JsonCharTable","HANLP_CHAR_TABLE_JSON, HANLP_CHAR_TABLE_TXT, b, cells, convert, line, mapper, src",,", 
, https://file.hankcs.com/corpus/char_table.json.zip, https://file.hankcs.com/corpus/char_table.zip#CharTable.txt, utf-8",3,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-09 19:07, #CharTable.txt'",
https://github.com/hankcs/HanLP,localization.py,0,0.0,4,1.5,173,65.04,0,0.0,3,1.13,86,32.33,0,0,4,0,,"dep, ner, pos, task",,,,"dep, ner, pos, task",,"AD, AS, BA, CC, CD, CS, DEC, DEG, DER, DEV, DT, ETC, FW, IJ, JJ, LB, LC, M, MSP, NN, NOI, NR, NS, NT, OD, ON, P, PN, PU, SB, SP, URL, VA, VC, VE, VV, advmod, amod, asp, assm, assmod, attr, ba, cc, ccomp, clf, comod, conj, cop, cpm, dep, det, dobj, dvpm, dvpmod, elf, etc, lccomp, lobj, loc, mmod, neg, ner, nn, nsubj, nsubjpass, nummod, ordmod, pass, pccomp, plmod, pobj, pos, prep, prnmod, prtmod, punct, range, rcmod, rcomp, root, srl, tmod, token, top, vmod, xsubj, 专有名词, 主题, 人名, 介词, 介词性修饰语, 介词性地点修饰, 介词性宾语, 介词补语, 从句补充, 从属连词, 代词, 位置补语, 依存句法树, 依赖关系, 关联修饰, 关联标记, 其他动词, 其他名词, 其他名词修饰语, 其他小品词, 副词, 动态助词, 动词修饰, 动词有无, 单词, 句末助词, 句首感叹词, 名词性主语, 名词性状语, 否定修饰, 命名实体, 噪声, 地名, 地字修饰, 地字动词短语, 复合名词修饰, 外来语, 小品词, 属性, 属格“的”, 并列关系, 并列联合动词, 并列连接词, 序数词, 形容词修饰语, 情态动词, 把字关系, 把字句, 控制主语, 插入词修饰, 数词修饰语, 数量词间接宾语, 方位词, 时态标记, 时间介词, 时间修饰语, 时间名词, 机构团体, 标点符号, 核心关系, 概数词, 直接宾语, 相关关系, 省略关系, 短句式表被动, 类别修饰, 系动词, 结果补语, 网址, 补语, 补语成分“的”, 表方式的“地”, 表示省略, 表结果的“得”, 表语形容词, 被动名词主语, 被动标记, 词性, 语义角色, 象声词, 连接性状语, 量词, 量词修饰, 长句式表被动, 限定词, 限定语",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-05 02:09","专有名词, 主题, 人名, 介词, 介词性修饰语, 介词性地点修饰, 介词性宾语, 介词补语, 从句补充, 从属连词, 代词, 位置补语, 依存句法树, 依赖关系, 关联修饰, 关联标记, 其他动词, 其他名词, 其他名词修饰语, 其他小品词, 副词, 动态助词, 动词修饰, 动词有无, 单词, 句末助词, 句首感叹词, 名词性主语, 名词性状语, 否定修饰, 命名实体, 噪声, 地名, 地字修饰, 地字动词短语, 复合名词修饰, 外来语, 小品词, 属性, 属格“的”, 并列关系, 并列联合动词, 并列连接词, 序数词, 形容词修饰语, 情态动词, 把字关系, 把字句, 控制主语, 插入词修饰, 数词修饰语, 数量词间接宾语, 方位词, 时态标记, 时间介词, 时间修饰语, 时间名词, 机构团体, 标点符号, 核心关系, 概数词, 直接宾语, 相关关系, 省略关系, 短句式表被动, 类别修饰, 系动词, 结果补语, 网址, 补语, 补语成分“的”, 表方式的“地”, 表示省略, 表结果的“得”, 表语形容词, 被动名词主语, 被动标记, 词性, 语义角色, 象声词, 连接性状语, 量词, 量词修饰, 长句式表被动, 限定词, 限定语"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-09 18:47",
https://github.com/hankcs/HanLP,optimization.py,0,0.0,75,58.59,19,14.84,12,9.38,22,17.19,0,0.0,12,2,17,7,,"AdamTF, AdamWeightDecay, WarmUp, __call__, __future__, __init__, _decay_weights_op, _decayed_lr_t, _do_use_weight_decay, _exclude_from_weight_decay, _fallback_apply_state, _get_lr, _include_in_weight_decay, _prepare_local, _resource_apply_dense, _resource_apply_sparse, _use_locking, amsgrad, apply_gradients, apply_state, assign_sub, base_dtype, beta_1, beta_2, classmethod, cls, coefficients, config, create_optimizer, custom_objects, decay, decay_schedule_fn, device, dict, do_decay, dtype, epsilon, exclude_from_weight_decay, from_config, get, get_config, global_step_float, grad, grads, grads_and_vars, include_in_weight_decay, indices, init_lr, initial_learning_rate, kwargs, learning_rate, learning_rate_fn, list, lr_t, name, num_train_steps, num_warmup_steps, param_name, power, re, self, step, super, tensorflow, tvars, update, var, var_device, var_dtype, warmup_learning_rate, warmup_percent_done, warmup_steps, warmup_steps_float, weight_decay_rate, zip","__future__.absolute_import, __future__.division, __future__.print_function, re.search, tf.cast, tf.clip_by_global_norm, tf.cond, tf.constant, tf.control_dependencies, tf.float32, tf.keras.optimizers.Adam, tf.keras.optimizers.legacy.Adam, tf.keras.optimizers.schedules.LearningRateSchedule, tf.keras.optimizers.schedules.PolynomialDecay, tf.math.pow, tf.name_scope, tf.no_op","__call__, __init__, _decay_weights_op, _do_use_weight_decay, _get_lr, _prepare_local, _resource_apply_dense, _resource_apply_sparse, apply_gradients, create_optimizer, from_config, get_config","AdamWeightDecay, WarmUp","AdamTF, apply_state, coefficients, config, custom_objects, decay, do_decay, global_step_float, grads, kwargs, learning_rate_fn, lr_t, optimizer, tvars, warmup_learning_rate, warmup_percent_done, warmup_steps_float","Adam enables L2 weight decay and clip_by_global_norm on gradients.

  Just adding the square of the weights to the loss function is *not* the
  correct way of using L2 regularization/weight decay with Adam, since that will
  interact with the m and v parameters in strange ways.

  Instead we want to decay the weights in a manner that doesn't interact with
  the m/v parameters. This is equivalent to adding the square of the weights to
  the loss with plain (non-momentum) SGD.

Args:

Returns:, Applys a warmup schedule on a given learning rate decay schedule., Creates an optimizer from its config with WarmUp custom object.

Args:
  config:

Returns:, Creates an optimizer with learning rate schedule.

Args:
  init_lr: 
  num_train_steps: 
  num_warmup_steps: 

Returns:, Functions and classes related to optimization (weight updates)., Retrieves the learning rate with the given state.

Args:
  var_device:
  var_dtype:
  apply_state:

Returns:, Whether to use L2 weight decay for `param_name`.

Args:
  param_name:

Returns:","Adam enables L2 weight decay and clip_by_global_norm on gradients.
    
      Just adding the square of the weights to the loss function is *not* the
      correct way of using L2 regularization/weight decay with Adam, since that will
      interact with the m and v parameters in strange ways.
    
      Instead we want to decay the weights in a manner that doesn't interact with
      the m/v parameters. This is equivalent to adding the square of the weights to
      the loss with plain (non-momentum) SGD.

    Args:

    Returns:

    , AdamWeightDecay, Applys a warmup schedule on a given learning rate decay schedule., Creates an optimizer from its config with WarmUp custom object.

        Args:
          config:

        Returns:

        , Creates an optimizer with learning rate schedule.

    Args:
      init_lr: 
      num_train_steps: 
      num_warmup_steps: 

    Returns:

    , Functions and classes related to optimization (weight updates)., Retrieves the learning rate with the given state.

        Args:
          var_device:
          var_dtype:
          apply_state:

        Returns:

        , WarmUp, Whether to use L2 weight decay for `param_name`.

        Args:
          param_name:

        Returns:

        , adam_weight_decay_rate, bias, decay_schedule_fn, initial_learning_rate, layer_norm, lr_t, name, power, warmup_steps, weight_decay_rate","0, 0.0, 0.001, 0.01, 0.9, 0.999, 1.0, 1e-06, 1e-07, False, None, True","#, #     http://www.apache.org/licenses/LICENSE-2.0, # ==============================================================================, # Copyright 2019 The TensorFlow Authors. All Rights Reserved., # Implements linear decay of the learning rate., # Implements polynomial warmup. i.e., if global_step < warmup_steps, the, # Licensed under the Apache License, Version 2.0 (the ""License"");, # See the License for the specific language governing permissions and, # Unless required by applicable law or agreed to in writing, software, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # avoid slowdown when using v2.11+ Keras optimizers on M1/M2 Macs, # distributed under the License is distributed on an ""AS IS"" BASIS,, # learning rate will be `global_step/num_warmup_steps * init_lr`., # limitations under the License., # you may not use this file except in compliance with the License., Adam enables L2 weight decay and clip_by_global_norm on gradients.

Just adding the square of the weights to the loss function is *not* the
correct way of using L2 regularization/weight decay with Adam, since that will
interact with the m and v parameters in strange ways.

Instead we want to decay the weights in a manner that doesn't interact with
the m/v parameters. This is equivalent to adding the square of the weights to
the loss with plain (non-momentum) SGD.

Args:

Returns:

, Creates an optimizer from its config with WarmUp custom object.

Args:
config:

Returns:

, Creates an optimizer with learning rate schedule.

Args:
init_lr:
num_train_steps:
num_warmup_steps:

Returns:

, Functions and classes related to optimization (weight updates).

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import re

import tensorflow as tf


class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):
Applys a warmup schedule on a given learning rate decay schedule., Retrieves the learning rate with the given state.

Args:
var_device:
var_dtype:
apply_state:

Returns:

, Whether to use L2 weight decay for `param_name`.

Args:
param_name:

Returns:

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,11,22.0,4,8.0,6,12.0,29,58.0,0,0.0,1,0,3,1,,"additional_args, clipnorm, create_optimizer, epsilon, hanlp, init_lr, learning_rate_fn, num_train_steps, num_warmup_steps, tensorflow, weight_decay_rate","hanlp.optimizers.adamw.optimization.AdamWeightDecay, hanlp.optimizers.adamw.optimization.WarmUp, tf.keras.optimizers.schedules.PolynomialDecay",create_optimizer,,"additional_args, learning_rate_fn, optimizer","Creates an optimizer with learning rate schedule.

Args:
  init_lr: 
  num_train_steps: 
  num_warmup_steps: 
  weight_decay_rate:  (Default value = 0.01)
  epsilon:  (Default value = 1e-6)
  clipnorm:  (Default value = None)

Returns:","Creates an optimizer with learning rate schedule.

    Args:
      init_lr: 
      num_train_steps: 
      num_warmup_steps: 
      weight_decay_rate:  (Default value = 0.01)
      epsilon:  (Default value = 1e-6)
      clipnorm:  (Default value = None)

    Returns:

    , LayerNorm, bias, clipnorm","0.0, 0.01, 0.9, 0.999, 1e-06, None","#, #                                   decay_schedule_fn=learning_rate_fn,, #                                   warmup_steps=num_warmup_steps), #         beta_1=0.9,, #         beta_2=0.999,, #         decay_steps=num_train_steps,, #         end_learning_rate=0.0), #         epsilon=1e-6,, #         exclude_from_weight_decay=['layer_norm', 'bias']), #         initial_learning_rate=init_lr,, #         learning_rate=learning_rate_fn,, #         learning_rate_fn = WarmUp(initial_learning_rate=init_lr,, #         weight_decay_rate=0.01,, #     """"""Creates an optimizer with learning rate schedule."""""", #     # Implements linear decay of the learning rate., #     if num_warmup_steps:, #     learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(, #     optimizer = AdamW(, #     return optimizer, #     wd_dict = get_weight_decays(model), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-11 18:44, # Implements linear decay of the learning rate., # def create_optimizer(model, init_lr, num_train_steps, num_warmup_steps):, # from hanlp.optimization.adamw.optimizers_v2 import AdamW, # from hanlp.optimization.adamw.utils import get_weight_decays, # {'LayerNorm/gamma:0', 'LayerNorm/beta:0'}, Creates an optimizer with learning rate schedule.

Args:
init_lr:
num_train_steps:
num_warmup_steps:
weight_decay_rate:  (Default value = 0.01)
epsilon:  (Default value = 1e-6)
clipnorm:  (Default value = None)

Returns:

",
https://github.com/hankcs/HanLP,smatch_eval.py,0,0.0,36,41.86,26,30.23,3,3.49,21,24.42,0,0.0,6,1,16,0,,"SmatchScores, ValueError, _FAST_SMATCH_SCRIPT, _SMATCH_SCRIPT, amr_version, append, float, format_fast_scores, format_official_scores, get_amr_utils, gold, hanlp, home, len, line, os, post_process, pred, property, score, scores, script, self, smatch_eval, split, stog_home, str, strip, text, typing, use_fast, util_dir, utils_tar_gz, vs, warnings, x","hanlp.metrics.f1.F1_, hanlp.metrics.mtl.MetricDict, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.pushd, hanlp.utils.io_util.run_cmd, hanlp.utils.log_util.flash, os.path.dirname, os.path.realpath, typing.Union, warnings.warn","format_fast_scores, format_official_scores, get_amr_utils, post_process, score, smatch_eval",SmatchScores,"_FAST_SMATCH_SCRIPT, _SMATCH_SCRIPT, cmd, gold, home, line, pred, score, scores, script, stog_home, text, util_dir, utils_tar_gz, vs, x",,", 
,  ,  --util_dir ,  --v 2,  -> , , , .post, 1.0, 2.0, 3.0, :, : , Failed to parse results from smatch: , Running evaluation script [blink][yellow]...[/yellow][/blink], Smatch, Unsupported AMR version , bash , https://file.hankcs.com/research/amr2020/amr_3.0_utils.tgz, https://github.com/ChunchuanLv/amr-evaluation-tool-enhanced/archive/master.zip#evaluation.sh, https://github.com/jcyk/AMR-gs/archive/master.zip, https://github.com/jcyk/AMR-gs/archive/master.zip#tools/fast_smatch/compute_smatch.sh, https://www.cs.jhu.edu/~s.zhang/data/AMR/amr_1.0_utils.tar.gz, https://www.cs.jhu.edu/~s.zhang/data/AMR/amr_2.0_utils.tar.gz, nan, python3 -u -m stog.data.dataset_readers.amr_parsing.postprocess.postprocess --amr_path ","1, 3, False","# -*- coding:utf-8 -*-, # Author: hankcs, # Concepts -> P: 0.075, R: 0.036, F: 0.049, # Date: 2020-08-24 12:47, # Document F-score: 0.121, # Frames -> P: 0.007, R: 0.007, F: 0.007, # IgnoreVars -> P: 0.005, R: 0.003, F: 0.003, # Named Ent. -> P: 0.222, R: 0.092, F: 0.130, # Negations -> P: 0.000, R: 0.000, F: 0.000, # No WSD -> P: 0.137, R: 0.108, F: 0.120, # Non_sense_frames -> P: 0.008, R: 0.008, F: 0.008, # Precision: 0.137, # Recall: 0.108, # Reentrancies -> P: 0.113, R: 0.060, F: 0.079, # SRL -> P: 0.145, R: 0.104, F: 0.121, # Smatch -> P: 0.136, R: 0.107, F: 0.120, # Unlabeled -> P: 0.229, R: 0.180, F: 0.202, # Wikification -> P: 0.000, R: 0.000, F: 0.000, # using fast smatch, #evaluation.sh', #tools/fast_smatch/compute_smatch.sh'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-24 12:47",
https://github.com/hankcs/HanLP,binary_chunking_f1.py,0,0.0,30,81.08,0,0.0,4,10.81,3,8.11,0,0.0,3,1,9,0,,"BinaryChunkingF1, __call__, batch, batch_pred, batch_pred_spans, collections, decode_spans, gold, gold_tags, hanlp, int, isinstance, items, lens, list, nonzero, offset, offsets, pred, pred_tags, self, set, size, staticmethod, super, tolist, torch, typing, update, zip","collections.defaultdict, hanlp.metrics.f1.F1, torch.LongTensor, torch.Tensor, typing.List, typing.Union","__call__, decode_spans, update",BinaryChunkingF1,"batch, batch_pred, batch_pred_spans, gold, l, lens, offset, offsets, pred",,,"0, 1, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-02 14:27",
https://github.com/hankcs/HanLP,bmes_tf.py,0,0.0,26,74.29,1,2.86,5,14.29,3,8.57,0,0.0,5,1,8,0,,"BMES_F1_TF, __init__, dtype, from_logits, hanlp, kwargs, len, name, nb_correct, nb_pred, nb_true, pred_entities, pred_tags, reset_states, result, score, self, set, suffix, super, tag_vocab, true_entities, true_tags, update_entities, update_tags, zip","hanlp.common.vocab_tf.VocabTF, hanlp.metrics.chunking.chunking_f1_tf.ChunkingF1_TF, hanlp.metrics.chunking.sequence_labeling.get_entities","__init__, reset_states, result, update_entities, update_tags",BMES_F1_TF,"nb_correct, nb_pred, nb_true, p, pred_entities, r, score, true_entities",,f1,"0, 2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-09-14 21:55",
https://github.com/hankcs/HanLP,chunking_f1.py,0,0.0,67,75.28,9,10.11,5,5.62,8,8.99,0,0.0,11,2,17,0,,"ChunkingF1, DetailedSpanF1, __call__, __init__, __str__, append, by_type, clear, close, collections, confusion_matrix, correct_chunk, correct_unlabeled, do_confusion_matrix, fscore, getvalue, gold, gold_chunks_unlabeled, gold_labels, gold_spans, gold_tags, group, group_by_span, group_by_tag, hanlp, int, io, items, keys, labels, len, nb_correct, nb_pred, nb_true, num_tokens, out, overall, prec, pred, pred_chunks_unlabeled, pred_labels, pred_spans, pred_tags, property, rec, report, reset, reset_state, score, self, set, sklearn, sorted, span, state, states, str, t_correct_chunk, t_total_gold, t_total_pred, text, token_counter, total_gold, total_pred, typing, write, zip","collections.defaultdict, hanlp.metrics.chunking.conlleval.DetailedF1, hanlp.metrics.chunking.conlleval.calculate_metrics, hanlp.metrics.chunking.conlleval.metrics, hanlp.metrics.chunking.sequence_labeling.get_entities, hanlp.metrics.f1.F1, hanlp.metrics.metric.Metric, io.StringIO, sklearn.metrics.confusion_matrix, typing.Dict, typing.List, typing.Set, typing.Tuple","__call__, __init__, __str__, confusion_matrix, group_by_span, group_by_tag, report, reset, reset_state, score, states","ChunkingF1, DetailedSpanF1","by_type, c, gold, gold_chunks_unlabeled, gold_spans, gold_tags, group, labels, out, overall, pred, pred_chunks_unlabeled, pred_spans, pred_tags, span, state, text",,"%17s: , FB1: %6.2f
, FB1: %6.2f  %d
, found: %d phrases; correct: %d.
, labeled overall, precision: %6.2f%%; , processed %d tokens with %d phrases; , recall: %6.2f%%; , unlabeled overall","0, 1, 100.0, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-11 22:14, # counts by type, # number of chunks in corpus, # number of correctly identified chunks, # number of identified chunks, # token counter (ignores sentence breaks)",
https://github.com/hankcs/HanLP,chunking_f1_tf.py,0,0.0,35,76.09,3,6.52,3,6.52,5,10.87,0,0.0,7,1,9,0,,"ChunkingF1_TF, __call__, __init__, _keras_mask, abc, append, batch, dtype, from_logits, get_token, hanlp, hasattr, int, kwargs, mask, name, numpy, pad_idx, result, sample_weight, self, sent, super, tag, tag_vocab, tags, tensorflow, to_tags, update_state, update_tags, update_the_state, y, y_pred, y_true, zip","abc.ABC, abc.abstractmethod, hanlp.common.vocab_tf.VocabTF, tf.Tensor, tf.argmax, tf.keras.metrics.Metric","__call__, __init__, result, to_tags, update_state, update_tags, update_the_state",ChunkingF1_TF,"batch, mask, sample_weight, sent, tag, tags, y, y_pred, y_true",,"ChunkingF1 requires masking, check your _keras_mask or compute_mask, _keras_mask, f1","1, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 23:09, # If model predicts <pad>, it will fail most metrics. So replace it with a valid one, # in this case, the model doesn't compute mask but provide a masking index, it's ok to",
https://github.com/hankcs/HanLP,conlleval.py,0,0.0,120,58.54,47,22.93,10,4.88,28,13.66,0,0.0,22,3,44,2,,"ANY_SPACE, DetailedF1, EvalCounts, Exception, FormatError, SpanF1, ValueError, __call__, __init__, __name__, __repr__, add, add_argument, argparse, args, argv, batch_update_state, by_type, calc_metrics, calculate_metrics, chunkStart, chunk_tag, clear, close, collections, correct, correct_chunk, correct_tags, correct_type, count, counts, end_correct, end_guessed, end_of_chunk, evaluate_iob2, evaluate_iobes, fb1, file, fn, fp, fscore, full, get, getvalue, gold, guessed, guessed_type, hanlp, in_correct, int, io, items, iterable, k, keys, label_encoding, last_correct, last_correct_type, last_guessed, last_guessed_type, len, list, main, metrics, object, open, out, overall, parse_args, parser, percent, prec, precision, pred, pred_seqs, pred_tag, prev_tag, prev_type, print, property, rec, recall, report, reset, reset_state, result, score, seen, self, set, sorted, split, split_tag, start_correct, start_guessed, start_of_chunk, state, states, str, super, sys, t_correct_chunk, t_total_gold, t_total_pred, text, token_counter, total, total_gold, total_pred, tp, true_seqs, true_tag, type_, typing, uniq, update_state, v, verbose, write, zip","argparse.ArgumentDefaultsHelpFormatter, argparse.ArgumentParser, collections.defaultdict, collections.namedtuple, hanlp.metrics.metric.Metric, hanlp.utils.span_util.bio_tags_to_spans, io.StringIO, sys.argv, sys.exit, sys.stdin, sys.stdout, typing.List, typing.Tuple, typing.Union","__call__, __init__, __repr__, batch_update_state, calc_metrics, calculate_metrics, end_of_chunk, evaluate_iob2, evaluate_iobes, main, metrics, parse_args, report, reset, reset_state, result, score, split_tag, start_of_chunk, states, uniq, update_state","EvalCounts, FormatError, SpanF1","ANY_SPACE, DetailedF1, arg, args, by_type, c, chunkStart, correct, correct_type, count, counts, end_correct, end_guessed, f, fb1, fn, fp, gold, guessed, guessed_type, in_correct, k, last_correct, last_correct_type, last_guessed, last_guessed_type, out, overall, p, parser, precision, pred, pred_tag, r, recall, result, seen, start_correct, start_guessed, state, text, tp, true_tag, v","compute overall precision, recall and FB1 (default values are 0.0)
if percent is True, return 100 * original decimal value

Args:
  tp: 
  p: 
  t: 
  percent:  (Default value = True)

Returns:, split chunk tag into IOBES prefix and chunk_type
e.g.
B-PER -> (B, PER)
O -> (O, None)

Args:
  chunk_tag: 

Returns:",",  F: ,  R: , %17s: , -, --boundary, --delimiter, --otag, -X-, -b, -d, -o, ., .2%, <SPACE>, ?, B, BIO, CHAR, E, FB1: %6.2f
, FB1: %6.2f  %d
, I, IOB2, IOBES, Metrics, O, P: , STR, Unrecognized label encoding , [, ], __main__, accuracy: %6.2f%%; , alternative outside tag, character delimiting items in input, compute overall precision, recall and FB1 (default values are 0.0)
    if percent is True, return 100 * original decimal value

    Args:
      tp: 
      p: 
      t: 
      percent:  (Default value = True)

    Returns:

    , evaluate tagging results using CoNLL criteria, file, found: %d phrases; correct: %d.
, precision: %6.2f%%; , processed %d tokens with %d phrases; , recall: %6.2f%%; , sentence boundary, split chunk tag into IOBES prefix and chunk_type
    e.g.
    B-PER -> (B, PER)
    O -> (O, None)

    Args:
      chunk_tag: 

    Returns:

    , tp fp fn prec rec fscore, utf-8","0, 0.0, 1, 1.0, 100, 100.0, 2, False, None, True","# - LaTeX output (-l argument) not supported, # - accept any space as delimiter by default, # - option to set boundary (-b argument), # - optional file argument (default STDIN), # - raw tags (-r argument) not supported, # Intentional differences:, # Python version of the evaluation script from CoNLL'00-, # arguments: previous and current chunk tags, previous and current types, # check if a chunk ended between the previous and current word, # check if a chunk started between the previous and current word, # corrected 1998-12-22: these chunks are assumed to have length 1, # counts by type, # currently processed chunks is correct until now, # number of chunks in corpus, # number of correct chunk tags, # number of correctly identified chunks, # number of identified chunks, # previous chunk tag in corpus, # previously identified chunk tag, # print(""startOfChunk?"", prevTag, tag, prevType, type), # print(chunkStart), # token counter (ignores sentence breaks), # torch convention: put pred before gold, # type of previous chunk tag in corpus, # type of previously identified chunk tag, #!/usr/bin/env python, compute overall precision, recall and FB1 (default values are 0.0)
if percent is True, return 100 * original decimal value

Args:
tp:
p:
t:
percent:  (Default value = True)

Returns:

, split chunk tag into IOBES prefix and chunk_type
e.g.
B-PER -> (B, PER)
O -> (O, None)

Args:
chunk_tag:

Returns:

",
https://github.com/hankcs/HanLP,iobes_tf.py,0,0.0,21,67.74,1,3.23,3,9.68,6,19.35,0,0.0,4,1,2,0,,"IOBES_F1_TF, __init__, dtype, from_logits, gold, hanlp, kwargs, name, pred, pred_tags, reset_state, reset_states, result, self, state, super, tag_vocab, true_tags, update_state, update_tags, zip","hanlp.common.vocab_tf.VocabTF, hanlp.metrics.chunking.chunking_f1_tf.ChunkingF1_TF, hanlp.metrics.chunking.conlleval.SpanF1","__init__, reset_states, result, update_tags",IOBES_F1_TF,"gold, pred",,f1,"False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-09-14 21:55, # pred_tags = list(itertools.chain.from_iterable(pred_tags)), # self.state.update_state(true_tags, pred_tags), # true_tags = list(itertools.chain.from_iterable(true_tags))",
https://github.com/hankcs/HanLP,sequence_labeling.py,0,0.0,74,50.68,35,23.97,6,4.11,31,21.23,0,0.0,10,0,44,10,,"accuracy_score, all, any, append, begin_offset, chunk, chunk_end, chunk_start, chunks, classification_report, collections, d1, d2, delimiter, digits, end, end_of_chunk, entities, enumerate, f1, f1_score, f1s, format, get_entities, head_fmt, headers, iobes_to_span, isinstance, item, items, join, last_line_heading, len, list, max, name_width, nb_correct, nb_pred, nb_true, numpy, p, performace_dict, performance_measure, precision_score, pred_entities, prev_tag, prev_type, ps, r, recall_score, report, row_fmt, rs, score, seq, set, start, start_of_chunk, sublist, suffix, sum, tag, tags, true_entities, type_, type_name, w, width, words, y_p, y_pred, y_t, y_true, zip","collections.defaultdict, np.average, np.sum","accuracy_score, classification_report, end_of_chunk, f1_score, get_entities, iobes_to_span, performance_measure, precision_score, recall_score, start_of_chunk",,"begin_offset, chunk, chunk_end, chunk_start, chunks, d1, d2, delimiter, end, entities, f1, f1s, head_fmt, headers, item, last_line_heading, name_width, nb_correct, nb_pred, nb_true, p, performace_dict, pred_entities, prev_tag, prev_type, ps, r, report, row_fmt, rs, score, seq, start, sublist, tag, true_entities, type_, type_name, w, width, y_p, y_pred, y_t, y_true","Accuracy classification score.

In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must *exactly* match the
corresponding set of labels in y_true.

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a tagger.

Returns:
  score: float.
  Example:

>>> from seqeval.metrics import accuracy_score
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> accuracy_score(y_true, y_pred)
    0.80, Build a text report showing the main classification metrics.

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a classifier.
  digits: int. Number of digits for formatting output floating point values. (Default value = 2)
  suffix:  (Default value = False)

Returns:
  report: string. Text summary of the precision, recall, F1 score for each class.
  Examples:

>>> from seqeval.metrics import classification_report
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> print(classification_report(y_true, y_pred))
                 precision    recall  f1-score   support
    <BLANKLINE>
           MISC       0.00      0.00      0.00         1
            PER       1.00      1.00      1.00         1
    <BLANKLINE>
      micro avg       0.50      0.50      0.50         2
      macro avg       0.50      0.50      0.50         2
    <BLANKLINE>, Checks if a chunk ended between the previous and current word.

Args:
  prev_tag: previous chunk tag.
  tag: current chunk tag.
  prev_type: previous type.
  type_: current type.

Returns:
  chunk_end: boolean., Checks if a chunk started between the previous and current word.

Args:
  prev_tag: previous chunk tag.
  tag: current chunk tag.
  prev_type: previous type.
  type_: current type.

Returns:
  chunk_start: boolean., Compute the F1 score.

The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is::

    F1 = 2 * (precision * recall) / (precision + recall)

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a tagger.
  average:  (Default value = 'micro')
  suffix:  (Default value = False)

Returns:
  score: float.
  Example:

>>> from seqeval.metrics import f1_score
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> f1_score(y_true, y_pred)
    0.50, Compute the performance metrics: TP, FP, FN, TN

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a tagger.

Returns:
  performance_dict: dict
  Example:

>>> from seqeval.metrics import performance_measure
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> performance_measure(y_true, y_pred)
    (3, 3, 1, 4), Compute the precision.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample.

The best value is 1 and the worst value is 0.

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a tagger.
  average:  (Default value = 'micro')
  suffix:  (Default value = False)

Returns:
  score: float.
  Example:

>>> from seqeval.metrics import precision_score
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> precision_score(y_true, y_pred)
    0.50, Compute the recall.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.

Args:
  y_true: 2d array. Ground truth (correct) target values.
  y_pred: 2d array. Estimated targets as returned by a tagger.
  average:  (Default value = 'micro')
  suffix:  (Default value = False)

Returns:
  score: float.
  Example:

>>> from seqeval.metrics import recall_score
    >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
    >>> recall_score(y_true, y_pred)
    0.50, Gets entities from sequence.

Args:
  seq(list): sequence of labels.
  suffix:  (Default value = False)

Returns:
  list: list of (chunk_type, chunk_start, chunk_end).
  Example:

>>> from seqeval.metrics.sequence_labeling import get_entities
    >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
    >>> get_entities(seq)
    [('PER', 0, 2), ('LOC', 3, 4)], Metrics to assess performance on sequence labeling task given prediction
Functions named as ``*_score`` return a scalar value to maximize: the higher
the better",", 
, 

,  ,  {:>9.{digits}f},  {:>9},  {:>9}
, ., Accuracy classification score.
    
    In multilabel classification, this function computes subset accuracy:
    the set of labels predicted for a sample must *exactly* match the
    corresponding set of labels in y_true.

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.

    Returns:
      score: float.
      Example:

    >>> from seqeval.metrics import accuracy_score
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> accuracy_score(y_true, y_pred)
        0.80
    , B, Build a text report showing the main classification metrics.

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a classifier.
      digits: int. Number of digits for formatting output floating point values. (Default value = 2)
      suffix:  (Default value = False)

    Returns:
      report: string. Text summary of the precision, recall, F1 score for each class.
      Examples:

    >>> from seqeval.metrics import classification_report
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> print(classification_report(y_true, y_pred))
                     precision    recall  f1-score   support
        <BLANKLINE>
               MISC       0.00      0.00      0.00         1
                PER       1.00      1.00      1.00         1
        <BLANKLINE>
          micro avg       0.50      0.50      0.50         2
          macro avg       0.50      0.50      0.50         2
        <BLANKLINE>
    , Checks if a chunk ended between the previous and current word.

    Args:
      prev_tag: previous chunk tag.
      tag: current chunk tag.
      prev_type: previous type.
      type_: current type.

    Returns:
      chunk_end: boolean.

    , Checks if a chunk started between the previous and current word.

    Args:
      prev_tag: previous chunk tag.
      tag: current chunk tag.
      prev_type: previous type.
      type_: current type.

    Returns:
      chunk_start: boolean.

    , Compute the F1 score.
    
    The F1 score can be interpreted as a weighted average of the precision and
    recall, where an F1 score reaches its best value at 1 and worst score at 0.
    The relative contribution of precision and recall to the F1 score are
    equal. The formula for the F1 score is::
    
        F1 = 2 * (precision * recall) / (precision + recall)

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.
      average:  (Default value = 'micro')
      suffix:  (Default value = False)

    Returns:
      score: float.
      Example:

    >>> from seqeval.metrics import f1_score
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> f1_score(y_true, y_pred)
        0.50
    , Compute the performance metrics: TP, FP, FN, TN

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.

    Returns:
      performance_dict: dict
      Example:

    >>> from seqeval.metrics import performance_measure
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> performance_measure(y_true, y_pred)
        (3, 3, 1, 4)
    , Compute the precision.
    
    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
    true positives and ``fp`` the number of false positives. The precision is
    intuitively the ability of the classifier not to label as positive a sample.
    
    The best value is 1 and the worst value is 0.

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.
      average:  (Default value = 'micro')
      suffix:  (Default value = False)

    Returns:
      score: float.
      Example:

    >>> from seqeval.metrics import precision_score
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> precision_score(y_true, y_pred)
        0.50
    , Compute the recall.
    
    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
    true positives and ``fn`` the number of false negatives. The recall is
    intuitively the ability of the classifier to find all the positive samples.
    
    The best value is 1 and the worst value is 0.

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.
      average:  (Default value = 'micro')
      suffix:  (Default value = False)

    Returns:
      score: float.
      Example:

    >>> from seqeval.metrics import recall_score
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> recall_score(y_true, y_pred)
        0.50
    , E, FN, FP, Gets entities from sequence.

    Args:
      seq(list): sequence of labels.
      suffix:  (Default value = False)

    Returns:
      list: list of (chunk_type, chunk_start, chunk_end).
      Example:

    >>> from seqeval.metrics.sequence_labeling import get_entities
        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
        >>> get_entities(seq)
        [('PER', 0, 2), ('LOC', 3, 4)]
    , I, Metrics to assess performance on sequence labeling task given prediction
Functions named as ``*_score`` return a scalar value to maximize: the higher
the better
, O, S, TN, TP, f1-score, macro avg, micro, micro avg, precision, recall, support, {:>{width}s} ","0, 1, 2, 3, False, True","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2018 chakki, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # compute averages, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # for nested list, # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # might be Chinese, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell, Accuracy classification score.

In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must *exactly* match the
corresponding set of labels in y_true.

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a tagger.

Returns:
score: float.
Example:

>>> from seqeval.metrics import accuracy_score
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> accuracy_score(y_true, y_pred)
0.80
, Build a text report showing the main classification metrics.

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a classifier.
digits: int. Number of digits for formatting output floating point values. (Default value = 2)
suffix:  (Default value = False)

Returns:
report: string. Text summary of the precision, recall, F1 score for each class.
Examples:

>>> from seqeval.metrics import classification_report
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> print(classification_report(y_true, y_pred))
precision    recall  f1-score   support
<BLANKLINE>
MISC       0.00      0.00      0.00         1
PER       1.00      1.00      1.00         1
<BLANKLINE>
micro avg       0.50      0.50      0.50         2
macro avg       0.50      0.50      0.50         2
<BLANKLINE>
, Checks if a chunk ended between the previous and current word.

Args:
prev_tag: previous chunk tag.
tag: current chunk tag.
prev_type: previous type.
type_: current type.

Returns:
chunk_end: boolean.

, Checks if a chunk started between the previous and current word.

Args:
prev_tag: previous chunk tag.
tag: current chunk tag.
prev_type: previous type.
type_: current type.

Returns:
chunk_start: boolean.

, Compute the F1 score.

The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is::

F1 = 2 * (precision * recall) / (precision + recall)

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a tagger.
average:  (Default value = 'micro')
suffix:  (Default value = False)

Returns:
score: float.
Example:

>>> from seqeval.metrics import f1_score
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> f1_score(y_true, y_pred)
0.50
, Compute the performance metrics: TP, FP, FN, TN

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a tagger.

Returns:
performance_dict: dict
Example:

>>> from seqeval.metrics import performance_measure
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]
>>> performance_measure(y_true, y_pred)
(3, 3, 1, 4)
, Compute the precision.

The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
true positives and ``fp`` the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample.

The best value is 1 and the worst value is 0.

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a tagger.
average:  (Default value = 'micro')
suffix:  (Default value = False)

Returns:
score: float.
Example:

>>> from seqeval.metrics import precision_score
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> precision_score(y_true, y_pred)
0.50
, Compute the recall.

The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
true positives and ``fn`` the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.

Args:
y_true: 2d array. Ground truth (correct) target values.
y_pred: 2d array. Estimated targets as returned by a tagger.
average:  (Default value = 'micro')
suffix:  (Default value = False)

Returns:
score: float.
Example:

>>> from seqeval.metrics import recall_score
>>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> recall_score(y_true, y_pred)
0.50
, Gets entities from sequence.

Args:
seq(list): sequence of labels.
suffix:  (Default value = False)

Returns:
list: list of (chunk_type, chunk_start, chunk_end).
Example:

>>> from seqeval.metrics.sequence_labeling import get_entities
>>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
>>> get_entities(seq)
[('PER', 0, 2), ('LOC', 3, 4)]
, Metrics to assess performance on sequence labeling task given prediction
Functions named as ``*_score`` return a scalar value to maximize: the higher
the better
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 03:49",
https://github.com/hankcs/HanLP,attachmentscore.py,0,0.0,31,56.36,3,5.45,2,3.64,19,34.55,0,0.0,11,1,2,0,,"AttachmentScore, __call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, arc_golds, arc_mask, arc_preds, correct_arcs, correct_rels, eps, eq, hanlp, las, len, mask, other, property, rel_golds, rel_mask, rel_preds, reset, score, self, sum, super, total, uas",hanlp.metrics.metric.Metric,"__call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, las, reset, score, uas",AttachmentScore,"arc_mask, rel_mask",," LAS: , .2%, UAS: ","0.0, 1e-12","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # noinspection PyMethodOverriding, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,conllx_eval.py,0,0.0,30,46.15,19,29.23,8,12.31,8,12.31,0,0.0,2,0,12,2,,"CONLLX_EVAL, RuntimeError, ValueError, copied_pred_file, copy_cols, endswith, enumerate, evaluate, fixed_gold_file, fixed_pred_file, gold_file, hanlp, idx, int, keep_comments, las, lines, name, next, open, pred_file, split, startswith, str, strip, tempfile, to_out, uas, write, zip","hanlp.utils.io_util.get_exitcode_stdout_stderr, hanlp.utils.io_util.get_resource, tempfile.NamedTemporaryFile","copy_cols, evaluate",,"CONLLX_EVAL, fixed_gold_file, fixed_pred_file, g, gold_file, idx, las, lines, p, pred_file, to_out, uas","Copy the first 6 columns from gold file to pred file

Args:
  gold_file: 
  pred_file: 
  copied_pred_file: 
  keep_comments:  (Default value = True)

Returns:, Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)

Args:
  gold_file(str): The gold conllx file
  pred_file(str): The pred conllx file

Returns:","	, 
,  -q -b -g ,  -s ,  and error message ,  and output ,  does not end a sentence at line , #, #bmstparser/src/utils/eval.pl, -, ., .conllu, Copy the first 6 columns from gold file to pred file

    Args:
      gold_file: 
      pred_file: 
      copied_pred_file: 
      keep_comments:  (Default value = True)

    Returns:

    
    , Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)

    Args:
      gold_file(str): The gold conllx file
      pred_file(str): The pred conllx file

    Returns:

    
    , Prediction file , eval.pl exited with error code , https://github.com/elikip/bist-parser/archive/master.zip, perl , w","0, 1, 3, 4, 5, 6, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-08 22:35, #') or '-' in g.split('\t')[0]:, #'):, #bmstparser/src/utils/eval.pl'), Copy the first 6 columns from gold file to pred file

Args:
gold_file:
pred_file:
copied_pred_file:
keep_comments:  (Default value = True)

Returns:


, Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)

Args:
gold_file(str): The gold conllx file
pred_file(str): The pred conllx file

Returns:


",
https://github.com/hankcs/HanLP,labeled_f1.py,0,0.0,40,80.0,5,10.0,2,4.0,3,6.0,0,0.0,18,1,6,0,,"LabeledF1, __call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, arc_golds, arc_preds, correct_arcs_wo_punc, correct_mask, correct_rels_wo_punc, dict, float, hanlp, las, lf, lp, lr, mask, mask_gold, mask_pred, other, property, rel_golds, rel_preds, reset, rp, score, self, sum, sum_gold_arcs_wo_punc, sum_pred_arcs_wo_punc, super, to_dict, uas, uf, up, ur",hanlp.metrics.metric.Metric,"__call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, las, lf, lp, lr, reset, score, to_dict, uas, uf, up, ur",LabeledF1,"correct_arcs_wo_punc, correct_mask, correct_rels_wo_punc, mask_gold, mask_pred, rp",," LF: , 4.2%, LF, UF, UF: ","0.0, 2","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-27 21:42",
https://github.com/hankcs/HanLP,labeled_f1_tf.py,0,0.0,41,77.36,5,9.43,4,7.55,3,5.66,0,0.0,18,1,6,0,,"LabeledF1TF, __call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, arc_golds, arc_preds, correct_arcs_wo_punc, correct_rels_wo_punc, dict, float, las, lf, lp, lr, mask, mask_gold, mask_pred, object, other, property, rel_golds, rel_preds, reset_states, rp, score, self, sum_gold_arcs_wo_punc, sum_pred_arcs_wo_punc, super, tensorflow, to_dict, transpose, uas, uf, unsqueeze, up, ur",tf.math.count_nonzero,"__call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, las, lf, lp, lr, reset_states, score, to_dict, uas, uf, up, ur",LabeledF1TF,"correct_arcs_wo_punc, correct_rels_wo_punc, mask, mask_gold, mask_pred, rp",," LF: , 6.2%, LF, UF, UF: ","0, 0.0, 1, 2","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-27 21:42",
https://github.com/hankcs/HanLP,labeled_score.py,0,0.0,33,76.74,5,11.63,2,4.65,3,6.98,0,0.0,12,1,2,0,,"LabeledScore, __call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, arc_golds, arc_mask, arc_preds, correct_arcs, correct_rels, dict, eps, int, las, len, mask, object, other, property, rel_golds, rel_mask, rel_preds, reset_states, score, self, super, tensorflow, to_dict, total, uas",tf.math.count_nonzero,"__call__, __ge__, __gt__, __init__, __le__, __lt__, __repr__, las, reset_states, score, to_dict, uas",LabeledScore,"arc_mask, rel_mask",," LAS: , 6.2%, LAS, UAS, UAS: ","0.0, 1e-05","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-27 00:49",
https://github.com/hankcs/HanLP,semdep_eval.py,0,0.0,51,51.52,18,18.18,8,8.08,22,22.22,0,0.0,2,0,34,1,,"Accuracy, F1, LAS, UAS, __future__, __name__, actual, codecs, collections, correct, correct_edges, current_sent, current_seq_correct, files, gf, gold_edge, gold_edges, gold_file, gold_files, gold_i, gold_line, gold_node, isinstance, labeled, len, main, n_correct_sequences, n_files, n_sequences, n_tokens, precision, predicted, readline, recall, rstrip, sdp_eval, seq_acc, set, sf, split, startswith, str, sys, sys_edge, sys_edges, sys_file, sys_files, sys_i, sys_line, sys_node, zip","__future__.absolute_import, __future__.division, __future__.print_function, codecs.open, collections.namedtuple, sys.argv","main, sdp_eval",,"Accuracy, F1, LAS, UAS, actual, correct, correct_edges, current_sent, current_seq_correct, files, gf, gold_edge, gold_edges, gold_file, gold_files, gold_i, gold_line, gold_node, n_correct_sequences, n_files, n_sequences, n_tokens, precision, predicted, recall, seq_acc, sf, sys_edge, sys_edges, sys_file, sys_files, sys_i, sys_line, sys_node","Modified from https://github.com/tdozat/Parser-v3/blob/2ff4061373e8aac8c962537a6220e1d5b196abf6/scripts/semdep_eval.py
Dozat claimed ""I tested it against the official eval script and it reported identical LF1"".

Args:
  gold_files: 
  sys_files: 
  labeled:  (Default value = False)

Returns:",", 	,  , #, 0, :, Accuracy, F1, LAS={:0.1f}, Modified from https://github.com/tdozat/Parser-v3/blob/2ff4061373e8aac8c962537a6220e1d5b196abf6/scripts/semdep_eval.py
    Dozat claimed ""I tested it against the official eval script and it reported identical LF1"".

    Args:
      gold_files: 
      sys_files: 
      labeled:  (Default value = False)

    Returns:

    
    , UAS={:0.1f}, _, __main__, precision, recall, seq_acc, utf-8, |","0, 1, 100, 1e-12, 2, 8, False, True","#, #     http://www.apache.org/licenses/LICENSE-2.0, # -*- coding: utf-8 -*-, # ===============================================================, # Compute the gold edges, # Compute the sys edges, # Copyright 2017 Timothy Dozat, # Licensed under the Apache License, Version 2.0 (the ""License"");, # See the License for the specific language governing permissions and, # Unless required by applicable law or agreed to in writing, software, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # assert sys_line[1] == gold_line[1], 'Files are misaligned at lines {}, {}'.format(gold_i, sys_i), # current_fp += len(sys_edges) - len(gold_edges & sys_edges), # distributed under the License is distributed on an ""AS IS"" BASIS,, # limitations under the License., # print(correct, predicted - correct, actual - correct), # you may not use this file except in compliance with the License., #!/usr/bin/env python, #') or sys_line.rstrip() == '' or sys_line.split('\t')[0] == '0':, #'):, Modified from https://github.com/tdozat/Parser-v3/blob/2ff4061373e8aac8c962537a6220e1d5b196abf6/scripts/semdep_eval.py
Dozat claimed ""I tested it against the official eval script and it reported identical LF1"".

Args:
gold_files:
sys_files:
labeled:  (Default value = False)

Returns:


",
https://github.com/hankcs/HanLP,span.py,0,0.0,38,53.52,10,14.08,4,5.63,19,26.76,0,0.0,13,1,11,0,,"SpanMetric, __call__, __init__, __repr__, collections, eps, gold, golds, hanlp, j, label, lcm, len, lf, lgold, list, lp, lpred, lr, ltp, n, n_lcm, n_ucm, pred, preds, property, reset, score, self, super, ucm, uf, ugold, up, upred, ur, utp, zip","collections.Counter, hanlp.metrics.metric.Metric","__call__, __init__, __repr__, lcm, lf, lp, lr, reset, score, ucm, uf, up, ur",SpanMetric,"gold, j, label, lgold, lpred, ltp, pred, s, ugold, upred, utp",," ,  LCM: ,  LF: ,  LR: ,  UF: ,  UR: , .2%, LP: , UCM: , UP: ","0.0, 1, 1e-12, 2","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # noinspection PyAttributeOutsideInit, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-27 00:48",
https://github.com/hankcs/HanLP,srlconll.py,0,0.0,23,46.94,14,28.57,6,12.24,6,12.24,0,0.0,2,0,9,0,,"RuntimeError, bin_path, conll_f1, conll_precision, conll_recall, dst, eval_info_gold_pred, eval_info_pred_gold, float, gold_path, hanlp, lib_path, official_conll_05_evaluate, ofile, open, os, pred_path, run_perl, script, script_root, src, strip, write","hanlp.utils.io_util.get_exitcode_stdout_stderr, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.run_cmd, os.environ, os.environ.get, os.path.expanduser","official_conll_05_evaluate, run_perl",,"bin_path, conll_f1, conll_precision, conll_recall, eval_info_gold_pred, eval_info_pred_gold, lib_path, ofile, script_root",,", 
,  , /bin, /bin/srl-eval.pl , /lib, :, PATH, PERL5LIB, http://www.lsi.upc.edu/~srlconll/srlconll-1.1.tgz, perl , perl -I, w, ~/.local/lib/perl5","0, 100, 2, 5, 6, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-16 18:44, # cpanm -l ~/.local Moose, # cpanm -l ~/.local MooseX::SemiAffordanceAccessor module, # cpanm -l ~/.local namespace::autoclean",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-16 18:44",
https://github.com/hankcs/HanLP,crf.py,0,0.0,67,41.36,20,12.35,7,4.32,66,40.74,2,1.23,9,1,25,4,,"CRF, ValueError, __class__, __init__, __name__, __repr__, __version__, _compute_normalizer, _compute_score, _validate, _viterbi_decode, append, batch_first, batch_size, best_last_tag, best_tags, best_tags_list, bool, broadcast_emission, broadcast_emissions, broadcast_score, decode, denominator, dim, emissions, end_transitions, forward, hist, history, idx, indices, int, item, last_tags, llh, long, mask, max, mean, new_ones, next_score, no_empty_seq, no_empty_seq_bf, num_tags, numerator, reduction, reset_parameters, reverse, reversed, score, self, seq_ends, seq_length, shape, size, start_transitions, str, sum, super, tags, torch, transitions, transpose, tuple, type_as, typing, unsqueeze","nn.Module, nn.Parameter, nn.init.uniform_, torch.ByteTensor, torch.FloatTensor, torch.LongTensor, torch.Tensor, torch.arange, torch.empty, torch.logsumexp, torch.ones_like, torch.uint8, torch.where, typing.List, typing.Optional","__init__, __repr__, _compute_normalizer, _compute_score, _validate, _viterbi_decode, decode, forward, reset_parameters",CRF,"__version__, batch_size, best_last_tag, best_tags, best_tags_list, broadcast_emission, broadcast_emissions, broadcast_score, denominator, emissions, hist, history, idx, indices, last_tags, llh, mask, next_score, no_empty_seq, no_empty_seq_bf, numerator, score, seq_ends, seq_length, tags","Compute the conditional log likelihood of a sequence of tags given emission scores.

Args:
    emissions (`~torch.Tensor`): Emission score tensor of size
        ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
        ``(batch_size, seq_length, num_tags)`` otherwise.
    tags (`~torch.LongTensor`): Sequence of tags tensor of size
        ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,
        ``(batch_size, seq_length)`` otherwise.
    mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
        if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.
    reduction: Specifies  the reduction to apply to the output:
        ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.
        ``sum``: the output will be summed over batches. ``mean``: the output will be
        averaged over batches. ``token_mean``: the output will be averaged over tokens.

Returns:
    `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if
    reduction is ``none``, ``()`` otherwise., Conditional random field.

This module implements a conditional random field [LMP01]_. The forward computation
of this class computes the log likelihood of the given sequence of tags and
emission score tensor. This class also has `~CRF.decode` method which finds
the best tag sequence given an emission score tensor using `Viterbi algorithm`_.

Args:
    num_tags: Number of tags.
    batch_first: Whether the first dimension corresponds to the size of a minibatch.

Attributes:
    start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size
        ``(num_tags,)``.
    end_transitions (`~torch.nn.Parameter`): End transition score tensor of size
        ``(num_tags,)``.
    transitions (`~torch.nn.Parameter`): Transition score tensor of size
        ``(num_tags, num_tags)``.


.. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).
   ""Conditional random fields: Probabilistic models for segmenting and
   labeling sequence data"". *Proc. 18th International Conf. on Machine
   Learning*. Morgan Kaufmann. pp. 282–289.

.. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm, Find the most likely tag sequence using Viterbi algorithm.

Args:
    emissions (`~torch.Tensor`): Emission score tensor of size
        ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
        ``(batch_size, seq_length, num_tags)`` otherwise.
    mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
        if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.

Returns:
    List of list containing the best tag sequence for each batch., Initialize the transition parameters.

The parameters will be initialized randomly from a uniform distribution
between -0.1 and 0.1."," and , (num_tags=, ), , got , 0.7.2, Compute the conditional log likelihood of a sequence of tags given emission scores.

        Args:
            emissions (`~torch.Tensor`): Emission score tensor of size
                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
                ``(batch_size, seq_length, num_tags)`` otherwise.
            tags (`~torch.LongTensor`): Sequence of tags tensor of size
                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,
                ``(batch_size, seq_length)`` otherwise.
            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.
            reduction: Specifies  the reduction to apply to the output:
                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.
                ``sum``: the output will be summed over batches. ``mean``: the output will be
                averaged over batches. ``token_mean``: the output will be averaged over tokens.

        Returns:
            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if
            reduction is ``none``, ``()`` otherwise.
        , Conditional random field.

    This module implements a conditional random field [LMP01]_. The forward computation
    of this class computes the log likelihood of the given sequence of tags and
    emission score tensor. This class also has `~CRF.decode` method which finds
    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.

    Args:
        num_tags: Number of tags.
        batch_first: Whether the first dimension corresponds to the size of a minibatch.

    Attributes:
        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size
            ``(num_tags,)``.
        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size
            ``(num_tags,)``.
        transitions (`~torch.nn.Parameter`): Transition score tensor of size
            ``(num_tags, num_tags)``.


    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).
       ""Conditional random fields: Probabilistic models for segmenting and
       labeling sequence data"". *Proc. 18th International Conf. on Machine
       Learning*. Morgan Kaufmann. pp. 282–289.

    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm
    , Find the most likely tag sequence using Viterbi algorithm.

        Args:
            emissions (`~torch.Tensor`): Emission score tensor of size
                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
                ``(batch_size, seq_length, num_tags)`` otherwise.
            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.

        Returns:
            List of list containing the best tag sequence for each batch.
        , Initialize the transition parameters.

        The parameters will be initialized randomly from a uniform distribution
        between -0.1 and 0.1.
        , emissions must have dimension of 3, got , expected last dimension of emissions is , invalid number of tags: , invalid reduction: , mask of the first timestep must all be on, mean, none, sum, the first two dimensions of emissions and mask must match, got , the first two dimensions of emissions and tags must match, got , token_mean","0, 0.1, 1, 2, 3, None, True","#, # (batch_size, num_tags) where for each batch, the j-th column stores, # Broadcast emission score for every possible current tag, # Broadcast score for every possible next tag, # Broadcast viterbi score for every possible next tag, # Compute the score tensor of size (batch_size, num_tags, num_tags) where, # Copied from https://github.com/kmkurn/pytorch-crf, # Copyright 2017 Kemal Kurniawan <kemal@kkurniawan.com>, # Emission score for next tag, only added if next timestep is valid (mask == 1), # End transition score, # Find the maximum score over all possible current tag, # Find the tag which maximizes the score at the last timestep; this is our best tag, # HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION, # INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A, # Now, compute the best path for each sample, # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE, # PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT, # Permission is hereby granted, free of charge, to any person obtaining a copy of, # Reverse the order because we start from the last timestep, # SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE., # Set score to the next score if this timestep is valid (mask == 1), # Start transition and first emission, # Start transition score and first emission, # Start transition score and first emission; score has size of, # Sum (log-sum-exp) over all possible tags, # Sum over all possible current tags, but we're in score space, so a sum, # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,, # The above copyright notice and this permission notice shall be included in all, # Transition score to next tag, only added if next timestep is valid (mask == 1), # Viterbi algorithm recursive case: we compute the score of the best tag sequence, # We trace back where the best last tag comes from, append that to our best tag, # all possible tag sequences so far, that end in tag i, # and emitting, # and save the index that produces the next score, # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of, # copies or substantial portions of the Software., # emissions: (seq_length, batch_size, num_tags), # for each sample, entry at row i and column j stores the score of the best, # for each sample, entry at row i and column j stores the sum of scores of all, # for every possible next tag, # for the last timestep, # history saves where the best tags candidate transitioned from; this is used, # mask: (seq_length, batch_size), # of the Software, and to permit persons to whom the Software is furnished to do, # possible tag sequences so far that end with transitioning from tag i to tag j, # score is a tensor of size (batch_size, num_tags) where for every batch,, # sequence, and trace it back again, and so on, # shape: (batch_size, 1, num_tags), # shape: (batch_size, num_tags), # shape: (batch_size, num_tags, 1), # shape: (batch_size, num_tags, num_tags), # shape: (batch_size,), # so, subject to the following conditions:, # tag sequence so far that ends with transitioning from tag i to tag j and emitting, # tags: (seq_length, batch_size), # the Software without restriction, including without limitation the rights to, # the score that the first timestep has tag j, # this software and associated documentation files (the ""Software""), to deal in, # use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies, # value at column j stores the score of the best tag sequence so far that ends, # when we trace back the best tag sequence, # with tag j, Compute the conditional log likelihood of a sequence of tags given emission scores.

Args:
emissions (`~torch.Tensor`): Emission score tensor of size
``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
``(batch_size, seq_length, num_tags)`` otherwise.
tags (`~torch.LongTensor`): Sequence of tags tensor of size
``(seq_length, batch_size)`` if ``batch_first`` is ``False``,
``(batch_size, seq_length)`` otherwise.
mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.
reduction: Specifies  the reduction to apply to the output:
``none|sum|mean|token_mean``. ``none``: no reduction will be applied.
``sum``: the output will be summed over batches. ``mean``: the output will be
averaged over batches. ``token_mean``: the output will be averaged over tokens.

Returns:
`~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if
reduction is ``none``, ``()`` otherwise.
, Conditional random field.

This module implements a conditional random field [LMP01]_. The forward computation
of this class computes the log likelihood of the given sequence of tags and
emission score tensor. This class also has `~CRF.decode` method which finds
the best tag sequence given an emission score tensor using `Viterbi algorithm`_.

Args:
num_tags: Number of tags.
batch_first: Whether the first dimension corresponds to the size of a minibatch.

Attributes:
start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size
``(num_tags,)``.
end_transitions (`~torch.nn.Parameter`): End transition score tensor of size
``(num_tags,)``.
transitions (`~torch.nn.Parameter`): Transition score tensor of size
``(num_tags, num_tags)``.


.. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).
""Conditional random fields: Probabilistic models for segmenting and
labeling sequence data"". *Proc. 18th International Conf. on Machine
Learning*. Morgan Kaufmann. pp. 282–289.

.. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm
, Find the most likely tag sequence using Viterbi algorithm.

Args:
emissions (`~torch.Tensor`): Emission score tensor of size
``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,
``(batch_size, seq_length, num_tags)`` otherwise.
mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``
if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.

Returns:
List of list containing the best tag sequence for each batch.
, Initialize the transition parameters.

The parameters will be initialized randomly from a uniform distribution
between -0.1 and 0.1.
","282–289, Conditional random field.

This module implements a conditional random field [LMP01]_. The forward computation
of this class computes the log likelihood of the given sequence of tags and
emission score tensor. This class also has `~CRF.decode` method which finds
the best tag sequence given an emission score tensor using `Viterbi algorithm`_.

Args:
num_tags: Number of tags.
batch_first: Whether the first dimension corresponds to the size of a minibatch.

Attributes:
start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size
``(num_tags,)``.
end_transitions (`~torch.nn.Parameter`): End transition score tensor of size
``(num_tags,)``.
transitions (`~torch.nn.Parameter`): Transition score tensor of size
``(num_tags, num_tags)``.


.. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).
""Conditional random fields: Probabilistic models for segmenting and
labeling sequence data"". *Proc. 18th International Conf. on Machine
Learning*. Morgan Kaufmann. pp. 282–289.

.. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm
"
https://github.com/hankcs/HanLP,crf_layer_tf.py,0,0.0,56,57.14,10,10.2,7,7.14,25,25.51,0,0.0,9,3,13,1,,"CRF, CRFLoss, CRFWrapper, ValueError, __call__, __init__, __name__, accuracy, add_weight, args, base_config, build, built, call, compute_mask, compute_output_shape, config, crf, dict, dtype, f_shape, func_name, get_config, hanlp, input_shape, input_spec, inputs, int, items, kwargs, len, list, mask, model, num_classes, object, output, output_dim, property, sample_weight, self, seq_len_shape, sequence_lengths, sequences, shape, super, supports_masking, tensorflow, training, transitions, type, viterbi_accuracy, viterbi_output, viterbi_sequence, y_pred, y_true","hanlp.layers.crf.crf_tf.crf_decode, hanlp.layers.crf.crf_tf.crf_log_likelihood, tf.TensorShape, tf.argmax, tf.cast, tf.convert_to_tensor, tf.int32, tf.keras.Model, tf.keras.backend.eval, tf.keras.backend.flatten, tf.keras.backend.in_train_phase, tf.keras.backend.one_hot, tf.keras.layers.InputSpec, tf.keras.layers.Layer, tf.keras.metrics.categorical_accuracy, tf.math.count_nonzero, tf.ones, tf.reduce_mean, tf.shape","__call__, __init__, accuracy, build, call, compute_mask, compute_output_shape, get_config, viterbi_accuracy","CRF, CRFLoss, CRFWrapper","base_config, config, f_shape, input_spec, output, seq_len_shape, sequence_lengths, sequences, shape, viterbi_output, viterbi_sequence, y_pred, y_true","Conditional Random Field layer (tf.keras)
`CRF` can be used as the last layer in a network (as a classifier). Input shape (features)
must be equal to the number of classes the CRF can predict (a linear layer is recommended).

Note: the loss and accuracy functions of networks using `CRF` must
use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)
as the classification of sequences are used with the layers internal weights.

Copyright: this is a modified version of
https://github.com/NervanaSystems/nlp-architect/blob/master/nlp_architect/nn/tensorflow/python/keras/layers/crf.py

Args:
  num_labels(int): the number of labels to tag each temporal input.
Input shape:
  num_labels(int): the number of labels to tag each temporal input.
Input shape:
nD tensor with shape `(batch_size, sentence length, num_classes)`.
Output shape:
  nD tensor with shape: `(batch_size, sentence length, num_classes)`.

Returns:","Conditional Random Field layer (tf.keras)
    `CRF` can be used as the last layer in a network (as a classifier). Input shape (features)
    must be equal to the number of classes the CRF can predict (a linear layer is recommended).
    
    Note: the loss and accuracy functions of networks using `CRF` must
    use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)
    as the classification of sequences are used with the layers internal weights.
    
    Copyright: this is a modified version of
    https://github.com/NervanaSystems/nlp-architect/blob/master/nlp_architect/nn/tensorflow/python/keras/layers/crf.py

    Args:
      num_labels(int): the number of labels to tag each temporal input.
    Input shape:
      num_labels(int): the number of labels to tag each temporal input.
    Input shape:
    nD tensor with shape `(batch_size, sentence length, num_classes)`.
    Output shape:
      nD tensor with shape: `(batch_size, sentence length, num_classes)`.

    Returns:

    , The last dimension of the input shape must be equal to output shape. Use a linear layer if needed., The last dimension of the inputs to `CRF` should be defined. Found `None`., glorot_uniform, int32, output_dim, supports_masking, transitions, viterbi_accuracy, your model has to support masking","0, 1, 2, 3, False, None, True","#, #                            sequence_lengths,, #                            tf.cast(y_true, dtype=tf.int32),, #                            transition_params=self.transitions), #         crf_log_likelihood(y_pred,, #     http://www.apache.org/licenses/LICENSE-2.0, #     log_likelihood, self.transitions = \, #     return tf.reduce_mean(-log_likelihood), #     y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype), # ******************************************************************************, # Copyright 2017-2018 Intel Corporation, # Just pass the received mask from previous layer, to the next layer or, # Licensed under the Apache License, Version 2.0 (the ""License"");, # See the License for the specific language governing permissions and, # Unless required by applicable law or agreed to in writing, software, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # def loss(self, y_true, y_pred):, # distributed under the License is distributed on an ""AS IS"" BASIS,, # limitations under the License., # manipulate it if this layer changes the shape of the input, # num of output labels, # pylint: disable=arguments-differ, # you may not use this file except in compliance with the License., Conditional Random Field layer (tf.keras)
`CRF` can be used as the last layer in a network (as a classifier). Input shape (features)
must be equal to the number of classes the CRF can predict (a linear layer is recommended).

Note: the loss and accuracy functions of networks using `CRF` must
use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)
as the classification of sequences are used with the layers internal weights.

Copyright: this is a modified version of
https://github.com/NervanaSystems/nlp-architect/blob/master/nlp_architect/nn/tensorflow/python/keras/layers/crf.py

Args:
num_labels(int): the number of labels to tag each temporal input.
Input shape:
num_labels(int): the number of labels to tag each temporal input.
Input shape:
nD tensor with shape `(batch_size, sentence length, num_classes)`.
Output shape:
nD tensor with shape: `(batch_size, sentence length, num_classes)`.

Returns:

",
https://github.com/hankcs/HanLP,crf_tf.py,0,0.0,89,57.79,19,12.34,6,3.9,40,25.97,0,0.0,19,1,52,16,,"CrfDecodeForwardRnnCell, __future__, __init__, _multi_seq_fn, _num_tags, _scan_fn, _single_seq_fn, _transition_params, all_alphas, alphas, append, backpointers, batch_size, best_score, binary_scores, bp, build, call, crf_binary_score, crf_decode, crf_decode_backward, crf_decode_forward, crf_forward, crf_fwd_cell, crf_fwd_layer, crf_log_likelihood, crf_log_norm, crf_multitag_sequence_score, crf_sequence_score, crf_unary_score, decode_tags, dtype, end_tag_indices, example_inds, filtered_inputs, first_input, flattened_inputs, flattened_tag_indices, flattened_transition_indices, flattened_transition_params, float, idxs, initial_state, initializer, input_shape, inputs, kwargs, last_score, len, log_likelihood, log_norm, masks, max_seq_len, new_alphas, new_state, new_tags, num_tags, num_transitions, numpy, offsets, output_size, potentials, property, rest_of_input, reverse, reversed, score, self, sequence_length, sequence_length_less_one, sequence_lengths, sequence_scores, shape, squeezed_potentials, start_tag_indices, state, state_size, super, tag_bitmap, tag_indices, tensorflow, transition_params, transition_scores, trellis, truncated_masks, unary_scores, viterbi, viterbi_decode, viterbi_score","__future__.absolute_import, __future__.division, __future__.print_function, np.argmax, np.expand_dims, np.int32, np.max, np.zeros_like, tf.Variable, tf.argmax, tf.bool, tf.cast, tf.concat, tf.constant, tf.expand_dims, tf.fill, tf.float32, tf.gather, tf.gather_nd, tf.int32, tf.int64, tf.keras.initializers.GlorotUniform, tf.keras.layers.AbstractRNNCell, tf.keras.layers.RNN, tf.less_equal, tf.maximum, tf.range, tf.reduce_logsumexp, tf.reduce_max, tf.reduce_sum, tf.reshape, tf.reverse_sequence, tf.scan, tf.sequence_mask, tf.shape, tf.slice, tf.squeeze, tf.stack, tf.transpose, tf.where, tf.zeros_like","__init__, _multi_seq_fn, _scan_fn, _single_seq_fn, build, call, crf_binary_score, crf_decode, crf_decode_backward, crf_decode_forward, crf_forward, crf_log_likelihood, crf_log_norm, crf_multitag_sequence_score, crf_sequence_score, crf_unary_score, output_size, state_size, viterbi_decode",CrfDecodeForwardRnnCell,"all_alphas, alphas, backpointers, batch_size, best_score, binary_scores, bp, crf_fwd_cell, crf_fwd_layer, decode_tags, end_tag_indices, example_inds, filtered_inputs, first_input, flattened_inputs, flattened_tag_indices, flattened_transition_indices, flattened_transition_params, idxs, initial_state, initializer, inputs, last_score, log_likelihood, log_norm, mask, masks, max_seq_len, new_alphas, new_state, new_tags, num_tags, num_transitions, offsets, rest_of_input, sequence_length, sequence_length_less_one, sequence_lengths, sequence_scores, squeezed_potentials, start_tag_indices, state, tag_bitmap, tag_indices, transition_params, transition_scores, trellis, truncated_masks, unary_scores, v, viterbi, viterbi_score","Build the CrfDecodeForwardRnnCell.

Args:
  inputs: A [batch_size, num_tags] matrix of unary potentials.
  state: A [batch_size, num_tags] matrix containing the previous step's
score values.

Returns:
  backpointers: A [batch_size, num_tags] matrix of backpointers.
  new_state: A [batch_size, num_tags] matrix of new score values., Computes backward decoding in a linear-chain CRF.

Args:
  inputs: A [batch_size, num_tags] matrix of
backpointer of next step (in time order).
  state: A [batch_size, 1] matrix of tag index of next step.

Returns:
  new_tags: A [batch_size, num_tags]
  tensor containing the new tag indices., Computes forward decoding in a linear-chain CRF.

Args:
  inputs: A [batch_size, num_tags] matrix of unary potentials.
  state: A [batch_size, num_tags] matrix containing the previous step's
score values.
  transition_params: A [num_tags, num_tags] matrix of binary potentials.
  sequence_lengths: A [batch_size] vector of true sequence lengths.

Returns:
  backpointers: A [batch_size, num_tags] matrix of backpointers.
  new_state: A [batch_size, num_tags] matrix of new score values., Computes the alpha values in a linear-chain CRF.

See http://www.cs.columbia.edu/~mcollins/fb.pdf for reference.

Args:
  inputs: A [batch_size, num_tags] matrix of unary potentials.
  state: A [batch_size, num_tags] matrix containing the previous alpha
values.
  transition_params: A [num_tags, num_tags] matrix of binary potentials.
This matrix is expanded into a [1, num_tags, num_tags] in preparation
for the broadcast summation occurring within the cell.
  sequence_lengths: A [batch_size] vector of true sequence lengths.

Returns:
  new_alphas: A [batch_size, num_tags] matrix containing the
  new alpha values., Computes the binary scores of tag sequences.

Args:
  tag_indices: A [batch_size, max_seq_len] matrix of tag indices.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  transition_params: 

Returns:
  binary_scores: A [batch_size] vector of binary scores., Computes the forward decoding in a linear-chain CRF., Computes the log-likelihood of tag sequences in a CRF.

Args:
  inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
  tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
we compute the log-likelihood.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  transition_params: A [num_tags, num_tags] transition matrix, (Default value = None)

Returns:
  log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of
  each example, given the sequence of tag indices.
  transition_params: A [num_tags, num_tags] transition matrix. This is
  either provided by the caller or created in this function., Computes the normalization for a CRF.

Args:
  inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  transition_params: 

Returns:
  log_norm: A [batch_size] vector of normalizers for a CRF., Computes the unary scores of tag sequences.

Args:
  tag_indices: A [batch_size, max_seq_len] matrix of tag indices.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  inputs: 

Returns:
  unary_scores: A [batch_size] vector of unary scores., Computes the unnormalized score for a tag sequence.

Args:
  inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
  tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
we compute the unnormalized score.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  transition_params: 

Returns:
  sequence_scores: A [batch_size] vector of unnormalized sequence scores., Computes the unnormalized score of all tag sequences matching
tag_bitmap.

tag_bitmap enables more than one tag to be considered correct at each time
step. This is useful when an observed output at a given time step is
consistent with more than one tag, and thus the log likelihood of that
observation must take into account all possible consistent tags.

Using one-hot vectors in tag_bitmap gives results identical to
crf_sequence_score.

Args:
  inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
  tag_bitmap: A [batch_size, max_seq_len, num_tags] boolean tensor
representing all active tags at each index for which to calculate the
unnormalized score.
  sequence_lengths: A [batch_size] vector of true sequence lengths.
  transition_params: 

Returns:
  sequence_scores: A [batch_size] vector of unnormalized sequence scores., Decode the highest scoring sequence of tags in TensorFlow.

This is a function for tensor.

Args:
  potentials: A [batch_size, max_seq_len, num_tags] tensor of
unary potentials.
  transition_params: A [num_tags, num_tags] matrix of
binary potentials.
  sequence_length: A [batch_size] vector of true sequence lengths.

Returns:
  decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.
  Contains the highest scoring tag indices.
  best_score: A [batch_size] vector, containing the score of `decode_tags`., Decode the highest scoring sequence of tags outside of TensorFlow.

This should only be used at test time.

Args:
  score: A [seq_len, num_tags] matrix of unary potentials.
  transition_params: A [num_tags, num_tags] matrix of binary potentials.

Returns:
  viterbi: A [seq_len] list of integers containing the highest scoring tag
  indices.
  viterbi_score: A float containing the score for the Viterbi sequence., Decoding of highest scoring sequence., Forward computation of alpha values., Initialize the CrfDecodeForwardRnnCell.

Args:
  transition_params: A [num_tags, num_tags] matrix of binary
    potentials. This matrix is expanded into a
    [1, num_tags, num_tags] in preparation for the broadcast
    summation occurring within the cell.","-inf, Build the CrfDecodeForwardRnnCell.

        Args:
          inputs: A [batch_size, num_tags] matrix of unary potentials.
          state: A [batch_size, num_tags] matrix containing the previous step's
        score values.

        Returns:
          backpointers: A [batch_size, num_tags] matrix of backpointers.
          new_state: A [batch_size, num_tags] matrix of new score values.

        , Computes backward decoding in a linear-chain CRF.

    Args:
      inputs: A [batch_size, num_tags] matrix of
    backpointer of next step (in time order).
      state: A [batch_size, 1] matrix of tag index of next step.

    Returns:
      new_tags: A [batch_size, num_tags]
      tensor containing the new tag indices.

    , Computes forward decoding in a linear-chain CRF.

    Args:
      inputs: A [batch_size, num_tags] matrix of unary potentials.
      state: A [batch_size, num_tags] matrix containing the previous step's
    score values.
      transition_params: A [num_tags, num_tags] matrix of binary potentials.
      sequence_lengths: A [batch_size] vector of true sequence lengths.

    Returns:
      backpointers: A [batch_size, num_tags] matrix of backpointers.
      new_state: A [batch_size, num_tags] matrix of new score values.

    , Computes the alpha values in a linear-chain CRF.
    
    See http://www.cs.columbia.edu/~mcollins/fb.pdf for reference.

    Args:
      inputs: A [batch_size, num_tags] matrix of unary potentials.
      state: A [batch_size, num_tags] matrix containing the previous alpha
    values.
      transition_params: A [num_tags, num_tags] matrix of binary potentials.
    This matrix is expanded into a [1, num_tags, num_tags] in preparation
    for the broadcast summation occurring within the cell.
      sequence_lengths: A [batch_size] vector of true sequence lengths.

    Returns:
      new_alphas: A [batch_size, num_tags] matrix containing the
      new alpha values.

    , Computes the binary scores of tag sequences.

    Args:
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: 

    Returns:
      binary_scores: A [batch_size] vector of binary scores.

    , Computes the forward decoding in a linear-chain CRF., Computes the log-likelihood of tag sequences in a CRF.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
    we compute the log-likelihood.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: A [num_tags, num_tags] transition matrix, (Default value = None)

    Returns:
      log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of
      each example, given the sequence of tag indices.
      transition_params: A [num_tags, num_tags] transition matrix. This is
      either provided by the caller or created in this function.

    , Computes the normalization for a CRF.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: 

    Returns:
      log_norm: A [batch_size] vector of normalizers for a CRF.

    , Computes the unary scores of tag sequences.

    Args:
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      inputs: 

    Returns:
      unary_scores: A [batch_size] vector of unary scores.

    , Computes the unnormalized score for a tag sequence.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
    we compute the unnormalized score.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: 

    Returns:
      sequence_scores: A [batch_size] vector of unnormalized sequence scores.

    , Computes the unnormalized score of all tag sequences matching
    tag_bitmap.
    
    tag_bitmap enables more than one tag to be considered correct at each time
    step. This is useful when an observed output at a given time step is
    consistent with more than one tag, and thus the log likelihood of that
    observation must take into account all possible consistent tags.
    
    Using one-hot vectors in tag_bitmap gives results identical to
    crf_sequence_score.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      tag_bitmap: A [batch_size, max_seq_len, num_tags] boolean tensor
    representing all active tags at each index for which to calculate the
    unnormalized score.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: 

    Returns:
      sequence_scores: A [batch_size] vector of unnormalized sequence scores.

    , Decode the highest scoring sequence of tags in TensorFlow.
    
    This is a function for tensor.

    Args:
      potentials: A [batch_size, max_seq_len, num_tags] tensor of
    unary potentials.
      transition_params: A [num_tags, num_tags] matrix of
    binary potentials.
      sequence_length: A [batch_size] vector of true sequence lengths.

    Returns:
      decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.
      Contains the highest scoring tag indices.
      best_score: A [batch_size] vector, containing the score of `decode_tags`.

    , Decode the highest scoring sequence of tags outside of TensorFlow.
    
    This should only be used at test time.

    Args:
      score: A [seq_len, num_tags] matrix of unary potentials.
      transition_params: A [num_tags, num_tags] matrix of binary potentials.

    Returns:
      viterbi: A [seq_len] list of integers containing the highest scoring tag
      indices.
      viterbi_score: A float containing the score for the Viterbi sequence.

    , Decoding of highest scoring sequence., Forward computation of alpha values., Initialize the CrfDecodeForwardRnnCell.

        Args:
          transition_params: A [num_tags, num_tags] matrix of binary
            potentials. This matrix is expanded into a
            [1, num_tags, num_tags] in preparation for the broadcast
            summation occurring within the cell.
        , tag_indices: A [batch_size, max_seq_len] matrix of tag indices., transitions","0, 1, 2, False, None, True","
assert len(tag_indices.shape) == 2, 'tag_indices: A [batch_size, max_seq_len] matrix of tag indices.'
tag_indices = tf.cast(tag_indices, dtype=tf.int32)
sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

batch_size = tf.shape(inputs)[0]
max_seq_len = tf.shape(inputs)[1]
num_tags = tf.shape(inputs)[2]

flattened_inputs = tf.reshape(inputs, [-1])

offsets = tf.expand_dims(tf.range(batch_size) * max_seq_len * num_tags, 1)
offsets += tf.expand_dims(tf.range(max_seq_len) * num_tags, 0)
# Use int32 or int64 based on tag_indices' dtype.
if tag_indices.dtype == tf.int64:
offsets = tf.cast(offsets, tf.int64)
flattened_tag_indices = tf.reshape(offsets + tag_indices, [-1])

unary_scores = tf.reshape(
tf.gather(flattened_inputs, flattened_tag_indices),
[batch_size, max_seq_len])

masks = tf.sequence_mask(
sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32)

unary_scores = tf.reduce_sum(unary_scores * masks, 1)
return unary_scores


def crf_binary_score(tag_indices, sequence_lengths, transition_params):
Computes the binary scores of tag sequences., 
num_tags = inputs.shape[2]

# cast type to handle different types
tag_indices = tf.cast(tag_indices, dtype=tf.int32)
sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

if transition_params is None:
initializer = tf.keras.initializers.GlorotUniform()
transition_params = tf.Variable(
initializer([num_tags, num_tags]), ""transitions"")

sequence_scores = crf_sequence_score(inputs, tag_indices, sequence_lengths,
transition_params)
log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)

# Normalize the scores to get the log-likelihood per example.
log_likelihood = sequence_scores - log_norm
return log_likelihood, transition_params


def crf_unary_score(tag_indices, sequence_lengths, inputs):
Computes the unary scores of tag sequences., 
sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

sequence_lengths = tf.maximum(
tf.constant(0, dtype=sequence_lengths.dtype), sequence_lengths - 2)
inputs = tf.transpose(inputs, [1, 0, 2])
transition_params = tf.expand_dims(transition_params, 0)

def _scan_fn(state, inputs):
state = tf.expand_dims(state, 2)
transition_scores = state + transition_params
new_alphas = inputs + tf.reduce_logsumexp(transition_scores, [1])
return new_alphas

all_alphas = tf.transpose(tf.scan(_scan_fn, inputs, state), [1, 0, 2])
idxs = tf.stack(
[tf.range(tf.shape(sequence_lengths)[0]), sequence_lengths], axis=1)
return tf.gather_nd(all_alphas, idxs)


def viterbi_decode(score, transition_params):
Decode the highest scoring sequence of tags outside of TensorFlow., 
tag_indices = tf.cast(tag_indices, dtype=tf.int32)
sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

num_tags = tf.shape(transition_params)[0]
num_transitions = tf.shape(tag_indices)[1] - 1

# Truncate by one on each side of the sequence to get the start and end
# indices of each transition.
start_tag_indices = tf.slice(tag_indices, [0, 0], [-1, num_transitions])
end_tag_indices = tf.slice(tag_indices, [0, 1], [-1, num_transitions])

# Encode the indices in a flattened representation.
flattened_transition_indices = start_tag_indices * \
num_tags + end_tag_indices
flattened_transition_params = tf.reshape(transition_params, [-1])

# Get the binary scores based on the flattened representation.
binary_scores = tf.gather(flattened_transition_params,
flattened_transition_indices)

masks = tf.sequence_mask(
sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32)
truncated_masks = tf.slice(masks, [0, 1], [-1, -1])
binary_scores = tf.reduce_sum(binary_scores * truncated_masks, 1)
return binary_scores


def crf_forward(inputs, state, transition_params, sequence_lengths):
Computes the alpha values in a linear-chain CRF., 
trellis = np.zeros_like(score)
backpointers = np.zeros_like(score, dtype=np.int32)
trellis[0] = score[0]

for t in range(1, score.shape[0]):
v = np.expand_dims(trellis[t - 1], 1) + transition_params
trellis[t] = score[t] + np.max(v, 0)
backpointers[t] = np.argmax(v, 0)

viterbi = [np.argmax(trellis[-1])]
for bp in reversed(backpointers[1:]):
viterbi.append(bp[viterbi[-1]])
viterbi.reverse()

viterbi_score = np.max(trellis[-1])
return viterbi, viterbi_score


class CrfDecodeForwardRnnCell(tf.keras.layers.AbstractRNNCell):
Computes the forward decoding in a linear-chain CRF., #, #     http://www.apache.org/licenses/LICENSE-2.0, # ==============================================================================, # Compute the logsumexp of all scores of sequences matching the given tags., # Compute the scores of the given tag sequence., # Copyright 2019 The TensorFlow Authors. All Rights Reserved., # If max_seq_len is 1, we skip the algorithm and simply reduce_logsumexp over, # If max_seq_len is 1, we skip the algorithm and simply return the argmax tag, # If max_seq_len is 1, we skip the score calculation and simply gather the, # Licensed under the Apache License, Version 2.0 (the ""License"");, # Mask `log_norm` of the sequences with length <= zero., # See the License for the specific language governing permissions and, # Split up the first and rest of the inputs in preparation for the forward, # TODO: Wrap functions in @tf.function once, # Unless required by applicable law or agreed to in writing, software, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # algorithm., # and the max activation., # distributed under the License is distributed on an ""AS IS"" BASIS,, # https://github.com/tensorflow/tensorflow/issues/29075 is resolved, # limitations under the License., # the ""initial state"" (the unary potentials)., # unary potentials of all active tags., # unary potentials of the single tag., # you may not use this file except in compliance with the License., Build the CrfDecodeForwardRnnCell.

Args:
inputs: A [batch_size, num_tags] matrix of unary potentials.
state: A [batch_size, num_tags] matrix containing the previous step's
score values.

Returns:
backpointers: A [batch_size, num_tags] matrix of backpointers.
new_state: A [batch_size, num_tags] matrix of new score values.

, Computes backward decoding in a linear-chain CRF.

Args:
inputs: A [batch_size, num_tags] matrix of
backpointer of next step (in time order).
state: A [batch_size, 1] matrix of tag index of next step.

Returns:
new_tags: A [batch_size, num_tags]
tensor containing the new tag indices.

, Computes forward decoding in a linear-chain CRF.

Args:
inputs: A [batch_size, num_tags] matrix of unary potentials.
state: A [batch_size, num_tags] matrix containing the previous step's
score values.
transition_params: A [num_tags, num_tags] matrix of binary potentials.
sequence_lengths: A [batch_size] vector of true sequence lengths.

Returns:
backpointers: A [batch_size, num_tags] matrix of backpointers.
new_state: A [batch_size, num_tags] matrix of new score values.

, Computes the normalization for a CRF.

Args:
inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
sequence_lengths: A [batch_size] vector of true sequence lengths.
transition_params:

Returns:
log_norm: A [batch_size] vector of normalizers for a CRF.

, Computes the unnormalized score for a tag sequence.

Args:
inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
we compute the unnormalized score.
sequence_lengths: A [batch_size] vector of true sequence lengths.
transition_params:

Returns:
sequence_scores: A [batch_size] vector of unnormalized sequence scores.

, Computes the unnormalized score of all tag sequences matching
tag_bitmap.

tag_bitmap enables more than one tag to be considered correct at each time
step. This is useful when an observed output at a given time step is
consistent with more than one tag, and thus the log likelihood of that
observation must take into account all possible consistent tags.

Using one-hot vectors in tag_bitmap gives results identical to
crf_sequence_score.

Args:
inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
to use as input to the CRF layer.
tag_bitmap: A [batch_size, max_seq_len, num_tags] boolean tensor
representing all active tags at each index for which to calculate the
unnormalized score.
sequence_lengths: A [batch_size] vector of true sequence lengths.
transition_params:

Returns:
sequence_scores: A [batch_size] vector of unnormalized sequence scores.

, Decode the highest scoring sequence of tags in TensorFlow.

This is a function for tensor.

Args:
potentials: A [batch_size, max_seq_len, num_tags] tensor of
unary potentials.
transition_params: A [num_tags, num_tags] matrix of
binary potentials.
sequence_length: A [batch_size] vector of true sequence lengths.

Returns:
decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.
Contains the highest scoring tag indices.
best_score: A [batch_size] vector, containing the score of `decode_tags`.

, Forward computation of alpha values.
rest_of_input = tf.slice(inputs, [0, 1, 0], [-1, -1, -1])
# Compute the alpha values in the forward algorithm in order to get the
# partition function.

alphas = crf_forward(rest_of_input, first_input, transition_params,
sequence_lengths)
log_norm = tf.reduce_logsumexp(alphas, [1])
# Mask `log_norm` of the sequences with length <= zero.
log_norm = tf.where(
tf.less_equal(sequence_lengths, 0), tf.zeros_like(log_norm),
log_norm)
return log_norm

if inputs.shape[1] == 1:
return _single_seq_fn()
else:
return _multi_seq_fn()


def crf_log_likelihood(inputs,
tag_indices,
sequence_lengths,
transition_params=None):
Computes the log-likelihood of tag sequences in a CRF., Initialize the CrfDecodeForwardRnnCell.

Args:
transition_params: A [num_tags, num_tags] matrix of binary
potentials. This matrix is expanded into a
[1, num_tags, num_tags] in preparation for the broadcast
summation occurring within the cell.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-18 22:55",
https://github.com/hankcs/HanLP,char_cnn.py,0,0.0,39,67.24,6,10.34,6,10.34,5,8.62,2,3.45,6,2,6,2,,"CharCNN, CharCNNEmbedding, Embedding, Module, ValueError, __init__, batch, conv_layer_activation, dict, embed, embedding_dim, encoder, field, forward, ge, get_output_dim, hanlp, hanlp_common, int, isinstance, len, mask, max, min_word_length, module, ngram_filter_sizes, num_filters, output_dim, property, self, super, tokens, torch, transform, typing, vocab_name, vocab_size, vocabs, x","hanlp.common.transform.ToChar, hanlp.common.transform.VocabDict, hanlp.common.vocab.Vocab, hanlp.layers.cnn_encoder.CnnEncoder, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.embedding.EmbeddingDim, hanlp.layers.time_distributed.TimeDistributed, hanlp_common.configurable.AutoConfigurable, torch.Tensor, torch.nn, typing.Callable, typing.Optional, typing.Tuple, typing.Union","__init__, forward, get_output_dim, module, transform, vocab_name","CharCNN, CharCNNEmbedding","embed, mask, min_word_length, tokens, vocab_name, x","A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.
The input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

See allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder, Apache 2.0

Args:
    field: The field in samples this encoder will work on.
    embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
    num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
        learned by that layer.
    ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
        default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
        ngrams of size 2 to 5 with some number of filters.
    conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
        Activation to use after the convolution layers.
    output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
        this size.  If this value is `None`, we will just return the result of the max pooling,
        giving an output of shape `len(ngram_filter_sizes) * num_filters`.
    vocab_size: The size of character vocab.

Returns:
    A tensor of shape `(batch_size, output_dim)`., Args:
    field: The character field in samples this encoder will work on.
    embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
    num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
        learned by that layer.
    ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
        default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
        ngrams of size 2 to 5 with some number of filters.
    conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
        Activation to use after the convolution layers.
    output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
        this size.  If this value is `None`, we will just return the result of the max pooling,
        giving an output of shape `len(ngram_filter_sizes) * num_filters`.
    min_word_length: For ngram filter with max size, the input (chars) is required to have at least max size
        chars.","

        Args:
            field: The character field in samples this encoder will work on.
            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
            num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
                learned by that layer.
            ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
                default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
                ngrams of size 2 to 5 with some number of filters.
            conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
                Activation to use after the convolution layers.
            output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
                this size.  If this value is `None`, we will just return the result of the max pooling,
                giving an output of shape `len(ngram_filter_sizes) * num_filters`.
            min_word_length: For ngram filter with max size, the input (chars) is required to have at least max size
                chars.
        , A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.
        The input to this module is of shape `(batch_size, num_tokens,
        input_dim)`, and the output is of shape `(batch_size, output_dim)`.

        The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
        out a vector of size num_filters. The number of times a convolution layer will be used
        is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
        outputs from the convolution layer and outputs the max.

        This operation is repeated for every ngram size passed, and consequently the dimensionality of
        the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
        (optionally) projected down to a lower dimensional output, specified by `output_dim`.

        We then use a fully connected layer to project in back to the desired output_dim.  For more
        details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
        Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

        See allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder, Apache 2.0

        Args:
            field: The field in samples this encoder will work on.
            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
            num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
                learned by that layer.
            ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
                default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
                ngrams of size 2 to 5 with some number of filters.
            conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
                Activation to use after the convolution layers.
            output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
                this size.  If this value is `None`, we will just return the result of the max pooling,
                giving an output of shape `len(ngram_filter_sizes) * num_filters`.
            vocab_size: The size of character vocab.

        Returns:
            A tensor of shape `(batch_size, output_dim)`.
        , ReLU, Unrecognized type for , _char, _char_id","0, 2, 3, 4, 5, None","

Args:
field: The character field in samples this encoder will work on.
embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
learned by that layer.
ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
ngrams of size 2 to 5 with some number of filters.
conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
Activation to use after the convolution layers.
output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
this size.  If this value is `None`, we will just return the result of the max pooling,
giving an output of shape `len(ngram_filter_sizes) * num_filters`.
min_word_length: For ngram filter with max size, the input (chars) is required to have at least max size
chars.
, # Adopted from https://github.com/allenai/allennlp under Apache Licence 2.0., # Changed the packaging and created a subclass CharCNNEmbedding, # the embedding layer, A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.
The input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

See allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder, Apache 2.0

Args:
field: The field in samples this encoder will work on.
embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
learned by that layer.
ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
ngrams of size 2 to 5 with some number of filters.
conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
Activation to use after the convolution layers.
output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
this size.  If this value is `None`, we will just return the result of the max pooling,
giving an output of shape `len(ngram_filter_sizes) * num_filters`.
vocab_size: The size of character vocab.

Returns:
A tensor of shape `(batch_size, output_dim)`.
","A `CnnEncoder` is a combination of multiple convolution layers and max pooling layers.
The input to this module is of shape `(batch_size, num_tokens,
input_dim)`, and the output is of shape `(batch_size, output_dim)`.

The CNN has one convolution layer for each ngram filter size. Each convolution operation gives
out a vector of size num_filters. The number of times a convolution layer will be used
is `num_tokens - ngram_size + 1`. The corresponding maxpooling layer aggregates all these
outputs from the convolution layer and outputs the max.

This operation is repeated for every ngram size passed, and consequently the dimensionality of
the output after maxpooling is `len(ngram_filter_sizes) * num_filters`.  This then gets
(optionally) projected down to a lower dimensional output, specified by `output_dim`.

We then use a fully connected layer to project in back to the desired output_dim.  For more
details, refer to ""A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
Networks for Sentence Classification"", Zhang and Wallace 2016, particularly Figure 1.

See allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder, Apache 2.0

Args:
field: The field in samples this encoder will work on.
embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
num_filters: This is the output dim for each convolutional layer, which is the number of ""filters""
learned by that layer.
ngram_filter_sizes: This specifies both the number of convolutional layers we will create and their sizes.  The
default of `(2, 3, 4, 5)` will have four convolutional layers, corresponding to encoding
ngrams of size 2 to 5 with some number of filters.
conv_layer_activation: `Activation`, optional (default=`torch.nn.ReLU`)
Activation to use after the convolution layers.
output_dim: After doing convolutions and pooling, we'll project the collected features into a vector of
this size.  If this value is `None`, we will just return the result of the max pooling,
giving an output of shape `len(ngram_filter_sizes) * num_filters`.
vocab_size: The size of character vocab.

Returns:
A tensor of shape `(batch_size, output_dim)`.
, Practitioners’"
https://github.com/hankcs/HanLP,char_cnn_tf.py,0,0.0,54,66.67,7,8.64,10,12.35,10,12.35,0,0.0,5,1,18,1,,"CharCNNEmbeddingTF, __init__, _keras_mask, base_config, call, char_embedding, char_vocab, chars, cnn, compute_output_shape, config, conv1d, dict, dim1, dim2, dim3, dropout, dtype, dynamic, embed, embedding, features, filters, final_shape, flat_shape, functools, get_config, hanlp, input_shape, inputs, items, kernel_size, kwargs, len, list, lookup, mask, masked_conv1d_and_max, name, ndims, pad_token, range, rate, self, shape, super, t_conv, t_max, tensorflow, to_tensor, trainable, weights, word_vocab, x","functools.reduce, hanlp.common.vocab_tf.VocabTF, hanlp.utils.tf_util.hanlp_register, tf.Tensor, tf.cast, tf.float32, tf.keras.layers.Conv1D, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.layers.Layer, tf.not_equal, tf.ragged.boolean_mask, tf.reduce_max, tf.reduce_min, tf.reshape, tf.shape, tf.strings.unicode_split","__init__, call, compute_output_shape, get_config, masked_conv1d_and_max",CharCNNEmbeddingTF,"base_config, chars, config, dim1, dim2, dim3, embed, features, final_shape, flat_shape, inputs, mask, ndims, shape, t, t_conv, t_max, weights","Applies 1d convolution and a masked max-pooling

https://github.com/guillaumegenthial/tf_ner/blob/master/models/chars_conv_lstm_crf/masked_conv.py

Args:
  t(tf.Tensor): A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]
  weights(tf.Tensor of tf.bool): A Tensor of shape [d1, d2, dn-1]
  filters(int): number of filters
  kernel_size(int): kernel size for the temporal convolution
  conv1d: 

Returns:","Applies 1d convolution and a masked max-pooling
    
    https://github.com/guillaumegenthial/tf_ner/blob/master/models/chars_conv_lstm_crf/masked_conv.py

    Args:
      t(tf.Tensor): A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]
      weights(tf.Tensor of tf.bool): A Tensor of shape [d1, d2, dn-1]
      filters(int): number of filters
      kernel_size(int): kernel size for the temporal convolution
      conv1d: 

    Returns:

    
    , UTF-8, char_embedding, dropout, filters, kernel_size, same","0.5, 1, 1.0, 100, 2, 3, 50, False, None, True","# -*- coding:utf-8 -*-, # Apply convolution, # Author: hankcs, # Date: 2019-12-20 21:15, # Get shape and parameters, # Reduce max -- set to zero if all padded, # Reshape input and apply weights, # Reshape the output, # Reshape weights, Applies 1d convolution and a masked max-pooling

https://github.com/guillaumegenthial/tf_ner/blob/master/models/chars_conv_lstm_crf/masked_conv.py

Args:
t(tf.Tensor): A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]
weights(tf.Tensor of tf.bool): A Tensor of shape [d1, d2, dn-1]
filters(int): number of filters
kernel_size(int): kernel size for the temporal convolution
conv1d:

Returns:


",
https://github.com/hankcs/HanLP,char_rnn.py,0,0.0,39,62.9,5,8.06,6,9.68,12,19.35,0,0.0,6,2,7,2,,"CharRNN, CharRNNEmbedding, ValueError, __init__, batch, char_mask, embed, embedding_dim, field, forward, gt, hanlp, hanlp_common, hidden_size, int, isinstance, len, lens, lstm, mask, masked_scatter_, max_word_length, module, ne, new_zeros, property, self, shape, size, sum, super, torch, transform, typing, unsqueeze, vocab_name, vocab_size, vocabs, x","hanlp.common.transform.ToChar, hanlp.common.transform.VocabDict, hanlp.common.vocab.Vocab, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.embedding.EmbeddingDim, hanlp_common.configurable.AutoConfigurable, nn.Embedding, nn.LSTM, nn.Module, torch.cat, torch.nn.utils.rnn.pack_padded_sequence, torch.unbind, typing.Callable, typing.Optional, typing.Union","__init__, embedding_dim, forward, module, transform, vocab_name","CharRNN, CharRNNEmbedding","char_mask, embed, h, lens, mask, vocab_name, x","Character level RNN embedding module builder.

Args:
    field: The field in samples this encoder will work on.
    embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
    hidden_size: The hidden size of RNNs.
    max_word_length: Character sequence longer than ``max_word_length`` will be truncated., Character level RNN embedding module.

Args:
    field: The field in samples this encoder will work on.
    vocab_size: The size of character vocab.
    embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
    hidden_size: The hidden size of RNNs.","Character level RNN embedding module builder.

        Args:
            field: The field in samples this encoder will work on.
            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
            hidden_size: The hidden size of RNNs.
            max_word_length: Character sequence longer than ``max_word_length`` will be truncated.
        , Character level RNN embedding module.

        Args:
            field: The field in samples this encoder will work on.
            vocab_size: The size of character vocab.
            embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
            hidden_size: The hidden size of RNNs.
        , Unrecognized type for , _char, _char_id","0, 1, 2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-02 23:49, # [batch_size, seq_len, fix_len], # [batch_size, seq_len, n_out], # [batch_size, seq_len], # [n, fix_len, n_embed], # [n, fix_len, n_out], # the embedding layer, # the lstm layer, Character level RNN embedding module builder.

Args:
field: The field in samples this encoder will work on.
embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
hidden_size: The hidden size of RNNs.
max_word_length: Character sequence longer than ``max_word_length`` will be truncated.
, Character level RNN embedding module.

Args:
field: The field in samples this encoder will work on.
vocab_size: The size of character vocab.
embed: An ``Embedding`` object or the feature size to create an ``Embedding`` object.
hidden_size: The hidden size of RNNs.
",
https://github.com/hankcs/HanLP,char_rnn_tf.py,0,0.0,47,71.21,5,7.58,10,15.15,4,6.06,0,0.0,3,1,18,0,,"CharRNNEmbeddingTF, __init__, _keras_mask, all_zeros, base_config, c_bw, c_fw, call, char_embedding, char_mask, char_mask_shape, char_rnn_units, char_vocab, chars, config, dict, dropout, dtype, dynamic, embed, embed_shape, embedding, get_config, h_bw, h_fw, hanlp, hidden, hole, inputs, items, kwargs, len, list, lookup, mask, name, non_all_zeros, output, pad_token, rate, rnn, self, super, tensorflow, to_tensor, trainable, word_vocab","hanlp.common.vocab_tf.VocabTF, hanlp.utils.tf_util.hanlp_register, tf.Tensor, tf.bool, tf.cast, tf.concat, tf.expand_dims, tf.int32, tf.keras.layers.Bidirectional, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.layers.LSTM, tf.keras.layers.Layer, tf.logical_or, tf.not_equal, tf.ragged.boolean_mask, tf.reduce_sum, tf.reshape, tf.shape, tf.strings.unicode_split, tf.zeros","__init__, call, get_config",CharRNNEmbeddingTF,"all_zeros, base_config, c_bw, c_fw, char_mask, char_mask_shape, chars, config, embed, embed_shape, h_bw, h_fw, hidden, hole, inputs, mask, non_all_zeros, output",,"UTF-8, bilstm, char_embedding, char_rnn_units, dropout","0, 0.5, 1, 100, 2, 25, 3, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-20 17:02, # hidden = output",
https://github.com/hankcs/HanLP,concat_embedding.py,0,0.0,31,77.5,1,2.5,5,12.5,3,7.5,0,0.0,6,1,9,0,,"ConcatEmbedding, __init__, append, base_config, build, call, compute_mask, compute_output_shape, config, dict, dim, dtype, dynamic, embed, embeddings, embeds, feature, get_config, hanlp, input_shape, inputs, isinstance, items, kwargs, list, name, self, super, supports_masking, tensorflow, trainable","hanlp.utils.tf_util.copy_mask, hanlp.utils.tf_util.hanlp_register, tf.concat, tf.keras.layers.Layer, tf.keras.utils.deserialize_keras_object","__init__, build, call, compute_mask, compute_output_shape, get_config",ConcatEmbedding,"base_config, config, dim, dynamic, embed, embeds, feature, mask, trainable",,embeddings,"0, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-20 17:08",
https://github.com/hankcs/HanLP,contextual_string_embedding.py,0,0.0,98,66.67,30,20.41,9,6.12,7,4.76,3,2.04,12,4,40,1,,"ContextualStringEmbedding, ContextualStringEmbeddingModule, ContextualStringEmbeddingTransform, RNNLanguageModel, __call__, __init__, __name__, _validate, append, args, b, b_chars, b_chars_id, b_ids, b_o, b_offset, b_offsets, bool, classmethod, config, cs, decode, dict, each, embed, embedding_dim, embedding_size, encoder, end_marker, extra_offset, f, f_chars, f_chars_id, f_ids, f_o, f_offset, f_offsets, field, file, flair, forward, gold, hanlp, hanlp_common, hidden_size, ids, input_text, int, is_forward_lm, items, key, keys, kwargs, len, lens, lm, load, load_language_model, load_state_dict, main, max, model, model_file, model_state, module, n_tokens, name, offset_backward, offset_forward, offsets, os, parameters, path, print, property, requires_grad_, rnn, rnn_output, run_lm, sample, save, self, sent, sentence_text, sents, src, state, state_dict, str, super, tests, token, tokens, torch, trainable, transform, typing, zip","hanlp.common.transform.FieldToIndex, hanlp.common.transform.TransformList, hanlp.common.vocab.Vocab, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.embedding.EmbeddingDim, hanlp.utils.io_util.get_resource, hanlp.utils.torch_util.batched_index_select, hanlp.utils.torch_util.pad_lists, hanlp_common.configurable.Configurable, nn.Embedding, nn.LSTM, nn.Module, os.path.join, tests.cdroot, torch.LongTensor, torch.Tensor, torch.allclose, torch.cat, torch.load, torch.nn.utils.rnn.pack_padded_sequence, torch.nn.utils.rnn.pad_packed_sequence, torch.save, typing.Callable, typing.Dict, typing.List","__call__, __init__, _validate, embed, embedding_dim, forward, load_language_model, main, module, run_lm, save, transform","ContextualStringEmbedding, ContextualStringEmbeddingModule, ContextualStringEmbeddingTransform, RNNLanguageModel","args, b, b_chars, b_ids, b_o, b_offsets, cs, each, emb, embed, end_marker, extra_offset, f, f_chars, f_ids, f_o, f_offsets, flair, gold, input_text, key, keys, lens, model, model_file, model_state, name, offset_backward, offset_forward, path, rnn_output, sample, sent, sentence_text, state, token, tokens, transform, vocab, x","Container module with an encoder, a recurrent module, and a decoder.","
,  , , , /home/hhe43/flair/gold.pt, /home/hhe43/flair/item2idx.pt, Container module with an encoder, a recurrent module, and a decoder., FASTTEXT_DEBUG_EMBEDDING_EN, FLAIR_LM_WMT11_EN, I love Berlin ., _, __main__, _b_char, _b_offset, _f_char, _f_offset, b_char_id, b_offset, backward.pt, embedding_size, f_char_id, f_chars, b_chars, f_offsets, b_offsets, f_offset, forward.pt, hidden_size, is_forward_lm, n_tokens, state_dict, token, token_, vocab.json","0, 1, 100, 1e-06, 2048, 4, False, None, True","#, # Flair is licensed under the following MIT License (MIT) Copyright © 2018 Zalando SE, https://tech.zalando.com, # Most codes of this file is adopted from flair, which is licenced under:, # Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:, # THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE., # The MIT License (MIT), # The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.","Flair is licensed under the following MIT License (MIT) Copyright © 2018 Zalando SE, https://tech.zalando.com, Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:, THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
https://github.com/hankcs/HanLP,contextual_string_embedding_tf.py,0,0.0,79,79.8,9,9.09,6,6.06,5,5.05,0,0.0,10,1,28,1,,"ContextualStringEmbeddingTF, __init__, _bw, _fw, _get_raw_string, _load_lm, _run_rnn, all, any, append, backward, backward_model, backward_model_path, base_config, bw, call, chars, compute_mask, compute_output_shape, config, dict, dim, dtype, dynamic, embed, embeddings, end, enumerate, filepath, forward_model, forward_model_path, fw, get_config, hanlp, hanlp_common, hidden, idx, input_shape, inputs, inputs_to_dataset, isinstance, items, kwargs, layers, len, lm, load, max, max_word_len, maxlen, model, model_from_config, name, numpy, off, offsets, output_dim, outputs, predict, property, raw, raw_string, self, sent, start, str_inputs, super, supports_masking, tensorflow, texts, tokenize_func, tokenizer, trainable, transform, typing, whitespace_after, word, x, zip","hanlp.components.rnn_language_model_tf.RNNLanguageModel, hanlp.utils.io_util.get_resource, hanlp.utils.tf_util.copy_mask, hanlp.utils.tf_util.hanlp_register, hanlp.utils.tf_util.str_tensor_2d_to_list, hanlp_common.constant.PAD, hanlp_common.util.infer_space_after, np.stack, np.zeros_like, tf.concat, tf.keras.Sequential, tf.keras.layers.LSTM, tf.keras.layers.Layer, tf.not_equal, tf.reverse, tf.stack, typing.List","__init__, _get_raw_string, _load_lm, _run_rnn, call, compute_mask, compute_output_shape, embed, get_config, output_dim",ContextualStringEmbeddingTF,"backward, base_config, bw, chars, config, dim, embed, embeddings, end, filepath, fw, hidden, idx, inputs, lm, maxlen, off, offsets, outputs, raw, raw_string, sent, start, str_inputs, tokenizer, whitespace_after, word, x","Embedding sentences (list of words) with contextualized string embedding

Args:
  texts: List of words, not chars
  texts: List[List[str]]: 

Returns:"," , At least one model is required, ContextualStringEmbedding works only in eager mode, Embedding sentences (list of words) with contextualized string embedding

        Args:
          texts: List of words, not chars
          texts: List[List[str]]: 

        Returns:

        
        , backward_model_path, forward, forward_model_path, max_word_len, rnn_units","0, 1, 10, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-19 03:24, # discard dense layer, Embedding sentences (list of words) with contextualized string embedding

Args:
texts: List of words, not chars
texts: List[List[str]]:

Returns:


",
https://github.com/hankcs/HanLP,contextual_word_embedding.py,0,0.0,58,70.73,4,4.88,3,3.66,17,20.73,0,0.0,8,2,7,2,,"ContextualWordEmbedding, ContextualWordEmbeddingModule, Module, ModuleList, __init__, _tokenizer_transform, _transformer_tokenizer, average_subwords, batch, child, cls_is_bos, config, device, dict, do_basic_tokenize, field, find_transformer, float, forward, found, from_pretrained, get, get_device, get_output_dim, get_tokenizer, hanlp, hanlp_common, hidden_size, input_ids, int, isinstance, kwargs, max_sequence_length, module, next, output, parameters, ret_prefix_mask, ret_raw_hidden_states, ret_subtokens, ret_subtokens_group, ret_token_span, scalar_mix, self, sep_is_eos, str, super, token_span, torch, trainable, training, transform, transformer, transformer_args, truncate_long_sequences, typing, use_fast, word_dropout","hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.layers.transformers.encoder.TransformerEncoder, hanlp.layers.transformers.pt_imports.AutoConfig_, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp_common.configurable.AutoConfigurable, torch.LongTensor, torch.Tensor, torch.device, torch.nn, typing.Any, typing.Dict, typing.List, typing.Optional, typing.Tuple, typing.Union","__init__, find_transformer, forward, get_device, get_output_dim, get_tokenizer, module, transform","ContextualWordEmbedding, ContextualWordEmbeddingModule","child, config, device, found, input_ids, output, token_span","A contextual word embedding builder which builds a
:class:`~hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule` and a
:class:`~hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer`.

Args:
    field: The field to work on. Usually some token fields.
    transformer:  An identifier of a ``PreTrainedModel``.
    average_subwords: ``True`` to average subword representations.
    scalar_mix: Layer attention.
    word_dropout: Dropout rate of randomly replacing a subword with MASK.
    max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
        window.
    truncate_long_sequences: ``True`` to return hidden states of each layer.
    cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
                ``False`` (default) means the first token is not [CLS], it will have its own embedding other than
                the embedding of [CLS].
    sep_is_eos: ``True`` means the last token of input is [SEP].
                ``False`` means it's not but [SEP] will be appended,
                ``None`` means it dependents on `input[-1] == [EOS]`.
    ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
    ret_subtokens: ``True`` to return list of subtokens belonging to each token.
    ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
    ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
    ret_raw_hidden_states: ``True`` to return hidden states of each layer.
    transformer_args: Extra arguments passed to the transformer.
    use_fast: Whether or not to try to load the fast version of the tokenizer.
    do_basic_tokenize: Whether to do basic tokenization before wordpiece.
    trainable: ``False`` to use static embeddings., A contextualized word embedding module.

Args:
    field: The field to work on. Usually some token fields.
    transformer:  An identifier of a ``PreTrainedModel``.
    transformer_tokenizer:
    average_subwords: ``True`` to average subword representations.
    scalar_mix: Layer attention.
    word_dropout: Dropout rate of randomly replacing a subword with MASK.
    max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
        window.
    ret_raw_hidden_states: ``True`` to return hidden states of each layer.
    transformer_args: Extra arguments passed to the transformer.
    trainable: ``False`` to use static embeddings.
    training: ``False`` to skip loading weights from pre-trained transformers.","A contextual word embedding builder which builds a
        :class:`~hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule` and a
        :class:`~hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer`.

        Args:
            field: The field to work on. Usually some token fields.
            transformer:  An identifier of a ``PreTrainedModel``.
            average_subwords: ``True`` to average subword representations.
            scalar_mix: Layer attention.
            word_dropout: Dropout rate of randomly replacing a subword with MASK.
            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
                window.
            truncate_long_sequences: ``True`` to return hidden states of each layer.
            cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
                        ``False`` (default) means the first token is not [CLS], it will have its own embedding other than
                        the embedding of [CLS].
            sep_is_eos: ``True`` means the last token of input is [SEP].
                        ``False`` means it's not but [SEP] will be appended,
                        ``None`` means it dependents on `input[-1] == [EOS]`.
            ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
            ret_subtokens: ``True`` to return list of subtokens belonging to each token.
            ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
            ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
            ret_raw_hidden_states: ``True`` to return hidden states of each layer.
            transformer_args: Extra arguments passed to the transformer.
            use_fast: Whether or not to try to load the fast version of the tokenizer.
            do_basic_tokenize: Whether to do basic tokenization before wordpiece.
            trainable: ``False`` to use static embeddings.
        , A contextualized word embedding module.

        Args:
            field: The field to work on. Usually some token fields.
            transformer:  An identifier of a ``PreTrainedModel``.
            transformer_tokenizer:
            average_subwords: ``True`` to average subword representations.
            scalar_mix: Layer attention.
            word_dropout: Dropout rate of randomly replacing a subword with MASK.
            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
                window.
            ret_raw_hidden_states: ``True`` to return hidden states of each layer.
            transformer_args: Extra arguments passed to the transformer.
            trainable: ``False`` to use static embeddings.
            training: ``False`` to skip loading weights from pre-trained transformers.
        , _input_ids, _token_span","False, None, True","#         output = [x.to(input_device) for x in output], #         output = output.to(input_device), #     else:, #     if isinstance(output, torch.Tensor):, #     input_ids = input_ids.to(this_device), #     token_span = token_span.to(this_device), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-05 13:50, # We might want to apply mask here, # if input_device != this_device:, # input_device = input_ids.device, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, # this_device = self.get_device(), A contextual word embedding builder which builds a
:class:`~hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule` and a
:class:`~hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer`.

Args:
field: The field to work on. Usually some token fields.
transformer:  An identifier of a ``PreTrainedModel``.
average_subwords: ``True`` to average subword representations.
scalar_mix: Layer attention.
word_dropout: Dropout rate of randomly replacing a subword with MASK.
max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
window.
truncate_long_sequences: ``True`` to return hidden states of each layer.
cls_is_bos: ``True`` means the first token of input is treated as [CLS] no matter what its surface form is.
``False`` (default) means the first token is not [CLS], it will have its own embedding other than
the embedding of [CLS].
sep_is_eos: ``True`` means the last token of input is [SEP].
``False`` means it's not but [SEP] will be appended,
``None`` means it dependents on `input[-1] == [EOS]`.
ret_token_span: ``True`` to return span of each token measured by subtoken offsets.
ret_subtokens: ``True`` to return list of subtokens belonging to each token.
ret_subtokens_group: ``True`` to return list of offsets of subtokens belonging to each token.
ret_prefix_mask: ``True`` to generate a mask where each non-zero element corresponds to a prefix of a token.
ret_raw_hidden_states: ``True`` to return hidden states of each layer.
transformer_args: Extra arguments passed to the transformer.
use_fast: Whether or not to try to load the fast version of the tokenizer.
do_basic_tokenize: Whether to do basic tokenization before wordpiece.
trainable: ``False`` to use static embeddings.
, A contextualized word embedding module.

Args:
field: The field to work on. Usually some token fields.
transformer:  An identifier of a ``PreTrainedModel``.
transformer_tokenizer:
average_subwords: ``True`` to average subword representations.
scalar_mix: Layer attention.
word_dropout: Dropout rate of randomly replacing a subword with MASK.
max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
window.
ret_raw_hidden_states: ``True`` to return hidden states of each layer.
transformer_args: Extra arguments passed to the transformer.
trainable: ``False`` to use static embeddings.
training: ``False`` to skip loading weights from pre-trained transformers.
",
https://github.com/hankcs/HanLP,embedding.py,0,0.0,46,73.02,5,7.94,2,3.17,10,15.87,0,0.0,9,4,10,5,,"ConcatModuleList, Embedding, EmbeddingDim, EmbeddingList, Module, ModuleList, __init__, _embeddings, abc, append, batch, child, cls, config, dict, dropout, each, embed, embedding_dim, embeddings, embeddings_, embeds, find_embedding_by_class, forward, found, from_config, get_output_dim, hanlp, hanlp_common, int, isinstance, kwargs, list, module, modules, property, remove, self, sum, super, to_list, torch, transform, transforms, typing, x","abc.ABC, abc.abstractmethod, hanlp.common.transform.TransformList, hanlp.layers.dropout.IndependentDropout, hanlp_common.configurable.AutoConfigurable, torch.cat, torch.nn, torch.nn.Module, typing.Callable, typing.Iterable, typing.List, typing.Optional","__init__, embedding_dim, embeddings, find_embedding_by_class, forward, get_output_dim, module, to_list, transform","ConcatModuleList, Embedding, EmbeddingDim, EmbeddingList","child, dropout, each, embed, embeddings, embeds, found, modules, transforms, x","A ``nn.ModuleList`` to bundle several embeddings modules.

Args:
    *modules: Embedding layers.
    dropout: Dropout applied on the concatenated embedding., An embedding builder to bundle several embedding builders.

Args:
    *embeddings_: A list of embedding builders.
    embeddings: Deserialization for a dict of embedding builders.
    dropout: Dropout applied on the concatenated embedding., Base class for embedding builders., Build a module for this embedding.

Args:
    **kwargs: Containing vocabs, training etc. Not finalized for now.

Returns:
    A module., Build a transform function for this embedding.

Args:
    **kwargs: Containing vocabs, training etc. Not finalized for now.

Returns:
    A transform function.","
        Base class for embedding builders.
        , A ``nn.ModuleList`` to bundle several embeddings modules.

        Args:
            *modules: Embedding layers.
            dropout: Dropout applied on the concatenated embedding.
        , An embedding builder to bundle several embedding builders.

        Args:
            *embeddings_: A list of embedding builders.
            embeddings: Deserialization for a dict of embedding builders.
            dropout: Dropout applied on the concatenated embedding.
        , Build a module for this embedding.

        Args:
            **kwargs: Containing vocabs, training etc. Not finalized for now.

        Returns:
            A module.
        , Build a transform function for this embedding.

        Args:
            **kwargs: Containing vocabs, training etc. Not finalized for now.

        Returns:
            A transform function.
        ","1, None","
Base class for embedding builders.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-02 13:04, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, A ``nn.ModuleList`` to bundle several embeddings modules.

Args:
*modules: Embedding layers.
dropout: Dropout applied on the concatenated embedding.
, An embedding builder to bundle several embedding builders.

Args:
*embeddings_: A list of embedding builders.
embeddings: Deserialization for a dict of embedding builders.
dropout: Dropout applied on the concatenated embedding.
, Build a module for this embedding.

Args:
**kwargs: Containing vocabs, training etc. Not finalized for now.

Returns:
A module.
, Build a transform function for this embedding.

Args:
**kwargs: Containing vocabs, training etc. Not finalized for now.

Returns:
A transform function.
",
https://github.com/hankcs/HanLP,fast_text.py,0,0.0,66,70.21,17,18.09,4,4.26,7,7.45,0,0.0,20,6,11,3,,"FastTextDataset, FastTextEmbedding, FastTextEmbeddingComponent, FastTextEmbeddingModule, FastTextTransform, Module, NotImplementedError, SelectFromBatchModule, __call__, __class__, __init__, __name__, __repr__, _fasttext, _model, batch, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, dataloader, device, devices, dict, dst, each, embed, embedding_dim, evaluate_dataloader, execute_training_loop, fasttext, filepath, fit_dataloader, get_output_dim, hanlp, hanlp_common, int, isinstance, key, kwargs, load_file, load_vocabs, load_weights, logging, mask, model, module, os, output_dim, outputs, predict, property, sample, self, src, str, super, sys, to, torch, transform, typing, vector, word","fasttext.load_model, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.TransformableDataset, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.EmbeddingNamedTransform, hanlp.common.vocab.Vocab, hanlp.layers.embeddings.embedding.Embedding, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.stdout_redirected, hanlp.utils.log_util.flash, hanlp_common.configurable.AutoConfigurable, logging.Logger, os.devnull, sys.stderr, torch.device, torch.nn, torch.nn.Module, torch.nn.utils.rnn.pad_sequence, torch.stack, torch.tensor, torch.utils.data.DataLoader, typing.Callable, typing.Optional","__call__, __init__, __repr__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, devices, embed, evaluate_dataloader, execute_training_loop, fit_dataloader, get_output_dim, load_file, load_vocabs, load_weights, module, predict, transform","FastTextDataset, FastTextEmbedding, FastTextEmbeddingComponent, FastTextEmbeddingModule, FastTextTransform, SelectFromBatchModule","batch, dataloader, dataset, dst, each, filepath, output_dim, outputs, s, vector, word","An embedding layer builder for fastText (:cite:`bojanowski2017enriching`).

Args:
    src: Field name.
    filepath: Filepath to pretrained fastText embeddings., An embedding layer for fastText (:cite:`bojanowski2017enriching`).

Args:
    key: Field name.
    embedding_dim: Size of this embedding layer, Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

Args:
    **kwargs:",",  Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

        Args:
            **kwargs:
        ,  [blink][yellow]...[/yellow][/blink], (, ), , embedding_dim=, An embedding layer builder for fastText (:cite:`bojanowski2017enriching`).

        Args:
            src: Field name.
            filepath: Filepath to pretrained fastText embeddings.
        , An embedding layer for fastText (:cite:`bojanowski2017enriching`).

        Args:
            key: Field name.
            embedding_dim: Size of this embedding layer
        , Loading fasttext model , Not supported., _fasttext, cpu, key=, king, model.pt, token, vocabs.json","0, False, None, True"," Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

Args:
**kwargs:
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-27 15:06, # It's a toy so doesn't really do batching, An embedding layer builder for fastText (:cite:`bojanowski2017enriching`).

Args:
src: Field name.
filepath: Filepath to pretrained fastText embeddings.
, An embedding layer for fastText (:cite:`bojanowski2017enriching`).

Args:
key: Field name.
embedding_dim: Size of this embedding layer
",
https://github.com/hankcs/HanLP,fast_text_tf.py,0,0.0,54,62.07,18,20.69,5,5.75,10,11.49,0,0.0,8,1,11,0,,"FastTextEmbeddingTF, __init__, _embed_np, _keras_mask, base_config, build, built, call, compute_mask, compute_output_shape, config, debug, decode, dict, embed, embed_fn, embed_np, encode, fasttext, filepath, get_config, hanlp, hanlp_common, hasattr, input_shape, inputs, isinstance, items, junk, kwargs, len, list, mask, mask_zero, masks, model, name, numpy, os, output, output_dim, outputs, padding, pop, reshape, self, shape, shape_type_conversion, str, super, sys, tensorflow, word, words","fasttext.load_model, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.stdout_redirected, hanlp.utils.log_util.logger, hanlp.utils.tf_util.hanlp_register, hanlp_common.constant.PAD, np.frompyfunc, np.ndarray, np.stack, os.devnull, os.path.basename, os.path.isfile, os.path.splitext, sys.stderr, tensorflow.python.keras.utils.tf_utils, tf.Tensor, tf.constant, tf.expand_dims, tf.float32, tf.keras.layers.Embedding, tf.not_equal, tf.string, tf.tile, tf.zeros_like","__init__, build, call, compute_mask, compute_output_shape, embed, embed_np, get_config",FastTextEmbeddingTF,"base_config, config, embed_fn, filepath, inputs, junk, mask, masks, name, output, outputs",," is not a file, Loading fasttext model from [{}]., Resolved path , activity_regularizer, batch_input_shape, embeddings_constraint, embeddings_initializer, embeddings_regularizer, filepath, input_dim, input_length, king, mask_zero, numpy, output_dim, padding, trainable, utf-8","0, 1, False, None, True","#     seq_len = 1, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-29 13:14, # fasttext print a blank line here, # if not seq_len:, # placeholder tensor, # return tf.zeros([1, seq_len, self.output_dim]), # seq_len = inputs.shape[-1], # vf = np.vectorize(self.embed, otypes=[np.ndarray])",
https://github.com/hankcs/HanLP,util.py,0,0.0,38,62.3,10,16.39,7,11.48,6,9.84,0,0.0,2,0,10,2,,"Embedding, ValueError, append, build_word2vec_with_vocab, extend_vocab, filepath, from_pretrained, get, get_idx, hanlp, ids, idx, index_select, index_word2vec_with_vocab, int, isinstance, items, len, lock, lower, lowercase, normalize, pad_idx, pop, pret_matrix, pret_vocab, safe_unk_token, size, str, token_to_idx, torch, trainable, typing, unk, unk_embeds, unk_id_offset, unlock, word_id","hanlp.common.vocab.Vocab, hanlp.utils.init_util.embedding_uniform, hanlp.utils.torch_util.load_word2vec, hanlp.utils.torch_util.load_word2vec_as_vocab_tensor, torch.LongTensor, torch.Tensor, torch.cat, torch.nn, torch.nn.functional.normalize, torch.norm, torch.std, torch.zeros, typing.Union","build_word2vec_with_vocab, index_word2vec_with_vocab",,"embed, embedding, ids, idx, init, pret_matrix, pret_vocab, unk_embeds, unk_id_offset, word_id","Args:
    filepath: The path to pretrained embedding.
    vocab: The vocabulary from training set.
    extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
    unk: UNK token.
    lowercase: Convert words in pretrained embeddings into lowercase.
    init: Indicate which initialization to use for oov tokens.
    normalize: ``True`` or a method to normalize the embedding matrix.

Returns:
    An embedding matrix., Build word2vec embedding and a vocab.

Args:
    embed:
    vocab: The vocabulary from training set.
    extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
    unk: UNK token.
    lowercase: Convert words in pretrained embeddings into lowercase.
    trainable: ``False`` to use static embeddings.
    init: Indicate which initialization to use for oov tokens.
    normalize: ``True`` or a method to normalize the embedding matrix.

Returns:
    An embedding matrix.","

    Args:
        filepath: The path to pretrained embedding.
        vocab: The vocabulary from training set.
        extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
        unk: UNK token.
        lowercase: Convert words in pretrained embeddings into lowercase.
        init: Indicate which initialization to use for oov tokens.
        normalize: ``True`` or a method to normalize the embedding matrix.

    Returns:
        An embedding matrix.

    , Build word2vec embedding and a vocab.

    Args:
        embed:
        vocab: The vocabulary from training set.
        extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
        unk: UNK token.
        lowercase: Convert words in pretrained embeddings into lowercase.
        trainable: ``False`` to use static embeddings.
        init: Indicate which initialization to use for oov tokens.
        normalize: ``True`` or a method to normalize the embedding matrix.

    Returns:
        An embedding matrix.

    , Unsupported init , Unsupported normalization method , Unsupported parameter type: , l2, norm, std, uniform, zeros","0, 1, 1e-12, 2, False, None, True","

Args:
filepath: The path to pretrained embedding.
vocab: The vocabulary from training set.
extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
unk: UNK token.
lowercase: Convert words in pretrained embeddings into lowercase.
init: Indicate which initialization to use for oov tokens.
normalize: ``True`` or a method to normalize the embedding matrix.

Returns:
An embedding matrix.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-09 15:45, # Retry lower case, Build word2vec embedding and a vocab.

Args:
embed:
vocab: The vocabulary from training set.
extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
unk: UNK token.
lowercase: Convert words in pretrained embeddings into lowercase.
trainable: ``False`` to use static embeddings.
init: Indicate which initialization to use for oov tokens.
normalize: ``True`` or a method to normalize the embedding matrix.

Returns:
An embedding matrix.

",
https://github.com/hankcs/HanLP,util_tf.py,0,0.0,31,51.67,12,20.0,5,8.33,12,20.0,0,0.0,4,0,5,0,,"__class__, __name__, _upgrade, any_embedding_in, array_ks, build_embedding, char_vocab, cls, config, dict, embed_name, embedding_trainable, embeddings_require_char_input, embeddings_require_string_input, endswith, get, hanlp, int, isinstance, items, len, list, map_x, names, run_eagerly, set, startswith, tensorflow, typing, update, word_vocab","hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.layers.embeddings.char_cnn_tf.CharCNNEmbeddingTF, hanlp.layers.embeddings.char_rnn_tf.CharRNNEmbeddingTF, hanlp.layers.embeddings.concat_embedding.ConcatEmbedding, hanlp.layers.embeddings.contextual_string_embedding_tf.ContextualStringEmbeddingTF, hanlp.layers.embeddings.fast_text_tf.FastTextEmbeddingTF, hanlp.layers.embeddings.word2vec_tf.StringWord2VecEmbeddingTF, hanlp.layers.embeddings.word2vec_tf.Word2VecEmbeddingTF, hanlp.layers.embeddings.word2vec_tf.Word2VecEmbeddingV1, tf.keras.layers.Embedding, tf.keras.utils.deserialize_keras_object, tf.keras.utils.get_custom_objects, typing.Union","any_embedding_in, build_embedding, embeddings_require_char_input, embeddings_require_string_input",,"_upgrade, config, embed_name, embeddings, names",,">, FastTextEmbedding, FastTextEmbeddingTF, HanLP>, TF, char_vocab, class_name, config, embedding should be str or int or dict, embedding_trainable, vocab, word_vocab","1, 2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-09 15:46, # Embedding specific configuration, # Embeddings need vocab, # Upgrade to 2.1, # Vocab won't present in the dict, # fasttext accept string instead of int, # fasttext can only run in eager mode, # those embeddings require string as input, # use the string version of Word2VecEmbedding instead, # word_vocab.unlock()",
https://github.com/hankcs/HanLP,word2vec.py,0,0.0,138,73.02,27,14.29,7,3.7,17,8.99,0,0.0,24,6,42,4,,"Embedding, GazetteerEmbedding, GazetterTransform, Module, NotImplementedError, Word2VecDataset, Word2VecEmbedding, Word2VecEmbeddingComponent, Word2VecEmbeddingModule, __call__, __init__, _apply, _matrix, _remove_short_tokens, _tokenize, _tokenizer, _vocab, append, batch, batch_size, block_word_id, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, count_nonzero, cpu, dataloader, device, dict, direction, doc2vec, embed, embedding_dim, evaluate_dataloader, execute_training_loop, ext_mask, ext_words, extend_vocab, field, filename, fit_dataloader, flat, float, fn, forward, from_pretrained, ge, hanlp, hanlp_common, hanlp_trie, ids, idx, idx_to_token, index_select, indices, init, int, isfile, isinstance, items, kwargs, len, lens, lexicons, list, load_config, load_file, load_vocabs, load_weights, logging, lowercase, map, masked_fill, math, matrix, model, module, most_similar, n_embed, n_words, normalize, num_embeddings, num_tokens_in_trn, object, os, pad, pad_idx, pad_token, padding, parse, parse_longest, path, predict, property, results, sample, save_dir, sc, scores, second_channel, self, similarities, similarity_less_than, size, skips_l2r, skips_r2l, staticmethod, str, sum, super, to, token_id, token_is_unk, tokenizer, tokens, tolist, topk, torch, trainable, transform, trie, typing, unk, unk_idx, unk_token, unsqueeze, update, value, vocabs, weight, word_dropout, words, x, zeros_, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.TransformableDataset, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.VocabDict, hanlp.common.vocab.Vocab, hanlp.layers.dropout.WordDropout, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.embedding.EmbeddingDim, hanlp.layers.embeddings.util.build_word2vec_with_vocab, hanlp.utils.log_util.flash, hanlp.utils.torch_util.load_word2vec_as_vocab_tensor, hanlp_common.configurable.AutoConfigurable, hanlp_common.constant.HANLP_VERBOSE, hanlp_trie.trie.Trie, logging.Logger, math.inf, torch.Tensor, torch.arange, torch.nn, torch.nn.Module, torch.nn.functional.cosine_similarity, torch.no_grad, torch.tensor, torch.utils.data.DataLoader, typing.Callable, typing.Dict, typing.List, typing.Optional, typing.Union","__call__, __init__, _apply, _remove_short_tokens, _tokenize, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, embedding_dim, evaluate_dataloader, execute_training_loop, fit_dataloader, forward, load_config, load_file, load_vocabs, load_weights, module, most_similar, predict, tokenizer, transform","GazetteerEmbedding, GazetterTransform, Word2VecDataset, Word2VecEmbedding, Word2VecEmbeddingComponent, Word2VecEmbeddingModule","_matrix, _vocab, batch, block_word_id, dataloader, dataset, device, direction, e, embed, embeddings, ext_mask, ext_words, flat, ids, idx, indices, lens, lexicons, matrix, model, n_embed, n_words, num_tokens_in_trn, padding, results, sc, scores, second_channel, similarities, skips_l2r, skips_r2l, token_id, token_is_unk, tokens, unk, value, vocab, word2vec, word_dropout, words, x","A word2vec style embedding builder which maps a token to its embedding through looking up a pre-defined
table.

Args:
    field: The field to work on. Usually some token fields.
    embed: A path to pre-trained embedding file or an integer defining the size of randomly initialized
        embedding.
    extend_vocab: Unlock vocabulary of training set to add those tokens in pre-trained embedding file.
    pad: The padding token.
    unk: The unknown token.
    lowercase: Convert words in pretrained embeddings into lowercase.
    trainable: ``False`` to use static embeddings.
    second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
    word_dropout: The probability of randomly replacing a token with ``UNK``.
    normalize: ``l2`` or ``std`` to normalize the embedding matrix.
    cpu: Reside on CPU instead of GPU.
    init: Indicate which initialization to use for oov tokens., A word2vec style embedding module which maps a token to its embedding through looking up a pre-defined table.

Args:
    field: The field to work on. Usually some token fields.
    embed: An ``Embedding`` layer.
    word_dropout: The probability of randomly replacing a token with ``UNK``.
    cpu: Reside on CPU instead of GPU.
    second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
    num_tokens_in_trn: The number of tokens in training set.
    unk_idx: The index of ``UNK``., Find the `topk` most similar words of a given word or phrase.

Args:
    words: A word or phrase or multiple words/phrases.
    topk: Number of top similar words.
    doc2vec: Enable doc2vec model for processing OOV and phrases.
    similarity_less_than: Only return words with a similarity less than this value.
    batch_size: Number of words or phrases per batch.

Returns:
    Similar words and similarities stored in a dict., Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

Args:
    **kwargs:",",  Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

        Args:
            **kwargs:
        ,  has to be set in order to make use of word_dropout, A word2vec style embedding builder which maps a token to its embedding through looking up a pre-defined
        table.

        Args:
            field: The field to work on. Usually some token fields.
            embed: A path to pre-trained embedding file or an integer defining the size of randomly initialized
                embedding.
            extend_vocab: Unlock vocabulary of training set to add those tokens in pre-trained embedding file.
            pad: The padding token.
            unk: The unknown token.
            lowercase: Convert words in pretrained embeddings into lowercase.
            trainable: ``False`` to use static embeddings.
            second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
            word_dropout: The probability of randomly replacing a token with ``UNK``.
            normalize: ``l2`` or ``std`` to normalize the embedding matrix.
            cpu: Reside on CPU instead of GPU.
            init: Indicate which initialization to use for oov tokens.
        , A word2vec style embedding module which maps a token to its embedding through looking up a pre-defined table.

        Args:
            field: The field to work on. Usually some token fields.
            embed: An ``Embedding`` layer.
            word_dropout: The probability of randomly replacing a token with ``UNK``.
            cpu: Reside on CPU instead of GPU.
            second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
            num_tokens_in_trn: The number of tokens in training set.
            unk_idx: The index of ``UNK``.
        , Building Trie-based tokenizer for Doc2Vec [blink][yellow]...[/yellow][/blink], Find the `topk` most similar words of a given word or phrase.

        Args:
            words: A word or phrase or multiple words/phrases.
            topk: Number of top similar words.
            doc2vec: Enable doc2vec model for processing OOV and phrases.
            similarity_less_than: Only return words with a similarity less than this value.
            batch_size: Number of words or phrases per batch.

        Returns:
            Similar words and similarities stored in a dict.
        , Not supported., _, _count, _id, _offset, block_word_id, char, classpath, config.json, embed, hanlp.layers.embeddings.word2vec.Word2VecEmbeddingComponent, l2, model.pt, skips_l2r, skips_r2l, token, token_id, unk_token of vocab , vocabs.json, zeros","0, 1, 10, 32, False, None, True"," Toy example of Word2VecEmbedding. It simply returns the embedding of a given word

Args:
**kwargs:
, #         return super(Word2VecEmbeddingModule, self).to(-1, **kwargs), #     exit(1), #     if self.cpu:, #     print(self.cpu), #     return super(Word2VecEmbeddingModule, self).to(device, **kwargs), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-09 13:38, # It's a toy so doesn't really do batching, # This might block all fn not limiting to moving between devices., # def to(self, device, **kwargs):, # noinspection PyMethodOverriding, # noinspection PyUnboundLocalVariable, A word2vec style embedding builder which maps a token to its embedding through looking up a pre-defined
table.

Args:
field: The field to work on. Usually some token fields.
embed: A path to pre-trained embedding file or an integer defining the size of randomly initialized
embedding.
extend_vocab: Unlock vocabulary of training set to add those tokens in pre-trained embedding file.
pad: The padding token.
unk: The unknown token.
lowercase: Convert words in pretrained embeddings into lowercase.
trainable: ``False`` to use static embeddings.
second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
word_dropout: The probability of randomly replacing a token with ``UNK``.
normalize: ``l2`` or ``std`` to normalize the embedding matrix.
cpu: Reside on CPU instead of GPU.
init: Indicate which initialization to use for oov tokens.
, A word2vec style embedding module which maps a token to its embedding through looking up a pre-defined table.

Args:
field: The field to work on. Usually some token fields.
embed: An ``Embedding`` layer.
word_dropout: The probability of randomly replacing a token with ``UNK``.
cpu: Reside on CPU instead of GPU.
second_channel: A trainable second channel for each token, which will be added to pretrained embeddings.
num_tokens_in_trn: The number of tokens in training set.
unk_idx: The index of ``UNK``.
, Find the `topk` most similar words of a given word or phrase.

Args:
words: A word or phrase or multiple words/phrases.
topk: Number of top similar words.
doc2vec: Enable doc2vec model for processing OOV and phrases.
similarity_less_than: Only return words with a similarity less than this value.
batch_size: Number of words or phrases per batch.

Returns:
Similar words and similarities stored in a dict.
",
https://github.com/hankcs/HanLP,word2vec_tf.py,0,0.0,91,73.39,13,10.48,7,5.65,13,10.48,0,0.0,11,3,20,0,,"StringWord2VecEmbeddingTF, Word2VecEmbeddingTF, Word2VecEmbeddingV1, __getitem__, __init__, _embeddings_initializer, _load, _output_dim, activity_regularizer, array_ks, array_np, array_tf, base_config, bias, bool, call, compute_mask, compute_output_shape, config, cpu, dict, dim, dtype, dynamic, embeddings_constraint, embeddings_initializer, embeddings_regularizer, enumerate, expand_vocab, filepath, get, get_config, get_idx, get_idx_without_add, get_vector, hanlp, hanlp_common, idx, input_dim, input_length, input_shape, inputs, isinstance, items, keys, kwargs, len, list, load_all, lock, locked, lookup, lower, lowercase, mask_zero, mutable, name, normalize, not_equal, numpy, os, output_dim, pad_idx, pad_token, path, pop, pret_embs, property, safe_unk_token, scale, self, shape, size, state, staticmethod, str, super, supports_masking, tensorflow, token, token_to_idx, token_to_idx_table, trainable, typing, unk, unk_idx, unlock, update, vectors, vocab, word_ids","hanlp.common.vocab_tf.VocabTF, hanlp.utils.io_util.get_resource, hanlp.utils.tf_util.hanlp_register, hanlp.utils.torch_util.load_word2vec, hanlp_common.util.DummyContext, np.float32, np.ndarray, np.random.get_state, np.random.seed, np.random.set_state, np.random.uniform, np.sqrt, np.std, np.zeros, os.path.basename, os.path.splitext, tensorflow.python.ops.math_ops, tf.Tensor, tf.device, tf.int32, tf.int64, tf.keras.initializers.Constant, tf.keras.initializers.get, tf.keras.layers.Embedding, tf.keras.layers.Layer, tf.nn.embedding_lookup, tf.not_equal, tf.string, typing.List, typing.Tuple, typing.Union","__getitem__, __init__, _load, call, compute_mask, compute_output_shape, dim, get_config, get_vector, shape, size","StringWord2VecEmbeddingTF, Word2VecEmbeddingTF, Word2VecEmbeddingV1","_output_dim, base_config, bias, config, dim, embeddings_initializer, filepath, idx, input_dim, inputs, name, output_dim, pret_embs, scale, state, token, vec, vectors, vocab, word_ids",," does not match ,  in., . , Expect tf.string but got tf., Please pass tf., VarianceScaling, cpu:0, embeddings_initializer, expand_vocab, filepath, input_dim = , lowercase, output_dim = ","0, 0.001, 1, 3.0, False, None, True","#     vec += bias, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-24 21:49, # Retry lower case, # if the `unk` token exists in the pretrained,, # if vec is not None:, # init matrix, # inputs._keras_mask = tf.not_equal(inputs, self.vocab.pad_idx), # insert to pret_embs, # noinspection PyTypeChecker, # then replace it with a self-defined one, usually the one in word vocab, # vec = np.random.uniform(-scale, scale, [dim])",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-24 21:48",
https://github.com/hankcs/HanLP,encoder.py,0,0.0,63,75.0,7,8.33,5,5.95,9,10.71,0,0.0,4,1,16,1,,"Module, TransformerEncoder, __init__, append, attention_mask, average_subwords, bool, build, build_transformer, build_transformer_tokenizer, cls, cls_token_id, config, config_or_str, dict, do_basic_tokenize, embeddings, encoder, enumerate, excludes, extend, forward, from_pretrained, hanlp, hasattr, input_ids, int, isinstance, kwargs, layer, mask_token_id, max_position_embeddings, max_sequence_length, mixture_range, oov, output_hidden_states, pad, pad_token_id, raw_hidden_states, replacement, requires_grad_, ret_raw_hidden_states, scalar_mix, self, sep, sep_token_id, staticmethod, str, super, token_span, token_type_ids, torch, trainable, training, transformer, transformer_args, transformer_tokenizer, tuple, typing, unk_token_id, use_fast, warnings, word_dropout","hanlp.layers.dropout.WordDropout, hanlp.layers.scalar_mix.ScalarMixWithDropout, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.layers.transformers.pt_imports.AutoModel_, hanlp.layers.transformers.pt_imports.AutoTokenizer, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.pt_imports.BertTokenizer, hanlp.layers.transformers.pt_imports.PreTrainedModel, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.layers.transformers.resource.get_tokenizer_mirror, hanlp.layers.transformers.utils.transformer_encode, torch.LongTensor, torch.nn, typing.Any, typing.Dict, typing.Optional, typing.Sequence, typing.Tuple, typing.Union","__init__, build_transformer, build_transformer_tokenizer, forward",TransformerEncoder,"cls, excludes, input_ids, kwargs, layers, max_sequence_length, oov, output_hidden_states, pad, raw_hidden_states, replacement, sep, transformer, transformer_args, word_dropout, x","A pre-trained transformer encoder.

Args:
    transformer: A ``PreTrainedModel`` or an identifier of a ``PreTrainedModel``.
    transformer_tokenizer: A ``PreTrainedTokenizer``.
    average_subwords: ``True`` to average subword representations.
    scalar_mix: Layer attention.
    word_dropout: Dropout rate of randomly replacing a subword with MASK.
    max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
        window. If ``None``, then the ``max_position_embeddings`` of the transformer will be used.
    ret_raw_hidden_states: ``True`` to return hidden states of each layer.
    transformer_args: Extra arguments passed to the transformer.
    trainable: ``False`` to use static embeddings.
    training: ``False`` to skip loading weights from pre-trained transformers.","A pre-trained transformer encoder.

        Args:
            transformer: A ``PreTrainedModel`` or an identifier of a ``PreTrainedModel``.
            transformer_tokenizer: A ``PreTrainedTokenizer``.
            average_subwords: ``True`` to average subword representations.
            scalar_mix: Layer attention.
            word_dropout: Dropout rate of randomly replacing a subword with MASK.
            max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
                window. If ``None``, then the ``max_position_embeddings`` of the transformer will be used.
            ret_raw_hidden_states: ``True`` to return hidden states of each layer.
            transformer_args: Extra arguments passed to the transformer.
            trainable: ``False`` to use static embeddings.
            training: ``False`` to skip loading weights from pre-trained transformers.
        , decoder, embeddings, encoder, mask, output_hidden_states, unk","0, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-22 21:06, # Electra English has to use unk, # For seq2seq model, use its encoder, # UDify uses [MASK], # noinspection PyAbstractClass, # noinspection PyUnboundLocalVariable, A pre-trained transformer encoder.

Args:
transformer: A ``PreTrainedModel`` or an identifier of a ``PreTrainedModel``.
transformer_tokenizer: A ``PreTrainedTokenizer``.
average_subwords: ``True`` to average subword representations.
scalar_mix: Layer attention.
word_dropout: Dropout rate of randomly replacing a subword with MASK.
max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
window. If ``None``, then the ``max_position_embeddings`` of the transformer will be used.
ret_raw_hidden_states: ``True`` to return hidden states of each layer.
transformer_args: Extra arguments passed to the transformer.
trainable: ``False`` to use static embeddings.
training: ``False`` to skip loading weights from pre-trained transformers.
",
https://github.com/hankcs/HanLP,loader_tf.py,0,0.0,19,63.33,4,13.33,4,13.33,3,10.0,0,0.0,1,0,8,0,,"build, build_transformer, from_pretrained, hanlp, l_bert, l_input_ids, l_mask_ids, l_token_type_ids, logits, max_seq_length, model, num_labels, output, seq, tagging, tensorflow, tokenizer, tokenizer_only, transformers","hanlp.layers.transformers.pt_imports.AutoModel_, hanlp.layers.transformers.pt_imports.AutoTokenizer_, tf.keras.Model, tf.keras.layers.Dense, tf.keras.layers.Input, tf.keras.layers.Lambda, transformers.TFAutoModel",build_transformer,,"l_bert, l_input_ids, l_mask_ids, l_token_type_ids, logits, model, output, tokenizer",,"input_ids, int32, mask_ids, token_type_ids","0, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-01-04 06:05",
https://github.com/hankcs/HanLP,pt_imports.py,0,0.0,24,55.81,9,20.93,2,4.65,8,18.6,0,0.0,1,3,5,0,,"AutoConfig_, AutoModel_, AutoTokenizer_, additional_config, classmethod, cls, dict, do_basic_tokenize, from_pretrained, hanlp, isinstance, kwargs, model_args, name_or_path, os, pretrained_model_name_or_path, startswith, str, super, training, transformer, transformers, use_fast, warnings","hanlp.layers.transformers.resource.get_model_mirror, hanlp.layers.transformers.resource.get_tokenizer_mirror, os.environ, os.environ.get, transformers.AlbertConfig, transformers.AutoConfig, transformers.AutoModel, transformers.AutoModelForSequenceClassification, transformers.AutoModelForTokenClassification, transformers.AutoTokenizer, transformers.BartModel, transformers.BertConfig, transformers.BertJapaneseTokenizer, transformers.BertModel, transformers.BertTokenizer, transformers.BertTokenizerFast, transformers.PreTrainedModel, transformers.PreTrainedTokenizer, transformers.PretrainedConfig, warnings.warn",from_pretrained,"AutoConfig_, AutoModel_, AutoTokenizer_","additional_config, cls, pretrained_model_name_or_path, tokenizer, transformer",,"Langboat/mengzi-bert-base, TOKENIZERS_PARALLELISM, `do_basic_tokenize=False` might not work when `use_fast=True`, basic, cl-tohoku/bert-base-japanese-char, false, uer/albert, voidful/albert_chinese_, word_tokenizer_type","None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-09 11:25, # Since it's char level model, it's OK to use char level tok instead of fugashi, # cls = BertJapaneseTokenizerFast, # cls = BertTokenizerFast, # from hanlp.utils.lang.ja.bert_tok import BertJapaneseTokenizerFast, # from transformers import BertTokenizerFast",
https://github.com/hankcs/HanLP,relative_transformer.py,0,0.0,98,56.98,15,8.72,13,7.56,31,18.02,15,8.72,6,5,34,9,,"AC, BD, BDE, B_, D_, Dropout, E, E_, LayerNorm, LeakyReLU, Linear, Module, ModuleList, Parameter, RelativeMultiHeadAttn, RelativeSinusoidalPositionalEmbedding, RelativeTransformer, RelativeTransformerEncoder, RelativeTransformerLayer, Sequential, _, __init__, _shift, _transpose_shift, after_norm, attn, batch_size, bsz, config, d_model, device, dropout, dropout_attn, dropout_layer, emb, embed, embedding_dim, feedforward_dim, ffn, forward, get_embedding, get_output_dim, half_dim, hanlp, head_dim, in_features, index_select, indice, init, init_seq_length, init_size, inputs, k_as_x, layer, layers, locals, long, mask, masked_fill, math, max_len, max_pos, n_head, narrow, new_zeros, norm1, norm2, num_embeddings, num_heads, num_layers, numel, origin_shift, padding_idx, pos_embed, positions, q, qkv, qkv_linear, qv, qv_linear, r_r_bias, r_w_bias, register_buffer, residual, rw_head_q, self, self_attn, seq_len, size, super, to, torch, unsqueeze, v, view, weights, xavier_normal_, zero_pad","F.softmax, hanlp.common.structure.ConfigTracker, math.log, torch.Tensor, torch.arange, torch.cat, torch.chunk, torch.cos, torch.einsum, torch.exp, torch.float, torch.matmul, torch.nn, torch.sin, torch.zeros","__init__, _shift, _transpose_shift, forward, get_embedding, get_output_dim","RelativeMultiHeadAttn, RelativeSinusoidalPositionalEmbedding, RelativeTransformer, RelativeTransformerEncoder, RelativeTransformerLayer","AC, BD, BDE, B_, D_, E, E_, _, attn, batch_size, bsz, d_model, dropout_attn, emb, embed, half_dim, indice, k, layer, max_len, max_pos, n_head, pos_embed, positions, q, qkv, qv, residual, rw_head_q, seq_len, v, weights, x, zero_pad","Args:
    in_features:
    num_heads:
    dropout:
    r_w_bias: n_head x head_dim or None
    r_r_bias: n_head x head_dim or None
    init_seq_length:
    k_as_x:, Args:
  x: batch_size x max_len
  mask: batch_size x max_len. 有value的地方为1
  x: Tensor: 
  mask: Tensor: 

Returns:, Args:
  x: batch_size x max_len x d_model
  mask: batch_size x max_len

Returns:, Args:
  x: batch_size x max_len x hidden_size
  mask: batch_size x max_len, 为0的地方为pad

Returns:
  batch_size x max_len x hidden_size, Build sinusoidal embeddings.
This matches the implementation in tensor2tensor, but differs slightly
from the description in Section 3.5 of ""Attention Is All You Need"".

Args:
  num_embeddings:
  embedding_dim:
  padding_idx:  (Default value = None)

Returns:, Input is expected to be of size [bsz x seqlen].

Args:
  inputs: Tensor:

Returns:, This module produces sinusoidal positional embeddings of any length.
Padding symbols are ignored.

Args:
    embedding_dim: embedding size of each position
    padding_idx:
Returns:, 类似
  -3   -2   -1   0   1   2
 -30  -20  -10  00  10  20
-300 -200 -100 000 100 200

转换为
  0  -10   -200
  1   00   -100
  2   10    000

Args:
  E: batch_size x n_head x max_len x 2max_len

Returns:
  batch_size x n_head x max_len x max_len, 类似
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
转换为
0   1  2
-1  0  1
-2 -1  0

Args:
  BD: batch_size x n_head x max_len x 2max_len

Returns:
  batch_size x n_head x max_len x max_len","

        Args:
          x: batch_size x max_len
          mask: batch_size x max_len. 有value的地方为1
          x: Tensor: 
          mask: Tensor: 

        Returns:

        , 

        Args:
          x: batch_size x max_len x d_model
          mask: batch_size x max_len

        Returns:

        , 

        Args:
          x: batch_size x max_len x hidden_size
          mask: batch_size x max_len, 为0的地方为pad

        Returns:
          batch_size x max_len x hidden_size

        , 
        Args:
            in_features:
            num_heads:
            dropout:
            r_w_bias: n_head x head_dim or None
            r_r_bias: n_head x head_dim or None
            init_seq_length:
            k_as_x:
        , -inf, Build sinusoidal embeddings.
        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of ""Attention Is All You Need"".

        Args:
          num_embeddings:
          embedding_dim:
          padding_idx:  (Default value = None)

        Returns:

        , Input is expected to be of size [bsz x seqlen].

        Args:
          inputs: Tensor:

        Returns:

        , This module produces sinusoidal positional embeddings of any length.
    Padding symbols are ignored.

    Args:
        embedding_dim: embedding size of each position
        padding_idx:
    Returns:

    , bnqd,bnkd->bnqk, bnqd,ld->bnql, in_features, nd,ld->nl, weights, 类似
          -3   -2   -1   0   1   2
         -30  -20  -10  00  10  20
        -300 -200 -100 000 100 200

        转换为
          0  -10   -200
          1   00   -100
          2   10    000

        Args:
          E: batch_size x n_head x max_len x 2max_len

        Returns:
          batch_size x n_head x max_len x max_len

        , 类似
        -3 -2 -1 0 1 2
        -3 -2 -1 0 1 2
        -3 -2 -1 0 1 2
        转换为
        0   1  2
        -1  0  1
        -2 -1  0

        Args:
          BD: batch_size x n_head x max_len x 2max_len

        Returns:
          batch_size x n_head x max_len x max_len

        ","0, 0.1, 0.2, 1, 10000, 1024, 2, 256, 3, 4, False, None, True","

Args:
x: batch_size x max_len
mask: batch_size x max_len. 有value的地方为1
x: Tensor:
mask: Tensor:

Returns:

, 

Args:
x: batch_size x max_len x d_model
mask: batch_size x max_len

Returns:

, 

Args:
x: batch_size x max_len x hidden_size
mask: batch_size x max_len, 为0的地方为pad

Returns:
batch_size x max_len x hidden_size

, 
Args:
in_features:
num_heads:
dropout:
r_w_bias: n_head x head_dim or None
r_r_bias: n_head x head_dim or None
init_seq_length:
k_as_x:
, # 2*seq_len, # A modified version of the implementation from the following paper:, # Biases are not shared, # Hang Yan, Bocao Deng, Xiaonan Li, Xipeng Qiu, # TENER: Adapting Transformer Encoder for Named Entity Recognition, # b x n x l x d, # b x n x l x d, n是head, # batch_size x max_len x d_model2, # batch_size x max_len x d_model3, # bsz x head  x max_len x 2max_len，每个query对每个shift的偏移, # bsz x head x max_len x 2max_len, key对relative的bias, # bsz x head x max_len x 2max_len, 要转换为bsz x head x max_len x max_len, # bsz x n_head x (2max_len+1) x max_len, # bsz x n_head x -1 x (max_len+1), # bsz x n_head x 2max_len x max_len, # bsz x n_head x max_len x max_len, # head x 2max_len, 每个head对位置的bias, # l x head_dim, # r_r_bias就是v, # r_w_bias就是u, # recompute/expand embeddings if needed, # zero pad, Build sinusoidal embeddings.
This matches the implementation in tensor2tensor, but differs slightly
from the description in Section 3.5 of ""Attention Is All You Need"".

Args:
num_embeddings:
embedding_dim:
padding_idx:  (Default value = None)

Returns:

, Input is expected to be of size [bsz x seqlen].

Args:
inputs: Tensor:

Returns:

, This module produces sinusoidal positional embeddings of any length.
Padding symbols are ignored.

Args:
embedding_dim: embedding size of each position
padding_idx:
Returns:

, 类似
-3   -2   -1   0   1   2
-30  -20  -10  00  10  20
-300 -200 -100 000 100 200

转换为
0  -10   -200
1   00   -100
2   10    000

Args:
E: batch_size x n_head x max_len x 2max_len

Returns:
batch_size x n_head x max_len x max_len

, 类似
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
转换为
0   1  2
-1  0  1
-2 -1  0

Args:
BD: batch_size x n_head x max_len x 2max_len

Returns:
batch_size x n_head x max_len x max_len

","

Args:
x: batch_size x max_len
mask: batch_size x max_len. 有value的地方为1
x: Tensor:
mask: Tensor:

Returns:

, 

Args:
x: batch_size x max_len x hidden_size
mask: batch_size x max_len, 为0的地方为pad

Returns:
batch_size x max_len x hidden_size

, b x n x l x d, n是head, bsz x head  x max_len x 2max_len，每个query对每个shift的偏移, bsz x head x max_len x 2max_len, key对relative的bias, bsz x head x max_len x 2max_len, 要转换为bsz x head x max_len x max_len, head x 2max_len, 每个head对位置的bias, r_r_bias就是v, r_w_bias就是u, 为0的地方为pad, 有value的地方为1, 类似, 类似
-3   -2   -1   0   1   2
-30  -20  -10  00  10  20
-300 -200 -100 000 100 200

转换为
0  -10   -200
1   00   -100
2   10    000

Args:
E: batch_size x n_head x max_len x 2max_len

Returns:
batch_size x n_head x max_len x max_len

, 类似
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
-3 -2 -1 0 1 2
转换为
0   1  2
-1  0  1
-2 -1  0

Args:
BD: batch_size x n_head x max_len x 2max_len

Returns:
batch_size x n_head x max_len x max_len

, 转换为"
https://github.com/hankcs/HanLP,resource.py,0,0.0,9,25.71,22,62.86,1,2.86,3,8.57,0,0.0,2,0,3,0,,"get, get_model_mirror, get_tokenizer_mirror, hanlp, hanlp_common, model_mirrors, str, tokenizer_mirrors, transformer","hanlp.utils.io_util.get_resource, hanlp_common.constant.HANLP_URL","get_model_mirror, get_tokenizer_mirror",,"m, model_mirrors, tokenizer_mirrors",,"bart5-chinese-small, cl-tohoku/bert-base-japanese-char, ernie-gram, hfl/chinese-electra-180g-base-discriminator, hfl/chinese-electra-180g-small-discriminator, mMiniLMv2L12-no-space, mMiniLMv2L6-no-space, transformers/bart5-chinese-small_20210723_203923.zip, transformers/bart5-chinese-small_tok_20210723_180743.zip, transformers/bert-base-japanese-char_20210602_215445.zip, transformers/electra_zh_base_20210706_125233.zip, transformers/electra_zh_small_20210706_125427.zip, transformers/ernie-gram_20220207_103518.zip, transformers/mMiniLMv2L12-no-space-tokenizer_20220616_095900.zip, transformers/mMiniLMv2L12-no-space_20220616_095924.zip, transformers/mMiniLMv2L6-no-space-tokenizer_20220616_094859.zip, transformers/mMiniLMv2L6-no-space_20220616_094949.zip, transformers/xlm-roberta-base-no-space-tokenizer_20220610_204241.zip, transformers/xlm-roberta-base-no-space_20220610_203944.zip, transformers/xlm-roberta-base_20210706_125502.zip, xlm-roberta-base, xlm-roberta-base-no-space",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-20 12:43",
https://github.com/hankcs/HanLP,tf_imports.py,0,0.0,1,25.0,0,0.0,0,0.0,3,75.0,0,0.0,0,0,0,0,,transformers,"transformers.AlbertConfig, transformers.AutoConfig, transformers.AutoTokenizer, transformers.BertConfig, transformers.BertTokenizer, transformers.BertTokenizerFast, transformers.PreTrainedTokenizer, transformers.PretrainedConfig, transformers.TFAlbertForMaskedLM, transformers.TFAlbertModel, transformers.TFAutoModel, transformers.TFAutoModelWithLMHead, transformers.TFBertModel, transformers.TFPreTrainedModel",,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-08 21:57",
https://github.com/hankcs/HanLP,utils.py,0,0.0,120,58.54,17,8.29,15,7.32,53,25.85,0,0.0,10,0,52,3,,"AdamW, B, F, L, adam_epsilon, all_encoder_layers, append, attention_mask, average_subwords, batch_size, build_optimizer_for_pretrained, build_optimizer_scheduler_with_transformer, callable, cls_hidden, cls_mask, collect_decay_params, collections, combine_initial_dims_to_1d_or_2d, config, device, dims, embed, end_tokens, eps, final_window, final_window_start, first_window, flatten, float, full_seq_len, gather, get_device_of, get_linear_schedule_with_warmup, get_optimizers, get_range_vector, grouped_parameters, h_span, hanlp, hidden_states, hs, index_select, initial_dims, input_ids, input_mask, int, is_no_decay, is_pretrained, isinstance, kwargs, last_window_size, layer_range, learning_rate, len, list, lr, max, max_context_windows, max_pieces, max_sequence_length, model, n_sub_tokens, name, named_parameters, nd, ne, needs_split, no_decay, no_decay_fn, num_training_steps, offsets, offsets2d, optimizer, optimizer_grouped_parameters, output_hidden_states, outputs, pad, padding_amount, parameters, params, pick_tensor_for_each_token, pretrained, range, range_vector, recombined_embeddings, repeat, reshape, restore_from_sliding_window, ret_cls, ret_raw_hidden_states, scheduler, select_indices, selected_embeddings, set, shape, size, split, split_input_ids, split_to_sliding_window, start_tokens, stride, stride_offset, sum, token_span, token_type_ids, torch, transformer_encode, transformer_lr, transformer_sliding_window, transformer_weight_decay, transformers, tuple, typing, uncombine_initial_dims, unpacked_embeddings, unsqueeze, view, warmup_steps, weight_decay, x, zero_mask","collections.defaultdict, hanlp.components.parsers.ud.udify_util, hanlp.layers.transformers.pt_imports.PreTrainedModel, torch.LongTensor, torch.Tensor, torch.any, torch.cat, torch.nn.Module, torch.nn.functional, torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR, torch.split, torch.stack, torch.sum, transformers.AdamW, transformers.get_linear_schedule_with_warmup, transformers.optimization, typing.Tuple, typing.Union","build_optimizer_for_pretrained, build_optimizer_scheduler_with_transformer, collect_decay_params, get_optimizers, no_decay_fn, pick_tensor_for_each_token, restore_from_sliding_window, split_to_sliding_window, transformer_encode, transformer_sliding_window",,"B, F, L, all_encoder_layers, attention_mask, batch_size, cls_hidden, cls_mask, dims, embed, final_window, final_window_start, first_window, full_seq_len, grouped_parameters, h_span, hs, initial_dims, input_ids, input_mask, is_no_decay, is_pretrained, last_window_size, layers, max_context_windows, n_sub_tokens, nd, needs_split, no_decay, no_decay_fn, offsets2d, optimizer, optimizer_grouped_parameters, outputs, padding_amount, params, pretrained, range_vector, recombined_embeddings, scheduler, select_indices, selected_embeddings, split_input_ids, stride, stride_offset, token_span, transformer_lr, transformer_weight_decay, unpacked_embeddings, warmup_steps, x, zero_mask","Args:
  transformer:
  input_ids: torch.LongTensor: 
  input_mask:  (Default value = None)
  offsets: torch.LongTensor:  (Default value = None)
  token_type_ids: torch.LongTensor:  (Default value = None)
  max_pieces:  (Default value = 512)
  start_tokens: int:  (Default value = 1)
  end_tokens: int:  (Default value = 1)
  ret_cls:  (Default value = None)

Returns:, Modified from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L232
Setup the optimizer and the learning rate scheduler.

We provide a reasonable default that works well., Run transformer and pool its outputs.

Args:
    transformer: A transformer model.
    input_ids: Indices of subwords.
    attention_mask: Mask for these subwords.
    token_type_ids: Type ids for each subword.
    token_span: The spans of tokens.
    layer_range: The range of layers to use. Note that the 0-th layer means embedding layer, so the last 3 layers
                of a 12-layer BERT will be (10, 13).
    max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
                window.
     average_subwords: ``True`` to average subword representations.
    ret_raw_hidden_states: ``True`` to return hidden states of each layer.

Returns:
    Pooled outputs.","

    Args:
      transformer:
      input_ids: torch.LongTensor: 
      input_mask:  (Default value = None)
      offsets: torch.LongTensor:  (Default value = None)
      token_type_ids: torch.LongTensor:  (Default value = None)
      max_pieces:  (Default value = 512)
      start_tokens: int:  (Default value = 1)
      end_tokens: int:  (Default value = 1)
      ret_cls:  (Default value = None)

    Returns:

    
    , 
    Modified from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L232
    Setup the optimizer and the learning rate scheduler.

    We provide a reasonable default that works well.
    , LayerNorm.bias, LayerNorm.weight, Run transformer and pool its outputs.

    Args:
        transformer: A transformer model.
        input_ids: Indices of subwords.
        attention_mask: Mask for these subwords.
        token_type_ids: Type ids for each subword.
        token_span: The spans of tokens.
        layer_range: The range of layers to use. Note that the 0-th layer means embedding layer, so the last 3 layers
                    of a 12-layer BERT will be (10, 13).
        max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
                    window.
         average_subwords: ``True`` to average subword representations.
        ret_raw_hidden_states: ``True`` to return hidden states of each layer.

    Returns:
        Pooled outputs.

    , bias, decay, lr, max, no_decay, no_decay has to be callable or a tuple of str, non_pretrained, params, pretrained, raw, warmup_steps has to fall in range (0, 1) when it is float., weight_decay","0, 0.0, 0.01, 0.1, 1, 1e+20, 1e-05, 1e-08, 2, 3, 512, 5e-05, False, None, True","

Args:
transformer:
input_ids: torch.LongTensor:
input_mask:  (Default value = None)
offsets: torch.LongTensor:  (Default value = None)
token_type_ids: torch.LongTensor:  (Default value = None)
max_pieces:  (Default value = 512)
start_tokens: int:  (Default value = 1)
end_tokens: int:  (Default value = 1)
ret_cls:  (Default value = None)

Returns:


, 
Modified from https://github.com/huggingface/transformers/blob/7b75aa9fa55bee577e2c7403301ed31103125a35/src/transformers/trainer.py#L232
Setup the optimizer and the learning rate scheduler.

We provide a reasonable default that works well.
, #     hs.append(pick_tensor_for_each_token(h, token_span, average_subwords)), #     token_type_ids = torch.zeros_like(input_ids), #  0     1 2    3  4   5    6    7     8     9   10   11   12    13 14  15, # ""[CLS] I went to the very fine [SEP] [CLS] the very fine store to eat [SEP]"", # (layers, batch_size * d1 * ... * dn, sequence_length, embedding_dim), # -*- coding:utf-8 -*-, # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim), # Author: hankcs, # Calculate an offset to extract the centermost embeddings of each window, # Date: 2020-06-15 21:22, # E.g., ""[CLS] I went to the [SEP] [CLS] to the store to [SEP] ..."", # Fast pick, # Find the stride as half the max pieces, ignoring the special start and end tokens, # First, unpack the output embeddings into one long sequence again, # For backwards compatability, # Next, select indices of the sequence such that it will result in embeddings representing the original, # Now combine the sequences along the batch dimension, # Prepare optimizer and schedule (linear warmup and decay), # Recombine the outputs of all layers, # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim), # Slow pick, # Split the flattened list by the window size, `max_pieces`, # TODO: split token type ids in transformer_sliding_window if token type ids are not always 1, # The embedder may receive an input tensor that has a sequence length longer than can, # This can then be fed into BERT without any sentence length issues. Keep in mind, # We can then split the sequence into sub-sequences of that length, and concatenate them, # We want all sequences to be the same length, so pad the last sequence, # along the batch dimension so we effectively have one huge batch of partial sentences., # and final windows with indices [0, 1] and [14, 15] respectively., # avoid dividing by zero, # be fit. In that case, we should expect the wordpiece indexer to create padded windows, # before calling the BERT model and then reshape back at the end., # for h in outputs:, # hs = [], # if token_type_ids is None:, # input_ids may have extra dimensions, so we reshape down to 2-d, # input_mask = (recombined_embeddings != 0).long(), # long sentences., # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length), # of length `max_pieces` for us, and have them concatenated into one long sequence., # offsets is (batch_size, d1, ..., dn, orig_sequence_length), # pylint: disable=arguments-differ, # recombined = torch.cat(combined, dim=2), # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length), # sentence. To capture maximal context, the indices will be the middle part of each embedded window, # sub-sequence (plus any leftover start and final edge windows), e.g.,, # that the memory consumption can dramatically increase for large batches with extremely, # tile token_span as x, # token_type_ids=util.combine_initial_dims_to_1d_or_2d(token_type_ids),, # with max_pieces = 8 should produce max context indices [2, 3, 4, 10, 11, 12] with additional start, Run transformer and pool its outputs.

Args:
transformer: A transformer model.
input_ids: Indices of subwords.
attention_mask: Mask for these subwords.
token_type_ids: Type ids for each subword.
token_span: The spans of tokens.
layer_range: The range of layers to use. Note that the 0-th layer means embedding layer, so the last 3 layers
of a 12-layer BERT will be (10, 13).
max_sequence_length: The maximum sequence length. Sequence longer than this will be handled by sliding
window.
average_subwords: ``True`` to average subword representations.
ret_raw_hidden_states: ``True`` to return hidden states of each layer.

Returns:
Pooled outputs.

",
https://github.com/hankcs/HanLP,utils_tf.py,0,0.0,62,51.24,19,15.7,7,5.79,33,27.27,0,0.0,4,0,18,2,,"adjust_tokens_for_transformers, append, args, build_adamw_optimizer, cleaned_words, clipnorm, cls_token, cls_token_at_end, cls_token_segment_id, config, config_is, convert_examples_to_features, convert_tokens_to_ids, dict, do_padding, epsilon, extend, hanlp, input_ids, input_mask, label, label_ids, label_map, labels, learning_rate, len, locals, lr_config, mask_padding_with_zero, max_seq_length, model, optimizer, pad_on_left, pad_token_id, pad_token_label_id, pad_token_segment_id, padding_length, segment_ids, sentence, sep_token, sep_token_extra, sequence_a_segment_id, set, special_tokens_count, startswith, tensorflow, tokenize, tokenizer, tokens, train_steps, type, unk_token, use_amp, v, warmup_steps, warning, weight_decay_rate, word, word_tokens, words, x, zip","hanlp.optimizers.adamw.create_optimizer, hanlp.utils.log_util.logger, tf.keras.mixed_precision.experimental.LossScaleOptimizer, tf.keras.utils.serialize_keras_object","adjust_tokens_for_transformers, build_adamw_optimizer, config_is, convert_examples_to_features",,"args, cleaned_words, input_ids, input_mask, label, label_ids, labels, lr_config, opt, pad_token_label_id, padding_length, segment_ids, special_tokens_count, tokens, v, word, word_tokens, x","Adjust tokens for BERT
See https://github.com/DoodleJZ/HPSG-Neural-Parser/blob/master/src_joint/Zparser.py#L1204

Args:
  sentence: 

Returns:, Loads a data file into a list of `InputBatch`s
    `cls_token_at_end` define the location of the CLS token:
        - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
        - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
    `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

Args:
  words: 
  max_seq_length: 
  tokenizer: 
  labels:  (Default value = None)
  label_map:  (Default value = None)
  cls_token_at_end:  (Default value = False)
  cls_token:  (Default value = ""[CLS]"")
  cls_token_segment_id:  (Default value = 1)
  sep_token:  (Default value = ""[SEP]"")
  sep_token_extra:  (Default value = False)
  pad_on_left:  (Default value = False)
  pad_token_id:  (Default value = 0)
  pad_token_segment_id:  (Default value = 0)
  pad_token_label_id:  (Default value = 0)
  sequence_a_segment_id:  (Default value = 0)
  mask_padding_with_zero:  (Default value = True)
  unk_token:  (Default value = '[UNK]')
  do_padding:  (Default value = True)

Returns:"," exceed the max sequence length of ,  tokens beforehand., 't, . The exceeded part will be truncated and ignored. You are recommended to split your long text into several sentences within , Adjust tokens for BERT
    See https://github.com/DoodleJZ/HPSG-Neural-Parser/blob/master/src_joint/Zparser.py#L1204

    Args:
      sentence: 

    Returns:

    
    , Input tokens , Loads a data file into a list of `InputBatch`s
        `cls_token_at_end` define the location of the CLS token:
            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

    Args:
      words: 
      max_seq_length: 
      tokenizer: 
      labels:  (Default value = None)
      label_map:  (Default value = None)
      cls_token_at_end:  (Default value = False)
      cls_token:  (Default value = ""[CLS]"")
      cls_token_segment_id:  (Default value = 1)
      sep_token:  (Default value = ""[SEP]"")
      sep_token_extra:  (Default value = False)
      pad_on_left:  (Default value = False)
      pad_token_id:  (Default value = 0)
      pad_token_segment_id:  (Default value = 0)
      pad_token_label_id:  (Default value = 0)
      sequence_a_segment_id:  (Default value = 0)
      mask_padding_with_zero:  (Default value = True)
      unk_token:  (Default value = '[UNK]')
      do_padding:  (Default value = True)

    Returns:

    , [CLS], [SEP], [UNK], _, bert, config, decay_schedule_fn, dynamic, failed for:
 , learning_rate, n, n't","0, 1, 2, 3, False, None, True","#, #  token_type_ids:   0   0   0   0  0     0   0, #  token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1, #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP], #  tokens:   [CLS] the dog is hairy . [SEP], # (a) For sequence pairs:, # (b) For single sequences:, # -*- coding:utf-8 -*-, # Account for [CLS] and [SEP] with ""- 2"" and with ""- 3"" for RoBERTa., # Author: hankcs, # Date: 2019-12-29 15:32, # For classification tasks, the first vector (corresponding to [CLS]) is, # The convention in BERT is:, # The mask has 1 for real tokens and 0 for padding tokens. Only real, # Use the real label id for the first token of the word, and padding ids for the remaining tokens, # Where ""token_type_ids"" are used to indicate whether this is the first, # Zero-pad up to the sequence length., # `type=1` were learned during pre-training and are added to the wordpiece, # embedding vector (and position vector). This is not *strictly* necessary, # it easier for the model to learn the concept of sequences., # loss scaling is currently required when using mixed precision, # opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08), # opt = tfa.optimizers.AdamW(learning_rate=3e-5, epsilon=1e-08, weight_decay=0.01), # roberta uses an extra separator b/w pairs of sentences, # sequence or the second sequence. The embedding vectors for `type=0` and, # since the [SEP] token unambiguously separates the sequences, but it makes, # some wired chars cause the tagger to return empty list, # the entire model is fine-tuned., # tokens are attended to., # used as as the ""sentence vector"". Note that this only makes sense because, # word = BERT_TOKEN_MAPPING.get(word, word), Adjust tokens for BERT
See https://github.com/DoodleJZ/HPSG-Neural-Parser/blob/master/src_joint/Zparser.py#L1204

Args:
sentence:

Returns:


, Loads a data file into a list of `InputBatch`s
`cls_token_at_end` define the location of the CLS token:
- False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]
- True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]
`cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)

Args:
words:
max_seq_length:
tokenizer:
labels:  (Default value = None)
label_map:  (Default value = None)
cls_token_at_end:  (Default value = False)
cls_token:  (Default value = ""[CLS]"")
cls_token_segment_id:  (Default value = 1)
sep_token:  (Default value = ""[SEP]"")
sep_token_extra:  (Default value = False)
pad_on_left:  (Default value = False)
pad_token_id:  (Default value = 0)
pad_token_segment_id:  (Default value = 0)
pad_token_label_id:  (Default value = 0)
sequence_a_segment_id:  (Default value = 0)
mask_padding_with_zero:  (Default value = True)
unk_token:  (Default value = '[UNK]')
do_padding:  (Default value = True)

Returns:

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,1,8.33,7,58.33,0,0.0,4,33.33,0,0.0,0,0,0,0,,logging,"logging.ERROR, logging.getLogger",,,,,"transformers.configuration_utils, transformers.file_utils, transformers.filelock, transformers.modeling_tf_utils, transformers.modeling_utils, transformers.tokenization_utils, transformers.tokenization_utils_base",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 15:17, # mute transformers",
https://github.com/hankcs/HanLP,sentiment.py,0,0.0,4,33.33,4,33.33,0,0.0,4,33.33,0,0.0,0,0,4,0,,"CHNSENTICORP_ERNIE_DEV, CHNSENTICORP_ERNIE_TEST, CHNSENTICORP_ERNIE_TRAIN, _ERNIE_TASK_DATA",,,,"CHNSENTICORP_ERNIE_DEV, CHNSENTICORP_ERNIE_TEST, CHNSENTICORP_ERNIE_TRAIN, _ERNIE_TASK_DATA",,"chnsenticorp/dev.tsv, chnsenticorp/test.tsv, chnsenticorp/train.tsv, https://ernie.bj.bcebos.com/task_data_zh.tgz#",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 21:03, #'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 11:49",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-04 13:39",
https://github.com/hankcs/HanLP,eos.py,0,0.0,37,60.66,8,13.11,8,13.11,5,8.2,3,4.92,2,1,13,2,,"SentenceBoundaryDetectionDataset, __init__, append, append_after_sentence, cache, collections, corpus, enumerate, eos_char_is_punct, eos_char_min_freq, eos_chars, eos_index, eos_offsets, erase, filepath, hanlp, isinstance, itertools, k, label_id, len, line, list, load_file, log, most_common, offset, rstrip, self, sents, strip, super, transform, typing, v, window, window_size","collections.Counter, hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.TimingFileIterator, hanlp.utils.log_util.cprint, hanlp.utils.string_util.ispunct, itertools.chain.from_iterable, typing.Callable, typing.List, typing.Union","__init__, load_file",SentenceBoundaryDetectionDataset,"corpus, eos_chars, eos_index, eos_offsets, f, k, label_id, line, offset, sents, v, window, window_size","Dataset for sentence boundary detection (eos).

Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    append_after_sentence: A :class:`str` to insert at the tail of each sentence. For example, English always
        have a space between sentences.
    eos_chars: Punctuations at the tail of sentences. If ``None``, then it will built from training samples.
    eos_char_min_freq: Minimal frequency to keep a eos char.
    eos_char_is_punct: Limit eos chars to punctuations.
    window_size: Window size to extract ngram features.
    kwargs: Not used., Load eos corpus.

Args:
    filepath: Path to the corpus.

.. highlight:: bash
.. code-block:: bash

    $ head -n 2 ctb8.txt
    中国经济简讯
    新华社北京十月二十九日电中国经济简讯","
,  != , Dataset for sentence boundary detection (eos).

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            append_after_sentence: A :class:`str` to insert at the tail of each sentence. For example, English always
                have a space between sentences.
            eos_chars: Punctuations at the tail of sentences. If ``None``, then it will built from training samples.
            eos_char_min_freq: Minimal frequency to keep a eos char.
            eos_char_is_punct: Limit eos chars to punctuations.
            window_size: Window size to extract ngram features.
            kwargs: Not used.
        , Load eos corpus.

        Args:
            filepath: Path to the corpus.

        .. highlight:: bash
        .. code-block:: bash

            $ head -n 2 ctb8.txt
            中国经济简讯
            新华社北京十月二十九日电中国经济简讯

        , [/yellow], char, eos_chars = [yellow], label_id","0, 0.0, 1, 1.0, 200, 5, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-26 18:12, Dataset for sentence boundary detection (eos).

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
append_after_sentence: A :class:`str` to insert at the tail of each sentence. For example, English always
have a space between sentences.
eos_chars: Punctuations at the tail of sentences. If ``None``, then it will built from training samples.
eos_char_min_freq: Minimal frequency to keep a eos char.
eos_char_is_punct: Limit eos chars to punctuations.
window_size: Window size to extract ngram features.
kwargs: Not used.
, Load eos corpus.

Args:
filepath: Path to the corpus.

.. highlight:: bash
.. code-block:: bash

$ head -n 2 ctb8.txt
中国经济简讯
新华社北京十月二十九日电中国经济简讯

","Load eos corpus.

Args:
filepath: Path to the corpus.

.. highlight:: bash
.. code-block:: bash

$ head -n 2 ctb8.txt
中国经济简讯
新华社北京十月二十九日电中国经济简讯

, 中国经济简讯, 新华社北京十月二十九日电中国经济简讯"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-26 18:11",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,7,38.89,7,38.89,0,0.0,4,22.22,0,0.0,0,0,7,0,,"PTB_CHAR_DEV, PTB_CHAR_TEST, PTB_CHAR_TRAIN, PTB_TOKEN_DEV, PTB_TOKEN_TEST, PTB_TOKEN_TRAIN, _PTB_HOME",,,,"PTB_CHAR_DEV, PTB_CHAR_TEST, PTB_CHAR_TRAIN, PTB_TOKEN_DEV, PTB_TOKEN_TEST, PTB_TOKEN_TRAIN, _PTB_HOME",,"data/ptb.char.test.txt, data/ptb.char.train.txt, data/ptb.char.valid.txt, data/ptb.test.txt, data/ptb.train.txt, data/ptb.valid.txt, http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz#",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-05 21:41, #'",
https://github.com/hankcs/HanLP,glue.py,0,0.0,11,45.83,7,29.17,0,0.0,6,25.0,0,0.0,1,1,7,0,,"MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV, MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_TEST, MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_TRAIN, SST2Dataset, STANFORD_SENTIMENT_TREEBANK_2_DEV, STANFORD_SENTIMENT_TREEBANK_2_TEST, STANFORD_SENTIMENT_TREEBANK_2_TRAIN, __name__, hanlp, main, print",hanlp.common.dataset.TableDataset,main,SST2Dataset,"MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_DEV, MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_TEST, MICROSOFT_RESEARCH_PARAPHRASE_CORPUS_TRAIN, STANFORD_SENTIMENT_TREEBANK_2_DEV, STANFORD_SENTIMENT_TREEBANK_2_TEST, STANFORD_SENTIMENT_TREEBANK_2_TRAIN, dataset",,"__main__, http://file.hankcs.com/corpus/SST2.zip#dev.tsv, http://file.hankcs.com/corpus/SST2.zip#test.tsv, http://file.hankcs.com/corpus/SST2.zip#train.tsv, http://file.hankcs.com/corpus/mrpc.zip#dev.tsv, http://file.hankcs.com/corpus/mrpc.zip#test.tsv, http://file.hankcs.com/corpus/mrpc.zip#train.tsv",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 11:47, #dev.tsv', #test.tsv', #train.tsv'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:08",
https://github.com/hankcs/HanLP,conll03.py,0,0.0,3,20.0,6,40.0,0,0.0,6,40.0,0,0.0,0,0,3,0,,"CONLL03_EN_DEV, CONLL03_EN_TEST, CONLL03_EN_TRAIN",,,,"CONLL03_EN_DEV, CONLL03_EN_TEST, CONLL03_EN_TRAIN",,"Dev set of CoNLL03 (:cite:`tjong-kim-sang-de-meulder-2003-introduction`), Test set of CoNLL03 (:cite:`tjong-kim-sang-de-meulder-2003-introduction`), Training set of CoNLL03 (:cite:`tjong-kim-sang-de-meulder-2003-introduction`), https://file.hankcs.com/corpus/conll03_en_iobes.zip#eng.dev.tsv, https://file.hankcs.com/corpus/conll03_en_iobes.zip#eng.test.tsv, https://file.hankcs.com/corpus/conll03_en_iobes.zip#eng.train.tsv",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-06 15:31, #eng.test.tsv', #eng.train.tsv', Training set of CoNLL03 (:cite:`tjong-kim-sang-de-meulder-2003-introduction`)
CONLL03_EN_DEV = 'https://file.hankcs.com/corpus/conll03_en_iobes.zip#eng.dev.tsv'
Dev set of CoNLL03 (:cite:`tjong-kim-sang-de-meulder-2003-introduction`)",
https://github.com/hankcs/HanLP,msra.py,0,0.0,14,25.45,26,47.27,0,0.0,15,27.27,0,0.0,0,0,14,0,,"MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST, MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_TOKEN_LEVEL_IOBES_DEV, MSRA_NER_TOKEN_LEVEL_IOBES_TEST, MSRA_NER_TOKEN_LEVEL_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_DEV, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TEST, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TRAIN, _MSRA_NER_HOME, _MSRA_NER_TOKEN_LEVEL_HOME",,,,"MSRA_NER_CHAR_LEVEL_DEV, MSRA_NER_CHAR_LEVEL_TEST, MSRA_NER_CHAR_LEVEL_TRAIN, MSRA_NER_TOKEN_LEVEL_IOBES_DEV, MSRA_NER_TOKEN_LEVEL_IOBES_TEST, MSRA_NER_TOKEN_LEVEL_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TEST, MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_TRAIN, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_DEV, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TEST, MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TRAIN, _MSRA_NER_HOME, _MSRA_NER_TOKEN_LEVEL_HOME",,"#dev.tsv, #test.tsv, #train.tsv, #word_level.dev.short.jsonlines, #word_level.dev.short.tsv, #word_level.dev.tsv, #word_level.test.short.jsonlines, #word_level.test.short.tsv, #word_level.test.tsv, #word_level.train.short.jsonlines, #word_level.train.short.tsv, #word_level.train.tsv, Dev set of MSRA (:cite:`levow-2006-third`) in character level., Dev set of MSRA (:cite:`levow-2006-third`) in token level., Dev set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format., Dev set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level., Test set of MSRA (:cite:`levow-2006-third`) in character level., Test set of MSRA (:cite:`levow-2006-third`) in token level., Test set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format., Test set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level., Training set of MSRA (:cite:`levow-2006-third`) in character level., Training set of MSRA (:cite:`levow-2006-third`) in token level., Training set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format., Training set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level., http://file.hankcs.com/corpus/msra_ner.zip, http://file.hankcs.com/corpus/msra_ner_token_level.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 23:13, #test.tsv', #train.tsv', #word_level.dev.short.jsonlines', #word_level.dev.tsv', #word_level.test.short.tsv', #word_level.train.short.tsv', Dev set of MSRA (:cite:`levow-2006-third`) in token level.
MSRA_NER_TOKEN_LEVEL_IOBES_TEST = f'{_MSRA_NER_TOKEN_LEVEL_HOME}#word_level.test.tsv'
Test set of MSRA (:cite:`levow-2006-third`) in token level., Dev set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format.
MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TEST = f'{_MSRA_NER_TOKEN_LEVEL_HOME}#word_level.test.short.jsonlines'
Test set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format., Test set of MSRA (:cite:`levow-2006-third`) in character level.

MSRA_NER_TOKEN_LEVEL_IOBES_TRAIN = f'{_MSRA_NER_TOKEN_LEVEL_HOME}#word_level.train.tsv'
Training set of MSRA (:cite:`levow-2006-third`) in token level., Test set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level.

MSRA_NER_TOKEN_LEVEL_SHORT_JSON_TRAIN = f'{_MSRA_NER_TOKEN_LEVEL_HOME}#word_level.train.short.jsonlines'
Training set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level and jsonlines format., Training set of MSRA (:cite:`levow-2006-third`) in character level.
MSRA_NER_CHAR_LEVEL_DEV = f'{_MSRA_NER_HOME}#dev.tsv'
Dev set of MSRA (:cite:`levow-2006-third`) in character level., Training set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level.
MSRA_NER_TOKEN_LEVEL_SHORT_IOBES_DEV = f'{_MSRA_NER_TOKEN_LEVEL_HOME}#word_level.dev.short.tsv'
Dev set of shorten (<= 128 tokens) MSRA (:cite:`levow-2006-third`) in token level.",
https://github.com/hankcs/HanLP,resume.py,0,0.0,5,29.41,7,41.18,0,0.0,5,29.41,0,0.0,0,0,4,0,,"RESUME_NER_DEV, RESUME_NER_TEST, RESUME_NER_TRAIN, _RESUME_NER_HOME, hanlp","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.generate_words_tags_from_tsv, hanlp.utils.io_util.get_resource",,,"RESUME_NER_DEV, RESUME_NER_TEST, RESUME_NER_TRAIN, _RESUME_NER_HOME",,"Dev set of Resume in char level., ResumeNER/dev.char.bmes, ResumeNER/test.char.bmes, ResumeNER/train.char.bmes, Test set of Resume in char level., Training set of Resume in char level., https://github.com/jiesutd/LatticeLSTM/archive/master.zip#",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-08 12:10, #', Training set of Resume in char level.
RESUME_NER_DEV = _RESUME_NER_HOME + 'ResumeNER/dev.char.bmes'
Dev set of Resume in char level.",
https://github.com/hankcs/HanLP,weibo.py,0,0.0,5,29.41,7,41.18,0,0.0,5,29.41,0,0.0,0,0,4,0,,"WEIBO_NER_DEV, WEIBO_NER_TEST, WEIBO_NER_TRAIN, _WEIBO_NER_HOME, hanlp","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.generate_words_tags_from_tsv, hanlp.utils.io_util.get_resource",,,"WEIBO_NER_DEV, WEIBO_NER_TEST, WEIBO_NER_TRAIN, _WEIBO_NER_HOME",,"Dev set of Weibo in char level., Test set of Weibo in char level., Training set of Weibo in char level., https://github.com/hltcoe/golden-horse/archive/master.zip#data/, weiboNER_2nd_conll.dev, weiboNER_2nd_conll.test, weiboNER_2nd_conll.train",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-03 23:33, #data/', Training set of Weibo in char level.
WEIBO_NER_DEV = _WEIBO_NER_HOME + 'weiboNER_2nd_conll.dev'
Dev set of Weibo in char level.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-06 15:32",
https://github.com/hankcs/HanLP,amr.py,0,0.0,162,71.68,45,19.91,8,3.54,11,4.87,0,0.0,19,1,98,0,,"AbstractMeaningRepresentationDataset, _concept_char_in, _concept_in, _concept_out, _cp_seq, _lem, _m, _mp_seq, _ner, _pos, _rel, _token_type_ids, _u, _v, _word_char, add, append, append_bos, attention_mask, augmented_concept, bart, batchify, bidx, bottom_right, bsz, chars, chars_for_tok, collections, concept, concept_i, concept_len, concept_mask, concept_with_rel, concepts, connected_nodes, copy, counts, cp, cp_seq, decoder_mask, device, each, edge, edge_i, edge_with_rel, encodings, enumerate, extra_arc, filepath, generate_oracle, get, get_concepts, get_frequency, get_idx, get_mask, good_concept, good_edge, group, hanlp, hanlp_common, idx2token, idx_to_token, index, input_ids, isinstance, items, j, key, label, labels, largest_component, largest_connected_component, last_concept_offset, lem, len, levi_amr, levi_graph, linearize, load_file, local_idx2token, local_token2idx, make_batch_for_bart, make_batch_for_squeeze, mask, max, max_input_len, max_seq_len, max_string_len, move_dict_to_device, mp_seq, n_components, ner, new_tokens, node_to_id, numpy, nxt, out_conc_len, p1, p2, pad_data, pad_idx, pos, prefix, range, raw_concept_len, raw_sent_len, read, real_concept, real_edge, rel, rel_vocab, remove_unconnected_components, reorder, ret, reverse_edge_for_levi_bfs, reversed, root_centered_sort, row, sample, scipy, sent_len, sep_token, sep_token_id, separate_concept_rel, shape, shuffle_sibling, snt_len, sorted, spans, squeeze, startswith, subtoken_to_tensor, to, to_levi, to_triples, token2idx, token_and_concept, token_field, token_input_ids, token_length, token_mask, token_token_span, token_type_ids, tokenizer, tokens, torch, training, tri_edge, triples, tuple, typing, un_kahn, unique, unk_idx, unk_rate, unlinearize, update, ur, vocabs, vur, x, zip","collections.defaultdict, copy.copy, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.TransformableDataset, hanlp.common.transform.VocabDict, hanlp.common.vocab.VocabWithFrequency, hanlp.components.amr.amr_parser.amrio.AMRIO, hanlp.components.amr.amr_parser.data.DUM, hanlp.components.amr.amr_parser.data.END, hanlp.components.amr.amr_parser.data.NIL, hanlp.components.amr.amr_parser.data.REL, hanlp.components.amr.amr_parser.data.list_to_tensor, hanlp.components.amr.amr_parser.data.lists_of_string_to_tensor, hanlp.components.amr.amr_parser.transformer.SelfAttentionMask, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp_common.constant.CLS, hanlp_common.util.merge_list_of_dict, np.array, np.full, np.int, np.ndarray, np.ones, np.stack, np.unique, np.where, scipy.sparse.csgraph._traversal.connected_components, scipy.sparse.csr_matrix, torch.Tensor, torch.bool, torch.device, torch.long, torch.stack, torch.tensor, torch.zeros, typing.List","append_bos, batchify, chars_for_tok, generate_oracle, get_concepts, largest_connected_component, levi_amr, linearize, load_file, make_batch_for_bart, make_batch_for_squeeze, move_dict_to_device, remove_unconnected_components, reverse_edge_for_levi_bfs, separate_concept_rel, subtoken_to_tensor, to_triples, un_kahn, unlinearize",AbstractMeaningRepresentationDataset,"_concept_char_in, _concept_in, _concept_out, _cp_seq, _lem, _m, _mp_seq, _ner, _pos, _rel, _tok, _token_type_ids, _u, _v, _word_char, amr, attention_mask, augmented_concept, bidx, bottom_right, bsz, chars, col, concept, concept_i, concept_len, concept_mask, concept_with_rel, concepts, connected_nodes, counts, cp, cp_seq, data, decoder_mask, device, each, edge, edge_i, edge_with_rel, encodings, good_concept, good_edge, graph, group, idx2token, input_ids, j, key, labels, largest_component, last_concept_offset, lem, local_idx2token, local_token2idx, mask, max_input_len, max_seq_len, mp_seq, n_components, ner, new_tokens, node_to_id, nxt, out_conc_len, p1, p2, pos, r, raw_concept_len, raw_sent_len, real_concept, real_edge, rel_vocab, reorder, ret, row, sent_len, snt_len, spans, token, token2idx, token_and_concept, token_field, token_input_ids, token_length, token_mask, token_token_span, token_type_ids, tokenizer, tokens, tri_edge, u, unique, ur, v, vur, x",,", Invalid typological order, _, _input_ids, _reverse_, _token_span, amr, attention_mask, concept, concept_and_rel, concept_char, concept_char_in, concept_in, concept_input_ids, concept_mask, concept_out, concept_token_span, copy_seq, cp_seq, cpu:0, decoder_mask, edge, idx2token, kahn, last_concept_offset, lem, lemma, local_idx2token, local_token2idx, mp_seq, ner, pos, predictable_concept, rel, snt_len, tok, token, token2idx, token_and_concept, token_and_concept_input_ids, token_and_concept_token_span, token_length, token_mask, token_type_ids, word_char","0, 0.0, 1, 2, 20, False, None, True","# (['want', 'rel=ARG1', 'rel=ARG0', 'believe', 'rel=ARG1', 'rel=ARG0', 'boy', 'girl'],, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-18 17:47, # This is a undirectional graph, so we can safely reverse edge, # [(0, 1, 0.9999417066574097), (0, 2, 0.9999995231628418), (1, 3, 0.9999992847442627), (3, 4, 1.0), (3, 5, 0.9999996423721313), (2, 6, 0.9996106624603271), (4, 6, 0.9999767541885376), (5, 7, 0.9999860525131226)]), # for [SEP], # l=1 => pos=l+1=2, # skip END concept, # skip [CLS] and [SEP], # v: [<dummy>, concept_0, ..., concept_l, ..., concept_{n-1}, <end>] u: [<dummy>, concept_0, ..., concept_l, ..., concept_{n-1}]",
https://github.com/hankcs/HanLP,ctb5.py,0,0.0,7,33.33,9,42.86,0,0.0,5,23.81,0,0.0,0,0,6,0,,"CIP_W2V_100_CN, CTB5_DEP_DEV, CTB5_DEP_TEST, CTB5_DEP_TRAIN, _CTB5_DEP_HOME, _CTB_HOME, hanlp_common",hanlp_common.constant.HANLP_URL,,,"CIP_W2V_100_CN, CTB5_DEP_DEV, CTB5_DEP_TEST, CTB5_DEP_TRAIN, _CTB5_DEP_HOME, _CTB_HOME",,"BPNN/data/ctb5/, BPNN/data/embed.txt, Dev set for ctb5 dependency parsing., Test set for ctb5 dependency parsing., Training set for ctb5 dependency parsing., dev.conll, embeddings/SUDA-LA-CIP_20200109_021624.zip#, test.conll, train.conll",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 18:44, #', Training set for ctb5 dependency parsing.
CTB5_DEP_DEV = _CTB5_DEP_HOME + 'dev.conll'
Dev set for ctb5 dependency parsing.",
https://github.com/hankcs/HanLP,ctb7.py,0,0.0,5,31.25,7,43.75,0,0.0,4,25.0,0,0.0,0,0,4,0,,"CTB7_DEP_DEV, CTB7_DEP_TEST, CTB7_DEP_TRAIN, _CTB7_HOME, hanlp",hanlp.datasets.parsing.ctb5._CTB_HOME,,,"CTB7_DEP_DEV, CTB7_DEP_TEST, CTB7_DEP_TRAIN, _CTB7_HOME",,"BPNN/data/ctb7/, Dev set for ctb7 dependency parsing., Test set for ctb7 dependency parsing., Training set for ctb7 dependency parsing., dev.conll, test.conll, train.conll",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 18:44, Training set for ctb7 dependency parsing.
CTB7_DEP_DEV = _CTB7_HOME + 'dev.conll'
Dev set for ctb7 dependency parsing.",
https://github.com/hankcs/HanLP,ctb8.py,0,0.0,17,28.81,31,52.54,0,0.0,11,18.64,0,0.0,0,0,16,0,,"CTB8_BRACKET_LINE_DEV, CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TEST, CTB8_BRACKET_LINE_NOEC_TRAIN, CTB8_BRACKET_LINE_TEST, CTB8_BRACKET_LINE_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_CWS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_POS_TRAIN, CTB8_SD330_DEV, CTB8_SD330_TEST, CTB8_SD330_TRAIN, _CTB8_HOME, hanlp",hanlp.datasets.parsing.loaders._ctb_utils.make_ctb,,,"CTB8_BRACKET_LINE_DEV, CTB8_BRACKET_LINE_NOEC_DEV, CTB8_BRACKET_LINE_NOEC_TEST, CTB8_BRACKET_LINE_NOEC_TRAIN, CTB8_BRACKET_LINE_TEST, CTB8_BRACKET_LINE_TRAIN, CTB8_CWS_DEV, CTB8_CWS_TEST, CTB8_CWS_TRAIN, CTB8_POS_DEV, CTB8_POS_TEST, CTB8_POS_TRAIN, CTB8_SD330_DEV, CTB8_SD330_TEST, CTB8_SD330_TRAIN, _CTB8_HOME",,"Dev set for ctb8 Chinese word segmentation., Dev set for ctb8 PoS tagging., Dev set for ctb8 constituency parsing with empty categories., Dev set for ctb8 constituency parsing without empty categories., Dev set for ctb8 in Stanford Dependencies 3.3.0 standard., Test set for ctb8 Chinese word segmentation., Test set for ctb8 PoS tagging., Test set for ctb8 constituency parsing with empty categories., Test set for ctb8 constituency parsing without empty categories., Test set for ctb8 in Stanford Dependencies 3.3.0 standard., Training set for ctb8 Chinese word segmentation., Training set for ctb8 PoS tagging., Training set for ctb8 constituency parsing with empty categories., Training set for ctb8 constituency parsing without empty categories., Training set for ctb8 in Stanford Dependencies 3.3.0 standard., https://wakespace.lib.wfu.edu/bitstream/handle/10339/39379/LDC2013T21.tgz#data/, tasks/cws/dev.txt, tasks/cws/test.txt, tasks/cws/train.txt, tasks/dep/dev.conllx, tasks/dep/test.conllx, tasks/dep/train.conllx, tasks/par/dev.noempty.txt, tasks/par/dev.txt, tasks/par/test.noempty.txt, tasks/par/test.txt, tasks/par/train.noempty.txt, tasks/par/train.txt, tasks/pos/dev.tsv, tasks/pos/test.tsv, tasks/pos/train.tsv",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-14 20:54, #data/', Dev set for ctb8 PoS tagging.
CTB8_POS_TEST = _CTB8_HOME + 'tasks/pos/test.tsv'
Test set for ctb8 PoS tagging., Dev set for ctb8 constituency parsing without empty categories.
CTB8_BRACKET_LINE_NOEC_TEST = _CTB8_HOME + 'tasks/par/test.noempty.txt'
Test set for ctb8 constituency parsing without empty categories., Test set for ctb8 Chinese word segmentation.

CTB8_POS_TRAIN = _CTB8_HOME + 'tasks/pos/train.tsv'
Training set for ctb8 PoS tagging., Test set for ctb8 constituency parsing with empty categories.

CTB8_BRACKET_LINE_NOEC_TRAIN = _CTB8_HOME + 'tasks/par/train.noempty.txt'
Training set for ctb8 constituency parsing without empty categories., Training set for ctb8 Chinese word segmentation.
CTB8_CWS_DEV = _CTB8_HOME + 'tasks/cws/dev.txt'
Dev set for ctb8 Chinese word segmentation., Training set for ctb8 constituency parsing with empty categories.
CTB8_BRACKET_LINE_DEV = _CTB8_HOME + 'tasks/par/dev.txt'
Dev set for ctb8 constituency parsing with empty categories., Training set for ctb8 in Stanford Dependencies 3.3.0 standard.
CTB8_SD330_DEV = _CTB8_HOME + 'tasks/dep/dev.conllx'
Dev set for ctb8 in Stanford Dependencies 3.3.0 standard.",
https://github.com/hankcs/HanLP,ctb9.py,0,0.0,19,30.65,32,51.61,0,0.0,11,17.74,0,0.0,0,0,16,0,,"CTB9_BRACKET_LINE_DEV, CTB9_BRACKET_LINE_NOEC_DEV, CTB9_BRACKET_LINE_NOEC_TEST, CTB9_BRACKET_LINE_NOEC_TRAIN, CTB9_BRACKET_LINE_TEST, CTB9_BRACKET_LINE_TRAIN, CTB9_CWS_DEV, CTB9_CWS_TEST, CTB9_CWS_TRAIN, CTB9_POS_DEV, CTB9_POS_TEST, CTB9_POS_TRAIN, CTB9_SD330_DEV, CTB9_SD330_TEST, CTB9_SD330_TRAIN, FileNotFoundError, _CTB9_HOME, hanlp, urllib","hanlp.datasets.parsing.loaders._ctb_utils.make_ctb, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.path_from_url, urllib.error.HTTPError",,,"CTB9_BRACKET_LINE_DEV, CTB9_BRACKET_LINE_NOEC_DEV, CTB9_BRACKET_LINE_NOEC_TEST, CTB9_BRACKET_LINE_NOEC_TRAIN, CTB9_BRACKET_LINE_TEST, CTB9_BRACKET_LINE_TRAIN, CTB9_CWS_DEV, CTB9_CWS_TEST, CTB9_CWS_TRAIN, CTB9_POS_DEV, CTB9_POS_TEST, CTB9_POS_TRAIN, CTB9_SD330_DEV, CTB9_SD330_TEST, CTB9_SD330_TRAIN, _CTB9_HOME",,"Chinese Treebank 9.0 is a copyright dataset owned by LDC which we cannot re-distribute. Please apply for a licence from LDC (https://catalog.ldc.upenn.edu/LDC2016T13) then download it to , Dev set for ctb9 Chinese word segmentation., Dev set for ctb9 PoS tagging., Dev set for ctb9 constituency parsing with empty categories., Dev set for ctb9 constituency parsing without empty categories., Dev set for ctb9 in Stanford Dependencies 3.3.0 standard., Test set for ctb9 Chinese word segmentation., Test set for ctb9 PoS tagging., Test set for ctb9 constituency parsing with empty categories., Test set for ctb9 constituency parsing without empty categories., Test set for ctb9 in Stanford Dependencies 3.3.0 standard., Training set for ctb9 Chinese word segmentation., Training set for ctb9 PoS tagging., Training set for ctb9 constituency parsing with empty categories., Training set for ctb9 constituency parsing without empty categories., Training set for ctb9 in Stanford Dependencies 3.3.0 standard., https://catalog.ldc.upenn.edu/LDC2016T13/ctb9.0_LDC2016T13.tgz#data/, tasks/cws/dev.txt, tasks/cws/test.txt, tasks/cws/train.txt, tasks/dep/dev.conllx, tasks/dep/test.conllx, tasks/dep/train.conllx, tasks/par/dev.noempty.txt, tasks/par/dev.txt, tasks/par/test.noempty.txt, tasks/par/test.txt, tasks/par/train.noempty.txt, tasks/par/train.txt, tasks/pos/dev.tsv, tasks/pos/test.tsv, tasks/pos/train.tsv",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-14 20:54, #data/', Dev set for ctb9 PoS tagging.
CTB9_POS_TEST = _CTB9_HOME + 'tasks/pos/test.tsv'
Test set for ctb9 PoS tagging., Dev set for ctb9 constituency parsing without empty categories.
CTB9_BRACKET_LINE_NOEC_TEST = _CTB9_HOME + 'tasks/par/test.noempty.txt'
Test set for ctb9 constituency parsing without empty categories., Test set for ctb9 Chinese word segmentation.

CTB9_POS_TRAIN = _CTB9_HOME + 'tasks/pos/train.tsv'
Training set for ctb9 PoS tagging., Test set for ctb9 constituency parsing with empty categories.

CTB9_BRACKET_LINE_NOEC_TRAIN = _CTB9_HOME + 'tasks/par/train.noempty.txt'
Training set for ctb9 constituency parsing without empty categories., Training set for ctb9 Chinese word segmentation.
CTB9_CWS_DEV = _CTB9_HOME + 'tasks/cws/dev.txt'
Dev set for ctb9 Chinese word segmentation., Training set for ctb9 constituency parsing with empty categories.
CTB9_BRACKET_LINE_DEV = _CTB9_HOME + 'tasks/par/dev.txt'
Dev set for ctb9 constituency parsing with empty categories., Training set for ctb9 in Stanford Dependencies 3.3.0 standard.
CTB9_SD330_DEV = _CTB9_HOME + 'tasks/dep/dev.conllx'
Dev set for ctb9 in Stanford Dependencies 3.3.0 standard.",
https://github.com/hankcs/HanLP,pmt1.py,0,0.0,41,54.67,18,24.0,7,9.33,9,12.0,0,0.0,1,0,23,0,,"PTM_V1_DEV, PTM_V1_RAW, PTM_V1_TEST, PTM_V1_TRAIN, _HOME, _make_ptm, append, arc, buffer, clear, dirname, done, enumerate, hanlp, hanlp_common, home, isfile, join, len, line, offset, open, os, out, part, path, portion, pos, prev_offset, raw, rel, sent, sents, split, src, str, strip, tok, write, x, zip","hanlp.utils.io_util.get_resource, hanlp.utils.log_util.cprint, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLWord",_make_ptm,,"PTM_V1_DEV, PTM_V1_RAW, PTM_V1_TEST, PTM_V1_TRAIN, _HOME, arc, buffer, done, home, line, offset, out, part, portion, pos, prev_offset, raw, rel, sent, sents, src, tok, x",,"

, #199801_dependency_treebank_2014pos.txt, #dev.conllx, #test.conllx, #train.conllx, )[/cyan] in , .conllx, :, The dev set of PKU Multi-view Chinese Treebank (PMT) 1.0 (:cite:`qiu-etal-2014-multi`)., The test set of PKU Multi-view Chinese Treebank (PMT) 1.0 (:cite:`qiu-etal-2014-multi`)., The training set of PKU Multi-view Chinese Treebank (PMT) 1.0 (:cite:`qiu-etal-2014-multi`)., [/yellow] sentences [cyan][, [yellow], dev, https://github.com/qiulikun/PKUMultiviewTreebank/archive/refs/heads/master.zip, test, train, w","0, 1, 12000, 13000, 14463, False, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-02-15 04:14, # Sentences 12001-13000 and 13001-14463 are used as the development and test set, respectively. The remaining, # sentences are used as training data., #199801_dependency_treebank_2014pos.txt', #dev.conllx', #test.conllx', #train.conllx'",
https://github.com/hankcs/HanLP,ptb.py,0,0.0,8,13.56,42,71.19,0,0.0,9,15.25,0,0.0,0,0,8,0,,"PTB_DEV, PTB_SD330_DEV, PTB_SD330_TEST, PTB_SD330_TRAIN, PTB_TEST, PTB_TOKEN_MAPPING, PTB_TRAIN, _PTB_HOME",,,,"PTB_DEV, PTB_SD330_DEV, PTB_SD330_TEST, PTB_SD330_TRAIN, PTB_TEST, PTB_TOKEN_MAPPING, PTB_TRAIN, _PTB_HOME",,""", ', '', (, ), --, -LCB-, -LRB-, -LSB-, -RCB-, -RRB-, -RSB-, 02-21.10way.clean, 22.auto.clean, 23.auto.clean, Dev set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., Dev set of PTB without empty categories. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., Test set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., Test set of PTB without empty categories. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., Training set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., Training set of PTB without empty categories. PoS tags are automatically predicted using 10-fold 
jackknifing (:cite:`collins-koo-2005-discriminative`)., [, ], `, ``, https://github.com/KhalilMrini/LAL-Parser/archive/master.zip#data/, ptb_dev_3.3.0.sd.clean, ptb_test_3.3.0.sd.clean, ptb_train_3.3.0.sd.clean, {, }, «, », –, —, ‘, ’, “, ”, „, ‹, ›",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-02-17 15:46, # em dash, # en dash, #data/', Dev set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold
jackknifing (:cite:`collins-koo-2005-discriminative`).'''
PTB_SD330_TEST = _PTB_HOME + 'ptb_test_3.3.0.sd.clean'
Test set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold, Test set of PTB without empty categories. PoS tags are automatically predicted using 10-fold
jackknifing (:cite:`collins-koo-2005-discriminative`).'''

PTB_SD330_TRAIN = _PTB_HOME + 'ptb_train_3.3.0.sd.clean'
Training set of PTB in Stanford Dependencies 3.3.0 format. PoS tags are automatically predicted using 10-fold, Training set of PTB without empty categories. PoS tags are automatically predicted using 10-fold
jackknifing (:cite:`collins-koo-2005-discriminative`).'''
PTB_DEV = _PTB_HOME + '22.auto.clean'
Dev set of PTB without empty categories. PoS tags are automatically predicted using 10-fold",
https://github.com/hankcs/HanLP,semeval15.py,0,0.0,31,55.36,14,25.0,3,5.36,8,14.29,0,0.0,3,0,14,0,,"append, append_bos_to_form_pos, ar, arc, arc_2nd, arc_key, arc_per_token, deps, dict, each, enumerate, from_dict, hanlp_common, i, int, len, merge_head_deprel_with_2nd, pad_rel, pos_key, r, range, rel, rel_2nd, rel_key, rel_per_token, sample, sample_str, split, unpack_deps_to_head_deprel, warnings, zip","hanlp_common.conll.CoNLLSentence, hanlp_common.constant.PAD, hanlp_common.constant.ROOT, warnings.warn","append_bos_to_form_pos, merge_head_deprel_with_2nd, unpack_deps_to_head_deprel",,"a, ar, arc, arc_2nd, arc_per_token, deps, each, i, pad_rel, r, rel, rel_2nd, rel_per_token, sample_str",,", which means joint mode might not be suitable. The sample is
, :, CPOS, DEPS, FORM, The main dependency conflicts with 2nd dependency at ID=, _, arc, arc_2nd, pos, rel, rel_2nd, token, |","False, None, True","#, #         pass, #     def load_file(self, filepath: str):, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-28 14:40, # class SemEval15Dataset(CoNLLParsingDataset):, # from hanlp.datasets.parsing.conll_dataset import CoNLLParsingDataset",
https://github.com/hankcs/HanLP,semeval16.py,0,0.0,38,42.7,29,32.58,4,4.49,18,20.22,0,0.0,1,0,26,0,,"SEMEVAL2016_FULL_DEV_CONLLU, SEMEVAL2016_FULL_TEST_CONLLU, SEMEVAL2016_FULL_TRAIN_CONLLU, SEMEVAL2016_NEWS_DEV, SEMEVAL2016_NEWS_DEV_CONLLU, SEMEVAL2016_NEWS_TEST, SEMEVAL2016_NEWS_TEST_CONLLU, SEMEVAL2016_NEWS_TRAIN, SEMEVAL2016_NEWS_TRAIN_CONLLU, SEMEVAL2016_TEXT_DEV, SEMEVAL2016_TEXT_DEV_CONLLU, SEMEVAL2016_TEXT_TEST, SEMEVAL2016_TEXT_TEST_CONLLU, SEMEVAL2016_TEXT_TRAIN, SEMEVAL2016_TEXT_TRAIN_CONLLU, _SEMEVAL2016_HOME, conllu, convert_conll_to_conllu, deprel, deps, dst, from_file, group, hanlp, hanlp_common, head, open, os, out, part, path, root, sent, sents, str, word, write, zip","hanlp.utils.io_util.get_resource, hanlp.utils.io_util.merge_files, hanlp_common.conll.CoNLLSentence, hanlp_common.io.eprint, os.path.basename, os.path.isfile, os.path.splitext",convert_conll_to_conllu,,"SEMEVAL2016_FULL_DEV_CONLLU, SEMEVAL2016_FULL_TEST_CONLLU, SEMEVAL2016_FULL_TRAIN_CONLLU, SEMEVAL2016_NEWS_DEV, SEMEVAL2016_NEWS_DEV_CONLLU, SEMEVAL2016_NEWS_TEST, SEMEVAL2016_NEWS_TEST_CONLLU, SEMEVAL2016_NEWS_TRAIN, SEMEVAL2016_NEWS_TRAIN_CONLLU, SEMEVAL2016_TEXT_DEV, SEMEVAL2016_TEXT_DEV_CONLLU, SEMEVAL2016_TEXT_TEST, SEMEVAL2016_TEXT_TEST_CONLLU, SEMEVAL2016_TEXT_TRAIN, SEMEVAL2016_TEXT_TRAIN_CONLLU, _SEMEVAL2016_HOME, conllu, dst, file, group, out, part, root, sent, sents, word",,"

,  ...,  and ,  into full dataset ,  to , #test/full.test.conllu, #test/news.test.conll, #test/news.test.conllu, #test/text.test.conll, #test/text.test.conllu, #train/full.train.conllu, #train/news.train.conll, #train/news.train.conllu, #train/text.train.conll, #train/text.train.conllu, #validation/full.valid.conllu, #validation/news.valid.conll, #validation/news.valid.conllu, #validation/text.valid.conll, #validation/text.valid.conllu, .conllu, /train/full., Concatenating , Converting , https://github.com/HIT-SCIR/SemEval-2016/archive/master.zip, test, train, valid, w","0, 1, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 00:51, #test/full.test.conllu', #test/news.test.conll', #test/news.test.conllu', #test/text.test.conll', #test/text.test.conllu', #train/full.train.conllu', #train/news.train.conll', #train/news.train.conllu', #train/text.train.conll', #train/text.train.conllu', #validation/full.valid.conllu', #validation/news.valid.conll', #validation/news.valid.conllu', #validation/text.valid.conll', #validation/text.valid.conllu'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 00:51",
https://github.com/hankcs/HanLP,ctb5.py,0,0.0,4,23.53,7,41.18,0,0.0,6,35.29,0,0.0,0,0,4,0,,"CTB5_POS_DEV, CTB5_POS_TEST, CTB5_POS_TRAIN, _CTB5_POS_HOME",,,,"CTB5_POS_DEV, CTB5_POS_TEST, CTB5_POS_TRAIN, _CTB5_POS_HOME",,"#dev.tsv, #test.tsv, #train.tsv, PoS dev set for CTB5., PoS test set for CTB5., PoS training set for CTB5., http://file.hankcs.com/corpus/ctb5.1-pos.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:51, #test.tsv', #train.tsv', PoS training set for CTB5.
CTB5_POS_DEV = f'{_CTB5_POS_HOME}#dev.tsv'
PoS dev set for CTB5.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:50",
https://github.com/hankcs/HanLP,hotpotqa.py,0,0.0,97,74.62,23,17.69,5,3.85,5,3.85,0,0.0,16,4,43,0,,"BuildGraph, DOCUMENT_TITLE, HOTPOT_QA_DISTRACTOR_DEV, HOTPOT_QA_FULLWIKI_DEV, HOTPOT_QA_TRAIN, HotpotQADataset, NON_SP_ROOT, NON_SP_WORD, Q_ROOT, Q_WORD, SP_ROOT, SP_WORD, Type, Vertex, __call__, __hash__, __init__, __str__, add, append, arc, batch, build_graph, connect, context, create_sp_label, debug, deprels, document, dst, each, enum, enumerate, extend, fd, filepath, flat_sentence, get, graph, hanlp, hanlp_common, heads, hotpotqa_collate_fn, id, ids, idx, index_, int, is_question, is_sp, is_sp_root, is_sp_root_candidate, is_word, label, len, load_file, lower, map, max, max_seq_len, object, offset, open, parsed_sents, print, q_root, raw, raw_sents, rel, sample, samples, self, sent, sents, seq_lengths, sp, sp_candidate_mask, sp_idx, sp_label, sp_sents, src_mask, str, sum, super, text, title, title_, to, token, token_offset, tokens, torch, type, ujson, v, x, zip","enum.Enum, enum.auto, hanlp.common.dataset.TransformableDataset, hanlp_common.util.merge_list_of_dict, torch.LongTensor, torch.bool, torch.float, torch.long, torch.nn.utils.rnn.pad_sequence, torch.zeros, ujson.load","__call__, __hash__, __init__, __str__, build_graph, connect, create_sp_label, flat_sentence, hotpotqa_collate_fn, is_question, is_sp, is_sp_root, is_sp_root_candidate, is_word, label, load_file","BuildGraph, HotpotQADataset, Type, Vertex","DOCUMENT_TITLE, HOTPOT_QA_DISTRACTOR_DEV, HOTPOT_QA_FULLWIKI_DEV, HOTPOT_QA_TRAIN, NON_SP_ROOT, NON_SP_WORD, Q_ROOT, Q_WORD, SP_ROOT, SP_WORD, arc, batch, context, deprels, document, fd, graph, heads, ids, idx, max_seq_len, offset, parsed_sents, q_root, raw, raw_sents, sample, sent, sents, seq_lengths, sp, sp_candidate_mask, sp_idx, sp_label, sp_sents, src_mask, text, title, token, token_offset, tokens, v, x",," , ., Question: , Supporting Fact: , adj, bos, context, graph, http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json, http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_fullwiki_v1.json, http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json, parsed_sentences, question, seq_lengths, sp_candidate_mask, sp_label, src_mask, supporting fact?, supporting_facts, token, token_id, token_offset, tokens","0, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-20 19:46, # record each vertex's token offset, # sp = torch.zeros([len(samples), max_seq_len], dtype=torch.bool)",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-03-20 19:17",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-22 19:15",
https://github.com/hankcs/HanLP,stsb.py,0,0.0,21,58.33,7,19.44,2,5.56,6,16.67,0,0.0,2,1,3,0,,"STS_B_DEV, STS_B_TEST, STS_B_TRAIN, SemanticTextualSimilarityDataset, __init__, cache, delimiter, enumerate, filepath, float, generate_idx, hanlp, load_file, self, sent_a_col, sent_b_col, similarity_col, str, super, transform, typing","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.read_cells, typing.Callable, typing.List, typing.Union","__init__, load_file",SemanticTextualSimilarityDataset,"STS_B_DEV, STS_B_TEST, STS_B_TRAIN",,"auto, http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz#sts-dev.csv, http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz#sts-test.csv, http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz#sts-train.csv, sent_a, sent_b, similarity","None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-20 16:25, #sts-dev.csv', #sts-test.csv', #sts-train.csv'",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-20 16:25",
https://github.com/hankcs/HanLP,ctb6.py,0,0.0,4,23.53,7,41.18,0,0.0,6,35.29,0,0.0,0,0,4,0,,"CTB6_CWS_DEV, CTB6_CWS_TEST, CTB6_CWS_TRAIN, _CTB6_CWS_HOME",,,,"CTB6_CWS_DEV, CTB6_CWS_TEST, CTB6_CWS_TRAIN, _CTB6_CWS_HOME",,"#dev.txt, #test.txt, #train.txt, CTB6 dev set., CTB6 test set., CTB6 training set., http://file.hankcs.com/corpus/ctb6_cws.zip",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-28 22:19, #test.txt', #train.txt', CTB6 training set.
CTB6_CWS_DEV = _CTB6_CWS_HOME + '#dev.txt'
CTB6 dev set.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-01 12:33",
https://github.com/hankcs/HanLP,chunking_dataset.py,0,0.0,33,80.49,3,7.32,2,4.88,3,7.32,0,0.0,3,1,13,0,,"ChunkingDataset, __init__, _generate_chars_tags, append, cache, char, chars, delimiter, enumerate, filepath, generate_idx, hanlp, idx, isinstance, len, list, load_file, max_seq_len, open, self, sent_delimiter, short_chars, short_tags, src, staticmethod, super, tag, tags, text, transform, typing, x, zip","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.get_resource, hanlp.utils.span_util.bmes_of, hanlp.utils.string_util.ispunct, typing.Callable, typing.List, typing.Union","__init__, _generate_chars_tags, load_file",ChunkingDataset,"char, chars, delimiter, filepath, idx, max_seq_len, sent_delimiter, short_chars, short_tags, src, tag, tags, text",,"char, tag, utf8","None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-03 18:50",
https://github.com/hankcs/HanLP,txt.py,0,0.0,42,46.15,14,15.38,3,3.3,14,15.38,18,19.78,5,1,8,3,,"NotImplementedError, TextTokenizingDataset, __init__, append, cache, char_level, delimiter, dict, erase, filepath, generate_idx, generate_tags_for_subtokens, get, hanlp, hard_constraint, len, line, load_file, log, max_seq_len, results, rstrip, sample, self, sent_delimiter, short_sents, split, subtoken_offsets, subtoken_offsets_group, subtoken_offsets_to_subtokens, subtokens_group, subtokens_group_to_subtokens, sum, super, tagging_scheme, text, token, token_subtoken_offsets, tokens, transform, typing, zip","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.TimingFileIterator, hanlp.utils.span_util.words_to_bi, hanlp.utils.span_util.words_to_bmes, hanlp.utils.string_util.split_long_sentence_into, typing.Callable, typing.List, typing.Union","__init__, generate_tags_for_subtokens, load_file, subtoken_offsets_to_subtokens, subtokens_group_to_subtokens",TextTokenizingDataset,"f, line, results, short_sents, subtoken_offsets, subtokens_group, token, tokens","A dataset for tagging tokenization tasks.

Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler.
    delimiter: Delimiter between tokens used to split a line in the corpus.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway., Create a sequence of x for tokenization task. Each x is an atomic subtoken that will be tagged with BMES or BI tags.

Args:
    sample: During prediction, it is a dict with 'token' being the input text, 'token_subtoken_offsets' being
     incremental offsets per each subtoken. During training, it is a dict with 'token' being a sequence of tokens,
     'token_subtoken_offsets' being non-incremental offsets per each subtoken, 'token_subtoken_offsets_group' being
     subtoken offsets grouped by each token.
    tagging_scheme:

Returns:, Load tokenized corpus. The format is one sentence per line, where each line consisits of tokens seperated
by a delimiter (usually space).

.. highlight:: bash
.. code-block:: bash

    $ head train.txt
    上海 浦东 开发 与 法制 建设 同步
    新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

Args:
    filepath: The path to the corpus.","
, 
    Create a sequence of x for tokenization task. Each x is an atomic subtoken that will be tagged with BMES or BI tags.

    Args:
        sample: During prediction, it is a dict with 'token' being the input text, 'token_subtoken_offsets' being
         incremental offsets per each subtoken. During training, it is a dict with 'token' being a sequence of tokens,
         'token_subtoken_offsets' being non-incremental offsets per each subtoken, 'token_subtoken_offsets_group' being
         subtoken offsets grouped by each token.
        tagging_scheme:

    Returns:

    , ., A dataset for tagging tokenization tasks.

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.
            delimiter: Delimiter between tokens used to split a line in the corpus.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
        , BI, BMES, Load tokenized corpus. The format is one sentence per line, where each line consisits of tokens seperated
        by a delimiter (usually space).

        .. highlight:: bash
        .. code-block:: bash

            $ head train.txt
            上海 浦东 开发 与 法制 建设 同步
            新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

        Args:
            filepath: The path to the corpus.
        , Unsupported tagging scheme , raw_token, tag, token, token_, token_subtoken_offsets, token_subtoken_offsets_group","20, False, None","
Create a sequence of x for tokenization task. Each x is an atomic subtoken that will be tagged with BMES or BI tags.

Args:
sample: During prediction, it is a dict with 'token' being the input text, 'token_subtoken_offsets' being
incremental offsets per each subtoken. During training, it is a dict with 'token' being a sequence of tokens,
'token_subtoken_offsets' being non-incremental offsets per each subtoken, 'token_subtoken_offsets_group' being
subtoken offsets grouped by each token.
tagging_scheme:

Returns:

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-01 12:35, # We could use token_token_span but we don't want token_token_span in the batch, # assert debug == tokens, # debug = [], # debug.extend(short_sents), # longest_sent = 0, # longest_sent = max(longest_sent, len(''.join(short_sents))), # longest_sent = max(longest_sent, len(''.join(tokens))), # print(f'Longest sent: {longest_sent} in {filepath}'), A dataset for tagging tokenization tasks.

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.
delimiter: Delimiter between tokens used to split a line in the corpus.
max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
be split here.
char_level: Whether the sequence length is measured at char level.
hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
in a sentence, it will be split at a token anyway.
, Load tokenized corpus. The format is one sentence per line, where each line consisits of tokens seperated
by a delimiter (usually space).

.. highlight:: bash
.. code-block:: bash

$ head train.txt
上海 浦东 开发 与 法制 建设 同步
新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

Args:
filepath: The path to the corpus.
","Load tokenized corpus. The format is one sentence per line, where each line consisits of tokens seperated
by a delimiter (usually space).

.. highlight:: bash
.. code-block:: bash

$ head train.txt
上海 浦东 开发 与 法制 建设 同步
新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

Args:
filepath: The path to the corpus.
, 、, 上海, 与, 二月, 十日, 同步, 建设, 开发, 张持坚, 新华社, 法制, 浦东, 电, 记者, 谢金虎, （, ）"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:06",
https://github.com/hankcs/HanLP,as_.py,0,0.0,7,24.14,13,44.83,0,0.0,9,31.03,0,0.0,0,0,6,0,,"SIGHAN2005_AS_DEV, SIGHAN2005_AS_DICT, SIGHAN2005_AS_TEST, SIGHAN2005_AS_TEST_INPUT, SIGHAN2005_AS_TRAIN, SIGHAN2005_AS_TRAIN_ALL, hanlp","hanlp.datasets.tokenization.sighan2005.SIGHAN2005, hanlp.datasets.tokenization.sighan2005.make",,,"SIGHAN2005_AS_DEV, SIGHAN2005_AS_DICT, SIGHAN2005_AS_TEST, SIGHAN2005_AS_TEST_INPUT, SIGHAN2005_AS_TRAIN, SIGHAN2005_AS_TRAIN_ALL",,"#, Dev set (last 10% of full official training set)., Dictionary built on trainings set., Full training set., Test input., Test set., Training set (first 90% of the full official training set)., gold/as_testing_gold.utf8, gold/as_training_words.utf8, testing/as_testing.utf8, training/as_training.utf8, training/as_training_10.txt, training/as_training_90.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:42, #"" + ""gold/as_training_words.utf8"", #"" + ""testing/as_testing.utf8"", #"" + ""training/as_training_90.txt"", Dictionary built on trainings set.
SIGHAN2005_AS_TRAIN_ALL = SIGHAN2005 + ""#"" + ""training/as_training.utf8""
Full training set., Test input.
SIGHAN2005_AS_TEST = SIGHAN2005 + ""#"" + ""gold/as_testing_gold.utf8""
Test set., Training set (first 90% of the full official training set).
SIGHAN2005_AS_DEV = SIGHAN2005 + ""#"" + ""training/as_training_10.txt""
Dev set (last 10% of full official training set).",
https://github.com/hankcs/HanLP,cityu.py,0,0.0,7,24.14,13,44.83,0,0.0,9,31.03,0,0.0,0,0,6,0,,"SIGHAN2005_CITYU_DEV, SIGHAN2005_CITYU_DICT, SIGHAN2005_CITYU_TEST, SIGHAN2005_CITYU_TEST_INPUT, SIGHAN2005_CITYU_TRAIN, SIGHAN2005_CITYU_TRAIN_ALL, hanlp","hanlp.datasets.tokenization.sighan2005.SIGHAN2005, hanlp.datasets.tokenization.sighan2005.make",,,"SIGHAN2005_CITYU_DEV, SIGHAN2005_CITYU_DICT, SIGHAN2005_CITYU_TEST, SIGHAN2005_CITYU_TEST_INPUT, SIGHAN2005_CITYU_TRAIN, SIGHAN2005_CITYU_TRAIN_ALL",,"#, Dev set (last 10% of full official training set)., Dictionary built on trainings set., Full training set., Test input., Test set., Training set (first 90% of the full official training set)., gold/cityu_test_gold.utf8, gold/cityu_training_words.utf8, testing/cityu_test.utf8, training/cityu_training.utf8, training/cityu_training_10.txt, training/cityu_training_90.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:42, #"" + ""gold/cityu_training_words.utf8"", #"" + ""testing/cityu_test.utf8"", #"" + ""training/cityu_training_90.txt"", Dictionary built on trainings set.
SIGHAN2005_CITYU_TRAIN_ALL = SIGHAN2005 + ""#"" + ""training/cityu_training.utf8""
Full training set., Test input.
SIGHAN2005_CITYU_TEST = SIGHAN2005 + ""#"" + ""gold/cityu_test_gold.utf8""
Test set., Training set (first 90% of the full official training set).
SIGHAN2005_CITYU_DEV = SIGHAN2005 + ""#"" + ""training/cityu_training_10.txt""
Dev set (last 10% of full official training set).",
https://github.com/hankcs/HanLP,msr.py,0,0.0,7,24.14,13,44.83,0,0.0,9,31.03,0,0.0,0,0,6,0,,"SIGHAN2005_MSR_DEV, SIGHAN2005_MSR_DICT, SIGHAN2005_MSR_TEST, SIGHAN2005_MSR_TEST_INPUT, SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_TRAIN_ALL, hanlp","hanlp.datasets.tokenization.sighan2005.SIGHAN2005, hanlp.datasets.tokenization.sighan2005.make",,,"SIGHAN2005_MSR_DEV, SIGHAN2005_MSR_DICT, SIGHAN2005_MSR_TEST, SIGHAN2005_MSR_TEST_INPUT, SIGHAN2005_MSR_TRAIN, SIGHAN2005_MSR_TRAIN_ALL",,"#, Dev set (last 10% of full official training set)., Dictionary built on trainings set., Full training set., Test input., Test set., Training set (first 90% of the full official training set)., gold/msr_test_gold.utf8, gold/msr_training_words.utf8, testing/msr_test.utf8, training/msr_training.utf8, training/msr_training_10.txt, training/msr_training_90.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:42, #"" + ""gold/msr_training_words.utf8"", #"" + ""testing/msr_test.utf8"", #"" + ""training/msr_training_90.txt"", Dictionary built on trainings set.
SIGHAN2005_MSR_TRAIN_ALL = SIGHAN2005 + ""#"" + ""training/msr_training.utf8""
Full training set., Test input.
SIGHAN2005_MSR_TEST = SIGHAN2005 + ""#"" + ""gold/msr_test_gold.utf8""
Test set., Training set (first 90% of the full official training set).
SIGHAN2005_MSR_DEV = SIGHAN2005 + ""#"" + ""training/msr_training_10.txt""
Dev set (last 10% of full official training set).",
https://github.com/hankcs/HanLP,pku.py,0,0.0,7,24.14,13,44.83,0,0.0,9,31.03,0,0.0,0,0,6,0,,"SIGHAN2005_PKU_DEV, SIGHAN2005_PKU_DICT, SIGHAN2005_PKU_TEST, SIGHAN2005_PKU_TEST_INPUT, SIGHAN2005_PKU_TRAIN, SIGHAN2005_PKU_TRAIN_ALL, hanlp","hanlp.datasets.tokenization.sighan2005.SIGHAN2005, hanlp.datasets.tokenization.sighan2005.make",,,"SIGHAN2005_PKU_DEV, SIGHAN2005_PKU_DICT, SIGHAN2005_PKU_TEST, SIGHAN2005_PKU_TEST_INPUT, SIGHAN2005_PKU_TRAIN, SIGHAN2005_PKU_TRAIN_ALL",,"#, Dev set (last 10% of full official training set)., Dictionary built on trainings set., Full training set., Test input., Test set., Training set (first 90% of the full official training set)., gold/pku_test_gold.utf8, gold/pku_training_words.utf8, testing/pku_test.utf8, training/pku_training.utf8, training/pku_training_10.txt, training/pku_training_90.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:42, #"" + ""gold/pku_training_words.utf8"", #"" + ""testing/pku_test.utf8"", #"" + ""training/pku_training_90.txt"", Dictionary built on trainings set.
SIGHAN2005_PKU_TRAIN_ALL = SIGHAN2005 + ""#"" + ""training/pku_training.utf8""
Full training set., Test input.
SIGHAN2005_PKU_TEST = SIGHAN2005 + ""#"" + ""gold/pku_test_gold.utf8""
Test set., Training set (first 90% of the full official training set).
SIGHAN2005_PKU_DEV = SIGHAN2005 + ""#"" + ""training/pku_training_10.txt""
Dev set (last 10% of full official training set).",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,11,34.38,13,40.62,4,12.5,4,12.5,0,0.0,1,0,5,0,,"SIGHAN2005, full, hanlp, info, make, os, replace, root, split, train, valid","hanlp.utils.io_util.get_resource, hanlp.utils.io_util.split_file, hanlp.utils.log_util.logger, os.path.isfile, os.path.join",make,,"SIGHAN2005, full, root, train, valid",," ,  into training set and valid set with 9:1 proportion, #, .utf8, 10.txt, 90.txt, Failed to make , Splitting , Successfully made , _90.txt, dev, http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip, train","0, 0.1, 0.9, 1","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-21 15:42, #')[-1])",
https://github.com/hankcs/HanLP,mcws_dataset.py,0,0.0,36,50.7,6,8.45,4,5.63,4,5.63,21,29.58,4,1,6,1,,"MultiCriteriaTextTokenizingDataset, __init__, append_criteria_token, bool, cache, char_level, criteria, criteria_token_map, criteria_tokens, delimiter, dict, eachpath, filepath, generate_idx, get, hanlp, hard_constraint, int, isinstance, items, keys, len, list, load_file, max_seq_len, os, sample, sent_delimiter, should_load_file, size, str, super, transform, tuple, typing, unused_tokens","hanlp.datasets.tokenization.loaders.txt.TextTokenizingDataset, hanlp.utils.io_util.get_resource, os.path.basename, os.path.dirname, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, append_criteria_token, load_file, should_load_file",MultiCriteriaTextTokenizingDataset,"criteria, eachpath, sample, size, token, unused_tokens","Load multi-criteria corpora specified in filepath.

Args:
    filepath: A list of files where filename is its criterion. Or a dict of filename-criterion pairs.

.. highlight:: bash
.. code-block:: bash

    $ tree -L 2 .
    .
    ├── cnc
    │   ├── dev.txt
    │   ├── test.txt
    │   ├── train-all.txt
    │   └── train.txt
    ├── ctb
    │   ├── dev.txt
    │   ├── test.txt
    │   ├── train-all.txt
    │   └── train.txt
    ├── sxu
    │   ├── dev.txt
    │   ├── test.txt
    │   ├── train-all.txt
    │   └── train.txt
    ├── udc
    │   ├── dev.txt
    │   ├── test.txt
    │   ├── train-all.txt
    │   └── train.txt
    ├── wtb
    │   ├── dev.txt
    │   ├── test.txt
    │   ├── train-all.txt
    │   └── train.txt
    └── zx
        ├── dev.txt
        ├── test.txt
        ├── train-all.txt
        └── train.txt

    $ head -n 2 ctb/dev.txt
    上海 浦东 开发 与 法制 建设 同步
    新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）",". Current criteria_token_map = , Load multi-criteria corpora specified in filepath.

        Args:
            filepath: A list of files where filename is its criterion. Or a dict of filename-criterion pairs.

        .. highlight:: bash
        .. code-block:: bash

            $ tree -L 2 .
            .
            ├── cnc
            │   ├── dev.txt
            │   ├── test.txt
            │   ├── train-all.txt
            │   └── train.txt
            ├── ctb
            │   ├── dev.txt
            │   ├── test.txt
            │   ├── train-all.txt
            │   └── train.txt
            ├── sxu
            │   ├── dev.txt
            │   ├── test.txt
            │   ├── train-all.txt
            │   └── train.txt
            ├── udc
            │   ├── dev.txt
            │   ├── test.txt
            │   ├── train-all.txt
            │   └── train.txt
            ├── wtb
            │   ├── dev.txt
            │   ├── test.txt
            │   ├── train-all.txt
            │   └── train.txt
            └── zx
                ├── dev.txt
                ├── test.txt
                ├── train-all.txt
                └── train.txt

            $ head -n 2 ctb/dev.txt
            上海 浦东 开发 与 法制 建设 同步
            新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

        , No unused token available for criteria , criteria, token_input_ids, token_token_type_ids","0, 1, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-21 19:11, Load multi-criteria corpora specified in filepath.

Args:
filepath: A list of files where filename is its criterion. Or a dict of filename-criterion pairs.

.. highlight:: bash
.. code-block:: bash

$ tree -L 2 .
.
├── cnc
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── ctb
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── sxu
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── udc
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── wtb
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
└── zx
├── dev.txt
├── test.txt
├── train-all.txt
└── train.txt

$ head -n 2 ctb/dev.txt
上海 浦东 开发 与 法制 建设 同步
新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

","Load multi-criteria corpora specified in filepath.

Args:
filepath: A list of files where filename is its criterion. Or a dict of filename-criterion pairs.

.. highlight:: bash
.. code-block:: bash

$ tree -L 2 .
.
├── cnc
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── ctb
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── sxu
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── udc
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
├── wtb
│   ├── dev.txt
│   ├── test.txt
│   ├── train-all.txt
│   └── train.txt
└── zx
├── dev.txt
├── test.txt
├── train-all.txt
└── train.txt

$ head -n 2 ctb/dev.txt
上海 浦东 开发 与 法制 建设 同步
新华社 上海 二月 十日 电 （ 记者 谢金虎 、 张持坚 ）

, │, └──, ├──, 、, 上海, 与, 二月, 十日, 同步, 建设, 开发, 张持坚, 新华社, 法制, 浦东, 电, 记者, 谢金虎, （, ）"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,25,46.3,25,46.3,0,0.0,4,7.41,0,0.0,0,0,25,0,,"CNC_DEV, CNC_TEST, CNC_TRAIN, CNC_TRAIN_ALL, CTB_DEV, CTB_TEST, CTB_TRAIN, CTB_TRAIN_ALL, SXU_DEV, SXU_TEST, SXU_TRAIN, SXU_TRAIN_ALL, UDC_DEV, UDC_TEST, UDC_TRAIN, UDC_TRAIN_ALL, WTB_DEV, WTB_TEST, WTB_TRAIN, WTB_TRAIN_ALL, ZX_DEV, ZX_TEST, ZX_TRAIN, ZX_TRAIN_ALL, _HOME",,,,"CNC_DEV, CNC_TEST, CNC_TRAIN, CNC_TRAIN_ALL, CTB_DEV, CTB_TEST, CTB_TRAIN, CTB_TRAIN_ALL, SXU_DEV, SXU_TEST, SXU_TRAIN, SXU_TRAIN_ALL, UDC_DEV, UDC_TEST, UDC_TRAIN, UDC_TRAIN_ALL, WTB_DEV, WTB_TEST, WTB_TRAIN, WTB_TRAIN_ALL, ZX_DEV, ZX_TEST, ZX_TRAIN, ZX_TRAIN_ALL, _HOME",,"cnc/dev.txt, cnc/test.txt, cnc/train-all.txt, cnc/train.txt, ctb/dev.txt, ctb/test.txt, ctb/train-all.txt, ctb/train.txt, https://github.com/hankcs/multi-criteria-cws/archive/naive-mix.zip#data/raw/, sxu/dev.txt, sxu/test.txt, sxu/train-all.txt, sxu/train.txt, udc/dev.txt, udc/test.txt, udc/train-all.txt, udc/train.txt, wtb/dev.txt, wtb/test.txt, wtb/train-all.txt, wtb/train.txt, zx/dev.txt, zx/test.txt, zx/train-all.txt, zx/train.txt",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-11 20:35, #data/raw/'",
https://github.com/hankcs/HanLP,conll2012.py,0,0.0,89,63.57,31,22.14,8,5.71,12,8.57,0,0.0,10,4,42,2,,"CoNLL2012BIOSRLDataset, CoNLL2012SRLBIODataset, CoNLL2012SRLDataset, SpanCandidatesGenerator, ValueError, __call__, __init__, _make_bio_labels, _remove_B_V, add, any, append, arg, args, argument_begin_offset, argument_end_offset, bel, build_sample, cache, cell, deduplicated_srl, dict, doc, doc_level_offset, dst, end, endswith, erase, fid, filename, filepath, files, filter_v_args, generate_idx, get, glob, group_pa_by_p, group_pa_by_p_, grouped_srl, hanlp, idx, items, json, label, labels, labels_per_p, len, line, list, load_file, log, map, max_span_width, num_docs, num_sentences, num_tokens_in_doc, os, pa_set, prd_bio_labels, prd_indices, predicate_offset, prev, prop, props, range, reader, sample, self, sense, sentence, sid, sorted, split, src, srl, srl_label, start, startswith, staticmethod, str, super, timer, token, tokens, typing, unpack_srl, update, x, zip","glob.glob, hanlp.common.dataset.TransformableDataset, hanlp.common.transform.NamedTransform, hanlp.utils.io_util.TimingFileIterator, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.read_tsv_as_sents, hanlp.utils.span_util.enumerate_spans, hanlp.utils.time_util.CountdownTimer, json.loads, os.path.basename, os.path.isdir, os.path.isfile, typing.Callable, typing.List, typing.Union","__call__, __init__, _make_bio_labels, _remove_B_V, build_sample, filter_v_args, group_pa_by_p, group_pa_by_p_, load_file, unpack_srl","CoNLL2012BIOSRLDataset, CoNLL2012SRLBIODataset, CoNLL2012SRLDataset, SpanCandidatesGenerator","arg, args, argument_begin_offset, argument_end_offset, bel, cell, deduplicated_srl, doc, dst, end, fid, filename, filepath, files, grouped_srl, idx, label, labels, labels_per_p, line, num_docs, num_sentences, num_tokens_in_doc, pa, pa_set, prd_bio_labels, prd_indices, predicate_offset, prev, prop, props, reader, sample, sense, sentence, sid, srl, srl_label, start, timer, token, x","Copied from https://github.com/hiroki13/span-based-srl/blob/2c8b677c4e00b6c607e09ef4f9fe3d54961e4f2e/src/utils/sent.py#L42

Args:
  prop: 1D: n_words; elem=bracket label

Returns:
  1D: n_words; elem=BIO label, Load ``.jsonlines`` CoNLL12-style corpus. Samples of this corpus can be found using the following scripts.

.. highlight:: python
.. code-block:: python

    import json
    from hanlp_common.document import Document
    from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
    from hanlp.utils.io_util import get_resource

    with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
        for line in src:
            doc = json.loads(line)
            print(Document(doc))
            break

Args:
    filepath: ``.jsonlines`` CoNLL12 corpus."," ,  documents, ,  has to be a directory of CoNLL 2012,  sentences [blink][yellow]...[/yellow][/blink], #, (, ), *, -, /**/*gold_conll, B-, B-V, C-V, Copied from https://github.com/hiroki13/span-based-srl/blob/2c8b677c4e00b6c607e09ef4f9fe3d54961e4f2e/src/utils/sent.py#L42

        Args:
          prop: 1D: n_words; elem=bracket label

        Returns:
          1D: n_words; elem=BIO label

        , I-, Load ``.jsonlines`` CoNLL12-style corpus. Samples of this corpus can be found using the following scripts.

        .. highlight:: python
        .. code-block:: python

            import json
            from hanlp_common.document import Document
            from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
            from hanlp.utils.io_util import get_resource

            with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
                for line in src:
                    doc = json.loads(line)
                    print(Document(doc))
                    break

        Args:
            filepath: ``.jsonlines`` CoNLL12 corpus.
        , Negative offset occurred, maybe doc_level_offset=False, O, Offset exceeds sentence length, maybe doc_level_offset=True, V, _span, argument_begin_offset, argument_end_offset, files loading[blink][yellow]...[/yellow][/blink], pos, predicate_offset, sentences, srl, srl_label, srl_set, token","0, 1, 11, 2, 3, 7, None, True","# 'srl_mask': len(srl_label),, # -*- coding:utf-8 -*-, # 0:DOCUMENT 1:PART 2:INDEX 3:WORD 4:POS 5:PARSE 6:LEMMA 7:FRAME 8:SENSE 9:SPEAKER 10:NE 11-N:ARGS N:COREF, # Author: hankcs, # Convert srl to exclusive format, # Date: 2020-06-22 19:15, # We can obtain mask by srl_label > 0, # We don't predict predicate, # noinspection PyMethodMayBeStatic, #'):, Copied from https://github.com/hiroki13/span-based-srl/blob/2c8b677c4e00b6c607e09ef4f9fe3d54961e4f2e/src/utils/sent.py#L42

Args:
prop: 1D: n_words; elem=bracket label

Returns:
1D: n_words; elem=BIO label

, Load ``.jsonlines`` CoNLL12-style corpus. Samples of this corpus can be found using the following scripts.

.. highlight:: python
.. code-block:: python

import json
from hanlp_common.document import Document
from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
from hanlp.utils.io_util import get_resource

with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
for line in src:
doc = json.loads(line)
print(Document(doc))
break

Args:
filepath: ``.jsonlines`` CoNLL12 corpus.
",
https://github.com/hankcs/HanLP,ontonotes_loader.py,0,0.0,123,48.24,39,15.29,13,5.1,76,29.8,4,1.57,11,2,71,10,,"Ontonotes, OntonotesSentence, __init__, __name__, _canonicalize_clusters, _conll_rows_to_sentence, _normalize_word, _process_coref_span_annotations_for_word, _process_span_annotations_for_word, all, annotation, annotation_index, annotations, any, append, bio_label, bool, cluster, cluster2, cluster_dict, cluster_id, cluster_with_overlapping_mention, clusters, codecs, collections, conll_components, conll_file, conll_rows, coref_span_tuples, coref_spans, coref_stacks, count, current_span_labels, data_file, dataset_document_iterator, dataset_iterator, dataset_path_iterator, document, document_id, end, endswith, fields, file_path, files, flat_sentences_tokens, flattened_sentences, float, framenet_id, fromstring, gold_clusters, hanlp, index, info, int, items, label, labels, left_brackets, lemmatised_word, len, line, list, logger, logging, make_coref_instance, max_sentences, max_span_width, mention, merged_clusters, named_entities, new_cluster, new_gold_clusters, open_file, os, parse_piece, parse_pieces, parse_tree, parse_word, phrasetree, pos_tag, pos_tags, predicate, predicate_framenet_ids, predicate_lemmas, remove_singleton_clusters, right_brackets, right_hand_side, root, row, segment, self, sentence, sentence_id, sentence_iterator, sentence_offset, sentences, span_field, span_labels, span_list, speaker, speakers, split, srl_frames, start, startswith, staticmethod, str, strip, sum, text_field, total_length, tuple, typing, update, verbal_predicates, word, word_index, word_is_verbal_predicate, word_sense, word_senses, words, x, zip","codecs.open, collections.defaultdict, hanlp.utils.span_util.TypedSpan, hanlp.utils.span_util.enumerate_spans, logging.getLogger, os.path.join, os.walk, phrasetree.tree.Tree, typing.DefaultDict, typing.Dict, typing.Iterator, typing.List, typing.Optional, typing.Set, typing.Tuple","__init__, _canonicalize_clusters, _conll_rows_to_sentence, _normalize_word, _process_coref_span_annotations_for_word, _process_span_annotations_for_word, dataset_document_iterator, dataset_iterator, dataset_path_iterator, make_coref_instance, sentence_iterator","Ontonotes, OntonotesSentence","annotation, annotation_index, bio_label, cluster, cluster2, cluster_dict, cluster_id, cluster_with_overlapping_mention, clusters, conll_components, conll_file, conll_rows, coref_span_tuples, coref_stacks, current_span_labels, data_file, document, document_id, end, fields, files, flat_sentences_tokens, flattened_sentences, framenet_id, gold_clusters, index, label, labels, left_brackets, lemmatised_word, line, logger, mention, merged_clusters, named_entities, new_cluster, new_gold_clusters, open_file, parse_piece, parse_pieces, parse_tree, parse_word, pos_tag, pos_tags, predicate, predicate_framenet_ids, predicate_lemmas, right_brackets, right_hand_side, root, row, segment, sentence, sentence_id, sentence_offset, sentences, span_field, span_labels, span_list, speaker, speakers, srl_frames, start, text_field, total_length, verbal_predicates, word, word_is_verbal_predicate, word_sense, word_senses, x","# Parameters

sentences : `List[List[str]]`, required.
    A list of lists representing the tokenised words and sentences in the document.
token_indexers : `Dict[str, TokenIndexer]`
    This is used to index the words in the document.  See :class:`TokenIndexer`.
max_span_width : `int`, required.
    The maximum width of candidate spans to consider.
gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)
    A list of all clusters in the document, represented as word spans with absolute indices
    in the entire document. Each cluster contains some number of spans, which can be nested
    and overlap. If there are exact matches between clusters, they will be resolved
    using `_canonicalize_clusters`.
wordpiece_modeling_tokenizer: `PretrainedTransformerTokenizer`, optional (default = None)
    If not None, this dataset reader does subword tokenization using the supplied tokenizer
    and distribute the labels to the resulting wordpieces. All the modeling will be based on
    wordpieces. If this is set to `False` (default), the user is expected to use
    `PretrainedTransformerMismatchedIndexer` and `PretrainedTransformerMismatchedEmbedder`,
    and the modeling will be on the word-level.
max_sentences: int, optional (default = None)
    The maximum number of sentences in each document to keep. By default keeps all sentences.
remove_singleton_clusters : `bool`, optional (default = True)
    Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows
    the removal of them.

# Returns

An `Instance` containing the following `Fields`:
    text : `TextField`
        The text of the full document.
    spans : `ListField[SpanField]`
        A ListField containing the spans represented as `SpanFields`
        with respect to the document text.
    span_labels : `SequenceLabelField`, optional
        The id of the cluster which each possible span belongs to, or -1 if it does
            not belong to a cluster. As these labels have variable length (it depends on
            how many spans we are considering), we represent this a as a `SequenceLabelField`
            with respect to the spans `ListField`., A class representing the annotations available for a single CONLL formatted sentence.

# Parameters

document_id : `str`
    This is a variation on the document filename
sentence_id : `int`
    The integer ID of the sentence within a document.
words : `List[str]`
    This is the tokens as segmented/tokenized in the Treebank.
pos_tags : `List[str]`
    This is the Penn-Treebank-style part of speech. When parse information is missing,
    all parts of speech except the one for which there is some sense or proposition
    annotation are marked with a XX tag. The verb is marked with just a VERB tag.
parse_tree : `nltk.Tree`
    An nltk Tree representing the parse. It includes POS tags as pre-terminal nodes.
    When the parse information is missing, the parse will be `None`.
predicate_lemmas : `List[Optional[str]]`
    The predicate lemma of the words for which we have semantic role
    information or word sense information. All other indices are `None`.
predicate_framenet_ids : `List[Optional[int]]`
    The PropBank frameset ID of the lemmas in `predicate_lemmas`, or `None`.
word_senses : `List[Optional[float]]`
    The word senses for the words in the sentence, or `None`. These are floats
    because the word sense can have values after the decimal, like `1.1`.
speakers : `List[Optional[str]]`
    The speaker information for the words in the sentence, if present, or `None`
    This is the speaker or author name where available. Mostly in Broadcast Conversation
    and Web Log data. When not available the rows are marked with an ""-"".
named_entities : `List[str]`
    The BIO tags for named entities in the sentence.
srl_frames : `List[Tuple[str, List[str]]]`
    A dictionary keyed by the verb in the sentence for the given
    Propbank frame labels, in a BIO format.
coref_spans : `Set[TypedSpan]`
    The spans for entity mentions involved in coreference resolution within the sentence.
    Each element is a tuple composed of (cluster_id, (start_index, end_index)). Indices
    are `inclusive`., An iterator over CONLL formatted files which yields documents, regardless
of the number of document annotations in a particular file. This is useful
for conll data which has been preprocessed, such as the preprocessing which
takes place for the 2012 CONLL Coreference Resolution task., An iterator over the entire dataset, yielding all sentences processed., An iterator over the sentences in an individual CONLL formatted file., An iterator returning file_paths in a directory
containing CONLL-formatted files., For a given coref label, add it to a currently open span(s), complete a span(s) or
ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks
dictionaries.

# Parameters

label : `str`
    The coref label for this word.
word_index : `int`
    The word index into the sentence.
clusters : `DefaultDict[int, List[Tuple[int, int]]]`
    A dictionary mapping cluster ids to lists of inclusive spans into the
    sentence.
coref_stacks : `DefaultDict[int, List[int]]`
    Stacks for each cluster id to hold the start indices of active spans (spans
    which we are inside of when processing a given word). Spans with the same id
    can be nested, which is why we collect these opening spans on a stack, e.g:

    [Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1, Given a sequence of different label types for a single word and the current
span label we are inside, compute the BIO tag for each label and append to a list.

# Parameters

annotations : `List[str]`
    A list of labels to compute BIO tags for.
span_labels : `List[List[str]]`
    A list of lists, one for each annotation, to incrementally collect
    the BIO tags for a sequence.
current_span_labels : `List[Optional[str]]`
    The currently open span per annotation type, or `None` if there is no open span., The data might include 2 annotated spans which are identical,
but have different ids. This checks all clusters for spans which are
identical, and if it finds any, merges the clusters containing the
identical spans., This `DatasetReader` is designed to read in the English OntoNotes v5.0 data
in the format used by the CoNLL 2011/2012 shared tasks. In order to use this
Reader, you must follow the instructions provided [here (v12 release):]
(https://cemantix.org/data/ontonotes.html), which will allow you to download
the CoNLL style annotations for the  OntoNotes v5.0 release -- LDC2013T19.tgz
obtained from LDC.

Once you have run the scripts on the extracted data, you will have a folder
structured as follows:

```
conll-formatted-ontonotes-5.0/
 ── data
   ├── development
       └── data
           └── english
               └── annotations
                   ├── bc
                   ├── bn
                   ├── mz
                   ├── nw
                   ├── pt
                   ├── tc
                   └── wb
   ├── test
       └── data
           └── english
               └── annotations
                   ├── bc
                   ├── bn
                   ├── mz
                   ├── nw
                   ├── pt
                   ├── tc
                   └── wb
   └── train
       └── data
           └── english
               └── annotations
                   ├── bc
                   ├── bn
                   ├── mz
                   ├── nw
                   ├── pt
                   ├── tc
                   └── wb
```

The file path provided to this class can then be any of the train, test or development
directories(or the top level data directory, if you are not utilizing the splits).

The data has the following format, ordered by column.

1.  Document ID : `str`
    This is a variation on the document filename
2.  Part number : `int`
    Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3.  Word number : `int`
    This is the word index of the word in that sentence.
4.  Word : `str`
    This is the token as segmented/tokenized in the Treebank. Initially the `*_skel` file
    contain the placeholder [WORD] which gets replaced by the actual token from the
    Treebank which is part of the OntoNotes release.
5.  POS Tag : `str`
    This is the Penn Treebank style part of speech. When parse information is missing,
    all part of speeches except the one for which there is some sense or proposition
    annotation are marked with a XX tag. The verb is marked with just a VERB tag.
6.  Parse bit : `str`
    This is the bracketed structure broken before the first open parenthesis in the parse,
    and the word/part-of-speech leaf replaced with a `*`. When the parse information is
    missing, the first word of a sentence is tagged as `(TOP*` and the last word is tagged
    as `*)` and all intermediate words are tagged with a `*`.
7.  Predicate lemma : `str`
    The predicate lemma is mentioned for the rows for which we have semantic role
    information or word sense information. All other rows are marked with a ""-"".
8.  Predicate Frameset ID : `int`
    The PropBank frameset ID of the predicate in Column 7.
9.  Word sense : `float`
    This is the word sense of the word in Column 3.
10. Speaker/Author : `str`
    This is the speaker or author name where available. Mostly in Broadcast Conversation
    and Web Log data. When not available the rows are marked with an ""-"".
11. Named Entities : `str`
    These columns identifies the spans representing various named entities. For documents
    which do not have named entity annotation, each line is represented with an `*`.
12. Predicate Arguments : `str`
    There is one column each of predicate argument structure information for the predicate
    mentioned in Column 7. If there are no predicates tagged in a sentence this is a
    single column with all rows marked with an `*`.
-1. Co-reference : `str`
    Co-reference chain information encoded in a parenthesis structure. For documents that do
     not have co-reference annotations, each line is represented with a ""-"".",", 
        An iterator over CONLL formatted files which yields documents, regardless
        of the number of document annotations in a particular file. This is useful
        for conll data which has been preprocessed, such as the preprocessing which
        takes place for the 2012 CONLL Coreference Resolution task.
        , 
        An iterator over the entire dataset, yielding all sentences processed.
        , 
        An iterator over the sentences in an individual CONLL formatted file.
        , 
        An iterator returning file_paths in a directory
        containing CONLL-formatted files.
        , 
        For a given coref label, add it to a currently open span(s), complete a span(s) or
        ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks
        dictionaries.

        # Parameters

        label : `str`
            The coref label for this word.
        word_index : `int`
            The word index into the sentence.
        clusters : `DefaultDict[int, List[Tuple[int, int]]]`
            A dictionary mapping cluster ids to lists of inclusive spans into the
            sentence.
        coref_stacks : `DefaultDict[int, List[int]]`
            Stacks for each cluster id to hold the start indices of active spans (spans
            which we are inside of when processing a given word). Spans with the same id
            can be nested, which is why we collect these opening spans on a stack, e.g:

            [Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1
        , 
        Given a sequence of different label types for a single word and the current
        span label we are inside, compute the BIO tag for each label and append to a list.

        # Parameters

        annotations : `List[str]`
            A list of labels to compute BIO tags for.
        span_labels : `List[List[str]]`
            A list of lists, one for each annotation, to incrementally collect
            the BIO tags for a sequence.
        current_span_labels : `List[Optional[str]]`
            The currently open span per annotation type, or `None` if there is no open span.
        , 
    # Parameters

    sentences : `List[List[str]]`, required.
        A list of lists representing the tokenised words and sentences in the document.
    token_indexers : `Dict[str, TokenIndexer]`
        This is used to index the words in the document.  See :class:`TokenIndexer`.
    max_span_width : `int`, required.
        The maximum width of candidate spans to consider.
    gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)
        A list of all clusters in the document, represented as word spans with absolute indices
        in the entire document. Each cluster contains some number of spans, which can be nested
        and overlap. If there are exact matches between clusters, they will be resolved
        using `_canonicalize_clusters`.
    wordpiece_modeling_tokenizer: `PretrainedTransformerTokenizer`, optional (default = None)
        If not None, this dataset reader does subword tokenization using the supplied tokenizer
        and distribute the labels to the resulting wordpieces. All the modeling will be based on
        wordpieces. If this is set to `False` (default), the user is expected to use
        `PretrainedTransformerMismatchedIndexer` and `PretrainedTransformerMismatchedEmbedder`,
        and the modeling will be on the word-level.
    max_sentences: int, optional (default = None)
        The maximum number of sentences in each document to keep. By default keeps all sentences.
    remove_singleton_clusters : `bool`, optional (default = True)
        Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows
        the removal of them.

    # Returns

    An `Instance` containing the following `Fields`:
        text : `TextField`
            The text of the full document.
        spans : `ListField[SpanField]`
            A ListField containing the spans represented as `SpanFields`
            with respect to the document text.
        span_labels : `SequenceLabelField`, optional
            The id of the cluster which each possible span belongs to, or -1 if it does
                not belong to a cluster. As these labels have variable length (it depends on
                how many spans we are considering), we represent this a as a `SequenceLabelField`
                with respect to the spans `ListField`.
    , 
    A class representing the annotations available for a single CONLL formatted sentence.

    # Parameters

    document_id : `str`
        This is a variation on the document filename
    sentence_id : `int`
        The integer ID of the sentence within a document.
    words : `List[str]`
        This is the tokens as segmented/tokenized in the Treebank.
    pos_tags : `List[str]`
        This is the Penn-Treebank-style part of speech. When parse information is missing,
        all parts of speech except the one for which there is some sense or proposition
        annotation are marked with a XX tag. The verb is marked with just a VERB tag.
    parse_tree : `nltk.Tree`
        An nltk Tree representing the parse. It includes POS tags as pre-terminal nodes.
        When the parse information is missing, the parse will be `None`.
    predicate_lemmas : `List[Optional[str]]`
        The predicate lemma of the words for which we have semantic role
        information or word sense information. All other indices are `None`.
    predicate_framenet_ids : `List[Optional[int]]`
        The PropBank frameset ID of the lemmas in `predicate_lemmas`, or `None`.
    word_senses : `List[Optional[float]]`
        The word senses for the words in the sentence, or `None`. These are floats
        because the word sense can have values after the decimal, like `1.1`.
    speakers : `List[Optional[str]]`
        The speaker information for the words in the sentence, if present, or `None`
        This is the speaker or author name where available. Mostly in Broadcast Conversation
        and Web Log data. When not available the rows are marked with an ""-"".
    named_entities : `List[str]`
        The BIO tags for named entities in the sentence.
    srl_frames : `List[Tuple[str, List[str]]]`
        A dictionary keyed by the verb in the sentence for the given
        Propbank frame labels, in a BIO format.
    coref_spans : `Set[TypedSpan]`
        The spans for entity mentions involved in coreference resolution within the sentence.
        Each element is a tuple composed of (cluster_id, (start_index, end_index)). Indices
        are `inclusive`.
    , 
    The data might include 2 annotated spans which are identical,
    but have different ids. This checks all clusters for spans which are
    identical, and if it finds any, merges the clusters containing the
    identical spans.
    , 
    This `DatasetReader` is designed to read in the English OntoNotes v5.0 data
    in the format used by the CoNLL 2011/2012 shared tasks. In order to use this
    Reader, you must follow the instructions provided [here (v12 release):]
    (https://cemantix.org/data/ontonotes.html), which will allow you to download
    the CoNLL style annotations for the  OntoNotes v5.0 release -- LDC2013T19.tgz
    obtained from LDC.

    Once you have run the scripts on the extracted data, you will have a folder
    structured as follows:

    ```
    conll-formatted-ontonotes-5.0/
     ── data
       ├── development
           └── data
               └── english
                   └── annotations
                       ├── bc
                       ├── bn
                       ├── mz
                       ├── nw
                       ├── pt
                       ├── tc
                       └── wb
       ├── test
           └── data
               └── english
                   └── annotations
                       ├── bc
                       ├── bn
                       ├── mz
                       ├── nw
                       ├── pt
                       ├── tc
                       └── wb
       └── train
           └── data
               └── english
                   └── annotations
                       ├── bc
                       ├── bn
                       ├── mz
                       ├── nw
                       ├── pt
                       ├── tc
                       └── wb
    ```

    The file path provided to this class can then be any of the train, test or development
    directories(or the top level data directory, if you are not utilizing the splits).

    The data has the following format, ordered by column.

    1.  Document ID : `str`
        This is a variation on the document filename
    2.  Part number : `int`
        Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
    3.  Word number : `int`
        This is the word index of the word in that sentence.
    4.  Word : `str`
        This is the token as segmented/tokenized in the Treebank. Initially the `*_skel` file
        contain the placeholder [WORD] which gets replaced by the actual token from the
        Treebank which is part of the OntoNotes release.
    5.  POS Tag : `str`
        This is the Penn Treebank style part of speech. When parse information is missing,
        all part of speeches except the one for which there is some sense or proposition
        annotation are marked with a XX tag. The verb is marked with just a VERB tag.
    6.  Parse bit : `str`
        This is the bracketed structure broken before the first open parenthesis in the parse,
        and the word/part-of-speech leaf replaced with a `*`. When the parse information is
        missing, the first word of a sentence is tagged as `(TOP*` and the last word is tagged
        as `*)` and all intermediate words are tagged with a `*`.
    7.  Predicate lemma : `str`
        The predicate lemma is mentioned for the rows for which we have semantic role
        information or word sense information. All other rows are marked with a ""-"".
    8.  Predicate Frameset ID : `int`
        The PropBank frameset ID of the predicate in Column 7.
    9.  Word sense : `float`
        This is the word sense of the word in Column 3.
    10. Speaker/Author : `str`
        This is the speaker or author name where available. Mostly in Broadcast Conversation
        and Web Log data. When not available the rows are marked with an ""-"".
    11. Named Entities : `str`
        These columns identifies the spans representing various named entities. For documents
        which do not have named entity annotation, each line is represented with an `*`.
    12. Predicate Arguments : `str`
        There is one column each of predicate argument structure information for the predicate
        mentioned in Column 7. If there are no predicates tagged in a sentence this is a
        single column with all rows marked with an `*`.
    -1. Co-reference : `str`
        Co-reference chain information encoded in a parenthesis structure. For documents that do
         not have co-reference annotations, each line is represented with a ""-"".
    ,  ,  (, #, #end document, (, ()*, (V, ), ) , *, -, -LRB-, -RRB-, /., /?, B-, I-, O, Reading CONLL sentences from dataset files at: %s, XX, clusters, gold_conll, r, span_labels, spans, text, utf8, |","0, 1, 10, 11, 3, 4, 5, 6, 7, 8, 9, None, True","
# Parameters

sentences : `List[List[str]]`, required.
A list of lists representing the tokenised words and sentences in the document.
token_indexers : `Dict[str, TokenIndexer]`
This is used to index the words in the document.  See :class:`TokenIndexer`.
max_span_width : `int`, required.
The maximum width of candidate spans to consider.
gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)
A list of all clusters in the document, represented as word spans with absolute indices
in the entire document. Each cluster contains some number of spans, which can be nested
and overlap. If there are exact matches between clusters, they will be resolved
using `_canonicalize_clusters`.
wordpiece_modeling_tokenizer: `PretrainedTransformerTokenizer`, optional (default = None)
If not None, this dataset reader does subword tokenization using the supplied tokenizer
and distribute the labels to the resulting wordpieces. All the modeling will be based on
wordpieces. If this is set to `False` (default), the user is expected to use
`PretrainedTransformerMismatchedIndexer` and `PretrainedTransformerMismatchedEmbedder`,
and the modeling will be on the word-level.
max_sentences: int, optional (default = None)
The maximum number of sentences in each document to keep. By default keeps all sentences.
remove_singleton_clusters : `bool`, optional (default = True)
Some datasets contain clusters that are singletons (i.e. no coreferents). This option allows
the removal of them.

# Returns

An `Instance` containing the following `Fields`:
text : `TextField`
The text of the full document.
spans : `ListField[SpanField]`
A ListField containing the spans represented as `SpanFields`
with respect to the document text.
span_labels : `SequenceLabelField`, optional
The id of the cluster which each possible span belongs to, or -1 if it does
not belong to a cluster. As these labels have variable length (it depends on
how many spans we are considering), we represent this a as a `SequenceLabelField`
with respect to the spans `ListField`.
, 
A class representing the annotations available for a single CONLL formatted sentence.

# Parameters

document_id : `str`
This is a variation on the document filename
sentence_id : `int`
The integer ID of the sentence within a document.
words : `List[str]`
This is the tokens as segmented/tokenized in the Treebank.
pos_tags : `List[str]`
This is the Penn-Treebank-style part of speech. When parse information is missing,
all parts of speech except the one for which there is some sense or proposition
annotation are marked with a XX tag. The verb is marked with just a VERB tag.
parse_tree : `nltk.Tree`
An nltk Tree representing the parse. It includes POS tags as pre-terminal nodes.
When the parse information is missing, the parse will be `None`.
predicate_lemmas : `List[Optional[str]]`
The predicate lemma of the words for which we have semantic role
information or word sense information. All other indices are `None`.
predicate_framenet_ids : `List[Optional[int]]`
The PropBank frameset ID of the lemmas in `predicate_lemmas`, or `None`.
word_senses : `List[Optional[float]]`
The word senses for the words in the sentence, or `None`. These are floats
because the word sense can have values after the decimal, like `1.1`.
speakers : `List[Optional[str]]`
The speaker information for the words in the sentence, if present, or `None`
This is the speaker or author name where available. Mostly in Broadcast Conversation
and Web Log data. When not available the rows are marked with an ""-"".
named_entities : `List[str]`
The BIO tags for named entities in the sentence.
srl_frames : `List[Tuple[str, List[str]]]`
A dictionary keyed by the verb in the sentence for the given
Propbank frame labels, in a BIO format.
coref_spans : `Set[TypedSpan]`
The spans for entity mentions involved in coreference resolution within the sentence.
Each element is a tuple composed of (cluster_id, (start_index, end_index)). Indices
are `inclusive`.
, 
An iterator over CONLL formatted files which yields documents, regardless
of the number of document annotations in a particular file. This is useful
for conll data which has been preprocessed, such as the preprocessing which
takes place for the 2012 CONLL Coreference Resolution task.
, 
An iterator over the entire dataset, yielding all sentences processed.
, 
An iterator over the sentences in an individual CONLL formatted file.
, 
An iterator returning file_paths in a directory
containing CONLL-formatted files.
, 
For a given coref label, add it to a currently open span(s), complete a span(s) or
ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks
dictionaries.

# Parameters

label : `str`
The coref label for this word.
word_index : `int`
The word index into the sentence.
clusters : `DefaultDict[int, List[Tuple[int, int]]]`
A dictionary mapping cluster ids to lists of inclusive spans into the
sentence.
coref_stacks : `DefaultDict[int, List[int]]`
Stacks for each cluster id to hold the start indices of active spans (spans
which we are inside of when processing a given word). Spans with the same id
can be nested, which is why we collect these opening spans on a stack, e.g:

[Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1
, 
Given a sequence of different label types for a single word and the current
span label we are inside, compute the BIO tag for each label and append to a list.

# Parameters

annotations : `List[str]`
A list of labels to compute BIO tags for.
span_labels : `List[List[str]]`
A list of lists, one for each annotation, to incrementally collect
the BIO tags for a sequence.
current_span_labels : `List[Optional[str]]`
The currently open span per annotation type, or `None` if there is no open span.
, 
The data might include 2 annotated spans which are identical,
but have different ids. This checks all clusters for spans which are
identical, and if it finds any, merges the clusters containing the
identical spans.
, 
This `DatasetReader` is designed to read in the English OntoNotes v5.0 data
in the format used by the CoNLL 2011/2012 shared tasks. In order to use this
Reader, you must follow the instructions provided [here (v12 release):]
(https://cemantix.org/data/ontonotes.html), which will allow you to download
the CoNLL style annotations for the  OntoNotes v5.0 release -- LDC2013T19.tgz
obtained from LDC.

Once you have run the scripts on the extracted data, you will have a folder
structured as follows:

```
conll-formatted-ontonotes-5.0/
── data
├── development
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
├── test
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
└── train
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
```

The file path provided to this class can then be any of the train, test or development
directories(or the top level data directory, if you are not utilizing the splits).

The data has the following format, ordered by column.

1.  Document ID : `str`
This is a variation on the document filename
2.  Part number : `int`
Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3.  Word number : `int`
This is the word index of the word in that sentence.
4.  Word : `str`
This is the token as segmented/tokenized in the Treebank. Initially the `*_skel` file
contain the placeholder [WORD] which gets replaced by the actual token from the
Treebank which is part of the OntoNotes release.
5.  POS Tag : `str`
This is the Penn Treebank style part of speech. When parse information is missing,
all part of speeches except the one for which there is some sense or proposition
annotation are marked with a XX tag. The verb is marked with just a VERB tag.
6.  Parse bit : `str`
This is the bracketed structure broken before the first open parenthesis in the parse,
and the word/part-of-speech leaf replaced with a `*`. When the parse information is
missing, the first word of a sentence is tagged as `(TOP*` and the last word is tagged
as `*)` and all intermediate words are tagged with a `*`.
7.  Predicate lemma : `str`
The predicate lemma is mentioned for the rows for which we have semantic role
information or word sense information. All other rows are marked with a ""-"".
8.  Predicate Frameset ID : `int`
The PropBank frameset ID of the predicate in Column 7.
9.  Word sense : `float`
This is the word sense of the word in Column 3.
10. Speaker/Author : `str`
This is the speaker or author name where available. Mostly in Broadcast Conversation
and Web Log data. When not available the rows are marked with an ""-"".
11. Named Entities : `str`
These columns identifies the spans representing various named entities. For documents
which do not have named entity annotation, each line is represented with an `*`.
12. Predicate Arguments : `str`
There is one column each of predicate argument structure information for the predicate
mentioned in Column 7. If there are no predicates tagged in a sentence this is a
single column with all rows marked with an `*`.
-1. Co-reference : `str`
Co-reference chain information encoded in a parenthesis structure. For documents that do
not have co-reference annotations, each line is represented with a ""-"".
, #     metadata[""clusters""] = gold_clusters, # ""metadata"": metadata_field,, # Already encountered overlap - no need to keep looking., # Cluster id -> List of (start_index, end_index) spans., # Cluster id -> List of start_indices which are open for this id., # Collect any stragglers or files which might not, # Create variables representing the current label for each label, # Entering into a span for a particular semantic role label., # Exiting a span, so we reset the current span label for this annotation., # If any annotation marks this word as a verb predicate,, # If there's no '(' token, but the current_span_label is not None,, # If this is the first word in the sentence, create, # Look at clusters we have already processed to, # Merge cluster we are currently processing into, # Non-empty line. Collect the annotation., # Replace brackets in text and pos tags, # Retrieve the start index from the document state and, # The FrameNet ID of the predicate., # The conll representation of coref spans allows spans to, # The current speaker, if available., # The lemmatised form of the words in the sentence which, # The pos tags of the words in the sentence., # The sense of the word, if available., # The span begins and ends at this word (single word span)., # The span begins at this word., # The span for this id is ending, but didn't start at this word., # The span is starting, so we record the index of the word., # The words in the sentence., # There are some bad annotations in the CONLL data., # These are a relic of the dataset pre-processing. Every, # They contain no information, so to make this explicit,, # We append the label and set the current span for this annotation., # We can't do this upfront, because we don't know how many, # We're outside a span., # add the span to the clusters for this id., # and one generated from the preprocessing called filename.gold_conll., # cluster for comparison., # components we are collecting, as a sentence can have, # empty lists to collect the NER and SRL BIO labels., # file will be duplicated - one file called filename.gold_skel, # first cluster in merged clusters, # get the actual propbank label., # have SRL or word sense information., # have the '#end document' format for the end of the file., # if gold_clusters is not None:, # in the overall parse tree being None., # metadata: Dict[str, Any] = {""original_text"": flattened_sentences}, # metadata_field = MetadataField(metadata), # of ordering the verbal predicates by their location in the, # only keep ')' if there are nested brackets with nothing in them., # overlap. If spans end or begin at the same word, they are, # see if they contain a mention in the current, # sentence, automatically aligning them with the annotations., # separated by a ""|""., # sequence we are collecting., # strip all bracketing information to, # the cluster in the processed list., # the pieces of the parse tree., # then we are inside a span., # variable numbers of SRL frames., # we just set the parse piece to be None which will result, # we need to record its index. This also has the side effect, # which contains this mention., # with a different token for parse trees., #""):, #end document""):","
This `DatasetReader` is designed to read in the English OntoNotes v5.0 data
in the format used by the CoNLL 2011/2012 shared tasks. In order to use this
Reader, you must follow the instructions provided [here (v12 release):]
(https://cemantix.org/data/ontonotes.html), which will allow you to download
the CoNLL style annotations for the  OntoNotes v5.0 release -- LDC2013T19.tgz
obtained from LDC.

Once you have run the scripts on the extracted data, you will have a folder
structured as follows:

```
conll-formatted-ontonotes-5.0/
── data
├── development
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
├── test
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
└── train
└── data
└── english
└── annotations
├── bc
├── bn
├── mz
├── nw
├── pt
├── tc
└── wb
```

The file path provided to this class can then be any of the train, test or development
directories(or the top level data directory, if you are not utilizing the splits).

The data has the following format, ordered by column.

1.  Document ID : `str`
This is a variation on the document filename
2.  Part number : `int`
Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3.  Word number : `int`
This is the word index of the word in that sentence.
4.  Word : `str`
This is the token as segmented/tokenized in the Treebank. Initially the `*_skel` file
contain the placeholder [WORD] which gets replaced by the actual token from the
Treebank which is part of the OntoNotes release.
5.  POS Tag : `str`
This is the Penn Treebank style part of speech. When parse information is missing,
all part of speeches except the one for which there is some sense or proposition
annotation are marked with a XX tag. The verb is marked with just a VERB tag.
6.  Parse bit : `str`
This is the bracketed structure broken before the first open parenthesis in the parse,
and the word/part-of-speech leaf replaced with a `*`. When the parse information is
missing, the first word of a sentence is tagged as `(TOP*` and the last word is tagged
as `*)` and all intermediate words are tagged with a `*`.
7.  Predicate lemma : `str`
The predicate lemma is mentioned for the rows for which we have semantic role
information or word sense information. All other rows are marked with a ""-"".
8.  Predicate Frameset ID : `int`
The PropBank frameset ID of the predicate in Column 7.
9.  Word sense : `float`
This is the word sense of the word in Column 3.
10. Speaker/Author : `str`
This is the speaker or author name where available. Mostly in Broadcast Conversation
and Web Log data. When not available the rows are marked with an ""-"".
11. Named Entities : `str`
These columns identifies the spans representing various named entities. For documents
which do not have named entity annotation, each line is represented with an `*`.
12. Predicate Arguments : `str`
There is one column each of predicate argument structure information for the predicate
mentioned in Column 7. If there are no predicates tagged in a sentence this is a
single column with all rows marked with an `*`.
-1. Co-reference : `str`
Co-reference chain information encoded in a parenthesis structure. For documents that do
not have co-reference annotations, each line is represented with a ""-"".
, ──, └──, ├──"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:05",
https://github.com/hankcs/HanLP,chinese.py,0,0.0,35,36.84,44,46.32,1,1.05,15,15.79,0,0.0,0,0,31,0,,"ONTONOTES5_CHINESE_DEV, ONTONOTES5_CHINESE_TEST, ONTONOTES5_CHINESE_TRAIN, ONTONOTES5_CONLL12_CHINESE_DEV, ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_TRAIN, ONTONOTES5_CONLL12_NER_CHINESE_DEV, ONTONOTES5_CONLL12_NER_CHINESE_TEST, ONTONOTES5_CONLL12_NER_CHINESE_TRAIN, ONTONOTES5_CONLL_CHINESE_DEV, ONTONOTES5_CONLL_CHINESE_TEST, ONTONOTES5_CONLL_CHINESE_TRAIN, ONTONOTES5_CON_CHINESE_DEV, ONTONOTES5_CON_CHINESE_TEST, ONTONOTES5_CON_CHINESE_TRAIN, ONTONOTES5_DEP_CHINESE_DEV, ONTONOTES5_DEP_CHINESE_TEST, ONTONOTES5_DEP_CHINESE_TRAIN, ONTONOTES5_NER_CHINESE_DEV, ONTONOTES5_NER_CHINESE_TEST, ONTONOTES5_NER_CHINESE_TRAIN, ONTONOTES5_POS_CHINESE_DEV, ONTONOTES5_POS_CHINESE_TEST, ONTONOTES5_POS_CHINESE_TRAIN, _ONTONOTES5_CHINESE_HOME, _ONTONOTES5_CONLL12_CHINESE_HOME, folder, hanlp, intended_chinese, intended_file_path, intended_home, os, shutil, unofficial_chinese, urllib","hanlp.datasets.srl.ontonotes5.CONLL12_HOME, hanlp.datasets.srl.ontonotes5.ONTONOTES5_HOME, hanlp.datasets.srl.ontonotes5._utils.batch_make_con_txt_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_dep_conllx_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_ner_tsv_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_pos_tsv_if_necessary, hanlp.datasets.srl.ontonotes5._utils.make_gold_conll, hanlp.datasets.srl.ontonotes5._utils.make_ontonotes_language_jsonlines, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.path_from_url, hanlp.utils.log_util.cprint, hanlp.utils.log_util.flash, os.path.dirname, os.path.join, os.path.splitext, shutil.copytree, urllib.error.HTTPError",,,"ONTONOTES5_CHINESE_DEV, ONTONOTES5_CHINESE_TEST, ONTONOTES5_CHINESE_TRAIN, ONTONOTES5_CONLL12_CHINESE_DEV, ONTONOTES5_CONLL12_CHINESE_TEST, ONTONOTES5_CONLL12_CHINESE_TRAIN, ONTONOTES5_CONLL12_NER_CHINESE_DEV, ONTONOTES5_CONLL12_NER_CHINESE_TEST, ONTONOTES5_CONLL12_NER_CHINESE_TRAIN, ONTONOTES5_CONLL_CHINESE_DEV, ONTONOTES5_CONLL_CHINESE_TEST, ONTONOTES5_CONLL_CHINESE_TRAIN, ONTONOTES5_CON_CHINESE_DEV, ONTONOTES5_CON_CHINESE_TEST, ONTONOTES5_CON_CHINESE_TRAIN, ONTONOTES5_DEP_CHINESE_DEV, ONTONOTES5_DEP_CHINESE_TEST, ONTONOTES5_DEP_CHINESE_TRAIN, ONTONOTES5_NER_CHINESE_DEV, ONTONOTES5_NER_CHINESE_TEST, ONTONOTES5_NER_CHINESE_TRAIN, ONTONOTES5_POS_CHINESE_DEV, ONTONOTES5_POS_CHINESE_TEST, ONTONOTES5_POS_CHINESE_TRAIN, _ONTONOTES5_CHINESE_HOME, _ONTONOTES5_CONLL12_CHINESE_HOME, folder, intended_chinese, intended_file_path, intended_home, unofficial_chinese",,",  [blink][yellow]...[/yellow][/blink],  to , .., /data/files/data/chinese/, Copying , Dev set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Luckily, an [red]unofficial[/red] Chinese version is provided on GitHub which will be used for demonstration purpose., Ontonotes 5.0 is a [red][bold]copyright[/bold][/red] dataset owned by LDC which we cannot re-distribute. Please apply for a licence from LDC (https://catalog.ldc.upenn.edu/LDC2016T13) then download it to , Test set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Training set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., annotations, chinese, chinese/, development.chinese.con.txt, development.chinese.conll12.jsonlines, development.chinese.conll12.ner.tsv, development.chinese.dep.conllx, development.chinese.v4.jsonlines, development.chinese.v4.ner.tsv, development.chinese.v4.pos.tsv, development.chinese.v4_gold_conll, files/data/chinese/, https://github.com/GuocaiL/Coref_Resolution/archive/master.zip#data/, metadata, ontonotes-release-5.0, test.chinese.con.txt, test.chinese.conll12.jsonlines, test.chinese.conll12.ner.tsv, test.chinese.dep.conllx, test.chinese.v4.jsonlines, test.chinese.v4.ner.tsv, test.chinese.v4.pos.tsv, test.chinese.v4_gold_conll, train.chinese.con.txt, train.chinese.conll12.jsonlines, train.chinese.conll12.ner.tsv, train.chinese.dep.conllx, train.chinese.v4.jsonlines, train.chinese.v4.ner.tsv, train.chinese.v4.pos.tsv, train.chinese.v4_gold_conll, v4, zh",False,"#     [ONTONOTES5_CON_CHINESE_TRAIN, ONTONOTES5_CON_CHINESE_DEV, ONTONOTES5_CON_CHINESE_TEST]), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-26 16:07, # ONTONOTES5_CON_CHINESE_NOEC_DEV = _ONTONOTES5_CONLL12_CHINESE_HOME + 'development.chinese.con.noempty.txt', # ONTONOTES5_CON_CHINESE_NOEC_TEST = _ONTONOTES5_CONLL12_CHINESE_HOME + 'test.chinese.con.noempty.txt', # ONTONOTES5_CON_CHINESE_NOEC_TRAIN = _ONTONOTES5_CONLL12_CHINESE_HOME + 'train.chinese.con.noempty.txt', # batch_remove_empty_category_if_necessary(, # print(intended_chinese), # print(os.path.dirname(intended_chinese)), # print(unofficial_chinese), #data/'), Dev set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).
ONTONOTES5_CONLL12_NER_CHINESE_TEST = _ONTONOTES5_CONLL12_CHINESE_HOME + 'test.chinese.conll12.ner.tsv'
Test set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Test set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).

ONTONOTES5_CONLL12_NER_CHINESE_TRAIN = _ONTONOTES5_CONLL12_CHINESE_HOME + 'train.chinese.conll12.ner.tsv'
Training set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Training set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).
ONTONOTES5_CONLL12_CHINESE_DEV = _ONTONOTES5_CONLL12_CHINESE_HOME + 'development.chinese.conll12.jsonlines'
Dev set of OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).",
https://github.com/hankcs/HanLP,english.py,0,0.0,30,41.1,33,45.21,2,2.74,8,10.96,0,0.0,0,0,27,0,,"ONTONOTES5_CONLL12_ENGLISH_DEV, ONTONOTES5_CONLL12_ENGLISH_TEST, ONTONOTES5_CONLL12_ENGLISH_TRAIN, ONTONOTES5_CONLL12_NER_ENGLISH_DEV, ONTONOTES5_CONLL12_NER_ENGLISH_TEST, ONTONOTES5_CONLL12_NER_ENGLISH_TRAIN, ONTONOTES5_CONLL_ENGLISH_DEV, ONTONOTES5_CONLL_ENGLISH_TEST, ONTONOTES5_CONLL_ENGLISH_TRAIN, ONTONOTES5_CON_ENGLISH_DEV, ONTONOTES5_CON_ENGLISH_TEST, ONTONOTES5_CON_ENGLISH_TRAIN, ONTONOTES5_DEP_ENGLISH_DEV, ONTONOTES5_DEP_ENGLISH_TEST, ONTONOTES5_DEP_ENGLISH_TRAIN, ONTONOTES5_ENGLISH_DEV, ONTONOTES5_ENGLISH_TEST, ONTONOTES5_ENGLISH_TRAIN, ONTONOTES5_NER_ENGLISH_DEV, ONTONOTES5_NER_ENGLISH_TEST, ONTONOTES5_NER_ENGLISH_TRAIN, ONTONOTES5_POS_ENGLISH_DEV, ONTONOTES5_POS_ENGLISH_TEST, ONTONOTES5_POS_ENGLISH_TRAIN, _ONTONOTES5_CONLL12_ENGLISH_HOME, _ONTONOTES5_ENGLISH_HOME, exit, hanlp, intended_file_path, urllib","hanlp.datasets.srl.ontonotes5.CONLL12_HOME, hanlp.datasets.srl.ontonotes5.ONTONOTES5_HOME, hanlp.datasets.srl.ontonotes5._utils.batch_make_con_txt_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_dep_conllx_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_ner_tsv_if_necessary, hanlp.datasets.srl.ontonotes5._utils.batch_make_pos_tsv_if_necessary, hanlp.datasets.srl.ontonotes5._utils.make_gold_conll, hanlp.datasets.srl.ontonotes5._utils.make_ontonotes_language_jsonlines, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.path_from_url, hanlp.utils.log_util.cprint, urllib.error.HTTPError",,,"ONTONOTES5_CONLL12_ENGLISH_DEV, ONTONOTES5_CONLL12_ENGLISH_TEST, ONTONOTES5_CONLL12_ENGLISH_TRAIN, ONTONOTES5_CONLL12_NER_ENGLISH_DEV, ONTONOTES5_CONLL12_NER_ENGLISH_TEST, ONTONOTES5_CONLL12_NER_ENGLISH_TRAIN, ONTONOTES5_CONLL_ENGLISH_DEV, ONTONOTES5_CONLL_ENGLISH_TEST, ONTONOTES5_CONLL_ENGLISH_TRAIN, ONTONOTES5_CON_ENGLISH_DEV, ONTONOTES5_CON_ENGLISH_TEST, ONTONOTES5_CON_ENGLISH_TRAIN, ONTONOTES5_DEP_ENGLISH_DEV, ONTONOTES5_DEP_ENGLISH_TEST, ONTONOTES5_DEP_ENGLISH_TRAIN, ONTONOTES5_ENGLISH_DEV, ONTONOTES5_ENGLISH_TEST, ONTONOTES5_ENGLISH_TRAIN, ONTONOTES5_NER_ENGLISH_DEV, ONTONOTES5_NER_ENGLISH_TEST, ONTONOTES5_NER_ENGLISH_TRAIN, ONTONOTES5_POS_ENGLISH_DEV, ONTONOTES5_POS_ENGLISH_TEST, ONTONOTES5_POS_ENGLISH_TRAIN, _ONTONOTES5_CONLL12_ENGLISH_HOME, _ONTONOTES5_ENGLISH_HOME, intended_file_path",,".., Dev set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Ontonotes 5.0 is a [red][bold]copyright[/bold][/red] dataset owned by LDC which we cannot re-distribute. Please apply for a licence from LDC (https://catalog.ldc.upenn.edu/LDC2016T13) then download it to , Test set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Training set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., development.english.con.txt, development.english.conll12.jsonlines, development.english.conll12.ner.tsv, development.english.dep.conllx, development.english.v4.jsonlines, development.english.v4.ner.tsv, development.english.v4.pos.tsv, development.english.v4_gold_conll, english, english/, files/data/english/, test.english.con.txt, test.english.conll12.jsonlines, test.english.conll12.ner.tsv, test.english.dep.conllx, test.english.v4.jsonlines, test.english.v4.ner.tsv, test.english.v4.pos.tsv, test.english.v4_gold_conll, train.english.con.txt, train.english.conll12.jsonlines, train.english.conll12.ner.tsv, train.english.dep.conllx, train.english.v4.jsonlines, train.english.v4.ner.tsv, train.english.v4.pos.tsv, train.english.v4_gold_conll, v4","1, False","#     [ONTONOTES5_CON_ENGLISH_TRAIN, ONTONOTES5_CON_ENGLISH_DEV, ONTONOTES5_CON_ENGLISH_TEST]), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-25 18:48, # batch_remove_empty_category_if_necessary(, Dev set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).
ONTONOTES5_CONLL12_NER_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.conll12.ner.tsv'
Test set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Test set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).

ONTONOTES5_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.v4.jsonlines'
ONTONOTES5_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.v4.jsonlines'
ONTONOTES5_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.v4.jsonlines'

ONTONOTES5_CONLL_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.v4_gold_conll'
ONTONOTES5_CONLL_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.v4_gold_conll'
ONTONOTES5_CONLL_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.v4_gold_conll'

ONTONOTES5_POS_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.v4.pos.tsv'
ONTONOTES5_POS_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.v4.pos.tsv'
ONTONOTES5_POS_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.v4.pos.tsv'

ONTONOTES5_CON_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.con.txt'
ONTONOTES5_CON_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.con.txt'
ONTONOTES5_CON_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.con.txt'

ONTONOTES5_DEP_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.dep.conllx'
ONTONOTES5_DEP_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.dep.conllx'
ONTONOTES5_DEP_ENGLISH_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.dep.conllx'

# ONTONOTES5_CON_ENGLISH_NOEC_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.con.noempty.txt'
# ONTONOTES5_CON_ENGLISH_NOEC_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.con.noempty.txt'
# ONTONOTES5_CON_ENGLISH_NOEC_TEST = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'test.english.con.noempty.txt'

ONTONOTES5_CONLL12_NER_ENGLISH_TRAIN = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'train.english.conll12.ner.tsv'
Training set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`)., Training set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).
ONTONOTES5_CONLL12_ENGLISH_DEV = _ONTONOTES5_CONLL12_ENGLISH_HOME + 'development.english.conll12.jsonlines'
Dev set of English OntoNotes5 used in CoNLL12 (:cite:`pradhan-etal-2012-conll`).",
https://github.com/hankcs/HanLP,_utils.py,0,0.0,223,63.17,108,30.59,16,4.53,6,1.7,0,0.0,44,2,105,1,,"AssertionError, BEGIN_DOCUMENT_REGEX, DocumentState, EnvironmentError, POS, RestoreToken, RuntimeError, __call__, __init__, __name__, add, all_mentions, annotation, append, arg, args, argument_buffers, argument_stacks, assert_empty, assert_finalizable, asterisk_idx, batch_load_raw_text, batch_make_con_txt_if_necessary, batch_make_coref_json_if_necessary, batch_make_dep_conllx_if_necessary, batch_make_ner_json_if_necessary, batch_make_ner_tsv_if_necessary, batch_make_pos_tsv_if_necessary, batch_make_srl_json_if_necessary, batch_remove_empty_category_if_necessary, begin_document_match, bit, bracketed, c1, c2, cell, close, close_parens, cluster_count, cluster_id, clusters, codecs, collections, con_txt_file, con_txt_files, conll12_json_file, conll12_ontonotes_path, conll_file, const_buffer, const_stack, constituents, convert, convert_jsonlines_to_IOBES, convert_to_jsonlines, coref, coref_stacks, count, current_idx, dk_prefix, doc, doc_count, doc_id, doc_ids, doc_ids_file, doc_ids_to_keys, doc_key, doc_level_offset, document, document_state, dst, each, ensure_python_points_to_python2, enumerate, example, existing, expect_sent, expect_sent_line, filename, filter_data, filtered_examples, finalize, finalize_sentence, finalized_state, find, flatten, framefile, full_file, get_doc_key, glob, group, handle_bit, handle_line, hanlp, hanlp_common, home, id_file, input_file, input_json_file, input_path, item, json, json_file, json_files, jsonline, jsonpath, key, label, label_set, labels, lang_dir, language, languages, lemma, lemma_buffer, len, line, list, load_raw_text, main, make_con_txt, make_con_txt_if_necessary, make_coref_json, make_coref_json_if_necessary, make_dep_conllx, make_dep_conllx_if_necessary, make_gold_conll, make_ner_json, make_ner_json_if_necessary, make_ner_tsv_if_necessary, make_ontonotes_jsonlines, make_ontonotes_language_jsonlines, make_pos_tsv, make_pos_tsv_if_necessary, make_raw_text_if_necessary, make_srl_json, make_srl_json_if_necessary, mapper, max, merged_clusters, missing_count, ner, ner_buffer, ner_count, ner_stack, next_idx, normalize_token, normalize_word, object, offset, onf_file, onf_files, ontonotes_document_generator, ontonotes_path, ontonotes_root, open, open_index, open_parens, os, output_file, output_json_file, output_path, parse, part, pattern, pop, pos, pos_buffer, pos_per_sent, pprint, pred, predicate_buffer, predicate_sense, range, re, readlines, replace, roleset, root, row, sample, segment, self, sent_parts, sentence_count, sentence_id, sentences, sorted, spans, speaker, speakers, split, src, srl, srl_count, stack, start, startswith, stats, str, strip, sublist, sum, super, sys, tag, tags, text, text_speakers, to_conll, token, token_id, tree, tuple, typing, update, v5_json_file, values, version, word, word_count, word_index, write, zip","codecs.open, collections.defaultdict, glob.glob, hanlp.common.transform.NormalizeToken, hanlp.datasets.parsing.loaders._ctb_utils.convert_to_dependency, hanlp.datasets.parsing.loaders._ctb_utils.remove_all_ec, hanlp.datasets.parsing.ptb.PTB_TOKEN_MAPPING, hanlp.utils.io_util.get_exitcode_stdout_stderr, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.merge_files, hanlp.utils.io_util.pushd, hanlp.utils.io_util.read_tsv_as_sents, hanlp.utils.io_util.replace_ext, hanlp.utils.io_util.run_cmd, hanlp.utils.log_util.flash, hanlp_common.io.eprint, hanlp_common.io.save_json, json.dumps, json.loads, os.makedirs, os.path.abspath, os.path.basename, os.path.dirname, os.path.isfile, os.path.join, os.path.splitext, pprint.pprint, re.compile, re.match, sys.argv, typing.Dict, typing.List, typing.Union","__call__, __init__, assert_empty, assert_finalizable, batch_load_raw_text, batch_make_con_txt_if_necessary, batch_make_coref_json_if_necessary, batch_make_dep_conllx_if_necessary, batch_make_ner_json_if_necessary, batch_make_ner_tsv_if_necessary, batch_make_pos_tsv_if_necessary, batch_make_srl_json_if_necessary, batch_remove_empty_category_if_necessary, convert_jsonlines_to_IOBES, convert_to_jsonlines, ensure_python_points_to_python2, filter_data, finalize, finalize_sentence, flatten, get_doc_key, handle_bit, handle_line, load_raw_text, main, make_con_txt, make_con_txt_if_necessary, make_coref_json, make_coref_json_if_necessary, make_dep_conllx, make_dep_conllx_if_necessary, make_gold_conll, make_ner_json, make_ner_json_if_necessary, make_ner_tsv_if_necessary, make_ontonotes_jsonlines, make_ontonotes_language_jsonlines, make_pos_tsv, make_pos_tsv_if_necessary, make_raw_text_if_necessary, make_srl_json, make_srl_json_if_necessary, normalize_word, ontonotes_document_generator","DocumentState, RestoreToken","BEGIN_DOCUMENT_REGEX, POS, all_mentions, arg, args, asterisk_idx, begin_document_match, bracketed, c1, c2, cell, close_parens, cluster_count, cluster_id, con_txt_file, conll12_json_file, conll12_ontonotes_path, coref, count, current_idx, dk_prefix, doc, doc_count, doc_id, doc_ids, doc_ids_to_keys, doc_key, document, document_state, dst, each, example, existing, expect_sent, expect_sent_line, filename, files, filtered_examples, finalized_state, framefile, full_file, home, id_file, input_file, item, json_file, jsonline, jsonpath, key, label, labels, lang_dir, language, languages, lemma, line, mapper, merged_clusters, missing_count, ner, ner_count, next_idx, offset, onf_files, ontonotes_path, ontonotes_root, open_index, open_parens, output_file, output_path, parse, pattern, pos, pos_per_sent, pred, predicate_sense, roleset, row, segment, sent, sent_parts, sentence_count, sentence_id, sentences, sents, speaker, split, src, srl, srl_count, start, stats, sublist, tag, tags, to_conll, token, token_id, transform, tree, v5_json_file, version, word, word_count, word_index","Filter OntoNotes5 data based on CoNLL2012 (coref) doc ids.
https://github.com/bcmi220/unisrl/blob/master/scripts/filter_conll2012_data.py

Args:
  input_json_file: All documents.
  output_json_file:
  doc_ids_file:

Returns:",", 	, 
,  ,  current_span = (open_index, word_index)
        if current_span in spans:
          spans[current_span] += ""_"" + label
        else:
          spans[current_span] = label
        spans[current_span] = label ,  files to ,  of ,  to ,  to CoNLL. See exceptions for detail,  to json file , #, #begin, #begin document \((.*)\); part (\d+), #end, #end document, (, ), *, **/*.onf, -, ., . Try:

	ln -sf ""$(which python2)"" ""$(which python)"", .., .con.txt, .conll12.jsonlines, .coref, .coref.jsonlines, .dep.conllx, .id, .jsonlines, .name, .ner.jsonlines, .ner.tsv, .noempty.txt, .onf, .pos.tsv, .prop, .srl.jsonlines, /, /-, /., /?, /annotations/*/*/*/*gold_conll, /coref/, /data/, 2 arguments required: ontonotes_path output_path, Applying CoNLL 12 official splits on , B-, Converting CoNLL file , Converting [blue], Documents: {}
Sentences: {}
Words: {}
NER: {}, PAS: {}, Clusters: {}, No annotations: {}, E-, Filter OntoNotes5 data based on CoNLL2012 (coref) doc ids.
    https://github.com/bcmi220/unisrl/blob/master/scripts/filter_conll2012_data.py

    Args:
      input_json_file: All documents.
      output_json_file:
      doc_ids_file:

    Returns:

    , I-, Labels:, Merging , Merging clusters (shouldn't happen very often.), No gold_conll files found in , O, Plain sentence:, Python 2, S-, Statistics:, Your python command needs to be Python2, not , [/blue] to CoNLL format, this might take half an hour [blink][yellow]...[/yellow][/blink], [red]Failed[/red] to convert , _, __main__, _gold_conll, annotations, annotations/, arabic, bash , categories, chinese, clusters, conll-2012-test, constituents, data/files/data, development, doc_key, en, english, https://file.hankcs.com/research/emnlp2021/conll.cemantix.org.zip#2012/download/ids/, https://gist.githubusercontent.com/hankcs/46b9137016c769e4b6137104daf43a92/raw/66369de6c24b5ec47696ae307591f0d72c6f3f02/ontonotes_to_conll.sh, lemma, max_sent_len_{}, ner, num_clusters, num_mentions, num_sents_{}, pos, python --version, r, sentences, speakers, srl, test, text.jsonlines, token, train, utf-8, utf8, v, v5, w, {}_{}, |","0, 1, 10, 11, 12, 2, 3, 4, 5, 6, 7, 8, 9, False, None, True","# Arrange the files in order of id files, # {}, #!/usr/bin/env python, #"")], #begin document \((.*)\); part (\d+)""), Filter OntoNotes5 data based on CoNLL2012 (coref) doc ids.
https://github.com/bcmi220/unisrl/blob/master/scripts/filter_conll2012_data.py

Args:
input_json_file: All documents.
output_json_file:
doc_ids_file:

Returns:

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,2,25.0,2,25.0,0,0.0,4,50.0,0,0.0,0,0,2,0,,"CONLL12_HOME, ONTONOTES5_HOME",,,,"CONLL12_HOME, ONTONOTES5_HOME",,"../conll-2012/, https://catalog.ldc.upenn.edu/LDC2013T19/LDC2013T19.tgz#/ontonotes-release-5.0/data/",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-26 16:07, #/ontonotes-release-5.0/data/'",
https://github.com/hankcs/HanLP,conll_dataset.py,0,0.0,46,57.5,24,30.0,3,3.75,7,8.75,0,0.0,6,1,14,3,,"CoNLLParsingDataset, __init__, __len__, _prune, abs, append_bos, append_bos_eos, bool, bos, cache, cell, data, di, dict, dj, endswith, enumerate, field, field_names, filepath, fp, generate_idx, get, get_sibs, hanlp, hanlp_common, heads, hi, hj, idx, int, j, len, load_file, log, pos_key, prune, range, sample, self, sent, sibs, str, super, transform, typing","hanlp.common.dataset.TransformableDataset, hanlp.components.parsers.conll.read_conll, hanlp.utils.io_util.TimingFileIterator, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.constant.ROOT, typing.Callable, typing.Dict, typing.List, typing.Union","__init__, __len__, append_bos, append_bos_eos, get_sibs, load_file",CoNLLParsingDataset,"cell, di, dj, field, field_names, fp, heads, hi, hj, idx, j, sample, sent, sibs","Args:
    sample:
    pos_key:
    bos: A special token inserted to the head of tokens.

Returns:, Both ``.conllx`` and ``.conllu`` are supported. Their descriptions can be found in
:class:`hanlp_common.conll.CoNLLWord` and :class:`hanlp_common.conll.CoNLLUWord` respectively.

Args:
    filepath: ``.conllx`` or ``.conllu`` file path., General class for CoNLL style dependency parsing datasets.

Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler.
    prune: A filter to prune unwanted samples.","

    Args:
        sample:
        pos_key:
        bos: A special token inserted to the head of tokens.

    Returns:

    ,  samples [blink][yellow]...[/yellow][/blink], .conllu, Both ``.conllx`` and ``.conllu`` are supported. Their descriptions can be found in
        :class:`hanlp_common.conll.CoNLLWord` and :class:`hanlp_common.conll.CoNLLUWord` respectively.

        Args:
            filepath: ``.conllx`` or ``.conllu`` file path.
        , CPOS, DEPREL, DEPS, FEATS, FORM, General class for CoNLL style dependency parsing datasets.

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.
            prune: A filter to prune unwanted samples.
        , HEAD, ID, LEMMA, MISC, PDEPREL, PHEAD, POS, UPOS, XPOS, arc, pos, rel, sib_id, token","0, 1, None","

Args:
sample:
pos_key:
bos: A special token inserted to the head of tokens.

Returns:

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-08 16:10, # See https://universaldependencies.org/format.html, Both ``.conllx`` and ``.conllu`` are supported. Their descriptions can be found in
:class:`hanlp_common.conll.CoNLLWord` and :class:`hanlp_common.conll.CoNLLUWord` respectively.

Args:
filepath: ``.conllx`` or ``.conllu`` file path.
, General class for CoNLL style dependency parsing datasets.

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.
prune: A filter to prune unwanted samples.
",
https://github.com/hankcs/HanLP,constituency_dataset.py,0,0.0,60,70.59,13,15.29,4,4.71,8,9.41,0,0.0,8,1,17,3,,"ConstituencyDataset, append_bos_eos, binarize, build_tree, chart, child, children, chomsky_normal_form, collapse_unary, copy, delete_labels, dict, endswith, enumerate, equal_labels, extend, factorize, filepath, fromstring, get, hanlp, hanlp_common, isinstance, iter, j, label, labels, leaves, len, line, list, load_file, next, node, nodes, open, phrasetree, pop, pos, range, remove_subcategory, reversed, root, sample, sequence, set_label, spans, split, src, str, strip, subtree, subtrees, tags, tokens, track, typing, unpack_tree_to_features, words, zip","hanlp.common.dataset.TransformableDataset, hanlp_common.constant.BOS, hanlp_common.constant.EOS, phrasetree.tree.Tree, typing.List","append_bos_eos, binarize, build_tree, factorize, load_file, remove_subcategory, track, unpack_tree_to_features",ConstituencyDataset,"chart, child, children, j, label, labels, leaves, line, node, nodes, root, spans, src, subtree, tags, tree, words","Builds a constituency tree from the sequence. The sequence is generated in pre-order.
During building the tree, the sequence is de-binarized to the original format (i.e.,
the suffixes ``|<>`` are ignored, the collapsed labels are recovered).

Args:
    tokens :
        All tokens in a sentence.
    sequence (list[tuple]):
        A list of tuples used for generating a tree.
        Each tuple consits of the indices of left/right span boundaries and label of the span.

Returns:
    A result constituency tree.

Examples:
    >>> tree = Tree.totree(['She', 'enjoys', 'playing', 'tennis', '.'], 'TOP')
    >>> sequence = [(0, 5, 'S'), (0, 4, 'S|<>'), (0, 1, 'NP'), (1, 4, 'VP'), (1, 2, 'VP|<>'),
                    (2, 4, 'S+VP'), (2, 3, 'VP|<>'), (3, 4, 'NP'), (4, 5, 'S|<>')]
    >>> print(Tree.build_tree(root, sequence))
    (TOP
      (S
        (NP (_ She))
        (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
        (_ .))), Conducts binarization over the tree.

First, the tree is transformed to satisfy `Chomsky Normal Form (CNF)`_.
Here we call :meth:`~tree.Tree.chomsky_normal_form` to conduct left-binarization.
Second, all unary productions in the tree are collapsed.

Args:
    tree (tree.Tree):
        The tree to be binarized.

Returns:
    The binarized tree.

Examples:
    >>> tree = Tree.fromstring('''
                                    (TOP
                                      (S
                                        (NP (_ She))
                                        (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
                                        (_ .)))
                                    ''')
    >>> print(Tree.binarize(tree))
    (TOP
      (S
        (S|<>
          (NP (_ She))
          (VP
            (VP|<> (_ enjoys))
            (S+VP (VP|<> (_ playing)) (NP (_ tennis)))))
        (S|<> (_ .))))

.. _Chomsky Normal Form (CNF):
    https://en.wikipedia.org/wiki/Chomsky_normal_form, Factorizes the tree into a sequence.
The tree is traversed in pre-order.

Args:
    tree (tree.Tree):
        The tree to be factorized.
    delete_labels (set[str]):
        A set of labels to be ignored. This is used for evaluation.
        If it is a pre-terminal label, delete the word along with the brackets.
        If it is a non-terminal label, just delete the brackets (don't delete childrens).
        In `EVALB`_, the default set is:
        {'TOP', 'S1', '-NONE-', ',', ':', '``', ""''"", '.', '?', '!', ''}
        Default: ``None``.
    equal_labels (dict[str, str]):
        The key-val pairs in the dict are considered equivalent (non-directional). This is used for evaluation.
        The default dict defined in `EVALB`_ is: {'ADVP': 'PRT'}
        Default: ``None``.

Returns:
    The sequence of the factorized tree.

Examples:
    >>> tree = Tree.fromstring('' (TOP
                                      (S
                                        (NP (_ She))
                                        (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
                                        (_ .)))
                                '')
    >>> Tree.factorize(tree)
    [(0, 5, 'TOP'), (0, 5, 'S'), (0, 1, 'NP'), (1, 4, 'VP'), (2, 4, 'S'), (2, 4, 'VP'), (3, 4, 'NP')]
    >>> Tree.factorize(tree, delete_labels={'TOP', 'S1', '-NONE-', ',', ':', '``', ""''"", '.', '?', '!', ''})
    [(0, 5, 'S'), (0, 1, 'NP'), (1, 4, 'VP'), (2, 4, 'S'), (2, 4, 'VP'), (3, 4, 'NP')]

.. _EVALB:
    https://nlp.cs.nyu.edu/evalb/","
    Builds a constituency tree from the sequence. The sequence is generated in pre-order.
    During building the tree, the sequence is de-binarized to the original format (i.e.,
    the suffixes ``|<>`` are ignored, the collapsed labels are recovered).

    Args:
        tokens :
            All tokens in a sentence.
        sequence (list[tuple]):
            A list of tuples used for generating a tree.
            Each tuple consits of the indices of left/right span boundaries and label of the span.

    Returns:
        A result constituency tree.

    Examples:
        >>> tree = Tree.totree(['She', 'enjoys', 'playing', 'tennis', '.'], 'TOP')
        >>> sequence = [(0, 5, 'S'), (0, 4, 'S|<>'), (0, 1, 'NP'), (1, 4, 'VP'), (1, 2, 'VP|<>'),
                        (2, 4, 'S+VP'), (2, 3, 'VP|<>'), (3, 4, 'NP'), (4, 5, 'S|<>')]
        >>> print(Tree.build_tree(root, sequence))
        (TOP
          (S
            (NP (_ She))
            (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
            (_ .)))
    , 
    Conducts binarization over the tree.

    First, the tree is transformed to satisfy `Chomsky Normal Form (CNF)`_.
    Here we call :meth:`~tree.Tree.chomsky_normal_form` to conduct left-binarization.
    Second, all unary productions in the tree are collapsed.

    Args:
        tree (tree.Tree):
            The tree to be binarized.

    Returns:
        The binarized tree.

    Examples:
        >>> tree = Tree.fromstring('''
                                        (TOP
                                          (S
                                            (NP (_ She))
                                            (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
                                            (_ .)))
                                        ''')
        >>> print(Tree.binarize(tree))
        (TOP
          (S
            (S|<>
              (NP (_ She))
              (VP
                (VP|<> (_ enjoys))
                (S+VP (VP|<> (_ playing)) (NP (_ tennis)))))
            (S|<> (_ .))))

    .. _Chomsky Normal Form (CNF):
        https://en.wikipedia.org/wiki/Chomsky_normal_form
    , 
    Factorizes the tree into a sequence.
    The tree is traversed in pre-order.

    Args:
        tree (tree.Tree):
            The tree to be factorized.
        delete_labels (set[str]):
            A set of labels to be ignored. This is used for evaluation.
            If it is a pre-terminal label, delete the word along with the brackets.
            If it is a non-terminal label, just delete the brackets (don't delete childrens).
            In `EVALB`_, the default set is:
            {'TOP', 'S1', '-NONE-', ',', ':', '``', ""''"", '.', '?', '!', ''}
            Default: ``None``.
        equal_labels (dict[str, str]):
            The key-val pairs in the dict are considered equivalent (non-directional). This is used for evaluation.
            The default dict defined in `EVALB`_ is: {'ADVP': 'PRT'}
            Default: ``None``.

    Returns:
        The sequence of the factorized tree.

    Examples:
        >>> tree = Tree.fromstring('' (TOP
                                          (S
                                            (NP (_ She))
                                            (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
                                            (_ .)))
                                    '')
        >>> Tree.factorize(tree)
        [(0, 5, 'TOP'), (0, 5, 'S'), (0, 1, 'NP'), (1, 4, 'VP'), (2, 4, 'S'), (2, 4, 'VP'), (3, 4, 'NP')]
        >>> Tree.factorize(tree, delete_labels={'TOP', 'S1', '-NONE-', ',', ':', '``', ""''"", '.', '?', '!', ''})
        [(0, 5, 'S'), (0, 1, 'NP'), (1, 4, 'VP'), (2, 4, 'S'), (2, 4, 'VP'), (3, 4, 'NP')]

    .. _EVALB:
        https://nlp.cs.nyu.edu/evalb/
    , +, -, TOP, _, _con_token, chart, constituency, left, token, |<>","0, 1, None, True","

def track(tree, i):
label = tree.label()
if delete_labels is not None and label in delete_labels:
label = None
if equal_labels is not None:
label = equal_labels.get(label, label)
if len(tree) == 1 and not isinstance(tree[0], Tree):
return (i + 1 if label is not None else i), []
j, spans = i, []
for child in tree:
if isinstance(child, Tree):
j, s = track(child, j)
spans += s
if label is not None and j > i:
spans = [(i, j, label)] + spans
return j, spans

return track(tree, 0)[1]


def build_tree(tokens: List[str], sequence):
r""""""
Builds a constituency tree from the sequence. The sequence is generated in pre-order.
During building the tree, the sequence is de-binarized to the original format (i.e.,
the suffixes ``|<>`` are ignored, the collapsed labels are recovered).

Args:
tokens :
All tokens in a sentence.
sequence (list[tuple]):
A list of tuples used for generating a tree.
Each tuple consits of the indices of left/right span boundaries and label of the span.

Returns:
A result constituency tree.

Examples:
>>> tree = Tree.totree(['She', 'enjoys', 'playing', 'tennis', '.'], 'TOP')
>>> sequence = [(0, 5, 'S'), (0, 4, 'S|<>'), (0, 1, 'NP'), (1, 4, 'VP'), (1, 2, 'VP|<>'),
(2, 4, 'S+VP'), (2, 3, 'VP|<>'), (3, 4, 'NP'), (4, 5, 'S|<>')]
>>> print(Tree.build_tree(root, sequence))
(TOP
(S
(NP (_ She))
(VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
(_ .)))
, #     label = label.split('-')[0], # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-28 19:27, # User passed in [], which is the tokenized result of '', # if no_subcategory:, )
>>> print(Tree.binarize(tree))
(TOP
(S
(S|<>
(NP (_ She))
(VP
(VP|<> (_ enjoys))
(S+VP (VP|<> (_ playing)) (NP (_ tennis)))))
(S|<> (_ .))))

.. _Chomsky Normal Form (CNF):
https://en.wikipedia.org/wiki/Chomsky_normal_form
",
https://github.com/hankcs/HanLP,_ctb_utils.py,0,0.0,89,39.73,67,29.91,8,3.57,59,26.34,1,0.45,14,0,42,3,,"Exception, RuntimeError, ValueError, _list_treebank_root, append, bin, buffer, cells, chtbs, cid, cid_domain, cids, clean_ctb_bracketed, cleaned_root, collections, conllx, content, convert_to_dependency, ctb_home, ctb_pos_to_text_format, ctb_root, cws, delimiter, dep_path, dev, domain, domains, dst, each, endswith, enumerate, ext, fid, fp, fromstring, hanlp, id_of_chtb, items, jclass, join, language, len, line, list_treebank, load_bracketed_trees, load_domains, log, make_ctb, make_ctb_tasks, open, os, out_root, par_path, part, pformat, phrasetree, pos, read, remove_all_ec, reverse_splits, script, shutil, sorted, sp_home, split, split_chtb, split_str_to_trees, splits, src, startswith, str, strip, sys, tag, task, tasks_root, test, text, timer, train, trees, typing, ud, unused, version, word, words, write, zip","collections.defaultdict, hanlp.components.parsers.conll.read_conll, hanlp.utils.io_util.get_exitcode_stdout_stderr, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.pushd, hanlp.utils.io_util.read_tsv_as_sents, hanlp.utils.io_util.run_cmd, hanlp.utils.log_util.cprint, hanlp.utils.time_util.CountdownTimer, os.listdir, os.makedirs, os.path.basename, os.path.isdir, os.path.isfile, os.path.join, os.path.splitext, phrasetree.tree.Tree, shutil.rmtree, sys.maxsize, typing.List","_list_treebank_root, clean_ctb_bracketed, convert_to_dependency, ctb_pos_to_text_format, id_of_chtb, list_treebank, load_bracketed_trees, load_domains, make_ctb, make_ctb_tasks, remove_all_ec, reverse_splits, split_chtb, split_str_to_trees",,"bin, buffer, cells, chtbs, cid, cid_domain, cids, cleaned_root, cmd, content, ctb_home, ctb_root, cws, dep_path, dev, domain, domains, each, ext, fid, fp, jclass, line, par_path, part, path, pos, script, sents, sp_home, src, tag, task, tasks_root, test, timer, train, tree, trees, unused, word, words","Convert ctb pos tagging corpus from tsv format to text format, where each word is followed by
its pos tag.
Args:
    path: File to be converted.
    delimiter: Delimiter between word and tag., Load file ids from a Chinese treebank grouped by domains.

Args:
    ctb_home: Root path to CTB.

Returns:
    A dict of sets, each represents a domain., Remove empty categories for all trees in this file and save them into a ""noempty"" file.

Args:
    path: File path.","	, 
, 

, 
    Convert ctb pos tagging corpus from tsv format to text format, where each word is followed by
    its pos tag.
    Args:
        path: File to be converted.
        delimiter: Delimiter between word and tag.
    , 
    Load file ids from a Chinese treebank grouped by domains.

    Args:
        ctb_home: Root path to CTB.

    Returns:
        A dict of sets, each represents a domain.
    , 
    Remove empty categories for all trees in this file and save them into a ""noempty"" file.

    Args:
        path: File path.
    , 
Do you have java installed? Do you have enough memory?,  ,  -basic -keepPunct,  -conllx,  -treeFile ,  = ,  files in CTB, we apply the following splits:,  for ,  to ,  using Stanford Parser Version , (, -, -NONE-, ., . It might take a while [blink][yellow]...[/yellow][/blink], . The err message is:
 , .conllx, .tsv, .txt, /* , 3.3.0, 4.2.0, 8, 9, <, Cleaning up CTB [blink][yellow]...[/yellow][/blink], Conversion failed with code , Converting , Done pre-processing CTB. Enjoy your research with [blue]HanLP[/blue]!, FW, For the , Preprocesing the [blue], Unsupported version , X, [/blue] set of CTB [blink][yellow]...[/yellow][/blink], [yellow]Each file id ending with 8/9 is put into dev/test respectively, the rest are put into train. Our splits ensure files are evenly split across each genre, which is recommended for production systems.[/yellow], _, bracketed, chtb, cleaned_bracket, cws, dep, dev, edu.stanford.nlp.trees.EnglishGrammaticalStructure, edu.stanford.nlp.trees.international.pennchinese.ChineseGrammaticalStructure, edu.stanford.nlp.trees.international.pennchinese.UniversalChineseGrammaticalStructure, edu.stanford.nlp.trees.ud.UniversalDependenciesConverter, https://file.hankcs.com/bin/remove_ec.zip, https://nlp.stanford.edu/software/stanford-parser-4.2.0.zip, https://nlp.stanford.edu/software/stanford-parser-full-2013-11-12.zip, java -cp , java -cp elit-ddr-0.0.5-SNAPSHOT.jar:elit-sdk-0.0.5-SNAPSHOT.jar:hanlp-1.7.8.jar:fastutil-8.1.1.jar:. demo.RemoveEmptyCategoriesTreebank , par, pos, tasks, test, train, utf-8, w, zh, {}	{}
","0, 1, 2, 3, 4, False, None, True","
Convert ctb pos tagging corpus from tsv format to text format, where each word is followed by
its pos tag.
Args:
path: File to be converted.
delimiter: Delimiter between word and tag.
, 
Load file ids from a Chinese treebank grouped by domains.

Args:
ctb_home: Root path to CTB.

Returns:
A dict of sets, each represents a domain.
, 
Remove empty categories for all trees in this file and save them into a ""noempty"" file.

Args:
path: File path.
, #, #                     # cids.update(map(lambda x: f'{x:04d}', range(start, end))), #                     cids.add(int(each)), #                     cids.update(range(start, end + 1)), #                     continue, #                     start, end = each.split('-'), #                     start, end = map(lambda x: int(x), [start, end]), #                 each = each.strip(), #                 else:, #                 if '-' in each:, #                 if not each:, #             continue, #             for each in line.split(','):, #         assert len(cids & total) == 0, f'Overlap found in {part}', #         cids = set(), #         cids = set(f'{x:04d}' for x in cids), #         for line in lines:, #         if not isinstance(text, str):, #         lines = text.replace('\n', '').split(), #         splits[part] = cids, #     ''', #     ''',, #     'dev': ''', #     'test': ''', #     'train': ''', #     for part, text in list(splits.items()):, #     return splits, #     total = set(), # -*- coding:utf-8 -*-, # 0001-0043, 0144-0169, 0271-0301,, # 0044-0143, 0170-0270, 0400-0899,, # 0301-0326, 2916-3030, 4100-4106,, # 0900-0931, 1018, 1020, 1036, 1044,, # 1001-1017, 1019, 1021-1035, 1037-, # 1043, 1045-1059, 1062-1071, 1073-, # 1060, 1061, 1072, 1118, 1119, 1132,, # 1117, 1120-1131, 1133-1140, 1143-, # 1141, 1142, 1148, 3031-3145, 4107-, # 1147, 1149-1151, 2000-2915, 4051-, # 4099, 4112-4180, 4198-4368, 5000-, # 4111, 4190-4197, 4391-4411, 5493-, # 4181-4189, 4369-4390, 5447-5492,, # 5446, 6000-6560, 7000-7013, # 5558, 6631-6700, 7015-7017, # 6561-6630, 7013-7014, # Author: hankcs, # CTB9_ACADEMIA_SPLITS = {, # Date: 2020-11-25 16:14, # NT-SHORT ---> NT, # See Shao et al., 2017, # _make_splits(CTB9_ACADEMIA_SPLITS), # def _make_splits(splits: Dict[str, str]):, # jar_path = get_resource(f'{sp_home}#stanford-parser.jar'), # raise IOError(f'{name} not in any splits'), # }, # 铜_NN 30_CD ｘ_X 25_CD ｘ_X 14_CD cm_NT 1999_NT",铜_NN 30_CD ｘ_X 25_CD ｘ_X 14_CD cm_NT 1999_NT
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:04",
https://github.com/hankcs/HanLP,ud210.py,0,0.0,509,33.53,1003,66.07,1,0.07,5,0.33,0,0.0,2,0,498,0,,"UD_210_AFRIKAANS_AFRIBOOMS_DEV, UD_210_AFRIKAANS_AFRIBOOMS_TEST, UD_210_AFRIKAANS_AFRIBOOMS_TRAIN, UD_210_AKKADIAN_PISANDUB_TEST, UD_210_AKKADIAN_RIAO_TEST, UD_210_AKUNTSU_TUDET_TEST, UD_210_ALBANIAN_TSA_TEST, UD_210_AMHARIC_ATT_TEST, UD_210_ANCIENT_GREEK_PERSEUS_DEV, UD_210_ANCIENT_GREEK_PERSEUS_TEST, UD_210_ANCIENT_GREEK_PERSEUS_TRAIN, UD_210_ANCIENT_GREEK_PROIEL_DEV, UD_210_ANCIENT_GREEK_PROIEL_TEST, UD_210_ANCIENT_GREEK_PROIEL_TRAIN, UD_210_ANCIENT_HEBREW_PTNK_DEV, UD_210_ANCIENT_HEBREW_PTNK_TEST, UD_210_ANCIENT_HEBREW_PTNK_TRAIN, UD_210_APURINA_UFPA_TEST, UD_210_ARABIC_NYUAD_DEV, UD_210_ARABIC_NYUAD_TEST, UD_210_ARABIC_NYUAD_TRAIN, UD_210_ARABIC_PADT_DEV, UD_210_ARABIC_PADT_TEST, UD_210_ARABIC_PADT_TRAIN, UD_210_ARABIC_PUD_TEST, UD_210_ARMENIAN_ARMTDP_DEV, UD_210_ARMENIAN_ARMTDP_TEST, UD_210_ARMENIAN_ARMTDP_TRAIN, UD_210_ARMENIAN_BSUT_DEV, UD_210_ARMENIAN_BSUT_TEST, UD_210_ARMENIAN_BSUT_TRAIN, UD_210_ASSYRIAN_AS_TEST, UD_210_BAMBARA_CRB_TEST, UD_210_BASQUE_BDT_DEV, UD_210_BASQUE_BDT_TEST, UD_210_BASQUE_BDT_TRAIN, UD_210_BEJA_NSC_TEST, UD_210_BELARUSIAN_HSE_DEV, UD_210_BELARUSIAN_HSE_TEST, UD_210_BELARUSIAN_HSE_TRAIN, UD_210_BENGALI_BRU_TEST, UD_210_BHOJPURI_BHTB_TEST, UD_210_BRETON_KEB_TEST, UD_210_BULGARIAN_BTB_DEV, UD_210_BULGARIAN_BTB_TEST, UD_210_BULGARIAN_BTB_TRAIN, UD_210_BURYAT_BDT_TEST, UD_210_BURYAT_BDT_TRAIN, UD_210_CANTONESE_HK_TEST, UD_210_CATALAN_ANCORA_DEV, UD_210_CATALAN_ANCORA_TEST, UD_210_CATALAN_ANCORA_TRAIN, UD_210_CEBUANO_GJA_TEST, UD_210_CHINESE_CFL_TEST, UD_210_CHINESE_GSDSIMP_DEV, UD_210_CHINESE_GSDSIMP_TEST, UD_210_CHINESE_GSDSIMP_TRAIN, UD_210_CHINESE_GSD_DEV, UD_210_CHINESE_GSD_TEST, UD_210_CHINESE_GSD_TRAIN, UD_210_CHINESE_HK_TEST, UD_210_CHINESE_PUD_TEST, UD_210_CHUKCHI_HSE_TEST, UD_210_CLASSICAL_CHINESE_KYOTO_DEV, UD_210_CLASSICAL_CHINESE_KYOTO_TEST, UD_210_CLASSICAL_CHINESE_KYOTO_TRAIN, UD_210_COPTIC_SCRIPTORIUM_DEV, UD_210_COPTIC_SCRIPTORIUM_TEST, UD_210_COPTIC_SCRIPTORIUM_TRAIN, UD_210_CROATIAN_SET_DEV, UD_210_CROATIAN_SET_TEST, UD_210_CROATIAN_SET_TRAIN, UD_210_CZECH_CAC_DEV, UD_210_CZECH_CAC_TEST, UD_210_CZECH_CAC_TRAIN, UD_210_CZECH_CLTT_DEV, UD_210_CZECH_CLTT_TEST, UD_210_CZECH_CLTT_TRAIN, UD_210_CZECH_FICTREE_DEV, UD_210_CZECH_FICTREE_TEST, UD_210_CZECH_FICTREE_TRAIN, UD_210_CZECH_PDT_DEV, UD_210_CZECH_PDT_TEST, UD_210_CZECH_PDT_TRAIN, UD_210_CZECH_PUD_TEST, UD_210_DANISH_DDT_DEV, UD_210_DANISH_DDT_TEST, UD_210_DANISH_DDT_TRAIN, UD_210_DUTCH_ALPINO_DEV, UD_210_DUTCH_ALPINO_TEST, UD_210_DUTCH_ALPINO_TRAIN, UD_210_DUTCH_LASSYSMALL_DEV, UD_210_DUTCH_LASSYSMALL_TEST, UD_210_DUTCH_LASSYSMALL_TRAIN, UD_210_ENGLISH_ATIS_DEV, UD_210_ENGLISH_ATIS_TEST, UD_210_ENGLISH_ATIS_TRAIN, UD_210_ENGLISH_ESL_DEV, UD_210_ENGLISH_ESL_TEST, UD_210_ENGLISH_ESL_TRAIN, UD_210_ENGLISH_EWT_DEV, UD_210_ENGLISH_EWT_TEST, UD_210_ENGLISH_EWT_TRAIN, UD_210_ENGLISH_GUMREDDIT_DEV, UD_210_ENGLISH_GUMREDDIT_TEST, UD_210_ENGLISH_GUMREDDIT_TRAIN, UD_210_ENGLISH_GUM_DEV, UD_210_ENGLISH_GUM_TEST, UD_210_ENGLISH_GUM_TRAIN, UD_210_ENGLISH_LINES_DEV, UD_210_ENGLISH_LINES_TEST, UD_210_ENGLISH_LINES_TRAIN, UD_210_ENGLISH_PARTUT_DEV, UD_210_ENGLISH_PARTUT_TEST, UD_210_ENGLISH_PARTUT_TRAIN, UD_210_ENGLISH_PRONOUNS_TEST, UD_210_ENGLISH_PUD_TEST, UD_210_ERZYA_JR_TEST, UD_210_ESTONIAN_EDT_DEV, UD_210_ESTONIAN_EDT_TEST, UD_210_ESTONIAN_EDT_TRAIN, UD_210_ESTONIAN_EWT_DEV, UD_210_ESTONIAN_EWT_TEST, UD_210_ESTONIAN_EWT_TRAIN, UD_210_FAROESE_FARPAHC_DEV, UD_210_FAROESE_FARPAHC_TEST, UD_210_FAROESE_FARPAHC_TRAIN, UD_210_FAROESE_OFT_TEST, UD_210_FINNISH_FTB_DEV, UD_210_FINNISH_FTB_TEST, UD_210_FINNISH_FTB_TRAIN, UD_210_FINNISH_OOD_TEST, UD_210_FINNISH_PUD_TEST, UD_210_FINNISH_TDT_DEV, UD_210_FINNISH_TDT_TEST, UD_210_FINNISH_TDT_TRAIN, UD_210_FRENCH_FQB_TEST, UD_210_FRENCH_FTB_DEV, UD_210_FRENCH_FTB_TEST, UD_210_FRENCH_FTB_TRAIN, UD_210_FRENCH_GSD_DEV, UD_210_FRENCH_GSD_TEST, UD_210_FRENCH_GSD_TRAIN, UD_210_FRENCH_PARISSTORIES_TEST, UD_210_FRENCH_PARISSTORIES_TRAIN, UD_210_FRENCH_PARTUT_DEV, UD_210_FRENCH_PARTUT_TEST, UD_210_FRENCH_PARTUT_TRAIN, UD_210_FRENCH_PUD_TEST, UD_210_FRENCH_RHAPSODIE_DEV, UD_210_FRENCH_RHAPSODIE_TEST, UD_210_FRENCH_RHAPSODIE_TRAIN, UD_210_FRENCH_SEQUOIA_DEV, UD_210_FRENCH_SEQUOIA_TEST, UD_210_FRENCH_SEQUOIA_TRAIN, UD_210_FRISIAN_DUTCH_FAME_TEST, UD_210_GALICIAN_CTG_DEV, UD_210_GALICIAN_CTG_TEST, UD_210_GALICIAN_CTG_TRAIN, UD_210_GALICIAN_TREEGAL_TEST, UD_210_GALICIAN_TREEGAL_TRAIN, UD_210_GERMAN_GSD_DEV, UD_210_GERMAN_GSD_TEST, UD_210_GERMAN_GSD_TRAIN, UD_210_GERMAN_HDT_DEV, UD_210_GERMAN_HDT_TEST, UD_210_GERMAN_HDT_TRAIN, UD_210_GERMAN_LIT_TEST, UD_210_GERMAN_PUD_TEST, UD_210_GOTHIC_PROIEL_DEV, UD_210_GOTHIC_PROIEL_TEST, UD_210_GOTHIC_PROIEL_TRAIN, UD_210_GREEK_GDT_DEV, UD_210_GREEK_GDT_TEST, UD_210_GREEK_GDT_TRAIN, UD_210_GUAJAJARA_TUDET_TEST, UD_210_GUARANI_OLDTUDET_TEST, UD_210_HEBREW_HTB_DEV, UD_210_HEBREW_HTB_TEST, UD_210_HEBREW_HTB_TRAIN, UD_210_HEBREW_IAHLTWIKI_DEV, UD_210_HEBREW_IAHLTWIKI_TEST, UD_210_HEBREW_IAHLTWIKI_TRAIN, UD_210_HINDI_ENGLISH_HIENCS_DEV, UD_210_HINDI_ENGLISH_HIENCS_TEST, UD_210_HINDI_ENGLISH_HIENCS_TRAIN, UD_210_HINDI_HDTB_DEV, UD_210_HINDI_HDTB_TEST, UD_210_HINDI_HDTB_TRAIN, UD_210_HINDI_PUD_TEST, UD_210_HITTITE_HITTB_TEST, UD_210_HUNGARIAN_SZEGED_DEV, UD_210_HUNGARIAN_SZEGED_TEST, UD_210_HUNGARIAN_SZEGED_TRAIN, UD_210_ICELANDIC_ICEPAHC_DEV, UD_210_ICELANDIC_ICEPAHC_TEST, UD_210_ICELANDIC_ICEPAHC_TRAIN, UD_210_ICELANDIC_MODERN_DEV, UD_210_ICELANDIC_MODERN_TEST, UD_210_ICELANDIC_MODERN_TRAIN, UD_210_ICELANDIC_PUD_TEST, UD_210_INDONESIAN_CSUI_TEST, UD_210_INDONESIAN_CSUI_TRAIN, UD_210_INDONESIAN_GSD_DEV, UD_210_INDONESIAN_GSD_TEST, UD_210_INDONESIAN_GSD_TRAIN, UD_210_INDONESIAN_PUD_TEST, UD_210_IRISH_IDT_DEV, UD_210_IRISH_IDT_TEST, UD_210_IRISH_IDT_TRAIN, UD_210_IRISH_TWITTIRISH_TEST, UD_210_ITALIAN_ISDT_DEV, UD_210_ITALIAN_ISDT_TEST, UD_210_ITALIAN_ISDT_TRAIN, UD_210_ITALIAN_MARKIT_DEV, UD_210_ITALIAN_MARKIT_TEST, UD_210_ITALIAN_MARKIT_TRAIN, UD_210_ITALIAN_PARTUT_DEV, UD_210_ITALIAN_PARTUT_TEST, UD_210_ITALIAN_PARTUT_TRAIN, UD_210_ITALIAN_POSTWITA_DEV, UD_210_ITALIAN_POSTWITA_TEST, UD_210_ITALIAN_POSTWITA_TRAIN, UD_210_ITALIAN_PUD_TEST, UD_210_ITALIAN_TWITTIRO_DEV, UD_210_ITALIAN_TWITTIRO_TEST, UD_210_ITALIAN_TWITTIRO_TRAIN, UD_210_ITALIAN_VALICO_TEST, UD_210_ITALIAN_VIT_DEV, UD_210_ITALIAN_VIT_TEST, UD_210_ITALIAN_VIT_TRAIN, UD_210_JAPANESE_BCCWJLUW_DEV, UD_210_JAPANESE_BCCWJLUW_TEST, UD_210_JAPANESE_BCCWJLUW_TRAIN, UD_210_JAPANESE_BCCWJ_DEV, UD_210_JAPANESE_BCCWJ_TEST, UD_210_JAPANESE_BCCWJ_TRAIN, UD_210_JAPANESE_GSDLUW_DEV, UD_210_JAPANESE_GSDLUW_TEST, UD_210_JAPANESE_GSDLUW_TRAIN, UD_210_JAPANESE_GSD_DEV, UD_210_JAPANESE_GSD_TEST, UD_210_JAPANESE_GSD_TRAIN, UD_210_JAPANESE_MODERN_TEST, UD_210_JAPANESE_PUDLUW_TEST, UD_210_JAPANESE_PUD_TEST, UD_210_JAVANESE_CSUI_TEST, UD_210_KAAPOR_TUDET_TEST, UD_210_KANGRI_KDTB_TEST, UD_210_KARELIAN_KKPP_TEST, UD_210_KARO_TUDET_TEST, UD_210_KAZAKH_KTB_TEST, UD_210_KAZAKH_KTB_TRAIN, UD_210_KHUNSARI_AHA_TEST, UD_210_KICHE_IU_TEST, UD_210_KOMI_PERMYAK_UH_TEST, UD_210_KOMI_ZYRIAN_IKDP_TEST, UD_210_KOMI_ZYRIAN_LATTICE_TEST, UD_210_KOREAN_GSD_DEV, UD_210_KOREAN_GSD_TEST, UD_210_KOREAN_GSD_TRAIN, UD_210_KOREAN_KAIST_DEV, UD_210_KOREAN_KAIST_TEST, UD_210_KOREAN_KAIST_TRAIN, UD_210_KOREAN_PUD_TEST, UD_210_KURMANJI_MG_TEST, UD_210_KURMANJI_MG_TRAIN, UD_210_LATIN_ITTB_DEV, UD_210_LATIN_ITTB_TEST, UD_210_LATIN_ITTB_TRAIN, UD_210_LATIN_LLCT_DEV, UD_210_LATIN_LLCT_TEST, UD_210_LATIN_LLCT_TRAIN, UD_210_LATIN_PERSEUS_TEST, UD_210_LATIN_PERSEUS_TRAIN, UD_210_LATIN_PROIEL_DEV, UD_210_LATIN_PROIEL_TEST, UD_210_LATIN_PROIEL_TRAIN, UD_210_LATIN_UDANTE_DEV, UD_210_LATIN_UDANTE_TEST, UD_210_LATIN_UDANTE_TRAIN, UD_210_LATVIAN_LVTB_DEV, UD_210_LATVIAN_LVTB_TEST, UD_210_LATVIAN_LVTB_TRAIN, UD_210_LIGURIAN_GLT_TEST, UD_210_LIGURIAN_GLT_TRAIN, UD_210_LITHUANIAN_ALKSNIS_DEV, UD_210_LITHUANIAN_ALKSNIS_TEST, UD_210_LITHUANIAN_ALKSNIS_TRAIN, UD_210_LITHUANIAN_HSE_DEV, UD_210_LITHUANIAN_HSE_TEST, UD_210_LITHUANIAN_HSE_TRAIN, UD_210_LIVVI_KKPP_TEST, UD_210_LIVVI_KKPP_TRAIN, UD_210_LOW_SAXON_LSDC_TEST, UD_210_MADI_JARAWARA_TEST, UD_210_MAKURAP_TUDET_TEST, UD_210_MALTESE_MUDT_DEV, UD_210_MALTESE_MUDT_TEST, UD_210_MALTESE_MUDT_TRAIN, UD_210_MANX_CADHAN_TEST, UD_210_MARATHI_UFAL_DEV, UD_210_MARATHI_UFAL_TEST, UD_210_MARATHI_UFAL_TRAIN, UD_210_MBYA_GUARANI_DOOLEY_TEST, UD_210_MBYA_GUARANI_THOMAS_TEST, UD_210_MOKSHA_JR_TEST, UD_210_MUNDURUKU_TUDET_TEST, UD_210_NAIJA_NSC_DEV, UD_210_NAIJA_NSC_TEST, UD_210_NAIJA_NSC_TRAIN, UD_210_NAYINI_AHA_TEST, UD_210_NEAPOLITAN_RB_TEST, UD_210_NORTH_SAMI_GIELLA_TEST, UD_210_NORTH_SAMI_GIELLA_TRAIN, UD_210_NORWEGIAN_BOKMAAL_DEV, UD_210_NORWEGIAN_BOKMAAL_TEST, UD_210_NORWEGIAN_BOKMAAL_TRAIN, UD_210_NORWEGIAN_NYNORSKLIA_DEV, UD_210_NORWEGIAN_NYNORSKLIA_TEST, UD_210_NORWEGIAN_NYNORSKLIA_TRAIN, UD_210_NORWEGIAN_NYNORSK_DEV, UD_210_NORWEGIAN_NYNORSK_TEST, UD_210_NORWEGIAN_NYNORSK_TRAIN, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_DEV, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_TEST, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_TRAIN, UD_210_OLD_EAST_SLAVIC_RNC_TEST, UD_210_OLD_EAST_SLAVIC_RNC_TRAIN, UD_210_OLD_EAST_SLAVIC_TOROT_DEV, UD_210_OLD_EAST_SLAVIC_TOROT_TEST, UD_210_OLD_EAST_SLAVIC_TOROT_TRAIN, UD_210_OLD_FRENCH_SRCMF_DEV, UD_210_OLD_FRENCH_SRCMF_TEST, UD_210_OLD_FRENCH_SRCMF_TRAIN, UD_210_OLD_TURKISH_TONQQ_TEST, UD_210_PERSIAN_PERDT_DEV, UD_210_PERSIAN_PERDT_TEST, UD_210_PERSIAN_PERDT_TRAIN, UD_210_PERSIAN_SERAJI_DEV, UD_210_PERSIAN_SERAJI_TEST, UD_210_PERSIAN_SERAJI_TRAIN, UD_210_POLISH_LFG_DEV, UD_210_POLISH_LFG_TEST, UD_210_POLISH_LFG_TRAIN, UD_210_POLISH_PDB_DEV, UD_210_POLISH_PDB_TEST, UD_210_POLISH_PDB_TRAIN, UD_210_POLISH_PUD_TEST, UD_210_POMAK_PHILOTIS_DEV, UD_210_POMAK_PHILOTIS_TEST, UD_210_POMAK_PHILOTIS_TRAIN, UD_210_PORTUGUESE_BOSQUE_DEV, UD_210_PORTUGUESE_BOSQUE_TEST, UD_210_PORTUGUESE_BOSQUE_TRAIN, UD_210_PORTUGUESE_GSD_DEV, UD_210_PORTUGUESE_GSD_TEST, UD_210_PORTUGUESE_GSD_TRAIN, UD_210_PORTUGUESE_PUD_TEST, UD_210_ROMANIAN_ART_TEST, UD_210_ROMANIAN_NONSTANDARD_DEV, UD_210_ROMANIAN_NONSTANDARD_TEST, UD_210_ROMANIAN_NONSTANDARD_TRAIN, UD_210_ROMANIAN_RRT_DEV, UD_210_ROMANIAN_RRT_TEST, UD_210_ROMANIAN_RRT_TRAIN, UD_210_ROMANIAN_SIMONERO_DEV, UD_210_ROMANIAN_SIMONERO_TEST, UD_210_ROMANIAN_SIMONERO_TRAIN, UD_210_RUSSIAN_GSD_DEV, UD_210_RUSSIAN_GSD_TEST, UD_210_RUSSIAN_GSD_TRAIN, UD_210_RUSSIAN_PUD_TEST, UD_210_RUSSIAN_SYNTAGRUS_DEV, UD_210_RUSSIAN_SYNTAGRUS_TEST, UD_210_RUSSIAN_SYNTAGRUS_TRAIN, UD_210_RUSSIAN_TAIGA_DEV, UD_210_RUSSIAN_TAIGA_TEST, UD_210_RUSSIAN_TAIGA_TRAIN, UD_210_SANSKRIT_UFAL_TEST, UD_210_SANSKRIT_VEDIC_TEST, UD_210_SANSKRIT_VEDIC_TRAIN, UD_210_SCOTTISH_GAELIC_ARCOSG_DEV, UD_210_SCOTTISH_GAELIC_ARCOSG_TEST, UD_210_SCOTTISH_GAELIC_ARCOSG_TRAIN, UD_210_SERBIAN_SET_DEV, UD_210_SERBIAN_SET_TEST, UD_210_SERBIAN_SET_TRAIN, UD_210_SKOLT_SAMI_GIELLAGAS_TEST, UD_210_SLOVAK_SNK_DEV, UD_210_SLOVAK_SNK_TEST, UD_210_SLOVAK_SNK_TRAIN, UD_210_SLOVENIAN_SSJ_DEV, UD_210_SLOVENIAN_SSJ_TEST, UD_210_SLOVENIAN_SSJ_TRAIN, UD_210_SLOVENIAN_SST_TEST, UD_210_SLOVENIAN_SST_TRAIN, UD_210_SOI_AHA_TEST, UD_210_SOUTH_LEVANTINE_ARABIC_MADAR_TEST, UD_210_SPANISH_ANCORA_DEV, UD_210_SPANISH_ANCORA_TEST, UD_210_SPANISH_ANCORA_TRAIN, UD_210_SPANISH_GSD_DEV, UD_210_SPANISH_GSD_TEST, UD_210_SPANISH_GSD_TRAIN, UD_210_SPANISH_PUD_TEST, UD_210_SWEDISH_LINES_DEV, UD_210_SWEDISH_LINES_TEST, UD_210_SWEDISH_LINES_TRAIN, UD_210_SWEDISH_PUD_TEST, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_210_SWEDISH_TALBANKEN_DEV, UD_210_SWEDISH_TALBANKEN_TEST, UD_210_SWEDISH_TALBANKEN_TRAIN, UD_210_SWISS_GERMAN_UZH_TEST, UD_210_TAGALOG_TRG_TEST, UD_210_TAGALOG_UGNAYAN_TEST, UD_210_TAMIL_MWTT_TEST, UD_210_TAMIL_TTB_DEV, UD_210_TAMIL_TTB_TEST, UD_210_TAMIL_TTB_TRAIN, UD_210_TATAR_NMCTT_TEST, UD_210_TEKO_TUDET_TEST, UD_210_TELUGU_MTG_DEV, UD_210_TELUGU_MTG_TEST, UD_210_TELUGU_MTG_TRAIN, UD_210_THAI_PUD_TEST, UD_210_TUPINAMBA_TUDET_TEST, UD_210_TURKISH_ATIS_DEV, UD_210_TURKISH_ATIS_TEST, UD_210_TURKISH_ATIS_TRAIN, UD_210_TURKISH_BOUN_DEV, UD_210_TURKISH_BOUN_TEST, UD_210_TURKISH_BOUN_TRAIN, UD_210_TURKISH_FRAMENET_DEV, UD_210_TURKISH_FRAMENET_TEST, UD_210_TURKISH_FRAMENET_TRAIN, UD_210_TURKISH_GB_TEST, UD_210_TURKISH_GERMAN_SAGT_DEV, UD_210_TURKISH_GERMAN_SAGT_TEST, UD_210_TURKISH_GERMAN_SAGT_TRAIN, UD_210_TURKISH_IMST_DEV, UD_210_TURKISH_IMST_TEST, UD_210_TURKISH_IMST_TRAIN, UD_210_TURKISH_KENET_DEV, UD_210_TURKISH_KENET_TEST, UD_210_TURKISH_KENET_TRAIN, UD_210_TURKISH_PENN_DEV, UD_210_TURKISH_PENN_TEST, UD_210_TURKISH_PENN_TRAIN, UD_210_TURKISH_PUD_TEST, UD_210_TURKISH_TOURISM_DEV, UD_210_TURKISH_TOURISM_TEST, UD_210_TURKISH_TOURISM_TRAIN, UD_210_UKRAINIAN_IU_DEV, UD_210_UKRAINIAN_IU_TEST, UD_210_UKRAINIAN_IU_TRAIN, UD_210_UMBRIAN_IKUVINA_TEST, UD_210_UPPER_SORBIAN_UFAL_TEST, UD_210_UPPER_SORBIAN_UFAL_TRAIN, UD_210_URDU_UDTB_DEV, UD_210_URDU_UDTB_TEST, UD_210_URDU_UDTB_TRAIN, UD_210_UYGHUR_UDT_DEV, UD_210_UYGHUR_UDT_TEST, UD_210_UYGHUR_UDT_TRAIN, UD_210_VIETNAMESE_VTB_DEV, UD_210_VIETNAMESE_VTB_TEST, UD_210_VIETNAMESE_VTB_TRAIN, UD_210_WARLPIRI_UFAL_TEST, UD_210_WELSH_CCG_DEV, UD_210_WELSH_CCG_TEST, UD_210_WELSH_CCG_TRAIN, UD_210_WESTERN_ARMENIAN_ARMTDP_DEV, UD_210_WESTERN_ARMENIAN_ARMTDP_TEST, UD_210_WESTERN_ARMENIAN_ARMTDP_TRAIN, UD_210_WOLOF_WTB_DEV, UD_210_WOLOF_WTB_TEST, UD_210_WOLOF_WTB_TRAIN, UD_210_XIBE_XDT_TEST, UD_210_YAKUT_YKTDT_TEST, UD_210_YORUBA_YTB_TEST, UD_210_YUPIK_SLI_TEST, _UD_210_HOME, _UD_210_URL, __name__, _list_dir, _path, basename, glob, hanlp, home, len, lstrip, main, open, os, out, prefix, sorted, sp, split, upper, write","glob.glob, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.uncompress, os.path.basename, os.path.isfile, os.path.join, os.rename","_list_dir, main",,"UD_210_AFRIKAANS_AFRIBOOMS_DEV, UD_210_AFRIKAANS_AFRIBOOMS_TEST, UD_210_AFRIKAANS_AFRIBOOMS_TRAIN, UD_210_AKKADIAN_PISANDUB_TEST, UD_210_AKKADIAN_RIAO_TEST, UD_210_AKUNTSU_TUDET_TEST, UD_210_ALBANIAN_TSA_TEST, UD_210_AMHARIC_ATT_TEST, UD_210_ANCIENT_GREEK_PERSEUS_DEV, UD_210_ANCIENT_GREEK_PERSEUS_TEST, UD_210_ANCIENT_GREEK_PERSEUS_TRAIN, UD_210_ANCIENT_GREEK_PROIEL_DEV, UD_210_ANCIENT_GREEK_PROIEL_TEST, UD_210_ANCIENT_GREEK_PROIEL_TRAIN, UD_210_ANCIENT_HEBREW_PTNK_DEV, UD_210_ANCIENT_HEBREW_PTNK_TEST, UD_210_ANCIENT_HEBREW_PTNK_TRAIN, UD_210_APURINA_UFPA_TEST, UD_210_ARABIC_NYUAD_DEV, UD_210_ARABIC_NYUAD_TEST, UD_210_ARABIC_NYUAD_TRAIN, UD_210_ARABIC_PADT_DEV, UD_210_ARABIC_PADT_TEST, UD_210_ARABIC_PADT_TRAIN, UD_210_ARABIC_PUD_TEST, UD_210_ARMENIAN_ARMTDP_DEV, UD_210_ARMENIAN_ARMTDP_TEST, UD_210_ARMENIAN_ARMTDP_TRAIN, UD_210_ARMENIAN_BSUT_DEV, UD_210_ARMENIAN_BSUT_TEST, UD_210_ARMENIAN_BSUT_TRAIN, UD_210_ASSYRIAN_AS_TEST, UD_210_BAMBARA_CRB_TEST, UD_210_BASQUE_BDT_DEV, UD_210_BASQUE_BDT_TEST, UD_210_BASQUE_BDT_TRAIN, UD_210_BEJA_NSC_TEST, UD_210_BELARUSIAN_HSE_DEV, UD_210_BELARUSIAN_HSE_TEST, UD_210_BELARUSIAN_HSE_TRAIN, UD_210_BENGALI_BRU_TEST, UD_210_BHOJPURI_BHTB_TEST, UD_210_BRETON_KEB_TEST, UD_210_BULGARIAN_BTB_DEV, UD_210_BULGARIAN_BTB_TEST, UD_210_BULGARIAN_BTB_TRAIN, UD_210_BURYAT_BDT_TEST, UD_210_BURYAT_BDT_TRAIN, UD_210_CANTONESE_HK_TEST, UD_210_CATALAN_ANCORA_DEV, UD_210_CATALAN_ANCORA_TEST, UD_210_CATALAN_ANCORA_TRAIN, UD_210_CEBUANO_GJA_TEST, UD_210_CHINESE_CFL_TEST, UD_210_CHINESE_GSDSIMP_DEV, UD_210_CHINESE_GSDSIMP_TEST, UD_210_CHINESE_GSDSIMP_TRAIN, UD_210_CHINESE_GSD_DEV, UD_210_CHINESE_GSD_TEST, UD_210_CHINESE_GSD_TRAIN, UD_210_CHINESE_HK_TEST, UD_210_CHINESE_PUD_TEST, UD_210_CHUKCHI_HSE_TEST, UD_210_CLASSICAL_CHINESE_KYOTO_DEV, UD_210_CLASSICAL_CHINESE_KYOTO_TEST, UD_210_CLASSICAL_CHINESE_KYOTO_TRAIN, UD_210_COPTIC_SCRIPTORIUM_DEV, UD_210_COPTIC_SCRIPTORIUM_TEST, UD_210_COPTIC_SCRIPTORIUM_TRAIN, UD_210_CROATIAN_SET_DEV, UD_210_CROATIAN_SET_TEST, UD_210_CROATIAN_SET_TRAIN, UD_210_CZECH_CAC_DEV, UD_210_CZECH_CAC_TEST, UD_210_CZECH_CAC_TRAIN, UD_210_CZECH_CLTT_DEV, UD_210_CZECH_CLTT_TEST, UD_210_CZECH_CLTT_TRAIN, UD_210_CZECH_FICTREE_DEV, UD_210_CZECH_FICTREE_TEST, UD_210_CZECH_FICTREE_TRAIN, UD_210_CZECH_PDT_DEV, UD_210_CZECH_PDT_TEST, UD_210_CZECH_PDT_TRAIN, UD_210_CZECH_PUD_TEST, UD_210_DANISH_DDT_DEV, UD_210_DANISH_DDT_TEST, UD_210_DANISH_DDT_TRAIN, UD_210_DUTCH_ALPINO_DEV, UD_210_DUTCH_ALPINO_TEST, UD_210_DUTCH_ALPINO_TRAIN, UD_210_DUTCH_LASSYSMALL_DEV, UD_210_DUTCH_LASSYSMALL_TEST, UD_210_DUTCH_LASSYSMALL_TRAIN, UD_210_ENGLISH_ATIS_DEV, UD_210_ENGLISH_ATIS_TEST, UD_210_ENGLISH_ATIS_TRAIN, UD_210_ENGLISH_ESL_DEV, UD_210_ENGLISH_ESL_TEST, UD_210_ENGLISH_ESL_TRAIN, UD_210_ENGLISH_EWT_DEV, UD_210_ENGLISH_EWT_TEST, UD_210_ENGLISH_EWT_TRAIN, UD_210_ENGLISH_GUMREDDIT_DEV, UD_210_ENGLISH_GUMREDDIT_TEST, UD_210_ENGLISH_GUMREDDIT_TRAIN, UD_210_ENGLISH_GUM_DEV, UD_210_ENGLISH_GUM_TEST, UD_210_ENGLISH_GUM_TRAIN, UD_210_ENGLISH_LINES_DEV, UD_210_ENGLISH_LINES_TEST, UD_210_ENGLISH_LINES_TRAIN, UD_210_ENGLISH_PARTUT_DEV, UD_210_ENGLISH_PARTUT_TEST, UD_210_ENGLISH_PARTUT_TRAIN, UD_210_ENGLISH_PRONOUNS_TEST, UD_210_ENGLISH_PUD_TEST, UD_210_ERZYA_JR_TEST, UD_210_ESTONIAN_EDT_DEV, UD_210_ESTONIAN_EDT_TEST, UD_210_ESTONIAN_EDT_TRAIN, UD_210_ESTONIAN_EWT_DEV, UD_210_ESTONIAN_EWT_TEST, UD_210_ESTONIAN_EWT_TRAIN, UD_210_FAROESE_FARPAHC_DEV, UD_210_FAROESE_FARPAHC_TEST, UD_210_FAROESE_FARPAHC_TRAIN, UD_210_FAROESE_OFT_TEST, UD_210_FINNISH_FTB_DEV, UD_210_FINNISH_FTB_TEST, UD_210_FINNISH_FTB_TRAIN, UD_210_FINNISH_OOD_TEST, UD_210_FINNISH_PUD_TEST, UD_210_FINNISH_TDT_DEV, UD_210_FINNISH_TDT_TEST, UD_210_FINNISH_TDT_TRAIN, UD_210_FRENCH_FQB_TEST, UD_210_FRENCH_FTB_DEV, UD_210_FRENCH_FTB_TEST, UD_210_FRENCH_FTB_TRAIN, UD_210_FRENCH_GSD_DEV, UD_210_FRENCH_GSD_TEST, UD_210_FRENCH_GSD_TRAIN, UD_210_FRENCH_PARISSTORIES_TEST, UD_210_FRENCH_PARISSTORIES_TRAIN, UD_210_FRENCH_PARTUT_DEV, UD_210_FRENCH_PARTUT_TEST, UD_210_FRENCH_PARTUT_TRAIN, UD_210_FRENCH_PUD_TEST, UD_210_FRENCH_RHAPSODIE_DEV, UD_210_FRENCH_RHAPSODIE_TEST, UD_210_FRENCH_RHAPSODIE_TRAIN, UD_210_FRENCH_SEQUOIA_DEV, UD_210_FRENCH_SEQUOIA_TEST, UD_210_FRENCH_SEQUOIA_TRAIN, UD_210_FRISIAN_DUTCH_FAME_TEST, UD_210_GALICIAN_CTG_DEV, UD_210_GALICIAN_CTG_TEST, UD_210_GALICIAN_CTG_TRAIN, UD_210_GALICIAN_TREEGAL_TEST, UD_210_GALICIAN_TREEGAL_TRAIN, UD_210_GERMAN_GSD_DEV, UD_210_GERMAN_GSD_TEST, UD_210_GERMAN_GSD_TRAIN, UD_210_GERMAN_HDT_DEV, UD_210_GERMAN_HDT_TEST, UD_210_GERMAN_HDT_TRAIN, UD_210_GERMAN_LIT_TEST, UD_210_GERMAN_PUD_TEST, UD_210_GOTHIC_PROIEL_DEV, UD_210_GOTHIC_PROIEL_TEST, UD_210_GOTHIC_PROIEL_TRAIN, UD_210_GREEK_GDT_DEV, UD_210_GREEK_GDT_TEST, UD_210_GREEK_GDT_TRAIN, UD_210_GUAJAJARA_TUDET_TEST, UD_210_GUARANI_OLDTUDET_TEST, UD_210_HEBREW_HTB_DEV, UD_210_HEBREW_HTB_TEST, UD_210_HEBREW_HTB_TRAIN, UD_210_HEBREW_IAHLTWIKI_DEV, UD_210_HEBREW_IAHLTWIKI_TEST, UD_210_HEBREW_IAHLTWIKI_TRAIN, UD_210_HINDI_ENGLISH_HIENCS_DEV, UD_210_HINDI_ENGLISH_HIENCS_TEST, UD_210_HINDI_ENGLISH_HIENCS_TRAIN, UD_210_HINDI_HDTB_DEV, UD_210_HINDI_HDTB_TEST, UD_210_HINDI_HDTB_TRAIN, UD_210_HINDI_PUD_TEST, UD_210_HITTITE_HITTB_TEST, UD_210_HUNGARIAN_SZEGED_DEV, UD_210_HUNGARIAN_SZEGED_TEST, UD_210_HUNGARIAN_SZEGED_TRAIN, UD_210_ICELANDIC_ICEPAHC_DEV, UD_210_ICELANDIC_ICEPAHC_TEST, UD_210_ICELANDIC_ICEPAHC_TRAIN, UD_210_ICELANDIC_MODERN_DEV, UD_210_ICELANDIC_MODERN_TEST, UD_210_ICELANDIC_MODERN_TRAIN, UD_210_ICELANDIC_PUD_TEST, UD_210_INDONESIAN_CSUI_TEST, UD_210_INDONESIAN_CSUI_TRAIN, UD_210_INDONESIAN_GSD_DEV, UD_210_INDONESIAN_GSD_TEST, UD_210_INDONESIAN_GSD_TRAIN, UD_210_INDONESIAN_PUD_TEST, UD_210_IRISH_IDT_DEV, UD_210_IRISH_IDT_TEST, UD_210_IRISH_IDT_TRAIN, UD_210_IRISH_TWITTIRISH_TEST, UD_210_ITALIAN_ISDT_DEV, UD_210_ITALIAN_ISDT_TEST, UD_210_ITALIAN_ISDT_TRAIN, UD_210_ITALIAN_MARKIT_DEV, UD_210_ITALIAN_MARKIT_TEST, UD_210_ITALIAN_MARKIT_TRAIN, UD_210_ITALIAN_PARTUT_DEV, UD_210_ITALIAN_PARTUT_TEST, UD_210_ITALIAN_PARTUT_TRAIN, UD_210_ITALIAN_POSTWITA_DEV, UD_210_ITALIAN_POSTWITA_TEST, UD_210_ITALIAN_POSTWITA_TRAIN, UD_210_ITALIAN_PUD_TEST, UD_210_ITALIAN_TWITTIRO_DEV, UD_210_ITALIAN_TWITTIRO_TEST, UD_210_ITALIAN_TWITTIRO_TRAIN, UD_210_ITALIAN_VALICO_TEST, UD_210_ITALIAN_VIT_DEV, UD_210_ITALIAN_VIT_TEST, UD_210_ITALIAN_VIT_TRAIN, UD_210_JAPANESE_BCCWJLUW_DEV, UD_210_JAPANESE_BCCWJLUW_TEST, UD_210_JAPANESE_BCCWJLUW_TRAIN, UD_210_JAPANESE_BCCWJ_DEV, UD_210_JAPANESE_BCCWJ_TEST, UD_210_JAPANESE_BCCWJ_TRAIN, UD_210_JAPANESE_GSDLUW_DEV, UD_210_JAPANESE_GSDLUW_TEST, UD_210_JAPANESE_GSDLUW_TRAIN, UD_210_JAPANESE_GSD_DEV, UD_210_JAPANESE_GSD_TEST, UD_210_JAPANESE_GSD_TRAIN, UD_210_JAPANESE_MODERN_TEST, UD_210_JAPANESE_PUDLUW_TEST, UD_210_JAPANESE_PUD_TEST, UD_210_JAVANESE_CSUI_TEST, UD_210_KAAPOR_TUDET_TEST, UD_210_KANGRI_KDTB_TEST, UD_210_KARELIAN_KKPP_TEST, UD_210_KARO_TUDET_TEST, UD_210_KAZAKH_KTB_TEST, UD_210_KAZAKH_KTB_TRAIN, UD_210_KHUNSARI_AHA_TEST, UD_210_KICHE_IU_TEST, UD_210_KOMI_PERMYAK_UH_TEST, UD_210_KOMI_ZYRIAN_IKDP_TEST, UD_210_KOMI_ZYRIAN_LATTICE_TEST, UD_210_KOREAN_GSD_DEV, UD_210_KOREAN_GSD_TEST, UD_210_KOREAN_GSD_TRAIN, UD_210_KOREAN_KAIST_DEV, UD_210_KOREAN_KAIST_TEST, UD_210_KOREAN_KAIST_TRAIN, UD_210_KOREAN_PUD_TEST, UD_210_KURMANJI_MG_TEST, UD_210_KURMANJI_MG_TRAIN, UD_210_LATIN_ITTB_DEV, UD_210_LATIN_ITTB_TEST, UD_210_LATIN_ITTB_TRAIN, UD_210_LATIN_LLCT_DEV, UD_210_LATIN_LLCT_TEST, UD_210_LATIN_LLCT_TRAIN, UD_210_LATIN_PERSEUS_TEST, UD_210_LATIN_PERSEUS_TRAIN, UD_210_LATIN_PROIEL_DEV, UD_210_LATIN_PROIEL_TEST, UD_210_LATIN_PROIEL_TRAIN, UD_210_LATIN_UDANTE_DEV, UD_210_LATIN_UDANTE_TEST, UD_210_LATIN_UDANTE_TRAIN, UD_210_LATVIAN_LVTB_DEV, UD_210_LATVIAN_LVTB_TEST, UD_210_LATVIAN_LVTB_TRAIN, UD_210_LIGURIAN_GLT_TEST, UD_210_LIGURIAN_GLT_TRAIN, UD_210_LITHUANIAN_ALKSNIS_DEV, UD_210_LITHUANIAN_ALKSNIS_TEST, UD_210_LITHUANIAN_ALKSNIS_TRAIN, UD_210_LITHUANIAN_HSE_DEV, UD_210_LITHUANIAN_HSE_TEST, UD_210_LITHUANIAN_HSE_TRAIN, UD_210_LIVVI_KKPP_TEST, UD_210_LIVVI_KKPP_TRAIN, UD_210_LOW_SAXON_LSDC_TEST, UD_210_MADI_JARAWARA_TEST, UD_210_MAKURAP_TUDET_TEST, UD_210_MALTESE_MUDT_DEV, UD_210_MALTESE_MUDT_TEST, UD_210_MALTESE_MUDT_TRAIN, UD_210_MANX_CADHAN_TEST, UD_210_MARATHI_UFAL_DEV, UD_210_MARATHI_UFAL_TEST, UD_210_MARATHI_UFAL_TRAIN, UD_210_MBYA_GUARANI_DOOLEY_TEST, UD_210_MBYA_GUARANI_THOMAS_TEST, UD_210_MOKSHA_JR_TEST, UD_210_MUNDURUKU_TUDET_TEST, UD_210_NAIJA_NSC_DEV, UD_210_NAIJA_NSC_TEST, UD_210_NAIJA_NSC_TRAIN, UD_210_NAYINI_AHA_TEST, UD_210_NEAPOLITAN_RB_TEST, UD_210_NORTH_SAMI_GIELLA_TEST, UD_210_NORTH_SAMI_GIELLA_TRAIN, UD_210_NORWEGIAN_BOKMAAL_DEV, UD_210_NORWEGIAN_BOKMAAL_TEST, UD_210_NORWEGIAN_BOKMAAL_TRAIN, UD_210_NORWEGIAN_NYNORSKLIA_DEV, UD_210_NORWEGIAN_NYNORSKLIA_TEST, UD_210_NORWEGIAN_NYNORSKLIA_TRAIN, UD_210_NORWEGIAN_NYNORSK_DEV, UD_210_NORWEGIAN_NYNORSK_TEST, UD_210_NORWEGIAN_NYNORSK_TRAIN, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_210_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_DEV, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_TEST, UD_210_OLD_EAST_SLAVIC_BIRCHBARK_TRAIN, UD_210_OLD_EAST_SLAVIC_RNC_TEST, UD_210_OLD_EAST_SLAVIC_RNC_TRAIN, UD_210_OLD_EAST_SLAVIC_TOROT_DEV, UD_210_OLD_EAST_SLAVIC_TOROT_TEST, UD_210_OLD_EAST_SLAVIC_TOROT_TRAIN, UD_210_OLD_FRENCH_SRCMF_DEV, UD_210_OLD_FRENCH_SRCMF_TEST, UD_210_OLD_FRENCH_SRCMF_TRAIN, UD_210_OLD_TURKISH_TONQQ_TEST, UD_210_PERSIAN_PERDT_DEV, UD_210_PERSIAN_PERDT_TEST, UD_210_PERSIAN_PERDT_TRAIN, UD_210_PERSIAN_SERAJI_DEV, UD_210_PERSIAN_SERAJI_TEST, UD_210_PERSIAN_SERAJI_TRAIN, UD_210_POLISH_LFG_DEV, UD_210_POLISH_LFG_TEST, UD_210_POLISH_LFG_TRAIN, UD_210_POLISH_PDB_DEV, UD_210_POLISH_PDB_TEST, UD_210_POLISH_PDB_TRAIN, UD_210_POLISH_PUD_TEST, UD_210_POMAK_PHILOTIS_DEV, UD_210_POMAK_PHILOTIS_TEST, UD_210_POMAK_PHILOTIS_TRAIN, UD_210_PORTUGUESE_BOSQUE_DEV, UD_210_PORTUGUESE_BOSQUE_TEST, UD_210_PORTUGUESE_BOSQUE_TRAIN, UD_210_PORTUGUESE_GSD_DEV, UD_210_PORTUGUESE_GSD_TEST, UD_210_PORTUGUESE_GSD_TRAIN, UD_210_PORTUGUESE_PUD_TEST, UD_210_ROMANIAN_ART_TEST, UD_210_ROMANIAN_NONSTANDARD_DEV, UD_210_ROMANIAN_NONSTANDARD_TEST, UD_210_ROMANIAN_NONSTANDARD_TRAIN, UD_210_ROMANIAN_RRT_DEV, UD_210_ROMANIAN_RRT_TEST, UD_210_ROMANIAN_RRT_TRAIN, UD_210_ROMANIAN_SIMONERO_DEV, UD_210_ROMANIAN_SIMONERO_TEST, UD_210_ROMANIAN_SIMONERO_TRAIN, UD_210_RUSSIAN_GSD_DEV, UD_210_RUSSIAN_GSD_TEST, UD_210_RUSSIAN_GSD_TRAIN, UD_210_RUSSIAN_PUD_TEST, UD_210_RUSSIAN_SYNTAGRUS_DEV, UD_210_RUSSIAN_SYNTAGRUS_TEST, UD_210_RUSSIAN_SYNTAGRUS_TRAIN, UD_210_RUSSIAN_TAIGA_DEV, UD_210_RUSSIAN_TAIGA_TEST, UD_210_RUSSIAN_TAIGA_TRAIN, UD_210_SANSKRIT_UFAL_TEST, UD_210_SANSKRIT_VEDIC_TEST, UD_210_SANSKRIT_VEDIC_TRAIN, UD_210_SCOTTISH_GAELIC_ARCOSG_DEV, UD_210_SCOTTISH_GAELIC_ARCOSG_TEST, UD_210_SCOTTISH_GAELIC_ARCOSG_TRAIN, UD_210_SERBIAN_SET_DEV, UD_210_SERBIAN_SET_TEST, UD_210_SERBIAN_SET_TRAIN, UD_210_SKOLT_SAMI_GIELLAGAS_TEST, UD_210_SLOVAK_SNK_DEV, UD_210_SLOVAK_SNK_TEST, UD_210_SLOVAK_SNK_TRAIN, UD_210_SLOVENIAN_SSJ_DEV, UD_210_SLOVENIAN_SSJ_TEST, UD_210_SLOVENIAN_SSJ_TRAIN, UD_210_SLOVENIAN_SST_TEST, UD_210_SLOVENIAN_SST_TRAIN, UD_210_SOI_AHA_TEST, UD_210_SOUTH_LEVANTINE_ARABIC_MADAR_TEST, UD_210_SPANISH_ANCORA_DEV, UD_210_SPANISH_ANCORA_TEST, UD_210_SPANISH_ANCORA_TRAIN, UD_210_SPANISH_GSD_DEV, UD_210_SPANISH_GSD_TEST, UD_210_SPANISH_GSD_TRAIN, UD_210_SPANISH_PUD_TEST, UD_210_SWEDISH_LINES_DEV, UD_210_SWEDISH_LINES_TEST, UD_210_SWEDISH_LINES_TRAIN, UD_210_SWEDISH_PUD_TEST, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_210_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_210_SWEDISH_TALBANKEN_DEV, UD_210_SWEDISH_TALBANKEN_TEST, UD_210_SWEDISH_TALBANKEN_TRAIN, UD_210_SWISS_GERMAN_UZH_TEST, UD_210_TAGALOG_TRG_TEST, UD_210_TAGALOG_UGNAYAN_TEST, UD_210_TAMIL_MWTT_TEST, UD_210_TAMIL_TTB_DEV, UD_210_TAMIL_TTB_TEST, UD_210_TAMIL_TTB_TRAIN, UD_210_TATAR_NMCTT_TEST, UD_210_TEKO_TUDET_TEST, UD_210_TELUGU_MTG_DEV, UD_210_TELUGU_MTG_TEST, UD_210_TELUGU_MTG_TRAIN, UD_210_THAI_PUD_TEST, UD_210_TUPINAMBA_TUDET_TEST, UD_210_TURKISH_ATIS_DEV, UD_210_TURKISH_ATIS_TEST, UD_210_TURKISH_ATIS_TRAIN, UD_210_TURKISH_BOUN_DEV, UD_210_TURKISH_BOUN_TEST, UD_210_TURKISH_BOUN_TRAIN, UD_210_TURKISH_FRAMENET_DEV, UD_210_TURKISH_FRAMENET_TEST, UD_210_TURKISH_FRAMENET_TRAIN, UD_210_TURKISH_GB_TEST, UD_210_TURKISH_GERMAN_SAGT_DEV, UD_210_TURKISH_GERMAN_SAGT_TEST, UD_210_TURKISH_GERMAN_SAGT_TRAIN, UD_210_TURKISH_IMST_DEV, UD_210_TURKISH_IMST_TEST, UD_210_TURKISH_IMST_TRAIN, UD_210_TURKISH_KENET_DEV, UD_210_TURKISH_KENET_TEST, UD_210_TURKISH_KENET_TRAIN, UD_210_TURKISH_PENN_DEV, UD_210_TURKISH_PENN_TEST, UD_210_TURKISH_PENN_TRAIN, UD_210_TURKISH_PUD_TEST, UD_210_TURKISH_TOURISM_DEV, UD_210_TURKISH_TOURISM_TEST, UD_210_TURKISH_TOURISM_TRAIN, UD_210_UKRAINIAN_IU_DEV, UD_210_UKRAINIAN_IU_TEST, UD_210_UKRAINIAN_IU_TRAIN, UD_210_UMBRIAN_IKUVINA_TEST, UD_210_UPPER_SORBIAN_UFAL_TEST, UD_210_UPPER_SORBIAN_UFAL_TRAIN, UD_210_URDU_UDTB_DEV, UD_210_URDU_UDTB_TEST, UD_210_URDU_UDTB_TRAIN, UD_210_UYGHUR_UDT_DEV, UD_210_UYGHUR_UDT_TEST, UD_210_UYGHUR_UDT_TRAIN, UD_210_VIETNAMESE_VTB_DEV, UD_210_VIETNAMESE_VTB_TEST, UD_210_VIETNAMESE_VTB_TRAIN, UD_210_WARLPIRI_UFAL_TEST, UD_210_WELSH_CCG_DEV, UD_210_WELSH_CCG_TEST, UD_210_WELSH_CCG_TRAIN, UD_210_WESTERN_ARMENIAN_ARMTDP_DEV, UD_210_WESTERN_ARMENIAN_ARMTDP_TEST, UD_210_WESTERN_ARMENIAN_ARMTDP_TRAIN, UD_210_WOLOF_WTB_DEV, UD_210_WOLOF_WTB_TEST, UD_210_WOLOF_WTB_TRAIN, UD_210_XIBE_XDT_TEST, UD_210_YAKUT_YKTDT_TEST, UD_210_YORUBA_YTB_TEST, UD_210_YUPIK_SLI_TEST, _UD_210_HOME, _UD_210_URL, _path, basename, name, out, path, prefix, sp, split",,",  ,  + "",  = ,  set of , "", ""
, #ud-treebanks-v2.10/, -, .""
, .conllu, .zip, /, /*, /ud-treebanks-v2.10/UD_*, UD_, UD_210 dev set of AFRIKAANS_AFRIBOOMS., UD_210 dev set of ANCIENT_GREEK_PERSEUS., UD_210 dev set of ANCIENT_GREEK_PROIEL., UD_210 dev set of ANCIENT_HEBREW_PTNK., UD_210 dev set of ARABIC_NYUAD., UD_210 dev set of ARABIC_PADT., UD_210 dev set of ARMENIAN_ARMTDP., UD_210 dev set of ARMENIAN_BSUT., UD_210 dev set of BASQUE_BDT., UD_210 dev set of BELARUSIAN_HSE., UD_210 dev set of BULGARIAN_BTB., UD_210 dev set of CATALAN_ANCORA., UD_210 dev set of CHINESE_GSD., UD_210 dev set of CHINESE_GSDSIMP., UD_210 dev set of CLASSICAL_CHINESE_KYOTO., UD_210 dev set of COPTIC_SCRIPTORIUM., UD_210 dev set of CROATIAN_SET., UD_210 dev set of CZECH_CAC., UD_210 dev set of CZECH_CLTT., UD_210 dev set of CZECH_FICTREE., UD_210 dev set of CZECH_PDT., UD_210 dev set of DANISH_DDT., UD_210 dev set of DUTCH_ALPINO., UD_210 dev set of DUTCH_LASSYSMALL., UD_210 dev set of ENGLISH_ATIS., UD_210 dev set of ENGLISH_ESL., UD_210 dev set of ENGLISH_EWT., UD_210 dev set of ENGLISH_GUM., UD_210 dev set of ENGLISH_GUMREDDIT., UD_210 dev set of ENGLISH_LINES., UD_210 dev set of ENGLISH_PARTUT., UD_210 dev set of ESTONIAN_EDT., UD_210 dev set of ESTONIAN_EWT., UD_210 dev set of FAROESE_FARPAHC., UD_210 dev set of FINNISH_FTB., UD_210 dev set of FINNISH_TDT., UD_210 dev set of FRENCH_FTB., UD_210 dev set of FRENCH_GSD., UD_210 dev set of FRENCH_PARTUT., UD_210 dev set of FRENCH_RHAPSODIE., UD_210 dev set of FRENCH_SEQUOIA., UD_210 dev set of GALICIAN_CTG., UD_210 dev set of GERMAN_GSD., UD_210 dev set of GERMAN_HDT., UD_210 dev set of GOTHIC_PROIEL., UD_210 dev set of GREEK_GDT., UD_210 dev set of HEBREW_HTB., UD_210 dev set of HEBREW_IAHLTWIKI., UD_210 dev set of HINDI_ENGLISH_HIENCS., UD_210 dev set of HINDI_HDTB., UD_210 dev set of HUNGARIAN_SZEGED., UD_210 dev set of ICELANDIC_ICEPAHC., UD_210 dev set of ICELANDIC_MODERN., UD_210 dev set of INDONESIAN_GSD., UD_210 dev set of IRISH_IDT., UD_210 dev set of ITALIAN_ISDT., UD_210 dev set of ITALIAN_MARKIT., UD_210 dev set of ITALIAN_PARTUT., UD_210 dev set of ITALIAN_POSTWITA., UD_210 dev set of ITALIAN_TWITTIRO., UD_210 dev set of ITALIAN_VIT., UD_210 dev set of JAPANESE_BCCWJ., UD_210 dev set of JAPANESE_BCCWJLUW., UD_210 dev set of JAPANESE_GSD., UD_210 dev set of JAPANESE_GSDLUW., UD_210 dev set of KOREAN_GSD., UD_210 dev set of KOREAN_KAIST., UD_210 dev set of LATIN_ITTB., UD_210 dev set of LATIN_LLCT., UD_210 dev set of LATIN_PROIEL., UD_210 dev set of LATIN_UDANTE., UD_210 dev set of LATVIAN_LVTB., UD_210 dev set of LITHUANIAN_ALKSNIS., UD_210 dev set of LITHUANIAN_HSE., UD_210 dev set of MALTESE_MUDT., UD_210 dev set of MARATHI_UFAL., UD_210 dev set of NAIJA_NSC., UD_210 dev set of NORWEGIAN_BOKMAAL., UD_210 dev set of NORWEGIAN_NYNORSK., UD_210 dev set of NORWEGIAN_NYNORSKLIA., UD_210 dev set of OLD_CHURCH_SLAVONIC_PROIEL., UD_210 dev set of OLD_EAST_SLAVIC_BIRCHBARK., UD_210 dev set of OLD_EAST_SLAVIC_TOROT., UD_210 dev set of OLD_FRENCH_SRCMF., UD_210 dev set of PERSIAN_PERDT., UD_210 dev set of PERSIAN_SERAJI., UD_210 dev set of POLISH_LFG., UD_210 dev set of POLISH_PDB., UD_210 dev set of POMAK_PHILOTIS., UD_210 dev set of PORTUGUESE_BOSQUE., UD_210 dev set of PORTUGUESE_GSD., UD_210 dev set of ROMANIAN_NONSTANDARD., UD_210 dev set of ROMANIAN_RRT., UD_210 dev set of ROMANIAN_SIMONERO., UD_210 dev set of RUSSIAN_GSD., UD_210 dev set of RUSSIAN_SYNTAGRUS., UD_210 dev set of RUSSIAN_TAIGA., UD_210 dev set of SCOTTISH_GAELIC_ARCOSG., UD_210 dev set of SERBIAN_SET., UD_210 dev set of SLOVAK_SNK., UD_210 dev set of SLOVENIAN_SSJ., UD_210 dev set of SPANISH_ANCORA., UD_210 dev set of SPANISH_GSD., UD_210 dev set of SWEDISH_LINES., UD_210 dev set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_210 dev set of SWEDISH_TALBANKEN., UD_210 dev set of TAMIL_TTB., UD_210 dev set of TELUGU_MTG., UD_210 dev set of TURKISH_ATIS., UD_210 dev set of TURKISH_BOUN., UD_210 dev set of TURKISH_FRAMENET., UD_210 dev set of TURKISH_GERMAN_SAGT., UD_210 dev set of TURKISH_IMST., UD_210 dev set of TURKISH_KENET., UD_210 dev set of TURKISH_PENN., UD_210 dev set of TURKISH_TOURISM., UD_210 dev set of UKRAINIAN_IU., UD_210 dev set of URDU_UDTB., UD_210 dev set of UYGHUR_UDT., UD_210 dev set of VIETNAMESE_VTB., UD_210 dev set of WELSH_CCG., UD_210 dev set of WESTERN_ARMENIAN_ARMTDP., UD_210 dev set of WOLOF_WTB., UD_210 test set of AFRIKAANS_AFRIBOOMS., UD_210 test set of AKKADIAN_PISANDUB., UD_210 test set of AKKADIAN_RIAO., UD_210 test set of AKUNTSU_TUDET., UD_210 test set of ALBANIAN_TSA., UD_210 test set of AMHARIC_ATT., UD_210 test set of ANCIENT_GREEK_PERSEUS., UD_210 test set of ANCIENT_GREEK_PROIEL., UD_210 test set of ANCIENT_HEBREW_PTNK., UD_210 test set of APURINA_UFPA., UD_210 test set of ARABIC_NYUAD., UD_210 test set of ARABIC_PADT., UD_210 test set of ARABIC_PUD., UD_210 test set of ARMENIAN_ARMTDP., UD_210 test set of ARMENIAN_BSUT., UD_210 test set of ASSYRIAN_AS., UD_210 test set of BAMBARA_CRB., UD_210 test set of BASQUE_BDT., UD_210 test set of BEJA_NSC., UD_210 test set of BELARUSIAN_HSE., UD_210 test set of BENGALI_BRU., UD_210 test set of BHOJPURI_BHTB., UD_210 test set of BRETON_KEB., UD_210 test set of BULGARIAN_BTB., UD_210 test set of BURYAT_BDT., UD_210 test set of CANTONESE_HK., UD_210 test set of CATALAN_ANCORA., UD_210 test set of CEBUANO_GJA., UD_210 test set of CHINESE_CFL., UD_210 test set of CHINESE_GSD., UD_210 test set of CHINESE_GSDSIMP., UD_210 test set of CHINESE_HK., UD_210 test set of CHINESE_PUD., UD_210 test set of CHUKCHI_HSE., UD_210 test set of CLASSICAL_CHINESE_KYOTO., UD_210 test set of COPTIC_SCRIPTORIUM., UD_210 test set of CROATIAN_SET., UD_210 test set of CZECH_CAC., UD_210 test set of CZECH_CLTT., UD_210 test set of CZECH_FICTREE., UD_210 test set of CZECH_PDT., UD_210 test set of CZECH_PUD., UD_210 test set of DANISH_DDT., UD_210 test set of DUTCH_ALPINO., UD_210 test set of DUTCH_LASSYSMALL., UD_210 test set of ENGLISH_ATIS., UD_210 test set of ENGLISH_ESL., UD_210 test set of ENGLISH_EWT., UD_210 test set of ENGLISH_GUM., UD_210 test set of ENGLISH_GUMREDDIT., UD_210 test set of ENGLISH_LINES., UD_210 test set of ENGLISH_PARTUT., UD_210 test set of ENGLISH_PRONOUNS., UD_210 test set of ENGLISH_PUD., UD_210 test set of ERZYA_JR., UD_210 test set of ESTONIAN_EDT., UD_210 test set of ESTONIAN_EWT., UD_210 test set of FAROESE_FARPAHC., UD_210 test set of FAROESE_OFT., UD_210 test set of FINNISH_FTB., UD_210 test set of FINNISH_OOD., UD_210 test set of FINNISH_PUD., UD_210 test set of FINNISH_TDT., UD_210 test set of FRENCH_FQB., UD_210 test set of FRENCH_FTB., UD_210 test set of FRENCH_GSD., UD_210 test set of FRENCH_PARISSTORIES., UD_210 test set of FRENCH_PARTUT., UD_210 test set of FRENCH_PUD., UD_210 test set of FRENCH_RHAPSODIE., UD_210 test set of FRENCH_SEQUOIA., UD_210 test set of FRISIAN_DUTCH_FAME., UD_210 test set of GALICIAN_CTG., UD_210 test set of GALICIAN_TREEGAL., UD_210 test set of GERMAN_GSD., UD_210 test set of GERMAN_HDT., UD_210 test set of GERMAN_LIT., UD_210 test set of GERMAN_PUD., UD_210 test set of GOTHIC_PROIEL., UD_210 test set of GREEK_GDT., UD_210 test set of GUAJAJARA_TUDET., UD_210 test set of GUARANI_OLDTUDET., UD_210 test set of HEBREW_HTB., UD_210 test set of HEBREW_IAHLTWIKI., UD_210 test set of HINDI_ENGLISH_HIENCS., UD_210 test set of HINDI_HDTB., UD_210 test set of HINDI_PUD., UD_210 test set of HITTITE_HITTB., UD_210 test set of HUNGARIAN_SZEGED., UD_210 test set of ICELANDIC_ICEPAHC., UD_210 test set of ICELANDIC_MODERN., UD_210 test set of ICELANDIC_PUD., UD_210 test set of INDONESIAN_CSUI., UD_210 test set of INDONESIAN_GSD., UD_210 test set of INDONESIAN_PUD., UD_210 test set of IRISH_IDT., UD_210 test set of IRISH_TWITTIRISH., UD_210 test set of ITALIAN_ISDT., UD_210 test set of ITALIAN_MARKIT., UD_210 test set of ITALIAN_PARTUT., UD_210 test set of ITALIAN_POSTWITA., UD_210 test set of ITALIAN_PUD., UD_210 test set of ITALIAN_TWITTIRO., UD_210 test set of ITALIAN_VALICO., UD_210 test set of ITALIAN_VIT., UD_210 test set of JAPANESE_BCCWJ., UD_210 test set of JAPANESE_BCCWJLUW., UD_210 test set of JAPANESE_GSD., UD_210 test set of JAPANESE_GSDLUW., UD_210 test set of JAPANESE_MODERN., UD_210 test set of JAPANESE_PUD., UD_210 test set of JAPANESE_PUDLUW., UD_210 test set of JAVANESE_CSUI., UD_210 test set of KAAPOR_TUDET., UD_210 test set of KANGRI_KDTB., UD_210 test set of KARELIAN_KKPP., UD_210 test set of KARO_TUDET., UD_210 test set of KAZAKH_KTB., UD_210 test set of KHUNSARI_AHA., UD_210 test set of KICHE_IU., UD_210 test set of KOMI_PERMYAK_UH., UD_210 test set of KOMI_ZYRIAN_IKDP., UD_210 test set of KOMI_ZYRIAN_LATTICE., UD_210 test set of KOREAN_GSD., UD_210 test set of KOREAN_KAIST., UD_210 test set of KOREAN_PUD., UD_210 test set of KURMANJI_MG., UD_210 test set of LATIN_ITTB., UD_210 test set of LATIN_LLCT., UD_210 test set of LATIN_PERSEUS., UD_210 test set of LATIN_PROIEL., UD_210 test set of LATIN_UDANTE., UD_210 test set of LATVIAN_LVTB., UD_210 test set of LIGURIAN_GLT., UD_210 test set of LITHUANIAN_ALKSNIS., UD_210 test set of LITHUANIAN_HSE., UD_210 test set of LIVVI_KKPP., UD_210 test set of LOW_SAXON_LSDC., UD_210 test set of MADI_JARAWARA., UD_210 test set of MAKURAP_TUDET., UD_210 test set of MALTESE_MUDT., UD_210 test set of MANX_CADHAN., UD_210 test set of MARATHI_UFAL., UD_210 test set of MBYA_GUARANI_DOOLEY., UD_210 test set of MBYA_GUARANI_THOMAS., UD_210 test set of MOKSHA_JR., UD_210 test set of MUNDURUKU_TUDET., UD_210 test set of NAIJA_NSC., UD_210 test set of NAYINI_AHA., UD_210 test set of NEAPOLITAN_RB., UD_210 test set of NORTH_SAMI_GIELLA., UD_210 test set of NORWEGIAN_BOKMAAL., UD_210 test set of NORWEGIAN_NYNORSK., UD_210 test set of NORWEGIAN_NYNORSKLIA., UD_210 test set of OLD_CHURCH_SLAVONIC_PROIEL., UD_210 test set of OLD_EAST_SLAVIC_BIRCHBARK., UD_210 test set of OLD_EAST_SLAVIC_RNC., UD_210 test set of OLD_EAST_SLAVIC_TOROT., UD_210 test set of OLD_FRENCH_SRCMF., UD_210 test set of OLD_TURKISH_TONQQ., UD_210 test set of PERSIAN_PERDT., UD_210 test set of PERSIAN_SERAJI., UD_210 test set of POLISH_LFG., UD_210 test set of POLISH_PDB., UD_210 test set of POLISH_PUD., UD_210 test set of POMAK_PHILOTIS., UD_210 test set of PORTUGUESE_BOSQUE., UD_210 test set of PORTUGUESE_GSD., UD_210 test set of PORTUGUESE_PUD., UD_210 test set of ROMANIAN_ART., UD_210 test set of ROMANIAN_NONSTANDARD., UD_210 test set of ROMANIAN_RRT., UD_210 test set of ROMANIAN_SIMONERO., UD_210 test set of RUSSIAN_GSD., UD_210 test set of RUSSIAN_PUD., UD_210 test set of RUSSIAN_SYNTAGRUS., UD_210 test set of RUSSIAN_TAIGA., UD_210 test set of SANSKRIT_UFAL., UD_210 test set of SANSKRIT_VEDIC., UD_210 test set of SCOTTISH_GAELIC_ARCOSG., UD_210 test set of SERBIAN_SET., UD_210 test set of SKOLT_SAMI_GIELLAGAS., UD_210 test set of SLOVAK_SNK., UD_210 test set of SLOVENIAN_SSJ., UD_210 test set of SLOVENIAN_SST., UD_210 test set of SOI_AHA., UD_210 test set of SOUTH_LEVANTINE_ARABIC_MADAR., UD_210 test set of SPANISH_ANCORA., UD_210 test set of SPANISH_GSD., UD_210 test set of SPANISH_PUD., UD_210 test set of SWEDISH_LINES., UD_210 test set of SWEDISH_PUD., UD_210 test set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_210 test set of SWEDISH_TALBANKEN., UD_210 test set of SWISS_GERMAN_UZH., UD_210 test set of TAGALOG_TRG., UD_210 test set of TAGALOG_UGNAYAN., UD_210 test set of TAMIL_MWTT., UD_210 test set of TAMIL_TTB., UD_210 test set of TATAR_NMCTT., UD_210 test set of TEKO_TUDET., UD_210 test set of TELUGU_MTG., UD_210 test set of THAI_PUD., UD_210 test set of TUPINAMBA_TUDET., UD_210 test set of TURKISH_ATIS., UD_210 test set of TURKISH_BOUN., UD_210 test set of TURKISH_FRAMENET., UD_210 test set of TURKISH_GB., UD_210 test set of TURKISH_GERMAN_SAGT., UD_210 test set of TURKISH_IMST., UD_210 test set of TURKISH_KENET., UD_210 test set of TURKISH_PENN., UD_210 test set of TURKISH_PUD., UD_210 test set of TURKISH_TOURISM., UD_210 test set of UKRAINIAN_IU., UD_210 test set of UMBRIAN_IKUVINA., UD_210 test set of UPPER_SORBIAN_UFAL., UD_210 test set of URDU_UDTB., UD_210 test set of UYGHUR_UDT., UD_210 test set of VIETNAMESE_VTB., UD_210 test set of WARLPIRI_UFAL., UD_210 test set of WELSH_CCG., UD_210 test set of WESTERN_ARMENIAN_ARMTDP., UD_210 test set of WOLOF_WTB., UD_210 test set of XIBE_XDT., UD_210 test set of YAKUT_YKTDT., UD_210 test set of YORUBA_YTB., UD_210 test set of YUPIK_SLI., UD_210 train set of AFRIKAANS_AFRIBOOMS., UD_210 train set of ANCIENT_GREEK_PERSEUS., UD_210 train set of ANCIENT_GREEK_PROIEL., UD_210 train set of ANCIENT_HEBREW_PTNK., UD_210 train set of ARABIC_NYUAD., UD_210 train set of ARABIC_PADT., UD_210 train set of ARMENIAN_ARMTDP., UD_210 train set of ARMENIAN_BSUT., UD_210 train set of BASQUE_BDT., UD_210 train set of BELARUSIAN_HSE., UD_210 train set of BULGARIAN_BTB., UD_210 train set of BURYAT_BDT., UD_210 train set of CATALAN_ANCORA., UD_210 train set of CHINESE_GSD., UD_210 train set of CHINESE_GSDSIMP., UD_210 train set of CLASSICAL_CHINESE_KYOTO., UD_210 train set of COPTIC_SCRIPTORIUM., UD_210 train set of CROATIAN_SET., UD_210 train set of CZECH_CAC., UD_210 train set of CZECH_CLTT., UD_210 train set of CZECH_FICTREE., UD_210 train set of CZECH_PDT., UD_210 train set of DANISH_DDT., UD_210 train set of DUTCH_ALPINO., UD_210 train set of DUTCH_LASSYSMALL., UD_210 train set of ENGLISH_ATIS., UD_210 train set of ENGLISH_ESL., UD_210 train set of ENGLISH_EWT., UD_210 train set of ENGLISH_GUM., UD_210 train set of ENGLISH_GUMREDDIT., UD_210 train set of ENGLISH_LINES., UD_210 train set of ENGLISH_PARTUT., UD_210 train set of ESTONIAN_EDT., UD_210 train set of ESTONIAN_EWT., UD_210 train set of FAROESE_FARPAHC., UD_210 train set of FINNISH_FTB., UD_210 train set of FINNISH_TDT., UD_210 train set of FRENCH_FTB., UD_210 train set of FRENCH_GSD., UD_210 train set of FRENCH_PARISSTORIES., UD_210 train set of FRENCH_PARTUT., UD_210 train set of FRENCH_RHAPSODIE., UD_210 train set of FRENCH_SEQUOIA., UD_210 train set of GALICIAN_CTG., UD_210 train set of GALICIAN_TREEGAL., UD_210 train set of GERMAN_GSD., UD_210 train set of GERMAN_HDT., UD_210 train set of GOTHIC_PROIEL., UD_210 train set of GREEK_GDT., UD_210 train set of HEBREW_HTB., UD_210 train set of HEBREW_IAHLTWIKI., UD_210 train set of HINDI_ENGLISH_HIENCS., UD_210 train set of HINDI_HDTB., UD_210 train set of HUNGARIAN_SZEGED., UD_210 train set of ICELANDIC_ICEPAHC., UD_210 train set of ICELANDIC_MODERN., UD_210 train set of INDONESIAN_CSUI., UD_210 train set of INDONESIAN_GSD., UD_210 train set of IRISH_IDT., UD_210 train set of ITALIAN_ISDT., UD_210 train set of ITALIAN_MARKIT., UD_210 train set of ITALIAN_PARTUT., UD_210 train set of ITALIAN_POSTWITA., UD_210 train set of ITALIAN_TWITTIRO., UD_210 train set of ITALIAN_VIT., UD_210 train set of JAPANESE_BCCWJ., UD_210 train set of JAPANESE_BCCWJLUW., UD_210 train set of JAPANESE_GSD., UD_210 train set of JAPANESE_GSDLUW., UD_210 train set of KAZAKH_KTB., UD_210 train set of KOREAN_GSD., UD_210 train set of KOREAN_KAIST., UD_210 train set of KURMANJI_MG., UD_210 train set of LATIN_ITTB., UD_210 train set of LATIN_LLCT., UD_210 train set of LATIN_PERSEUS., UD_210 train set of LATIN_PROIEL., UD_210 train set of LATIN_UDANTE., UD_210 train set of LATVIAN_LVTB., UD_210 train set of LIGURIAN_GLT., UD_210 train set of LITHUANIAN_ALKSNIS., UD_210 train set of LITHUANIAN_HSE., UD_210 train set of LIVVI_KKPP., UD_210 train set of MALTESE_MUDT., UD_210 train set of MARATHI_UFAL., UD_210 train set of NAIJA_NSC., UD_210 train set of NORTH_SAMI_GIELLA., UD_210 train set of NORWEGIAN_BOKMAAL., UD_210 train set of NORWEGIAN_NYNORSK., UD_210 train set of NORWEGIAN_NYNORSKLIA., UD_210 train set of OLD_CHURCH_SLAVONIC_PROIEL., UD_210 train set of OLD_EAST_SLAVIC_BIRCHBARK., UD_210 train set of OLD_EAST_SLAVIC_RNC., UD_210 train set of OLD_EAST_SLAVIC_TOROT., UD_210 train set of OLD_FRENCH_SRCMF., UD_210 train set of PERSIAN_PERDT., UD_210 train set of PERSIAN_SERAJI., UD_210 train set of POLISH_LFG., UD_210 train set of POLISH_PDB., UD_210 train set of POMAK_PHILOTIS., UD_210 train set of PORTUGUESE_BOSQUE., UD_210 train set of PORTUGUESE_GSD., UD_210 train set of ROMANIAN_NONSTANDARD., UD_210 train set of ROMANIAN_RRT., UD_210 train set of ROMANIAN_SIMONERO., UD_210 train set of RUSSIAN_GSD., UD_210 train set of RUSSIAN_SYNTAGRUS., UD_210 train set of RUSSIAN_TAIGA., UD_210 train set of SANSKRIT_VEDIC., UD_210 train set of SCOTTISH_GAELIC_ARCOSG., UD_210 train set of SERBIAN_SET., UD_210 train set of SLOVAK_SNK., UD_210 train set of SLOVENIAN_SSJ., UD_210 train set of SLOVENIAN_SST., UD_210 train set of SPANISH_ANCORA., UD_210 train set of SPANISH_GSD., UD_210 train set of SWEDISH_LINES., UD_210 train set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_210 train set of SWEDISH_TALBANKEN., UD_210 train set of TAMIL_TTB., UD_210 train set of TELUGU_MTG., UD_210 train set of TURKISH_ATIS., UD_210 train set of TURKISH_BOUN., UD_210 train set of TURKISH_FRAMENET., UD_210 train set of TURKISH_GERMAN_SAGT., UD_210 train set of TURKISH_IMST., UD_210 train set of TURKISH_KENET., UD_210 train set of TURKISH_PENN., UD_210 train set of TURKISH_TOURISM., UD_210 train set of UKRAINIAN_IU., UD_210 train set of UPPER_SORBIAN_UFAL., UD_210 train set of URDU_UDTB., UD_210 train set of UYGHUR_UDT., UD_210 train set of VIETNAMESE_VTB., UD_210 train set of WELSH_CCG., UD_210 train set of WESTERN_ARMENIAN_ARMTDP., UD_210 train set of WOLOF_WTB., UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu, UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu, UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu, UD_Akkadian-PISANDUB/akk_pisandub-ud-test.conllu, UD_Akkadian-RIAO/akk_riao-ud-test.conllu, UD_Akuntsu-TuDeT/aqz_tudet-ud-test.conllu, UD_Albanian-TSA/sq_tsa-ud-test.conllu, UD_Amharic-ATT/am_att-ud-test.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-dev.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-test.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-train.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-dev.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-test.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-train.conllu, UD_Ancient_Hebrew-PTNK/hbo_ptnk-ud-dev.conllu, UD_Ancient_Hebrew-PTNK/hbo_ptnk-ud-test.conllu, UD_Ancient_Hebrew-PTNK/hbo_ptnk-ud-train.conllu, UD_Apurina-UFPA/apu_ufpa-ud-test.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-dev.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-test.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu, UD_Arabic-PADT/ar_padt-ud-dev.conllu, UD_Arabic-PADT/ar_padt-ud-test.conllu, UD_Arabic-PADT/ar_padt-ud-train.conllu, UD_Arabic-PUD/ar_pud-ud-test.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-dev.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-test.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-train.conllu, UD_Armenian-BSUT/hy_bsut-ud-dev.conllu, UD_Armenian-BSUT/hy_bsut-ud-test.conllu, UD_Armenian-BSUT/hy_bsut-ud-train.conllu, UD_Assyrian-AS/aii_as-ud-test.conllu, UD_Bambara-CRB/bm_crb-ud-test.conllu, UD_Basque-BDT/eu_bdt-ud-dev.conllu, UD_Basque-BDT/eu_bdt-ud-test.conllu, UD_Basque-BDT/eu_bdt-ud-train.conllu, UD_Beja-NSC/bej_nsc-ud-test.conllu, UD_Belarusian-HSE/be_hse-ud-dev.conllu, UD_Belarusian-HSE/be_hse-ud-test.conllu, UD_Belarusian-HSE/be_hse-ud-train.conllu, UD_Bengali-BRU/bn_bru-ud-test.conllu, UD_Bhojpuri-BHTB/bho_bhtb-ud-test.conllu, UD_Breton-KEB/br_keb-ud-test.conllu, UD_Bulgarian-BTB/bg_btb-ud-dev.conllu, UD_Bulgarian-BTB/bg_btb-ud-test.conllu, UD_Bulgarian-BTB/bg_btb-ud-train.conllu, UD_Buryat-BDT/bxr_bdt-ud-test.conllu, UD_Buryat-BDT/bxr_bdt-ud-train.conllu, UD_Cantonese-HK/yue_hk-ud-test.conllu, UD_Catalan-AnCora/ca_ancora-ud-dev.conllu, UD_Catalan-AnCora/ca_ancora-ud-test.conllu, UD_Catalan-AnCora/ca_ancora-ud-train.conllu, UD_Cebuano-GJA/ceb_gja-ud-test.conllu, UD_Chinese-CFL/zh_cfl-ud-test.conllu, UD_Chinese-GSD/zh_gsd-ud-dev.conllu, UD_Chinese-GSD/zh_gsd-ud-test.conllu, UD_Chinese-GSD/zh_gsd-ud-train.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-dev.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-test.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu, UD_Chinese-HK/zh_hk-ud-test.conllu, UD_Chinese-PUD/zh_pud-ud-test.conllu, UD_Chukchi-HSE/ckt_hse-ud-test.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-dev.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-test.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-train.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-dev.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-test.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-train.conllu, UD_Croatian-SET/hr_set-ud-dev.conllu, UD_Croatian-SET/hr_set-ud-test.conllu, UD_Croatian-SET/hr_set-ud-train.conllu, UD_Czech-CAC/cs_cac-ud-dev.conllu, UD_Czech-CAC/cs_cac-ud-test.conllu, UD_Czech-CAC/cs_cac-ud-train.conllu, UD_Czech-CLTT/cs_cltt-ud-dev.conllu, UD_Czech-CLTT/cs_cltt-ud-test.conllu, UD_Czech-CLTT/cs_cltt-ud-train.conllu, UD_Czech-FicTree/cs_fictree-ud-dev.conllu, UD_Czech-FicTree/cs_fictree-ud-test.conllu, UD_Czech-FicTree/cs_fictree-ud-train.conllu, UD_Czech-PDT/cs_pdt-ud-dev.conllu, UD_Czech-PDT/cs_pdt-ud-test.conllu, UD_Czech-PDT/cs_pdt-ud-train.conllu, UD_Czech-PUD/cs_pud-ud-test.conllu, UD_Danish-DDT/da_ddt-ud-dev.conllu, UD_Danish-DDT/da_ddt-ud-test.conllu, UD_Danish-DDT/da_ddt-ud-train.conllu, UD_Dutch-Alpino/nl_alpino-ud-dev.conllu, UD_Dutch-Alpino/nl_alpino-ud-test.conllu, UD_Dutch-Alpino/nl_alpino-ud-train.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-dev.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-test.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-train.conllu, UD_English-Atis/en_atis-ud-dev.conllu, UD_English-Atis/en_atis-ud-test.conllu, UD_English-Atis/en_atis-ud-train.conllu, UD_English-ESL/en_esl-ud-dev.conllu, UD_English-ESL/en_esl-ud-test.conllu, UD_English-ESL/en_esl-ud-train.conllu, UD_English-EWT/en_ewt-ud-dev.conllu, UD_English-EWT/en_ewt-ud-test.conllu, UD_English-EWT/en_ewt-ud-train.conllu, UD_English-GUM/en_gum-ud-dev.conllu, UD_English-GUM/en_gum-ud-test.conllu, UD_English-GUM/en_gum-ud-train.conllu, UD_English-GUMReddit/en_gumreddit-ud-dev.conllu, UD_English-GUMReddit/en_gumreddit-ud-test.conllu, UD_English-GUMReddit/en_gumreddit-ud-train.conllu, UD_English-LinES/en_lines-ud-dev.conllu, UD_English-LinES/en_lines-ud-test.conllu, UD_English-LinES/en_lines-ud-train.conllu, UD_English-PUD/en_pud-ud-test.conllu, UD_English-ParTUT/en_partut-ud-dev.conllu, UD_English-ParTUT/en_partut-ud-test.conllu, UD_English-ParTUT/en_partut-ud-train.conllu, UD_English-Pronouns/en_pronouns-ud-test.conllu, UD_Erzya-JR/myv_jr-ud-test.conllu, UD_Estonian-EDT/et_edt-ud-dev.conllu, UD_Estonian-EDT/et_edt-ud-test.conllu, UD_Estonian-EDT/et_edt-ud-train.conllu, UD_Estonian-EWT/et_ewt-ud-dev.conllu, UD_Estonian-EWT/et_ewt-ud-test.conllu, UD_Estonian-EWT/et_ewt-ud-train.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-dev.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-test.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-train.conllu, UD_Faroese-OFT/fo_oft-ud-test.conllu, UD_Finnish-FTB/fi_ftb-ud-dev.conllu, UD_Finnish-FTB/fi_ftb-ud-test.conllu, UD_Finnish-FTB/fi_ftb-ud-train.conllu, UD_Finnish-OOD/fi_ood-ud-test.conllu, UD_Finnish-PUD/fi_pud-ud-test.conllu, UD_Finnish-TDT/fi_tdt-ud-dev.conllu, UD_Finnish-TDT/fi_tdt-ud-test.conllu, UD_Finnish-TDT/fi_tdt-ud-train.conllu, UD_French-FQB/fr_fqb-ud-test.conllu, UD_French-FTB/fr_ftb-ud-dev.conllu, UD_French-FTB/fr_ftb-ud-test.conllu, UD_French-FTB/fr_ftb-ud-train.conllu, UD_French-GSD/fr_gsd-ud-dev.conllu, UD_French-GSD/fr_gsd-ud-test.conllu, UD_French-GSD/fr_gsd-ud-train.conllu, UD_French-PUD/fr_pud-ud-test.conllu, UD_French-ParTUT/fr_partut-ud-dev.conllu, UD_French-ParTUT/fr_partut-ud-test.conllu, UD_French-ParTUT/fr_partut-ud-train.conllu, UD_French-ParisStories/fr_parisstories-ud-test.conllu, UD_French-ParisStories/fr_parisstories-ud-train.conllu, UD_French-Rhapsodie/fr_rhapsodie-ud-dev.conllu, UD_French-Rhapsodie/fr_rhapsodie-ud-test.conllu, UD_French-Rhapsodie/fr_rhapsodie-ud-train.conllu, UD_French-Sequoia/fr_sequoia-ud-dev.conllu, UD_French-Sequoia/fr_sequoia-ud-test.conllu, UD_French-Sequoia/fr_sequoia-ud-train.conllu, UD_Frisian_Dutch-Fame/qfn_fame-ud-test.conllu, UD_Galician-CTG/gl_ctg-ud-dev.conllu, UD_Galician-CTG/gl_ctg-ud-test.conllu, UD_Galician-CTG/gl_ctg-ud-train.conllu, UD_Galician-TreeGal/gl_treegal-ud-test.conllu, UD_Galician-TreeGal/gl_treegal-ud-train.conllu, UD_German-GSD/de_gsd-ud-dev.conllu, UD_German-GSD/de_gsd-ud-test.conllu, UD_German-GSD/de_gsd-ud-train.conllu, UD_German-HDT/de_hdt-ud-dev.conllu, UD_German-HDT/de_hdt-ud-test.conllu, UD_German-HDT/de_hdt-ud-train.conllu, UD_German-LIT/de_lit-ud-test.conllu, UD_German-PUD/de_pud-ud-test.conllu, UD_Gothic-PROIEL/got_proiel-ud-dev.conllu, UD_Gothic-PROIEL/got_proiel-ud-test.conllu, UD_Gothic-PROIEL/got_proiel-ud-train.conllu, UD_Greek-GDT/el_gdt-ud-dev.conllu, UD_Greek-GDT/el_gdt-ud-test.conllu, UD_Greek-GDT/el_gdt-ud-train.conllu, UD_Guajajara-TuDeT/gub_tudet-ud-test.conllu, UD_Guarani-OldTuDeT/gn_oldtudet-ud-test.conllu, UD_Hebrew-HTB/he_htb-ud-dev.conllu, UD_Hebrew-HTB/he_htb-ud-test.conllu, UD_Hebrew-HTB/he_htb-ud-train.conllu, UD_Hebrew-IAHLTwiki/he_iahltwiki-ud-dev.conllu, UD_Hebrew-IAHLTwiki/he_iahltwiki-ud-test.conllu, UD_Hebrew-IAHLTwiki/he_iahltwiki-ud-train.conllu, UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu, UD_Hindi-HDTB/hi_hdtb-ud-test.conllu, UD_Hindi-HDTB/hi_hdtb-ud-train.conllu, UD_Hindi-PUD/hi_pud-ud-test.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-dev.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-test.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-train.conllu, UD_Hittite-HitTB/hit_hittb-ud-test.conllu, UD_Hungarian-Szeged/hu_szeged-ud-dev.conllu, UD_Hungarian-Szeged/hu_szeged-ud-test.conllu, UD_Hungarian-Szeged/hu_szeged-ud-train.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-dev.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-test.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-train.conllu, UD_Icelandic-Modern/is_modern-ud-dev.conllu, UD_Icelandic-Modern/is_modern-ud-test.conllu, UD_Icelandic-Modern/is_modern-ud-train.conllu, UD_Icelandic-PUD/is_pud-ud-test.conllu, UD_Indonesian-CSUI/id_csui-ud-test.conllu, UD_Indonesian-CSUI/id_csui-ud-train.conllu, UD_Indonesian-GSD/id_gsd-ud-dev.conllu, UD_Indonesian-GSD/id_gsd-ud-test.conllu, UD_Indonesian-GSD/id_gsd-ud-train.conllu, UD_Indonesian-PUD/id_pud-ud-test.conllu, UD_Irish-IDT/ga_idt-ud-dev.conllu, UD_Irish-IDT/ga_idt-ud-test.conllu, UD_Irish-IDT/ga_idt-ud-train.conllu, UD_Irish-TwittIrish/ga_twittirish-ud-test.conllu, UD_Italian-ISDT/it_isdt-ud-dev.conllu, UD_Italian-ISDT/it_isdt-ud-test.conllu, UD_Italian-ISDT/it_isdt-ud-train.conllu, UD_Italian-MarkIT/it_markit-ud-dev.conllu, UD_Italian-MarkIT/it_markit-ud-test.conllu, UD_Italian-MarkIT/it_markit-ud-train.conllu, UD_Italian-PUD/it_pud-ud-test.conllu, UD_Italian-ParTUT/it_partut-ud-dev.conllu, UD_Italian-ParTUT/it_partut-ud-test.conllu, UD_Italian-ParTUT/it_partut-ud-train.conllu, UD_Italian-PoSTWITA/it_postwita-ud-dev.conllu, UD_Italian-PoSTWITA/it_postwita-ud-test.conllu, UD_Italian-PoSTWITA/it_postwita-ud-train.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-dev.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-test.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-train.conllu, UD_Italian-VIT/it_vit-ud-dev.conllu, UD_Italian-VIT/it_vit-ud-test.conllu, UD_Italian-VIT/it_vit-ud-train.conllu, UD_Italian-Valico/it_valico-ud-test.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-dev.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-test.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-train.conllu, UD_Japanese-BCCWJLUW/ja_bccwjluw-ud-dev.conllu, UD_Japanese-BCCWJLUW/ja_bccwjluw-ud-test.conllu, UD_Japanese-BCCWJLUW/ja_bccwjluw-ud-train.conllu, UD_Japanese-GSD/ja_gsd-ud-dev.conllu, UD_Japanese-GSD/ja_gsd-ud-test.conllu, UD_Japanese-GSD/ja_gsd-ud-train.conllu, UD_Japanese-GSDLUW/ja_gsdluw-ud-dev.conllu, UD_Japanese-GSDLUW/ja_gsdluw-ud-test.conllu, UD_Japanese-GSDLUW/ja_gsdluw-ud-train.conllu, UD_Japanese-Modern/ja_modern-ud-test.conllu, UD_Japanese-PUD/ja_pud-ud-test.conllu, UD_Japanese-PUDLUW/ja_pudluw-ud-test.conllu, UD_Javanese-CSUI/jv_csui-ud-test.conllu, UD_Kaapor-TuDeT/urb_tudet-ud-test.conllu, UD_Kangri-KDTB/xnr_kdtb-ud-test.conllu, UD_Karelian-KKPP/krl_kkpp-ud-test.conllu, UD_Karo-TuDeT/arr_tudet-ud-test.conllu, UD_Kazakh-KTB/kk_ktb-ud-test.conllu, UD_Kazakh-KTB/kk_ktb-ud-train.conllu, UD_Khunsari-AHA/kfm_aha-ud-test.conllu, UD_Kiche-IU/quc_iu-ud-test.conllu, UD_Komi_Permyak-UH/koi_uh-ud-test.conllu, UD_Komi_Zyrian-IKDP/kpv_ikdp-ud-test.conllu, UD_Komi_Zyrian-Lattice/kpv_lattice-ud-test.conllu, UD_Korean-GSD/ko_gsd-ud-dev.conllu, UD_Korean-GSD/ko_gsd-ud-test.conllu, UD_Korean-GSD/ko_gsd-ud-train.conllu, UD_Korean-Kaist/ko_kaist-ud-dev.conllu, UD_Korean-Kaist/ko_kaist-ud-test.conllu, UD_Korean-Kaist/ko_kaist-ud-train.conllu, UD_Korean-PUD/ko_pud-ud-test.conllu, UD_Kurmanji-MG/kmr_mg-ud-test.conllu, UD_Kurmanji-MG/kmr_mg-ud-train.conllu, UD_Latin-ITTB/la_ittb-ud-dev.conllu, UD_Latin-ITTB/la_ittb-ud-test.conllu, UD_Latin-ITTB/la_ittb-ud-train.conllu, UD_Latin-LLCT/la_llct-ud-dev.conllu, UD_Latin-LLCT/la_llct-ud-test.conllu, UD_Latin-LLCT/la_llct-ud-train.conllu, UD_Latin-PROIEL/la_proiel-ud-dev.conllu, UD_Latin-PROIEL/la_proiel-ud-test.conllu, UD_Latin-PROIEL/la_proiel-ud-train.conllu, UD_Latin-Perseus/la_perseus-ud-test.conllu, UD_Latin-Perseus/la_perseus-ud-train.conllu, UD_Latin-UDante/la_udante-ud-dev.conllu, UD_Latin-UDante/la_udante-ud-test.conllu, UD_Latin-UDante/la_udante-ud-train.conllu, UD_Latvian-LVTB/lv_lvtb-ud-dev.conllu, UD_Latvian-LVTB/lv_lvtb-ud-test.conllu, UD_Latvian-LVTB/lv_lvtb-ud-train.conllu, UD_Ligurian-GLT/lij_glt-ud-test.conllu, UD_Ligurian-GLT/lij_glt-ud-train.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-dev.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-test.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-train.conllu, UD_Lithuanian-HSE/lt_hse-ud-dev.conllu, UD_Lithuanian-HSE/lt_hse-ud-test.conllu, UD_Lithuanian-HSE/lt_hse-ud-train.conllu, UD_Livvi-KKPP/olo_kkpp-ud-test.conllu, UD_Livvi-KKPP/olo_kkpp-ud-train.conllu, UD_Low_Saxon-LSDC/nds_lsdc-ud-test.conllu, UD_Madi-Jarawara/jaa_jarawara-ud-test.conllu, UD_Makurap-TuDeT/mpu_tudet-ud-test.conllu, UD_Maltese-MUDT/mt_mudt-ud-dev.conllu, UD_Maltese-MUDT/mt_mudt-ud-test.conllu, UD_Maltese-MUDT/mt_mudt-ud-train.conllu, UD_Manx-Cadhan/gv_cadhan-ud-test.conllu, UD_Marathi-UFAL/mr_ufal-ud-dev.conllu, UD_Marathi-UFAL/mr_ufal-ud-test.conllu, UD_Marathi-UFAL/mr_ufal-ud-train.conllu, UD_Mbya_Guarani-Dooley/gun_dooley-ud-test.conllu, UD_Mbya_Guarani-Thomas/gun_thomas-ud-test.conllu, UD_Moksha-JR/mdf_jr-ud-test.conllu, UD_Munduruku-TuDeT/myu_tudet-ud-test.conllu, UD_Naija-NSC/pcm_nsc-ud-dev.conllu, UD_Naija-NSC/pcm_nsc-ud-test.conllu, UD_Naija-NSC/pcm_nsc-ud-train.conllu, UD_Nayini-AHA/nyq_aha-ud-test.conllu, UD_Neapolitan-RB/nap_rb-ud-test.conllu, UD_North_Sami-Giella/sme_giella-ud-test.conllu, UD_North_Sami-Giella/sme_giella-ud-train.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-dev.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-test.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-dev.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-test.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-train.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-dev.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-test.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-train.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-dev.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-test.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-train.conllu, UD_Old_East_Slavic-Birchbark/orv_birchbark-ud-dev.conllu, UD_Old_East_Slavic-Birchbark/orv_birchbark-ud-test.conllu, UD_Old_East_Slavic-Birchbark/orv_birchbark-ud-train.conllu, UD_Old_East_Slavic-RNC/orv_rnc-ud-test.conllu, UD_Old_East_Slavic-RNC/orv_rnc-ud-train.conllu, UD_Old_East_Slavic-TOROT/orv_torot-ud-dev.conllu, UD_Old_East_Slavic-TOROT/orv_torot-ud-test.conllu, UD_Old_East_Slavic-TOROT/orv_torot-ud-train.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-dev.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-test.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-train.conllu, UD_Old_Turkish-Tonqq/otk_tonqq-ud-test.conllu, UD_Persian-PerDT/fa_perdt-ud-dev.conllu, UD_Persian-PerDT/fa_perdt-ud-test.conllu, UD_Persian-PerDT/fa_perdt-ud-train.conllu, UD_Persian-Seraji/fa_seraji-ud-dev.conllu, UD_Persian-Seraji/fa_seraji-ud-test.conllu, UD_Persian-Seraji/fa_seraji-ud-train.conllu, UD_Polish-LFG/pl_lfg-ud-dev.conllu, UD_Polish-LFG/pl_lfg-ud-test.conllu, UD_Polish-LFG/pl_lfg-ud-train.conllu, UD_Polish-PDB/pl_pdb-ud-dev.conllu, UD_Polish-PDB/pl_pdb-ud-test.conllu, UD_Polish-PDB/pl_pdb-ud-train.conllu, UD_Polish-PUD/pl_pud-ud-test.conllu, UD_Pomak-Philotis/qpm_philotis-ud-dev.conllu, UD_Pomak-Philotis/qpm_philotis-ud-test.conllu, UD_Pomak-Philotis/qpm_philotis-ud-train.conllu, UD_Portuguese-Bosque/pt_bosque-ud-dev.conllu, UD_Portuguese-Bosque/pt_bosque-ud-test.conllu, UD_Portuguese-Bosque/pt_bosque-ud-train.conllu, UD_Portuguese-GSD/pt_gsd-ud-dev.conllu, UD_Portuguese-GSD/pt_gsd-ud-test.conllu, UD_Portuguese-GSD/pt_gsd-ud-train.conllu, UD_Portuguese-PUD/pt_pud-ud-test.conllu, UD_Romanian-ArT/ro_art-ud-test.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-dev.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-test.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-train.conllu, UD_Romanian-RRT/ro_rrt-ud-dev.conllu, UD_Romanian-RRT/ro_rrt-ud-test.conllu, UD_Romanian-RRT/ro_rrt-ud-train.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-dev.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-test.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-train.conllu, UD_Russian-GSD/ru_gsd-ud-dev.conllu, UD_Russian-GSD/ru_gsd-ud-test.conllu, UD_Russian-GSD/ru_gsd-ud-train.conllu, UD_Russian-PUD/ru_pud-ud-test.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu, UD_Russian-Taiga/ru_taiga-ud-dev.conllu, UD_Russian-Taiga/ru_taiga-ud-test.conllu, UD_Russian-Taiga/ru_taiga-ud-train.conllu, UD_Sanskrit-UFAL/sa_ufal-ud-test.conllu, UD_Sanskrit-Vedic/sa_vedic-ud-test.conllu, UD_Sanskrit-Vedic/sa_vedic-ud-train.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-dev.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-test.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-train.conllu, UD_Serbian-SET/sr_set-ud-dev.conllu, UD_Serbian-SET/sr_set-ud-test.conllu, UD_Serbian-SET/sr_set-ud-train.conllu, UD_Skolt_Sami-Giellagas/sms_giellagas-ud-test.conllu, UD_Slovak-SNK/sk_snk-ud-dev.conllu, UD_Slovak-SNK/sk_snk-ud-test.conllu, UD_Slovak-SNK/sk_snk-ud-train.conllu, UD_Slovenian-SSJ/sl_ssj-ud-dev.conllu, UD_Slovenian-SSJ/sl_ssj-ud-test.conllu, UD_Slovenian-SSJ/sl_ssj-ud-train.conllu, UD_Slovenian-SST/sl_sst-ud-test.conllu, UD_Slovenian-SST/sl_sst-ud-train.conllu, UD_Soi-AHA/soj_aha-ud-test.conllu, UD_South_Levantine_Arabic-MADAR/ajp_madar-ud-test.conllu, UD_Spanish-AnCora/es_ancora-ud-dev.conllu, UD_Spanish-AnCora/es_ancora-ud-test.conllu, UD_Spanish-AnCora/es_ancora-ud-train.conllu, UD_Spanish-GSD/es_gsd-ud-dev.conllu, UD_Spanish-GSD/es_gsd-ud-test.conllu, UD_Spanish-GSD/es_gsd-ud-train.conllu, UD_Spanish-PUD/es_pud-ud-test.conllu, UD_Swedish-LinES/sv_lines-ud-dev.conllu, UD_Swedish-LinES/sv_lines-ud-test.conllu, UD_Swedish-LinES/sv_lines-ud-train.conllu, UD_Swedish-PUD/sv_pud-ud-test.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-dev.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-test.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-dev.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-test.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-train.conllu, UD_Swiss_German-UZH/gsw_uzh-ud-test.conllu, UD_Tagalog-TRG/tl_trg-ud-test.conllu, UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu, UD_Tamil-MWTT/ta_mwtt-ud-test.conllu, UD_Tamil-TTB/ta_ttb-ud-dev.conllu, UD_Tamil-TTB/ta_ttb-ud-test.conllu, UD_Tamil-TTB/ta_ttb-ud-train.conllu, UD_Tatar-NMCTT/tt_nmctt-ud-test.conllu, UD_Teko-TuDeT/eme_tudet-ud-test.conllu, UD_Telugu-MTG/te_mtg-ud-dev.conllu, UD_Telugu-MTG/te_mtg-ud-test.conllu, UD_Telugu-MTG/te_mtg-ud-train.conllu, UD_Thai-PUD/th_pud-ud-test.conllu, UD_Tupinamba-TuDeT/tpn_tudet-ud-test.conllu, UD_Turkish-Atis/tr_atis-ud-dev.conllu, UD_Turkish-Atis/tr_atis-ud-test.conllu, UD_Turkish-Atis/tr_atis-ud-train.conllu, UD_Turkish-BOUN/tr_boun-ud-dev.conllu, UD_Turkish-BOUN/tr_boun-ud-test.conllu, UD_Turkish-BOUN/tr_boun-ud-train.conllu, UD_Turkish-FrameNet/tr_framenet-ud-dev.conllu, UD_Turkish-FrameNet/tr_framenet-ud-test.conllu, UD_Turkish-FrameNet/tr_framenet-ud-train.conllu, UD_Turkish-GB/tr_gb-ud-test.conllu, UD_Turkish-IMST/tr_imst-ud-dev.conllu, UD_Turkish-IMST/tr_imst-ud-test.conllu, UD_Turkish-IMST/tr_imst-ud-train.conllu, UD_Turkish-Kenet/tr_kenet-ud-dev.conllu, UD_Turkish-Kenet/tr_kenet-ud-test.conllu, UD_Turkish-Kenet/tr_kenet-ud-train.conllu, UD_Turkish-PUD/tr_pud-ud-test.conllu, UD_Turkish-Penn/tr_penn-ud-dev.conllu, UD_Turkish-Penn/tr_penn-ud-test.conllu, UD_Turkish-Penn/tr_penn-ud-train.conllu, UD_Turkish-Tourism/tr_tourism-ud-dev.conllu, UD_Turkish-Tourism/tr_tourism-ud-test.conllu, UD_Turkish-Tourism/tr_tourism-ud-train.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-dev.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-test.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-train.conllu, UD_Ukrainian-IU/uk_iu-ud-dev.conllu, UD_Ukrainian-IU/uk_iu-ud-test.conllu, UD_Ukrainian-IU/uk_iu-ud-train.conllu, UD_Umbrian-IKUVINA/xum_ikuvina-ud-test.conllu, UD_Upper_Sorbian-UFAL/hsb_ufal-ud-test.conllu, UD_Upper_Sorbian-UFAL/hsb_ufal-ud-train.conllu, UD_Urdu-UDTB/ur_udtb-ud-dev.conllu, UD_Urdu-UDTB/ur_udtb-ud-test.conllu, UD_Urdu-UDTB/ur_udtb-ud-train.conllu, UD_Uyghur-UDT/ug_udt-ud-dev.conllu, UD_Uyghur-UDT/ug_udt-ud-test.conllu, UD_Uyghur-UDT/ug_udt-ud-train.conllu, UD_Vietnamese-VTB/vi_vtb-ud-dev.conllu, UD_Vietnamese-VTB/vi_vtb-ud-test.conllu, UD_Vietnamese-VTB/vi_vtb-ud-train.conllu, UD_Warlpiri-UFAL/wbp_ufal-ud-test.conllu, UD_Welsh-CCG/cy_ccg-ud-dev.conllu, UD_Welsh-CCG/cy_ccg-ud-test.conllu, UD_Welsh-CCG/cy_ccg-ud-train.conllu, UD_Western_Armenian-ArmTDP/hyw_armtdp-ud-dev.conllu, UD_Western_Armenian-ArmTDP/hyw_armtdp-ud-test.conllu, UD_Western_Armenian-ArmTDP/hyw_armtdp-ud-train.conllu, UD_Wolof-WTB/wo_wtb-ud-dev.conllu, UD_Wolof-WTB/wo_wtb-ud-test.conllu, UD_Wolof-WTB/wo_wtb-ud-train.conllu, UD_Xibe-XDT/sjo_xdt-ud-test.conllu, UD_Yakut-YKTDT/sah_yktdt-ud-test.conllu, UD_Yoruba-YTB/yo_ytb-ud-test.conllu, UD_Yupik-SLI/ess_sli-ud-test.conllu, _, _HOME, _UD_210_HOME, __main__, a, dev, https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4758/allzip, test, train, ud-treebanks-v2.10.tgz, ud210.py",0,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-07 21:03, # noinspection PyShadowingNames, #ud-treebanks-v2.10/'",
https://github.com/hankcs/HanLP,ud210m.py,0,0.0,6,37.5,7,43.75,0,0.0,3,18.75,0,0.0,0,0,4,0,,"UD_210_MULTILINGUAL_DEV, UD_210_MULTILINGUAL_TEST, UD_210_MULTILINGUAL_TRAIN, _UD_210_MULTILINGUAL_HOME, hanlp, os","hanlp.datasets.parsing.ud.concat_treebanks, hanlp.datasets.parsing.ud.ud210._UD_210_HOME, os.path.join",,,"UD_210_MULTILINGUAL_DEV, UD_210_MULTILINGUAL_TEST, UD_210_MULTILINGUAL_TRAIN, _UD_210_MULTILINGUAL_HOME",,"2.10, Dev set of multilingual UD_210 obtained by concatenating all dev sets., Test set of multilingual UD_210 obtained by concatenating all test sets., Training set of multilingual UD_210 obtained by concatenating all training sets., dev.conllu, test.conllu, train.conllu",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-21 20:39",
https://github.com/hankcs/HanLP,ud23.py,0,0.0,327,33.96,326,33.85,1,0.1,309,32.09,0,0.0,2,0,315,0,,"UD_23_AFRIKAANS_AFRIBOOMS_DEV, UD_23_AFRIKAANS_AFRIBOOMS_TEST, UD_23_AFRIKAANS_AFRIBOOMS_TRAIN, UD_23_AKKADIAN_PISANDUB_TEST, UD_23_AMHARIC_ATT_TEST, UD_23_ANCIENT_GREEK_PERSEUS_DEV, UD_23_ANCIENT_GREEK_PERSEUS_TEST, UD_23_ANCIENT_GREEK_PERSEUS_TRAIN, UD_23_ANCIENT_GREEK_PROIEL_DEV, UD_23_ANCIENT_GREEK_PROIEL_TEST, UD_23_ANCIENT_GREEK_PROIEL_TRAIN, UD_23_ARABIC_NYUAD_DEV, UD_23_ARABIC_NYUAD_TEST, UD_23_ARABIC_NYUAD_TRAIN, UD_23_ARABIC_PADT_DEV, UD_23_ARABIC_PADT_TEST, UD_23_ARABIC_PADT_TRAIN, UD_23_ARABIC_PUD_TEST, UD_23_ARMENIAN_ARMTDP_TEST, UD_23_ARMENIAN_ARMTDP_TRAIN, UD_23_BAMBARA_CRB_TEST, UD_23_BASQUE_BDT_DEV, UD_23_BASQUE_BDT_TEST, UD_23_BASQUE_BDT_TRAIN, UD_23_BELARUSIAN_HSE_DEV, UD_23_BELARUSIAN_HSE_TEST, UD_23_BELARUSIAN_HSE_TRAIN, UD_23_BRETON_KEB_TEST, UD_23_BULGARIAN_BTB_DEV, UD_23_BULGARIAN_BTB_TEST, UD_23_BULGARIAN_BTB_TRAIN, UD_23_BURYAT_BDT_TEST, UD_23_BURYAT_BDT_TRAIN, UD_23_CANTONESE_HK_TEST, UD_23_CATALAN_ANCORA_DEV, UD_23_CATALAN_ANCORA_TEST, UD_23_CATALAN_ANCORA_TRAIN, UD_23_CHINESE_CFL_TEST, UD_23_CHINESE_GSD_DEV, UD_23_CHINESE_GSD_TEST, UD_23_CHINESE_GSD_TRAIN, UD_23_CHINESE_HK_TEST, UD_23_CHINESE_PUD_TEST, UD_23_COPTIC_SCRIPTORIUM_DEV, UD_23_COPTIC_SCRIPTORIUM_TEST, UD_23_COPTIC_SCRIPTORIUM_TRAIN, UD_23_CROATIAN_SET_DEV, UD_23_CROATIAN_SET_TEST, UD_23_CROATIAN_SET_TRAIN, UD_23_CZECH_CAC_DEV, UD_23_CZECH_CAC_TEST, UD_23_CZECH_CAC_TRAIN, UD_23_CZECH_CLTT_DEV, UD_23_CZECH_CLTT_TEST, UD_23_CZECH_CLTT_TRAIN, UD_23_CZECH_FICTREE_DEV, UD_23_CZECH_FICTREE_TEST, UD_23_CZECH_FICTREE_TRAIN, UD_23_CZECH_PDT_DEV, UD_23_CZECH_PDT_TEST, UD_23_CZECH_PDT_TRAIN, UD_23_CZECH_PUD_TEST, UD_23_DANISH_DDT_DEV, UD_23_DANISH_DDT_TEST, UD_23_DANISH_DDT_TRAIN, UD_23_DUTCH_ALPINO_DEV, UD_23_DUTCH_ALPINO_TEST, UD_23_DUTCH_ALPINO_TRAIN, UD_23_DUTCH_LASSYSMALL_DEV, UD_23_DUTCH_LASSYSMALL_TEST, UD_23_DUTCH_LASSYSMALL_TRAIN, UD_23_ENGLISH_ESL_DEV, UD_23_ENGLISH_ESL_TEST, UD_23_ENGLISH_ESL_TRAIN, UD_23_ENGLISH_EWT_DEV, UD_23_ENGLISH_EWT_TEST, UD_23_ENGLISH_EWT_TRAIN, UD_23_ENGLISH_GUM_DEV, UD_23_ENGLISH_GUM_TEST, UD_23_ENGLISH_GUM_TRAIN, UD_23_ENGLISH_LINES_DEV, UD_23_ENGLISH_LINES_TEST, UD_23_ENGLISH_LINES_TRAIN, UD_23_ENGLISH_PARTUT_DEV, UD_23_ENGLISH_PARTUT_TEST, UD_23_ENGLISH_PARTUT_TRAIN, UD_23_ENGLISH_PUD_TEST, UD_23_ERZYA_JR_TEST, UD_23_ESTONIAN_EDT_DEV, UD_23_ESTONIAN_EDT_TEST, UD_23_ESTONIAN_EDT_TRAIN, UD_23_FAROESE_OFT_TEST, UD_23_FINNISH_FTB_DEV, UD_23_FINNISH_FTB_TEST, UD_23_FINNISH_FTB_TRAIN, UD_23_FINNISH_PUD_TEST, UD_23_FINNISH_TDT_DEV, UD_23_FINNISH_TDT_TEST, UD_23_FINNISH_TDT_TRAIN, UD_23_FRENCH_FTB_DEV, UD_23_FRENCH_FTB_TEST, UD_23_FRENCH_FTB_TRAIN, UD_23_FRENCH_GSD_DEV, UD_23_FRENCH_GSD_TEST, UD_23_FRENCH_GSD_TRAIN, UD_23_FRENCH_PARTUT_DEV, UD_23_FRENCH_PARTUT_TEST, UD_23_FRENCH_PARTUT_TRAIN, UD_23_FRENCH_PUD_TEST, UD_23_FRENCH_SEQUOIA_DEV, UD_23_FRENCH_SEQUOIA_TEST, UD_23_FRENCH_SEQUOIA_TRAIN, UD_23_FRENCH_SPOKEN_DEV, UD_23_FRENCH_SPOKEN_TEST, UD_23_FRENCH_SPOKEN_TRAIN, UD_23_GALICIAN_CTG_DEV, UD_23_GALICIAN_CTG_TEST, UD_23_GALICIAN_CTG_TRAIN, UD_23_GALICIAN_TREEGAL_TEST, UD_23_GALICIAN_TREEGAL_TRAIN, UD_23_GERMAN_GSD_DEV, UD_23_GERMAN_GSD_TEST, UD_23_GERMAN_GSD_TRAIN, UD_23_GERMAN_PUD_TEST, UD_23_GOTHIC_PROIEL_DEV, UD_23_GOTHIC_PROIEL_TEST, UD_23_GOTHIC_PROIEL_TRAIN, UD_23_GREEK_GDT_DEV, UD_23_GREEK_GDT_TEST, UD_23_GREEK_GDT_TRAIN, UD_23_HEBREW_HTB_DEV, UD_23_HEBREW_HTB_TEST, UD_23_HEBREW_HTB_TRAIN, UD_23_HINDI_ENGLISH_HIENCS_DEV, UD_23_HINDI_ENGLISH_HIENCS_TEST, UD_23_HINDI_ENGLISH_HIENCS_TRAIN, UD_23_HINDI_HDTB_DEV, UD_23_HINDI_HDTB_TEST, UD_23_HINDI_HDTB_TRAIN, UD_23_HINDI_PUD_TEST, UD_23_HUNGARIAN_SZEGED_DEV, UD_23_HUNGARIAN_SZEGED_TEST, UD_23_HUNGARIAN_SZEGED_TRAIN, UD_23_INDONESIAN_GSD_DEV, UD_23_INDONESIAN_GSD_TEST, UD_23_INDONESIAN_GSD_TRAIN, UD_23_INDONESIAN_PUD_TEST, UD_23_IRISH_IDT_TEST, UD_23_IRISH_IDT_TRAIN, UD_23_ITALIAN_ISDT_DEV, UD_23_ITALIAN_ISDT_TEST, UD_23_ITALIAN_ISDT_TRAIN, UD_23_ITALIAN_PARTUT_DEV, UD_23_ITALIAN_PARTUT_TEST, UD_23_ITALIAN_PARTUT_TRAIN, UD_23_ITALIAN_POSTWITA_DEV, UD_23_ITALIAN_POSTWITA_TEST, UD_23_ITALIAN_POSTWITA_TRAIN, UD_23_ITALIAN_PUD_TEST, UD_23_JAPANESE_BCCWJ_DEV, UD_23_JAPANESE_BCCWJ_TEST, UD_23_JAPANESE_BCCWJ_TRAIN, UD_23_JAPANESE_GSD_DEV, UD_23_JAPANESE_GSD_TEST, UD_23_JAPANESE_GSD_TRAIN, UD_23_JAPANESE_MODERN_TEST, UD_23_JAPANESE_PUD_TEST, UD_23_KAZAKH_KTB_TEST, UD_23_KAZAKH_KTB_TRAIN, UD_23_KOMI_ZYRIAN_IKDP_TEST, UD_23_KOMI_ZYRIAN_LATTICE_TEST, UD_23_KOREAN_GSD_DEV, UD_23_KOREAN_GSD_TEST, UD_23_KOREAN_GSD_TRAIN, UD_23_KOREAN_KAIST_DEV, UD_23_KOREAN_KAIST_TEST, UD_23_KOREAN_KAIST_TRAIN, UD_23_KOREAN_PUD_TEST, UD_23_KURMANJI_MG_TEST, UD_23_KURMANJI_MG_TRAIN, UD_23_LATIN_ITTB_DEV, UD_23_LATIN_ITTB_TEST, UD_23_LATIN_ITTB_TRAIN, UD_23_LATIN_PERSEUS_TEST, UD_23_LATIN_PERSEUS_TRAIN, UD_23_LATIN_PROIEL_DEV, UD_23_LATIN_PROIEL_TEST, UD_23_LATIN_PROIEL_TRAIN, UD_23_LATVIAN_LVTB_DEV, UD_23_LATVIAN_LVTB_TEST, UD_23_LATVIAN_LVTB_TRAIN, UD_23_LITHUANIAN_HSE_DEV, UD_23_LITHUANIAN_HSE_TEST, UD_23_LITHUANIAN_HSE_TRAIN, UD_23_MALTESE_MUDT_DEV, UD_23_MALTESE_MUDT_TEST, UD_23_MALTESE_MUDT_TRAIN, UD_23_MARATHI_UFAL_DEV, UD_23_MARATHI_UFAL_TEST, UD_23_MARATHI_UFAL_TRAIN, UD_23_NAIJA_NSC_TEST, UD_23_NORTH_SAMI_GIELLA_TEST, UD_23_NORTH_SAMI_GIELLA_TRAIN, UD_23_NORWEGIAN_BOKMAAL_DEV, UD_23_NORWEGIAN_BOKMAAL_TEST, UD_23_NORWEGIAN_BOKMAAL_TRAIN, UD_23_NORWEGIAN_NYNORSKLIA_TEST, UD_23_NORWEGIAN_NYNORSKLIA_TRAIN, UD_23_NORWEGIAN_NYNORSK_DEV, UD_23_NORWEGIAN_NYNORSK_TEST, UD_23_NORWEGIAN_NYNORSK_TRAIN, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_23_OLD_FRENCH_SRCMF_DEV, UD_23_OLD_FRENCH_SRCMF_TEST, UD_23_OLD_FRENCH_SRCMF_TRAIN, UD_23_PERSIAN_SERAJI_DEV, UD_23_PERSIAN_SERAJI_TEST, UD_23_PERSIAN_SERAJI_TRAIN, UD_23_POLISH_LFG_DEV, UD_23_POLISH_LFG_TEST, UD_23_POLISH_LFG_TRAIN, UD_23_POLISH_SZ_DEV, UD_23_POLISH_SZ_TEST, UD_23_POLISH_SZ_TRAIN, UD_23_PORTUGUESE_BOSQUE_DEV, UD_23_PORTUGUESE_BOSQUE_TEST, UD_23_PORTUGUESE_BOSQUE_TRAIN, UD_23_PORTUGUESE_GSD_DEV, UD_23_PORTUGUESE_GSD_TEST, UD_23_PORTUGUESE_GSD_TRAIN, UD_23_PORTUGUESE_PUD_TEST, UD_23_ROMANIAN_NONSTANDARD_DEV, UD_23_ROMANIAN_NONSTANDARD_TEST, UD_23_ROMANIAN_NONSTANDARD_TRAIN, UD_23_ROMANIAN_RRT_DEV, UD_23_ROMANIAN_RRT_TEST, UD_23_ROMANIAN_RRT_TRAIN, UD_23_RUSSIAN_GSD_DEV, UD_23_RUSSIAN_GSD_TEST, UD_23_RUSSIAN_GSD_TRAIN, UD_23_RUSSIAN_PUD_TEST, UD_23_RUSSIAN_SYNTAGRUS_DEV, UD_23_RUSSIAN_SYNTAGRUS_TEST, UD_23_RUSSIAN_SYNTAGRUS_TRAIN, UD_23_RUSSIAN_TAIGA_TEST, UD_23_RUSSIAN_TAIGA_TRAIN, UD_23_SANSKRIT_UFAL_TEST, UD_23_SERBIAN_SET_DEV, UD_23_SERBIAN_SET_TEST, UD_23_SERBIAN_SET_TRAIN, UD_23_SLOVAK_SNK_DEV, UD_23_SLOVAK_SNK_TEST, UD_23_SLOVAK_SNK_TRAIN, UD_23_SLOVENIAN_SSJ_DEV, UD_23_SLOVENIAN_SSJ_TEST, UD_23_SLOVENIAN_SSJ_TRAIN, UD_23_SLOVENIAN_SST_TEST, UD_23_SLOVENIAN_SST_TRAIN, UD_23_SPANISH_ANCORA_DEV, UD_23_SPANISH_ANCORA_TEST, UD_23_SPANISH_ANCORA_TRAIN, UD_23_SPANISH_GSD_DEV, UD_23_SPANISH_GSD_TEST, UD_23_SPANISH_GSD_TRAIN, UD_23_SPANISH_PUD_TEST, UD_23_SWEDISH_LINES_DEV, UD_23_SWEDISH_LINES_TEST, UD_23_SWEDISH_LINES_TRAIN, UD_23_SWEDISH_PUD_TEST, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_23_SWEDISH_TALBANKEN_DEV, UD_23_SWEDISH_TALBANKEN_TEST, UD_23_SWEDISH_TALBANKEN_TRAIN, UD_23_TAGALOG_TRG_TEST, UD_23_TAMIL_TTB_DEV, UD_23_TAMIL_TTB_TEST, UD_23_TAMIL_TTB_TRAIN, UD_23_TELUGU_MTG_DEV, UD_23_TELUGU_MTG_TEST, UD_23_TELUGU_MTG_TRAIN, UD_23_THAI_PUD_TEST, UD_23_TURKISH_IMST_DEV, UD_23_TURKISH_IMST_TEST, UD_23_TURKISH_IMST_TRAIN, UD_23_TURKISH_PUD_TEST, UD_23_UKRAINIAN_IU_DEV, UD_23_UKRAINIAN_IU_TEST, UD_23_UKRAINIAN_IU_TRAIN, UD_23_UPPER_SORBIAN_UFAL_TEST, UD_23_UPPER_SORBIAN_UFAL_TRAIN, UD_23_URDU_UDTB_DEV, UD_23_URDU_UDTB_TEST, UD_23_URDU_UDTB_TRAIN, UD_23_UYGHUR_UDT_DEV, UD_23_UYGHUR_UDT_TEST, UD_23_UYGHUR_UDT_TRAIN, UD_23_VIETNAMESE_VTB_DEV, UD_23_VIETNAMESE_VTB_TEST, UD_23_VIETNAMESE_VTB_TRAIN, UD_23_WARLPIRI_UFAL_TEST, UD_23_YORUBA_YTB_TEST, _UD_23_HOME, _UD_24_HOME, __name__, _list_dir, basename, f, glob, hanlp, home, len, lstrip, main, open, os, out, path, prefix, sorted, sp, split, upper, write","glob.glob, hanlp.utils.io_util.get_resource, os.path.basename","_list_dir, main",,"UD_23_AFRIKAANS_AFRIBOOMS_DEV, UD_23_AFRIKAANS_AFRIBOOMS_TEST, UD_23_AFRIKAANS_AFRIBOOMS_TRAIN, UD_23_AKKADIAN_PISANDUB_TEST, UD_23_AMHARIC_ATT_TEST, UD_23_ANCIENT_GREEK_PERSEUS_DEV, UD_23_ANCIENT_GREEK_PERSEUS_TEST, UD_23_ANCIENT_GREEK_PERSEUS_TRAIN, UD_23_ANCIENT_GREEK_PROIEL_DEV, UD_23_ANCIENT_GREEK_PROIEL_TEST, UD_23_ANCIENT_GREEK_PROIEL_TRAIN, UD_23_ARABIC_NYUAD_DEV, UD_23_ARABIC_NYUAD_TEST, UD_23_ARABIC_NYUAD_TRAIN, UD_23_ARABIC_PADT_DEV, UD_23_ARABIC_PADT_TEST, UD_23_ARABIC_PADT_TRAIN, UD_23_ARABIC_PUD_TEST, UD_23_ARMENIAN_ARMTDP_TEST, UD_23_ARMENIAN_ARMTDP_TRAIN, UD_23_BAMBARA_CRB_TEST, UD_23_BASQUE_BDT_DEV, UD_23_BASQUE_BDT_TEST, UD_23_BASQUE_BDT_TRAIN, UD_23_BELARUSIAN_HSE_DEV, UD_23_BELARUSIAN_HSE_TEST, UD_23_BELARUSIAN_HSE_TRAIN, UD_23_BRETON_KEB_TEST, UD_23_BULGARIAN_BTB_DEV, UD_23_BULGARIAN_BTB_TEST, UD_23_BULGARIAN_BTB_TRAIN, UD_23_BURYAT_BDT_TEST, UD_23_BURYAT_BDT_TRAIN, UD_23_CANTONESE_HK_TEST, UD_23_CATALAN_ANCORA_DEV, UD_23_CATALAN_ANCORA_TEST, UD_23_CATALAN_ANCORA_TRAIN, UD_23_CHINESE_CFL_TEST, UD_23_CHINESE_GSD_DEV, UD_23_CHINESE_GSD_TEST, UD_23_CHINESE_GSD_TRAIN, UD_23_CHINESE_HK_TEST, UD_23_CHINESE_PUD_TEST, UD_23_COPTIC_SCRIPTORIUM_DEV, UD_23_COPTIC_SCRIPTORIUM_TEST, UD_23_COPTIC_SCRIPTORIUM_TRAIN, UD_23_CROATIAN_SET_DEV, UD_23_CROATIAN_SET_TEST, UD_23_CROATIAN_SET_TRAIN, UD_23_CZECH_CAC_DEV, UD_23_CZECH_CAC_TEST, UD_23_CZECH_CAC_TRAIN, UD_23_CZECH_CLTT_DEV, UD_23_CZECH_CLTT_TEST, UD_23_CZECH_CLTT_TRAIN, UD_23_CZECH_FICTREE_DEV, UD_23_CZECH_FICTREE_TEST, UD_23_CZECH_FICTREE_TRAIN, UD_23_CZECH_PDT_DEV, UD_23_CZECH_PDT_TEST, UD_23_CZECH_PDT_TRAIN, UD_23_CZECH_PUD_TEST, UD_23_DANISH_DDT_DEV, UD_23_DANISH_DDT_TEST, UD_23_DANISH_DDT_TRAIN, UD_23_DUTCH_ALPINO_DEV, UD_23_DUTCH_ALPINO_TEST, UD_23_DUTCH_ALPINO_TRAIN, UD_23_DUTCH_LASSYSMALL_DEV, UD_23_DUTCH_LASSYSMALL_TEST, UD_23_DUTCH_LASSYSMALL_TRAIN, UD_23_ENGLISH_ESL_DEV, UD_23_ENGLISH_ESL_TEST, UD_23_ENGLISH_ESL_TRAIN, UD_23_ENGLISH_EWT_DEV, UD_23_ENGLISH_EWT_TEST, UD_23_ENGLISH_EWT_TRAIN, UD_23_ENGLISH_GUM_DEV, UD_23_ENGLISH_GUM_TEST, UD_23_ENGLISH_GUM_TRAIN, UD_23_ENGLISH_LINES_DEV, UD_23_ENGLISH_LINES_TEST, UD_23_ENGLISH_LINES_TRAIN, UD_23_ENGLISH_PARTUT_DEV, UD_23_ENGLISH_PARTUT_TEST, UD_23_ENGLISH_PARTUT_TRAIN, UD_23_ENGLISH_PUD_TEST, UD_23_ERZYA_JR_TEST, UD_23_ESTONIAN_EDT_DEV, UD_23_ESTONIAN_EDT_TEST, UD_23_ESTONIAN_EDT_TRAIN, UD_23_FAROESE_OFT_TEST, UD_23_FINNISH_FTB_DEV, UD_23_FINNISH_FTB_TEST, UD_23_FINNISH_FTB_TRAIN, UD_23_FINNISH_PUD_TEST, UD_23_FINNISH_TDT_DEV, UD_23_FINNISH_TDT_TEST, UD_23_FINNISH_TDT_TRAIN, UD_23_FRENCH_FTB_DEV, UD_23_FRENCH_FTB_TEST, UD_23_FRENCH_FTB_TRAIN, UD_23_FRENCH_GSD_DEV, UD_23_FRENCH_GSD_TEST, UD_23_FRENCH_GSD_TRAIN, UD_23_FRENCH_PARTUT_DEV, UD_23_FRENCH_PARTUT_TEST, UD_23_FRENCH_PARTUT_TRAIN, UD_23_FRENCH_PUD_TEST, UD_23_FRENCH_SEQUOIA_DEV, UD_23_FRENCH_SEQUOIA_TEST, UD_23_FRENCH_SEQUOIA_TRAIN, UD_23_FRENCH_SPOKEN_DEV, UD_23_FRENCH_SPOKEN_TEST, UD_23_FRENCH_SPOKEN_TRAIN, UD_23_GALICIAN_CTG_DEV, UD_23_GALICIAN_CTG_TEST, UD_23_GALICIAN_CTG_TRAIN, UD_23_GALICIAN_TREEGAL_TEST, UD_23_GALICIAN_TREEGAL_TRAIN, UD_23_GERMAN_GSD_DEV, UD_23_GERMAN_GSD_TEST, UD_23_GERMAN_GSD_TRAIN, UD_23_GERMAN_PUD_TEST, UD_23_GOTHIC_PROIEL_DEV, UD_23_GOTHIC_PROIEL_TEST, UD_23_GOTHIC_PROIEL_TRAIN, UD_23_GREEK_GDT_DEV, UD_23_GREEK_GDT_TEST, UD_23_GREEK_GDT_TRAIN, UD_23_HEBREW_HTB_DEV, UD_23_HEBREW_HTB_TEST, UD_23_HEBREW_HTB_TRAIN, UD_23_HINDI_ENGLISH_HIENCS_DEV, UD_23_HINDI_ENGLISH_HIENCS_TEST, UD_23_HINDI_ENGLISH_HIENCS_TRAIN, UD_23_HINDI_HDTB_DEV, UD_23_HINDI_HDTB_TEST, UD_23_HINDI_HDTB_TRAIN, UD_23_HINDI_PUD_TEST, UD_23_HUNGARIAN_SZEGED_DEV, UD_23_HUNGARIAN_SZEGED_TEST, UD_23_HUNGARIAN_SZEGED_TRAIN, UD_23_INDONESIAN_GSD_DEV, UD_23_INDONESIAN_GSD_TEST, UD_23_INDONESIAN_GSD_TRAIN, UD_23_INDONESIAN_PUD_TEST, UD_23_IRISH_IDT_TEST, UD_23_IRISH_IDT_TRAIN, UD_23_ITALIAN_ISDT_DEV, UD_23_ITALIAN_ISDT_TEST, UD_23_ITALIAN_ISDT_TRAIN, UD_23_ITALIAN_PARTUT_DEV, UD_23_ITALIAN_PARTUT_TEST, UD_23_ITALIAN_PARTUT_TRAIN, UD_23_ITALIAN_POSTWITA_DEV, UD_23_ITALIAN_POSTWITA_TEST, UD_23_ITALIAN_POSTWITA_TRAIN, UD_23_ITALIAN_PUD_TEST, UD_23_JAPANESE_BCCWJ_DEV, UD_23_JAPANESE_BCCWJ_TEST, UD_23_JAPANESE_BCCWJ_TRAIN, UD_23_JAPANESE_GSD_DEV, UD_23_JAPANESE_GSD_TEST, UD_23_JAPANESE_GSD_TRAIN, UD_23_JAPANESE_MODERN_TEST, UD_23_JAPANESE_PUD_TEST, UD_23_KAZAKH_KTB_TEST, UD_23_KAZAKH_KTB_TRAIN, UD_23_KOMI_ZYRIAN_IKDP_TEST, UD_23_KOMI_ZYRIAN_LATTICE_TEST, UD_23_KOREAN_GSD_DEV, UD_23_KOREAN_GSD_TEST, UD_23_KOREAN_GSD_TRAIN, UD_23_KOREAN_KAIST_DEV, UD_23_KOREAN_KAIST_TEST, UD_23_KOREAN_KAIST_TRAIN, UD_23_KOREAN_PUD_TEST, UD_23_KURMANJI_MG_TEST, UD_23_KURMANJI_MG_TRAIN, UD_23_LATIN_ITTB_DEV, UD_23_LATIN_ITTB_TEST, UD_23_LATIN_ITTB_TRAIN, UD_23_LATIN_PERSEUS_TEST, UD_23_LATIN_PERSEUS_TRAIN, UD_23_LATIN_PROIEL_DEV, UD_23_LATIN_PROIEL_TEST, UD_23_LATIN_PROIEL_TRAIN, UD_23_LATVIAN_LVTB_DEV, UD_23_LATVIAN_LVTB_TEST, UD_23_LATVIAN_LVTB_TRAIN, UD_23_LITHUANIAN_HSE_DEV, UD_23_LITHUANIAN_HSE_TEST, UD_23_LITHUANIAN_HSE_TRAIN, UD_23_MALTESE_MUDT_DEV, UD_23_MALTESE_MUDT_TEST, UD_23_MALTESE_MUDT_TRAIN, UD_23_MARATHI_UFAL_DEV, UD_23_MARATHI_UFAL_TEST, UD_23_MARATHI_UFAL_TRAIN, UD_23_NAIJA_NSC_TEST, UD_23_NORTH_SAMI_GIELLA_TEST, UD_23_NORTH_SAMI_GIELLA_TRAIN, UD_23_NORWEGIAN_BOKMAAL_DEV, UD_23_NORWEGIAN_BOKMAAL_TEST, UD_23_NORWEGIAN_BOKMAAL_TRAIN, UD_23_NORWEGIAN_NYNORSKLIA_TEST, UD_23_NORWEGIAN_NYNORSKLIA_TRAIN, UD_23_NORWEGIAN_NYNORSK_DEV, UD_23_NORWEGIAN_NYNORSK_TEST, UD_23_NORWEGIAN_NYNORSK_TRAIN, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_23_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_23_OLD_FRENCH_SRCMF_DEV, UD_23_OLD_FRENCH_SRCMF_TEST, UD_23_OLD_FRENCH_SRCMF_TRAIN, UD_23_PERSIAN_SERAJI_DEV, UD_23_PERSIAN_SERAJI_TEST, UD_23_PERSIAN_SERAJI_TRAIN, UD_23_POLISH_LFG_DEV, UD_23_POLISH_LFG_TEST, UD_23_POLISH_LFG_TRAIN, UD_23_POLISH_SZ_DEV, UD_23_POLISH_SZ_TEST, UD_23_POLISH_SZ_TRAIN, UD_23_PORTUGUESE_BOSQUE_DEV, UD_23_PORTUGUESE_BOSQUE_TEST, UD_23_PORTUGUESE_BOSQUE_TRAIN, UD_23_PORTUGUESE_GSD_DEV, UD_23_PORTUGUESE_GSD_TEST, UD_23_PORTUGUESE_GSD_TRAIN, UD_23_PORTUGUESE_PUD_TEST, UD_23_ROMANIAN_NONSTANDARD_DEV, UD_23_ROMANIAN_NONSTANDARD_TEST, UD_23_ROMANIAN_NONSTANDARD_TRAIN, UD_23_ROMANIAN_RRT_DEV, UD_23_ROMANIAN_RRT_TEST, UD_23_ROMANIAN_RRT_TRAIN, UD_23_RUSSIAN_GSD_DEV, UD_23_RUSSIAN_GSD_TEST, UD_23_RUSSIAN_GSD_TRAIN, UD_23_RUSSIAN_PUD_TEST, UD_23_RUSSIAN_SYNTAGRUS_DEV, UD_23_RUSSIAN_SYNTAGRUS_TEST, UD_23_RUSSIAN_SYNTAGRUS_TRAIN, UD_23_RUSSIAN_TAIGA_TEST, UD_23_RUSSIAN_TAIGA_TRAIN, UD_23_SANSKRIT_UFAL_TEST, UD_23_SERBIAN_SET_DEV, UD_23_SERBIAN_SET_TEST, UD_23_SERBIAN_SET_TRAIN, UD_23_SLOVAK_SNK_DEV, UD_23_SLOVAK_SNK_TEST, UD_23_SLOVAK_SNK_TRAIN, UD_23_SLOVENIAN_SSJ_DEV, UD_23_SLOVENIAN_SSJ_TEST, UD_23_SLOVENIAN_SSJ_TRAIN, UD_23_SLOVENIAN_SST_TEST, UD_23_SLOVENIAN_SST_TRAIN, UD_23_SPANISH_ANCORA_DEV, UD_23_SPANISH_ANCORA_TEST, UD_23_SPANISH_ANCORA_TRAIN, UD_23_SPANISH_GSD_DEV, UD_23_SPANISH_GSD_TEST, UD_23_SPANISH_GSD_TRAIN, UD_23_SPANISH_PUD_TEST, UD_23_SWEDISH_LINES_DEV, UD_23_SWEDISH_LINES_TEST, UD_23_SWEDISH_LINES_TRAIN, UD_23_SWEDISH_PUD_TEST, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_23_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_23_SWEDISH_TALBANKEN_DEV, UD_23_SWEDISH_TALBANKEN_TEST, UD_23_SWEDISH_TALBANKEN_TRAIN, UD_23_TAGALOG_TRG_TEST, UD_23_TAMIL_TTB_DEV, UD_23_TAMIL_TTB_TEST, UD_23_TAMIL_TTB_TRAIN, UD_23_TELUGU_MTG_DEV, UD_23_TELUGU_MTG_TEST, UD_23_TELUGU_MTG_TRAIN, UD_23_THAI_PUD_TEST, UD_23_TURKISH_IMST_DEV, UD_23_TURKISH_IMST_TEST, UD_23_TURKISH_IMST_TRAIN, UD_23_TURKISH_PUD_TEST, UD_23_UKRAINIAN_IU_DEV, UD_23_UKRAINIAN_IU_TEST, UD_23_UKRAINIAN_IU_TRAIN, UD_23_UPPER_SORBIAN_UFAL_TEST, UD_23_UPPER_SORBIAN_UFAL_TRAIN, UD_23_URDU_UDTB_DEV, UD_23_URDU_UDTB_TEST, UD_23_URDU_UDTB_TRAIN, UD_23_UYGHUR_UDT_DEV, UD_23_UYGHUR_UDT_TEST, UD_23_UYGHUR_UDT_TRAIN, UD_23_VIETNAMESE_VTB_DEV, UD_23_VIETNAMESE_VTB_TEST, UD_23_VIETNAMESE_VTB_TRAIN, UD_23_WARLPIRI_UFAL_TEST, UD_23_YORUBA_YTB_TEST, _UD_23_HOME, _UD_24_HOME, basename, f, name, out, path, prefix, sp, split",,",  + ""#,  = , ""
, #UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu, #UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu, #UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu, #UD_Akkadian-PISANDUB/akk_pisandub-ud-test.conllu, #UD_Amharic-ATT/am_att-ud-test.conllu, #UD_Ancient_Greek-PROIEL/grc_proiel-ud-dev.conllu, #UD_Ancient_Greek-PROIEL/grc_proiel-ud-test.conllu, #UD_Ancient_Greek-PROIEL/grc_proiel-ud-train.conllu, #UD_Ancient_Greek-Perseus/grc_perseus-ud-dev.conllu, #UD_Ancient_Greek-Perseus/grc_perseus-ud-test.conllu, #UD_Ancient_Greek-Perseus/grc_perseus-ud-train.conllu, #UD_Arabic-NYUAD/ar_nyuad-ud-dev.conllu, #UD_Arabic-NYUAD/ar_nyuad-ud-test.conllu, #UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu, #UD_Arabic-PADT/ar_padt-ud-dev.conllu, #UD_Arabic-PADT/ar_padt-ud-test.conllu, #UD_Arabic-PADT/ar_padt-ud-train.conllu, #UD_Arabic-PUD/ar_pud-ud-test.conllu, #UD_Armenian-ArmTDP/hy_armtdp-ud-test.conllu, #UD_Armenian-ArmTDP/hy_armtdp-ud-train.conllu, #UD_Bambara-CRB/bm_crb-ud-test.conllu, #UD_Basque-BDT/eu_bdt-ud-dev.conllu, #UD_Basque-BDT/eu_bdt-ud-test.conllu, #UD_Basque-BDT/eu_bdt-ud-train.conllu, #UD_Belarusian-HSE/be_hse-ud-dev.conllu, #UD_Belarusian-HSE/be_hse-ud-test.conllu, #UD_Belarusian-HSE/be_hse-ud-train.conllu, #UD_Breton-KEB/br_keb-ud-test.conllu, #UD_Bulgarian-BTB/bg_btb-ud-dev.conllu, #UD_Bulgarian-BTB/bg_btb-ud-test.conllu, #UD_Bulgarian-BTB/bg_btb-ud-train.conllu, #UD_Buryat-BDT/bxr_bdt-ud-test.conllu, #UD_Buryat-BDT/bxr_bdt-ud-train.conllu, #UD_Cantonese-HK/yue_hk-ud-test.conllu, #UD_Catalan-AnCora/ca_ancora-ud-dev.conllu, #UD_Catalan-AnCora/ca_ancora-ud-test.conllu, #UD_Catalan-AnCora/ca_ancora-ud-train.conllu, #UD_Chinese-CFL/zh_cfl-ud-test.conllu, #UD_Chinese-GSD/zh_gsd-ud-dev.conllu, #UD_Chinese-GSD/zh_gsd-ud-test.conllu, #UD_Chinese-GSD/zh_gsd-ud-train.conllu, #UD_Chinese-HK/zh_hk-ud-test.conllu, #UD_Chinese-PUD/zh_pud-ud-test.conllu, #UD_Coptic-Scriptorium/cop_scriptorium-ud-dev.conllu, #UD_Coptic-Scriptorium/cop_scriptorium-ud-test.conllu, #UD_Coptic-Scriptorium/cop_scriptorium-ud-train.conllu, #UD_Croatian-SET/hr_set-ud-dev.conllu, #UD_Croatian-SET/hr_set-ud-test.conllu, #UD_Croatian-SET/hr_set-ud-train.conllu, #UD_Czech-CAC/cs_cac-ud-dev.conllu, #UD_Czech-CAC/cs_cac-ud-test.conllu, #UD_Czech-CAC/cs_cac-ud-train.conllu, #UD_Czech-CLTT/cs_cltt-ud-dev.conllu, #UD_Czech-CLTT/cs_cltt-ud-test.conllu, #UD_Czech-CLTT/cs_cltt-ud-train.conllu, #UD_Czech-FicTree/cs_fictree-ud-dev.conllu, #UD_Czech-FicTree/cs_fictree-ud-test.conllu, #UD_Czech-FicTree/cs_fictree-ud-train.conllu, #UD_Czech-PDT/cs_pdt-ud-dev.conllu, #UD_Czech-PDT/cs_pdt-ud-test.conllu, #UD_Czech-PDT/cs_pdt-ud-train.conllu, #UD_Czech-PUD/cs_pud-ud-test.conllu, #UD_Danish-DDT/da_ddt-ud-dev.conllu, #UD_Danish-DDT/da_ddt-ud-test.conllu, #UD_Danish-DDT/da_ddt-ud-train.conllu, #UD_Dutch-Alpino/nl_alpino-ud-dev.conllu, #UD_Dutch-Alpino/nl_alpino-ud-test.conllu, #UD_Dutch-Alpino/nl_alpino-ud-train.conllu, #UD_Dutch-LassySmall/nl_lassysmall-ud-dev.conllu, #UD_Dutch-LassySmall/nl_lassysmall-ud-test.conllu, #UD_Dutch-LassySmall/nl_lassysmall-ud-train.conllu, #UD_English-ESL/en_esl-ud-dev.conllu, #UD_English-ESL/en_esl-ud-test.conllu, #UD_English-ESL/en_esl-ud-train.conllu, #UD_English-EWT/en_ewt-ud-dev.conllu, #UD_English-EWT/en_ewt-ud-test.conllu, #UD_English-EWT/en_ewt-ud-train.conllu, #UD_English-GUM/en_gum-ud-dev.conllu, #UD_English-GUM/en_gum-ud-test.conllu, #UD_English-GUM/en_gum-ud-train.conllu, #UD_English-LinES/en_lines-ud-dev.conllu, #UD_English-LinES/en_lines-ud-test.conllu, #UD_English-LinES/en_lines-ud-train.conllu, #UD_English-PUD/en_pud-ud-test.conllu, #UD_English-ParTUT/en_partut-ud-dev.conllu, #UD_English-ParTUT/en_partut-ud-test.conllu, #UD_English-ParTUT/en_partut-ud-train.conllu, #UD_Erzya-JR/myv_jr-ud-test.conllu, #UD_Estonian-EDT/et_edt-ud-dev.conllu, #UD_Estonian-EDT/et_edt-ud-test.conllu, #UD_Estonian-EDT/et_edt-ud-train.conllu, #UD_Faroese-OFT/fo_oft-ud-test.conllu, #UD_Finnish-FTB/fi_ftb-ud-dev.conllu, #UD_Finnish-FTB/fi_ftb-ud-test.conllu, #UD_Finnish-FTB/fi_ftb-ud-train.conllu, #UD_Finnish-PUD/fi_pud-ud-test.conllu, #UD_Finnish-TDT/fi_tdt-ud-dev.conllu, #UD_Finnish-TDT/fi_tdt-ud-test.conllu, #UD_Finnish-TDT/fi_tdt-ud-train.conllu, #UD_French-FTB/fr_ftb-ud-dev.conllu, #UD_French-FTB/fr_ftb-ud-test.conllu, #UD_French-FTB/fr_ftb-ud-train.conllu, #UD_French-GSD/fr_gsd-ud-dev.conllu, #UD_French-GSD/fr_gsd-ud-test.conllu, #UD_French-GSD/fr_gsd-ud-train.conllu, #UD_French-PUD/fr_pud-ud-test.conllu, #UD_French-ParTUT/fr_partut-ud-dev.conllu, #UD_French-ParTUT/fr_partut-ud-test.conllu, #UD_French-ParTUT/fr_partut-ud-train.conllu, #UD_French-Sequoia/fr_sequoia-ud-dev.conllu, #UD_French-Sequoia/fr_sequoia-ud-test.conllu, #UD_French-Sequoia/fr_sequoia-ud-train.conllu, #UD_French-Spoken/fr_spoken-ud-dev.conllu, #UD_French-Spoken/fr_spoken-ud-test.conllu, #UD_French-Spoken/fr_spoken-ud-train.conllu, #UD_Galician-CTG/gl_ctg-ud-dev.conllu, #UD_Galician-CTG/gl_ctg-ud-test.conllu, #UD_Galician-CTG/gl_ctg-ud-train.conllu, #UD_Galician-TreeGal/gl_treegal-ud-test.conllu, #UD_Galician-TreeGal/gl_treegal-ud-train.conllu, #UD_German-GSD/de_gsd-ud-dev.conllu, #UD_German-GSD/de_gsd-ud-test.conllu, #UD_German-GSD/de_gsd-ud-train.conllu, #UD_German-PUD/de_pud-ud-test.conllu, #UD_Gothic-PROIEL/got_proiel-ud-dev.conllu, #UD_Gothic-PROIEL/got_proiel-ud-test.conllu, #UD_Gothic-PROIEL/got_proiel-ud-train.conllu, #UD_Greek-GDT/el_gdt-ud-dev.conllu, #UD_Greek-GDT/el_gdt-ud-test.conllu, #UD_Greek-GDT/el_gdt-ud-train.conllu, #UD_Hebrew-HTB/he_htb-ud-dev.conllu, #UD_Hebrew-HTB/he_htb-ud-test.conllu, #UD_Hebrew-HTB/he_htb-ud-train.conllu, #UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu, #UD_Hindi-HDTB/hi_hdtb-ud-test.conllu, #UD_Hindi-HDTB/hi_hdtb-ud-train.conllu, #UD_Hindi-PUD/hi_pud-ud-test.conllu, #UD_Hindi_English-HIENCS/qhe_hiencs-ud-dev.conllu, #UD_Hindi_English-HIENCS/qhe_hiencs-ud-test.conllu, #UD_Hindi_English-HIENCS/qhe_hiencs-ud-train.conllu, #UD_Hungarian-Szeged/hu_szeged-ud-dev.conllu, #UD_Hungarian-Szeged/hu_szeged-ud-test.conllu, #UD_Hungarian-Szeged/hu_szeged-ud-train.conllu, #UD_Indonesian-GSD/id_gsd-ud-dev.conllu, #UD_Indonesian-GSD/id_gsd-ud-test.conllu, #UD_Indonesian-GSD/id_gsd-ud-train.conllu, #UD_Indonesian-PUD/id_pud-ud-test.conllu, #UD_Irish-IDT/ga_idt-ud-test.conllu, #UD_Irish-IDT/ga_idt-ud-train.conllu, #UD_Italian-ISDT/it_isdt-ud-dev.conllu, #UD_Italian-ISDT/it_isdt-ud-test.conllu, #UD_Italian-ISDT/it_isdt-ud-train.conllu, #UD_Italian-PUD/it_pud-ud-test.conllu, #UD_Italian-ParTUT/it_partut-ud-dev.conllu, #UD_Italian-ParTUT/it_partut-ud-test.conllu, #UD_Italian-ParTUT/it_partut-ud-train.conllu, #UD_Italian-PoSTWITA/it_postwita-ud-dev.conllu, #UD_Italian-PoSTWITA/it_postwita-ud-test.conllu, #UD_Italian-PoSTWITA/it_postwita-ud-train.conllu, #UD_Japanese-BCCWJ/ja_bccwj-ud-dev.conllu, #UD_Japanese-BCCWJ/ja_bccwj-ud-test.conllu, #UD_Japanese-BCCWJ/ja_bccwj-ud-train.conllu, #UD_Japanese-GSD/ja_gsd-ud-dev.conllu, #UD_Japanese-GSD/ja_gsd-ud-test.conllu, #UD_Japanese-GSD/ja_gsd-ud-train.conllu, #UD_Japanese-Modern/ja_modern-ud-test.conllu, #UD_Japanese-PUD/ja_pud-ud-test.conllu, #UD_Kazakh-KTB/kk_ktb-ud-test.conllu, #UD_Kazakh-KTB/kk_ktb-ud-train.conllu, #UD_Komi_Zyrian-IKDP/kpv_ikdp-ud-test.conllu, #UD_Komi_Zyrian-Lattice/kpv_lattice-ud-test.conllu, #UD_Korean-GSD/ko_gsd-ud-dev.conllu, #UD_Korean-GSD/ko_gsd-ud-test.conllu, #UD_Korean-GSD/ko_gsd-ud-train.conllu, #UD_Korean-Kaist/ko_kaist-ud-dev.conllu, #UD_Korean-Kaist/ko_kaist-ud-test.conllu, #UD_Korean-Kaist/ko_kaist-ud-train.conllu, #UD_Korean-PUD/ko_pud-ud-test.conllu, #UD_Kurmanji-MG/kmr_mg-ud-test.conllu, #UD_Kurmanji-MG/kmr_mg-ud-train.conllu, #UD_Latin-ITTB/la_ittb-ud-dev.conllu, #UD_Latin-ITTB/la_ittb-ud-test.conllu, #UD_Latin-ITTB/la_ittb-ud-train.conllu, #UD_Latin-PROIEL/la_proiel-ud-dev.conllu, #UD_Latin-PROIEL/la_proiel-ud-test.conllu, #UD_Latin-PROIEL/la_proiel-ud-train.conllu, #UD_Latin-Perseus/la_perseus-ud-test.conllu, #UD_Latin-Perseus/la_perseus-ud-train.conllu, #UD_Latvian-LVTB/lv_lvtb-ud-dev.conllu, #UD_Latvian-LVTB/lv_lvtb-ud-test.conllu, #UD_Latvian-LVTB/lv_lvtb-ud-train.conllu, #UD_Lithuanian-HSE/lt_hse-ud-dev.conllu, #UD_Lithuanian-HSE/lt_hse-ud-test.conllu, #UD_Lithuanian-HSE/lt_hse-ud-train.conllu, #UD_Maltese-MUDT/mt_mudt-ud-dev.conllu, #UD_Maltese-MUDT/mt_mudt-ud-test.conllu, #UD_Maltese-MUDT/mt_mudt-ud-train.conllu, #UD_Marathi-UFAL/mr_ufal-ud-dev.conllu, #UD_Marathi-UFAL/mr_ufal-ud-test.conllu, #UD_Marathi-UFAL/mr_ufal-ud-train.conllu, #UD_Naija-NSC/pcm_nsc-ud-test.conllu, #UD_North_Sami-Giella/sme_giella-ud-test.conllu, #UD_North_Sami-Giella/sme_giella-ud-train.conllu, #UD_Norwegian-Bokmaal/no_bokmaal-ud-dev.conllu, #UD_Norwegian-Bokmaal/no_bokmaal-ud-test.conllu, #UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu, #UD_Norwegian-Nynorsk/no_nynorsk-ud-dev.conllu, #UD_Norwegian-Nynorsk/no_nynorsk-ud-test.conllu, #UD_Norwegian-Nynorsk/no_nynorsk-ud-train.conllu, #UD_Norwegian-NynorskLIA/no_nynorsklia-ud-test.conllu, #UD_Norwegian-NynorskLIA/no_nynorsklia-ud-train.conllu, #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-dev.conllu, #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-test.conllu, #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-train.conllu, #UD_Old_French-SRCMF/fro_srcmf-ud-dev.conllu, #UD_Old_French-SRCMF/fro_srcmf-ud-test.conllu, #UD_Old_French-SRCMF/fro_srcmf-ud-train.conllu, #UD_Persian-Seraji/fa_seraji-ud-dev.conllu, #UD_Persian-Seraji/fa_seraji-ud-test.conllu, #UD_Persian-Seraji/fa_seraji-ud-train.conllu, #UD_Polish-LFG/pl_lfg-ud-dev.conllu, #UD_Polish-LFG/pl_lfg-ud-test.conllu, #UD_Polish-LFG/pl_lfg-ud-train.conllu, #UD_Polish-SZ/pl_sz-ud-dev.conllu, #UD_Polish-SZ/pl_sz-ud-test.conllu, #UD_Polish-SZ/pl_sz-ud-train.conllu, #UD_Portuguese-Bosque/pt_bosque-ud-dev.conllu, #UD_Portuguese-Bosque/pt_bosque-ud-test.conllu, #UD_Portuguese-Bosque/pt_bosque-ud-train.conllu, #UD_Portuguese-GSD/pt_gsd-ud-dev.conllu, #UD_Portuguese-GSD/pt_gsd-ud-test.conllu, #UD_Portuguese-GSD/pt_gsd-ud-train.conllu, #UD_Portuguese-PUD/pt_pud-ud-test.conllu, #UD_Romanian-Nonstandard/ro_nonstandard-ud-dev.conllu, #UD_Romanian-Nonstandard/ro_nonstandard-ud-test.conllu, #UD_Romanian-Nonstandard/ro_nonstandard-ud-train.conllu, #UD_Romanian-RRT/ro_rrt-ud-dev.conllu, #UD_Romanian-RRT/ro_rrt-ud-test.conllu, #UD_Romanian-RRT/ro_rrt-ud-train.conllu, #UD_Russian-GSD/ru_gsd-ud-dev.conllu, #UD_Russian-GSD/ru_gsd-ud-test.conllu, #UD_Russian-GSD/ru_gsd-ud-train.conllu, #UD_Russian-PUD/ru_pud-ud-test.conllu, #UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu, #UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu, #UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu, #UD_Russian-Taiga/ru_taiga-ud-test.conllu, #UD_Russian-Taiga/ru_taiga-ud-train.conllu, #UD_Sanskrit-UFAL/sa_ufal-ud-test.conllu, #UD_Serbian-SET/sr_set-ud-dev.conllu, #UD_Serbian-SET/sr_set-ud-test.conllu, #UD_Serbian-SET/sr_set-ud-train.conllu, #UD_Slovak-SNK/sk_snk-ud-dev.conllu, #UD_Slovak-SNK/sk_snk-ud-test.conllu, #UD_Slovak-SNK/sk_snk-ud-train.conllu, #UD_Slovenian-SSJ/sl_ssj-ud-dev.conllu, #UD_Slovenian-SSJ/sl_ssj-ud-test.conllu, #UD_Slovenian-SSJ/sl_ssj-ud-train.conllu, #UD_Slovenian-SST/sl_sst-ud-test.conllu, #UD_Slovenian-SST/sl_sst-ud-train.conllu, #UD_Spanish-AnCora/es_ancora-ud-dev.conllu, #UD_Spanish-AnCora/es_ancora-ud-test.conllu, #UD_Spanish-AnCora/es_ancora-ud-train.conllu, #UD_Spanish-GSD/es_gsd-ud-dev.conllu, #UD_Spanish-GSD/es_gsd-ud-test.conllu, #UD_Spanish-GSD/es_gsd-ud-train.conllu, #UD_Spanish-PUD/es_pud-ud-test.conllu, #UD_Swedish-LinES/sv_lines-ud-dev.conllu, #UD_Swedish-LinES/sv_lines-ud-test.conllu, #UD_Swedish-LinES/sv_lines-ud-train.conllu, #UD_Swedish-PUD/sv_pud-ud-test.conllu, #UD_Swedish-Talbanken/sv_talbanken-ud-dev.conllu, #UD_Swedish-Talbanken/sv_talbanken-ud-test.conllu, #UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu, #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-dev.conllu, #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-test.conllu, #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-train.conllu, #UD_Tagalog-TRG/tl_trg-ud-test.conllu, #UD_Tamil-TTB/ta_ttb-ud-dev.conllu, #UD_Tamil-TTB/ta_ttb-ud-test.conllu, #UD_Tamil-TTB/ta_ttb-ud-train.conllu, #UD_Telugu-MTG/te_mtg-ud-dev.conllu, #UD_Telugu-MTG/te_mtg-ud-test.conllu, #UD_Telugu-MTG/te_mtg-ud-train.conllu, #UD_Thai-PUD/th_pud-ud-test.conllu, #UD_Turkish-IMST/tr_imst-ud-dev.conllu, #UD_Turkish-IMST/tr_imst-ud-test.conllu, #UD_Turkish-IMST/tr_imst-ud-train.conllu, #UD_Turkish-PUD/tr_pud-ud-test.conllu, #UD_Ukrainian-IU/uk_iu-ud-dev.conllu, #UD_Ukrainian-IU/uk_iu-ud-test.conllu, #UD_Ukrainian-IU/uk_iu-ud-train.conllu, #UD_Upper_Sorbian-UFAL/hsb_ufal-ud-test.conllu, #UD_Upper_Sorbian-UFAL/hsb_ufal-ud-train.conllu, #UD_Urdu-UDTB/ur_udtb-ud-dev.conllu, #UD_Urdu-UDTB/ur_udtb-ud-test.conllu, #UD_Urdu-UDTB/ur_udtb-ud-train.conllu, #UD_Uyghur-UDT/ug_udt-ud-dev.conllu, #UD_Uyghur-UDT/ug_udt-ud-test.conllu, #UD_Uyghur-UDT/ug_udt-ud-train.conllu, #UD_Vietnamese-VTB/vi_vtb-ud-dev.conllu, #UD_Vietnamese-VTB/vi_vtb-ud-test.conllu, #UD_Vietnamese-VTB/vi_vtb-ud-train.conllu, #UD_Warlpiri-UFAL/wbp_ufal-ud-test.conllu, #UD_Yoruba-YTB/yo_ytb-ud-test.conllu, -, .conllu, /, /*, /UD_*, UD_, _, _HOME, _UD_23_HOME, __main__, a, dev, https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2895/ud-treebanks-v2.3.tgz?sequence=1&isAllowed=y, https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2988/ud-treebanks-v2.4.tgz?sequence=4&isAllowed=y, test, train, ud23.py",0,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-21 20:26, #UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu"", #UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu"", #UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu"", #UD_Akkadian-PISANDUB/akk_pisandub-ud-test.conllu"", #UD_Amharic-ATT/am_att-ud-test.conllu"", #UD_Ancient_Greek-PROIEL/grc_proiel-ud-dev.conllu"", #UD_Ancient_Greek-PROIEL/grc_proiel-ud-test.conllu"", #UD_Ancient_Greek-PROIEL/grc_proiel-ud-train.conllu"", #UD_Ancient_Greek-Perseus/grc_perseus-ud-dev.conllu"", #UD_Ancient_Greek-Perseus/grc_perseus-ud-test.conllu"", #UD_Ancient_Greek-Perseus/grc_perseus-ud-train.conllu"", #UD_Arabic-NYUAD/ar_nyuad-ud-dev.conllu"", #UD_Arabic-NYUAD/ar_nyuad-ud-test.conllu"", #UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu"", #UD_Arabic-PADT/ar_padt-ud-dev.conllu"", #UD_Arabic-PADT/ar_padt-ud-test.conllu"", #UD_Arabic-PADT/ar_padt-ud-train.conllu"", #UD_Arabic-PUD/ar_pud-ud-test.conllu"", #UD_Armenian-ArmTDP/hy_armtdp-ud-test.conllu"", #UD_Armenian-ArmTDP/hy_armtdp-ud-train.conllu"", #UD_Bambara-CRB/bm_crb-ud-test.conllu"", #UD_Basque-BDT/eu_bdt-ud-dev.conllu"", #UD_Basque-BDT/eu_bdt-ud-test.conllu"", #UD_Basque-BDT/eu_bdt-ud-train.conllu"", #UD_Belarusian-HSE/be_hse-ud-dev.conllu"", #UD_Belarusian-HSE/be_hse-ud-test.conllu"", #UD_Belarusian-HSE/be_hse-ud-train.conllu"", #UD_Breton-KEB/br_keb-ud-test.conllu"", #UD_Bulgarian-BTB/bg_btb-ud-dev.conllu"", #UD_Bulgarian-BTB/bg_btb-ud-test.conllu"", #UD_Bulgarian-BTB/bg_btb-ud-train.conllu"", #UD_Buryat-BDT/bxr_bdt-ud-test.conllu"", #UD_Buryat-BDT/bxr_bdt-ud-train.conllu"", #UD_Cantonese-HK/yue_hk-ud-test.conllu"", #UD_Catalan-AnCora/ca_ancora-ud-dev.conllu"", #UD_Catalan-AnCora/ca_ancora-ud-test.conllu"", #UD_Catalan-AnCora/ca_ancora-ud-train.conllu"", #UD_Chinese-CFL/zh_cfl-ud-test.conllu"", #UD_Chinese-GSD/zh_gsd-ud-dev.conllu"", #UD_Chinese-GSD/zh_gsd-ud-test.conllu"", #UD_Chinese-GSD/zh_gsd-ud-train.conllu"", #UD_Chinese-HK/zh_hk-ud-test.conllu"", #UD_Chinese-PUD/zh_pud-ud-test.conllu"", #UD_Coptic-Scriptorium/cop_scriptorium-ud-dev.conllu"", #UD_Coptic-Scriptorium/cop_scriptorium-ud-test.conllu"", #UD_Coptic-Scriptorium/cop_scriptorium-ud-train.conllu"", #UD_Croatian-SET/hr_set-ud-dev.conllu"", #UD_Croatian-SET/hr_set-ud-test.conllu"", #UD_Croatian-SET/hr_set-ud-train.conllu"", #UD_Czech-CAC/cs_cac-ud-dev.conllu"", #UD_Czech-CAC/cs_cac-ud-test.conllu"", #UD_Czech-CAC/cs_cac-ud-train.conllu"", #UD_Czech-CLTT/cs_cltt-ud-dev.conllu"", #UD_Czech-CLTT/cs_cltt-ud-test.conllu"", #UD_Czech-CLTT/cs_cltt-ud-train.conllu"", #UD_Czech-FicTree/cs_fictree-ud-dev.conllu"", #UD_Czech-FicTree/cs_fictree-ud-test.conllu"", #UD_Czech-FicTree/cs_fictree-ud-train.conllu"", #UD_Czech-PDT/cs_pdt-ud-dev.conllu"", #UD_Czech-PDT/cs_pdt-ud-test.conllu"", #UD_Czech-PDT/cs_pdt-ud-train.conllu"", #UD_Czech-PUD/cs_pud-ud-test.conllu"", #UD_Danish-DDT/da_ddt-ud-dev.conllu"", #UD_Danish-DDT/da_ddt-ud-test.conllu"", #UD_Danish-DDT/da_ddt-ud-train.conllu"", #UD_Dutch-Alpino/nl_alpino-ud-dev.conllu"", #UD_Dutch-Alpino/nl_alpino-ud-test.conllu"", #UD_Dutch-Alpino/nl_alpino-ud-train.conllu"", #UD_Dutch-LassySmall/nl_lassysmall-ud-dev.conllu"", #UD_Dutch-LassySmall/nl_lassysmall-ud-test.conllu"", #UD_Dutch-LassySmall/nl_lassysmall-ud-train.conllu"", #UD_English-ESL/en_esl-ud-dev.conllu"", #UD_English-ESL/en_esl-ud-test.conllu"", #UD_English-ESL/en_esl-ud-train.conllu"", #UD_English-EWT/en_ewt-ud-dev.conllu"", #UD_English-EWT/en_ewt-ud-test.conllu"", #UD_English-EWT/en_ewt-ud-train.conllu"", #UD_English-GUM/en_gum-ud-dev.conllu"", #UD_English-GUM/en_gum-ud-test.conllu"", #UD_English-GUM/en_gum-ud-train.conllu"", #UD_English-LinES/en_lines-ud-dev.conllu"", #UD_English-LinES/en_lines-ud-test.conllu"", #UD_English-LinES/en_lines-ud-train.conllu"", #UD_English-PUD/en_pud-ud-test.conllu"", #UD_English-ParTUT/en_partut-ud-dev.conllu"", #UD_English-ParTUT/en_partut-ud-test.conllu"", #UD_English-ParTUT/en_partut-ud-train.conllu"", #UD_Erzya-JR/myv_jr-ud-test.conllu"", #UD_Estonian-EDT/et_edt-ud-dev.conllu"", #UD_Estonian-EDT/et_edt-ud-test.conllu"", #UD_Estonian-EDT/et_edt-ud-train.conllu"", #UD_Faroese-OFT/fo_oft-ud-test.conllu"", #UD_Finnish-FTB/fi_ftb-ud-dev.conllu"", #UD_Finnish-FTB/fi_ftb-ud-test.conllu"", #UD_Finnish-FTB/fi_ftb-ud-train.conllu"", #UD_Finnish-PUD/fi_pud-ud-test.conllu"", #UD_Finnish-TDT/fi_tdt-ud-dev.conllu"", #UD_Finnish-TDT/fi_tdt-ud-test.conllu"", #UD_Finnish-TDT/fi_tdt-ud-train.conllu"", #UD_French-FTB/fr_ftb-ud-dev.conllu"", #UD_French-FTB/fr_ftb-ud-test.conllu"", #UD_French-FTB/fr_ftb-ud-train.conllu"", #UD_French-GSD/fr_gsd-ud-dev.conllu"", #UD_French-GSD/fr_gsd-ud-test.conllu"", #UD_French-GSD/fr_gsd-ud-train.conllu"", #UD_French-PUD/fr_pud-ud-test.conllu"", #UD_French-ParTUT/fr_partut-ud-dev.conllu"", #UD_French-ParTUT/fr_partut-ud-test.conllu"", #UD_French-ParTUT/fr_partut-ud-train.conllu"", #UD_French-Sequoia/fr_sequoia-ud-dev.conllu"", #UD_French-Sequoia/fr_sequoia-ud-test.conllu"", #UD_French-Sequoia/fr_sequoia-ud-train.conllu"", #UD_French-Spoken/fr_spoken-ud-dev.conllu"", #UD_French-Spoken/fr_spoken-ud-test.conllu"", #UD_French-Spoken/fr_spoken-ud-train.conllu"", #UD_Galician-CTG/gl_ctg-ud-dev.conllu"", #UD_Galician-CTG/gl_ctg-ud-test.conllu"", #UD_Galician-CTG/gl_ctg-ud-train.conllu"", #UD_Galician-TreeGal/gl_treegal-ud-test.conllu"", #UD_Galician-TreeGal/gl_treegal-ud-train.conllu"", #UD_German-GSD/de_gsd-ud-dev.conllu"", #UD_German-GSD/de_gsd-ud-test.conllu"", #UD_German-GSD/de_gsd-ud-train.conllu"", #UD_German-PUD/de_pud-ud-test.conllu"", #UD_Gothic-PROIEL/got_proiel-ud-dev.conllu"", #UD_Gothic-PROIEL/got_proiel-ud-test.conllu"", #UD_Gothic-PROIEL/got_proiel-ud-train.conllu"", #UD_Greek-GDT/el_gdt-ud-dev.conllu"", #UD_Greek-GDT/el_gdt-ud-test.conllu"", #UD_Greek-GDT/el_gdt-ud-train.conllu"", #UD_Hebrew-HTB/he_htb-ud-dev.conllu"", #UD_Hebrew-HTB/he_htb-ud-test.conllu"", #UD_Hebrew-HTB/he_htb-ud-train.conllu"", #UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu"", #UD_Hindi-HDTB/hi_hdtb-ud-test.conllu"", #UD_Hindi-HDTB/hi_hdtb-ud-train.conllu"", #UD_Hindi-PUD/hi_pud-ud-test.conllu"", #UD_Hindi_English-HIENCS/qhe_hiencs-ud-dev.conllu"", #UD_Hindi_English-HIENCS/qhe_hiencs-ud-test.conllu"", #UD_Hindi_English-HIENCS/qhe_hiencs-ud-train.conllu"", #UD_Hungarian-Szeged/hu_szeged-ud-dev.conllu"", #UD_Hungarian-Szeged/hu_szeged-ud-test.conllu"", #UD_Hungarian-Szeged/hu_szeged-ud-train.conllu"", #UD_Indonesian-GSD/id_gsd-ud-dev.conllu"", #UD_Indonesian-GSD/id_gsd-ud-test.conllu"", #UD_Indonesian-GSD/id_gsd-ud-train.conllu"", #UD_Indonesian-PUD/id_pud-ud-test.conllu"", #UD_Irish-IDT/ga_idt-ud-test.conllu"", #UD_Irish-IDT/ga_idt-ud-train.conllu"", #UD_Italian-ISDT/it_isdt-ud-dev.conllu"", #UD_Italian-ISDT/it_isdt-ud-test.conllu"", #UD_Italian-ISDT/it_isdt-ud-train.conllu"", #UD_Italian-PUD/it_pud-ud-test.conllu"", #UD_Italian-ParTUT/it_partut-ud-dev.conllu"", #UD_Italian-ParTUT/it_partut-ud-test.conllu"", #UD_Italian-ParTUT/it_partut-ud-train.conllu"", #UD_Italian-PoSTWITA/it_postwita-ud-dev.conllu"", #UD_Italian-PoSTWITA/it_postwita-ud-test.conllu"", #UD_Italian-PoSTWITA/it_postwita-ud-train.conllu"", #UD_Japanese-BCCWJ/ja_bccwj-ud-dev.conllu"", #UD_Japanese-BCCWJ/ja_bccwj-ud-test.conllu"", #UD_Japanese-BCCWJ/ja_bccwj-ud-train.conllu"", #UD_Japanese-GSD/ja_gsd-ud-dev.conllu"", #UD_Japanese-GSD/ja_gsd-ud-test.conllu"", #UD_Japanese-GSD/ja_gsd-ud-train.conllu"", #UD_Japanese-Modern/ja_modern-ud-test.conllu"", #UD_Japanese-PUD/ja_pud-ud-test.conllu"", #UD_Kazakh-KTB/kk_ktb-ud-test.conllu"", #UD_Kazakh-KTB/kk_ktb-ud-train.conllu"", #UD_Komi_Zyrian-IKDP/kpv_ikdp-ud-test.conllu"", #UD_Komi_Zyrian-Lattice/kpv_lattice-ud-test.conllu"", #UD_Korean-GSD/ko_gsd-ud-dev.conllu"", #UD_Korean-GSD/ko_gsd-ud-test.conllu"", #UD_Korean-GSD/ko_gsd-ud-train.conllu"", #UD_Korean-Kaist/ko_kaist-ud-dev.conllu"", #UD_Korean-Kaist/ko_kaist-ud-test.conllu"", #UD_Korean-Kaist/ko_kaist-ud-train.conllu"", #UD_Korean-PUD/ko_pud-ud-test.conllu"", #UD_Kurmanji-MG/kmr_mg-ud-test.conllu"", #UD_Kurmanji-MG/kmr_mg-ud-train.conllu"", #UD_Latin-ITTB/la_ittb-ud-dev.conllu"", #UD_Latin-ITTB/la_ittb-ud-test.conllu"", #UD_Latin-ITTB/la_ittb-ud-train.conllu"", #UD_Latin-PROIEL/la_proiel-ud-dev.conllu"", #UD_Latin-PROIEL/la_proiel-ud-test.conllu"", #UD_Latin-PROIEL/la_proiel-ud-train.conllu"", #UD_Latin-Perseus/la_perseus-ud-test.conllu"", #UD_Latin-Perseus/la_perseus-ud-train.conllu"", #UD_Latvian-LVTB/lv_lvtb-ud-dev.conllu"", #UD_Latvian-LVTB/lv_lvtb-ud-test.conllu"", #UD_Latvian-LVTB/lv_lvtb-ud-train.conllu"", #UD_Lithuanian-HSE/lt_hse-ud-dev.conllu"", #UD_Lithuanian-HSE/lt_hse-ud-test.conllu"", #UD_Lithuanian-HSE/lt_hse-ud-train.conllu"", #UD_Maltese-MUDT/mt_mudt-ud-dev.conllu"", #UD_Maltese-MUDT/mt_mudt-ud-test.conllu"", #UD_Maltese-MUDT/mt_mudt-ud-train.conllu"", #UD_Marathi-UFAL/mr_ufal-ud-dev.conllu"", #UD_Marathi-UFAL/mr_ufal-ud-test.conllu"", #UD_Marathi-UFAL/mr_ufal-ud-train.conllu"", #UD_Naija-NSC/pcm_nsc-ud-test.conllu"", #UD_North_Sami-Giella/sme_giella-ud-test.conllu"", #UD_North_Sami-Giella/sme_giella-ud-train.conllu"", #UD_Norwegian-Bokmaal/no_bokmaal-ud-dev.conllu"", #UD_Norwegian-Bokmaal/no_bokmaal-ud-test.conllu"", #UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu"", #UD_Norwegian-Nynorsk/no_nynorsk-ud-dev.conllu"", #UD_Norwegian-Nynorsk/no_nynorsk-ud-test.conllu"", #UD_Norwegian-Nynorsk/no_nynorsk-ud-train.conllu"", #UD_Norwegian-NynorskLIA/no_nynorsklia-ud-test.conllu"", #UD_Norwegian-NynorskLIA/no_nynorsklia-ud-train.conllu"", #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-dev.conllu"", #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-test.conllu"", #UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-train.conllu"", #UD_Old_French-SRCMF/fro_srcmf-ud-dev.conllu"", #UD_Old_French-SRCMF/fro_srcmf-ud-test.conllu"", #UD_Old_French-SRCMF/fro_srcmf-ud-train.conllu"", #UD_Persian-Seraji/fa_seraji-ud-dev.conllu"", #UD_Persian-Seraji/fa_seraji-ud-test.conllu"", #UD_Persian-Seraji/fa_seraji-ud-train.conllu"", #UD_Polish-LFG/pl_lfg-ud-dev.conllu"", #UD_Polish-LFG/pl_lfg-ud-test.conllu"", #UD_Polish-LFG/pl_lfg-ud-train.conllu"", #UD_Polish-SZ/pl_sz-ud-dev.conllu"", #UD_Polish-SZ/pl_sz-ud-test.conllu"", #UD_Polish-SZ/pl_sz-ud-train.conllu"", #UD_Portuguese-Bosque/pt_bosque-ud-dev.conllu"", #UD_Portuguese-Bosque/pt_bosque-ud-test.conllu"", #UD_Portuguese-Bosque/pt_bosque-ud-train.conllu"", #UD_Portuguese-GSD/pt_gsd-ud-dev.conllu"", #UD_Portuguese-GSD/pt_gsd-ud-test.conllu"", #UD_Portuguese-GSD/pt_gsd-ud-train.conllu"", #UD_Portuguese-PUD/pt_pud-ud-test.conllu"", #UD_Romanian-Nonstandard/ro_nonstandard-ud-dev.conllu"", #UD_Romanian-Nonstandard/ro_nonstandard-ud-test.conllu"", #UD_Romanian-Nonstandard/ro_nonstandard-ud-train.conllu"", #UD_Romanian-RRT/ro_rrt-ud-dev.conllu"", #UD_Romanian-RRT/ro_rrt-ud-test.conllu"", #UD_Romanian-RRT/ro_rrt-ud-train.conllu"", #UD_Russian-GSD/ru_gsd-ud-dev.conllu"", #UD_Russian-GSD/ru_gsd-ud-test.conllu"", #UD_Russian-GSD/ru_gsd-ud-train.conllu"", #UD_Russian-PUD/ru_pud-ud-test.conllu"", #UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu"", #UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu"", #UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu"", #UD_Russian-Taiga/ru_taiga-ud-test.conllu"", #UD_Russian-Taiga/ru_taiga-ud-train.conllu"", #UD_Sanskrit-UFAL/sa_ufal-ud-test.conllu"", #UD_Serbian-SET/sr_set-ud-dev.conllu"", #UD_Serbian-SET/sr_set-ud-test.conllu"", #UD_Serbian-SET/sr_set-ud-train.conllu"", #UD_Slovak-SNK/sk_snk-ud-dev.conllu"", #UD_Slovak-SNK/sk_snk-ud-test.conllu"", #UD_Slovak-SNK/sk_snk-ud-train.conllu"", #UD_Slovenian-SSJ/sl_ssj-ud-dev.conllu"", #UD_Slovenian-SSJ/sl_ssj-ud-test.conllu"", #UD_Slovenian-SSJ/sl_ssj-ud-train.conllu"", #UD_Slovenian-SST/sl_sst-ud-test.conllu"", #UD_Slovenian-SST/sl_sst-ud-train.conllu"", #UD_Spanish-AnCora/es_ancora-ud-dev.conllu"", #UD_Spanish-AnCora/es_ancora-ud-test.conllu"", #UD_Spanish-AnCora/es_ancora-ud-train.conllu"", #UD_Spanish-GSD/es_gsd-ud-dev.conllu"", #UD_Spanish-GSD/es_gsd-ud-test.conllu"", #UD_Spanish-GSD/es_gsd-ud-train.conllu"", #UD_Spanish-PUD/es_pud-ud-test.conllu"", #UD_Swedish-LinES/sv_lines-ud-dev.conllu"", #UD_Swedish-LinES/sv_lines-ud-test.conllu"", #UD_Swedish-LinES/sv_lines-ud-train.conllu"", #UD_Swedish-PUD/sv_pud-ud-test.conllu"", #UD_Swedish-Talbanken/sv_talbanken-ud-dev.conllu"", #UD_Swedish-Talbanken/sv_talbanken-ud-test.conllu"", #UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu"", #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-dev.conllu"", #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-test.conllu"", #UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-train.conllu"", #UD_Tagalog-TRG/tl_trg-ud-test.conllu"", #UD_Tamil-TTB/ta_ttb-ud-dev.conllu"", #UD_Tamil-TTB/ta_ttb-ud-test.conllu"", #UD_Tamil-TTB/ta_ttb-ud-train.conllu"", #UD_Telugu-MTG/te_mtg-ud-dev.conllu"", #UD_Telugu-MTG/te_mtg-ud-test.conllu"", #UD_Telugu-MTG/te_mtg-ud-train.conllu"", #UD_Thai-PUD/th_pud-ud-test.conllu"", #UD_Turkish-IMST/tr_imst-ud-dev.conllu"", #UD_Turkish-IMST/tr_imst-ud-test.conllu"", #UD_Turkish-IMST/tr_imst-ud-train.conllu"", #UD_Turkish-PUD/tr_pud-ud-test.conllu"", #UD_Ukrainian-IU/uk_iu-ud-dev.conllu"", #UD_Ukrainian-IU/uk_iu-ud-test.conllu"", #UD_Ukrainian-IU/uk_iu-ud-train.conllu"", #UD_Upper_Sorbian-UFAL/hsb_ufal-ud-test.conllu"", #UD_Upper_Sorbian-UFAL/hsb_ufal-ud-train.conllu"", #UD_Urdu-UDTB/ur_udtb-ud-dev.conllu"", #UD_Urdu-UDTB/ur_udtb-ud-test.conllu"", #UD_Urdu-UDTB/ur_udtb-ud-train.conllu"", #UD_Uyghur-UDT/ug_udt-ud-dev.conllu"", #UD_Uyghur-UDT/ug_udt-ud-test.conllu"", #UD_Uyghur-UDT/ug_udt-ud-train.conllu"", #UD_Vietnamese-VTB/vi_vtb-ud-dev.conllu"", #UD_Vietnamese-VTB/vi_vtb-ud-test.conllu"", #UD_Vietnamese-VTB/vi_vtb-ud-train.conllu"", #UD_Warlpiri-UFAL/wbp_ufal-ud-test.conllu"", #UD_Yoruba-YTB/yo_ytb-ud-test.conllu"", #{basename}/{sp}""\n')",
https://github.com/hankcs/HanLP,ud23m.py,0,0.0,7,50.0,4,28.57,0,0.0,3,21.43,0,0.0,0,0,4,0,,"UD_23_MULTILINGUAL_DEV, UD_23_MULTILINGUAL_TEST, UD_23_MULTILINGUAL_TRAIN, _UD_23_MULTILINGUAL_HOME, hanlp, os, ud23","hanlp.datasets.parsing.ud.concat_treebanks, os.path.join, ud23._UD_23_HOME",,,"UD_23_MULTILINGUAL_DEV, UD_23_MULTILINGUAL_TEST, UD_23_MULTILINGUAL_TRAIN, _UD_23_MULTILINGUAL_HOME",,"2.3, dev.conllu, test.conllu, train.conllu",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-21 20:39",
https://github.com/hankcs/HanLP,ud27.py,0,0.0,427,33.57,839,65.96,1,0.08,5,0.39,0,0.0,2,0,416,0,,"UD_27_AFRIKAANS_AFRIBOOMS_DEV, UD_27_AFRIKAANS_AFRIBOOMS_TEST, UD_27_AFRIKAANS_AFRIBOOMS_TRAIN, UD_27_AKKADIAN_PISANDUB_TEST, UD_27_AKKADIAN_RIAO_TEST, UD_27_AKUNTSU_TUDET_TEST, UD_27_ALBANIAN_TSA_TEST, UD_27_AMHARIC_ATT_TEST, UD_27_ANCIENT_GREEK_PERSEUS_DEV, UD_27_ANCIENT_GREEK_PERSEUS_TEST, UD_27_ANCIENT_GREEK_PERSEUS_TRAIN, UD_27_ANCIENT_GREEK_PROIEL_DEV, UD_27_ANCIENT_GREEK_PROIEL_TEST, UD_27_ANCIENT_GREEK_PROIEL_TRAIN, UD_27_APURINA_UFPA_TEST, UD_27_ARABIC_NYUAD_DEV, UD_27_ARABIC_NYUAD_TEST, UD_27_ARABIC_NYUAD_TRAIN, UD_27_ARABIC_PADT_DEV, UD_27_ARABIC_PADT_TEST, UD_27_ARABIC_PADT_TRAIN, UD_27_ARABIC_PUD_TEST, UD_27_ARMENIAN_ARMTDP_DEV, UD_27_ARMENIAN_ARMTDP_TEST, UD_27_ARMENIAN_ARMTDP_TRAIN, UD_27_ASSYRIAN_AS_TEST, UD_27_BAMBARA_CRB_TEST, UD_27_BASQUE_BDT_DEV, UD_27_BASQUE_BDT_TEST, UD_27_BASQUE_BDT_TRAIN, UD_27_BELARUSIAN_HSE_DEV, UD_27_BELARUSIAN_HSE_TEST, UD_27_BELARUSIAN_HSE_TRAIN, UD_27_BHOJPURI_BHTB_TEST, UD_27_BRETON_KEB_TEST, UD_27_BULGARIAN_BTB_DEV, UD_27_BULGARIAN_BTB_TEST, UD_27_BULGARIAN_BTB_TRAIN, UD_27_BURYAT_BDT_TEST, UD_27_BURYAT_BDT_TRAIN, UD_27_CANTONESE_HK_TEST, UD_27_CATALAN_ANCORA_DEV, UD_27_CATALAN_ANCORA_TEST, UD_27_CATALAN_ANCORA_TRAIN, UD_27_CHINESE_CFL_TEST, UD_27_CHINESE_GSDSIMP_DEV, UD_27_CHINESE_GSDSIMP_TEST, UD_27_CHINESE_GSDSIMP_TRAIN, UD_27_CHINESE_GSD_DEV, UD_27_CHINESE_GSD_TEST, UD_27_CHINESE_GSD_TRAIN, UD_27_CHINESE_HK_TEST, UD_27_CHINESE_PUD_TEST, UD_27_CHUKCHI_HSE_TEST, UD_27_CLASSICAL_CHINESE_KYOTO_DEV, UD_27_CLASSICAL_CHINESE_KYOTO_TEST, UD_27_CLASSICAL_CHINESE_KYOTO_TRAIN, UD_27_COPTIC_SCRIPTORIUM_DEV, UD_27_COPTIC_SCRIPTORIUM_TEST, UD_27_COPTIC_SCRIPTORIUM_TRAIN, UD_27_CROATIAN_SET_DEV, UD_27_CROATIAN_SET_TEST, UD_27_CROATIAN_SET_TRAIN, UD_27_CZECH_CAC_DEV, UD_27_CZECH_CAC_TEST, UD_27_CZECH_CAC_TRAIN, UD_27_CZECH_CLTT_DEV, UD_27_CZECH_CLTT_TEST, UD_27_CZECH_CLTT_TRAIN, UD_27_CZECH_FICTREE_DEV, UD_27_CZECH_FICTREE_TEST, UD_27_CZECH_FICTREE_TRAIN, UD_27_CZECH_PDT_DEV, UD_27_CZECH_PDT_TEST, UD_27_CZECH_PDT_TRAIN, UD_27_CZECH_PUD_TEST, UD_27_DANISH_DDT_DEV, UD_27_DANISH_DDT_TEST, UD_27_DANISH_DDT_TRAIN, UD_27_DUTCH_ALPINO_DEV, UD_27_DUTCH_ALPINO_TEST, UD_27_DUTCH_ALPINO_TRAIN, UD_27_DUTCH_LASSYSMALL_DEV, UD_27_DUTCH_LASSYSMALL_TEST, UD_27_DUTCH_LASSYSMALL_TRAIN, UD_27_ENGLISH_ESL_DEV, UD_27_ENGLISH_ESL_TEST, UD_27_ENGLISH_ESL_TRAIN, UD_27_ENGLISH_EWT_DEV, UD_27_ENGLISH_EWT_TEST, UD_27_ENGLISH_EWT_TRAIN, UD_27_ENGLISH_GUMREDDIT_DEV, UD_27_ENGLISH_GUMREDDIT_TEST, UD_27_ENGLISH_GUMREDDIT_TRAIN, UD_27_ENGLISH_GUM_DEV, UD_27_ENGLISH_GUM_TEST, UD_27_ENGLISH_GUM_TRAIN, UD_27_ENGLISH_LINES_DEV, UD_27_ENGLISH_LINES_TEST, UD_27_ENGLISH_LINES_TRAIN, UD_27_ENGLISH_PARTUT_DEV, UD_27_ENGLISH_PARTUT_TEST, UD_27_ENGLISH_PARTUT_TRAIN, UD_27_ENGLISH_PRONOUNS_TEST, UD_27_ENGLISH_PUD_TEST, UD_27_ERZYA_JR_TEST, UD_27_ESTONIAN_EDT_DEV, UD_27_ESTONIAN_EDT_TEST, UD_27_ESTONIAN_EDT_TRAIN, UD_27_ESTONIAN_EWT_DEV, UD_27_ESTONIAN_EWT_TEST, UD_27_ESTONIAN_EWT_TRAIN, UD_27_FAROESE_FARPAHC_DEV, UD_27_FAROESE_FARPAHC_TEST, UD_27_FAROESE_FARPAHC_TRAIN, UD_27_FAROESE_OFT_TEST, UD_27_FINNISH_FTB_DEV, UD_27_FINNISH_FTB_TEST, UD_27_FINNISH_FTB_TRAIN, UD_27_FINNISH_OOD_TEST, UD_27_FINNISH_PUD_TEST, UD_27_FINNISH_TDT_DEV, UD_27_FINNISH_TDT_TEST, UD_27_FINNISH_TDT_TRAIN, UD_27_FRENCH_FQB_TEST, UD_27_FRENCH_FTB_DEV, UD_27_FRENCH_FTB_TEST, UD_27_FRENCH_FTB_TRAIN, UD_27_FRENCH_GSD_DEV, UD_27_FRENCH_GSD_TEST, UD_27_FRENCH_GSD_TRAIN, UD_27_FRENCH_PARTUT_DEV, UD_27_FRENCH_PARTUT_TEST, UD_27_FRENCH_PARTUT_TRAIN, UD_27_FRENCH_PUD_TEST, UD_27_FRENCH_SEQUOIA_DEV, UD_27_FRENCH_SEQUOIA_TEST, UD_27_FRENCH_SEQUOIA_TRAIN, UD_27_FRENCH_SPOKEN_DEV, UD_27_FRENCH_SPOKEN_TEST, UD_27_FRENCH_SPOKEN_TRAIN, UD_27_GALICIAN_CTG_DEV, UD_27_GALICIAN_CTG_TEST, UD_27_GALICIAN_CTG_TRAIN, UD_27_GALICIAN_TREEGAL_TEST, UD_27_GALICIAN_TREEGAL_TRAIN, UD_27_GERMAN_GSD_DEV, UD_27_GERMAN_GSD_TEST, UD_27_GERMAN_GSD_TRAIN, UD_27_GERMAN_HDT_DEV, UD_27_GERMAN_HDT_TEST, UD_27_GERMAN_HDT_TRAIN, UD_27_GERMAN_LIT_TEST, UD_27_GERMAN_PUD_TEST, UD_27_GOTHIC_PROIEL_DEV, UD_27_GOTHIC_PROIEL_TEST, UD_27_GOTHIC_PROIEL_TRAIN, UD_27_GREEK_GDT_DEV, UD_27_GREEK_GDT_TEST, UD_27_GREEK_GDT_TRAIN, UD_27_HEBREW_HTB_DEV, UD_27_HEBREW_HTB_TEST, UD_27_HEBREW_HTB_TRAIN, UD_27_HINDI_ENGLISH_HIENCS_DEV, UD_27_HINDI_ENGLISH_HIENCS_TEST, UD_27_HINDI_ENGLISH_HIENCS_TRAIN, UD_27_HINDI_HDTB_DEV, UD_27_HINDI_HDTB_TEST, UD_27_HINDI_HDTB_TRAIN, UD_27_HINDI_PUD_TEST, UD_27_HUNGARIAN_SZEGED_DEV, UD_27_HUNGARIAN_SZEGED_TEST, UD_27_HUNGARIAN_SZEGED_TRAIN, UD_27_ICELANDIC_ICEPAHC_DEV, UD_27_ICELANDIC_ICEPAHC_TEST, UD_27_ICELANDIC_ICEPAHC_TRAIN, UD_27_ICELANDIC_PUD_TEST, UD_27_INDONESIAN_CSUI_TEST, UD_27_INDONESIAN_CSUI_TRAIN, UD_27_INDONESIAN_GSD_DEV, UD_27_INDONESIAN_GSD_TEST, UD_27_INDONESIAN_GSD_TRAIN, UD_27_INDONESIAN_PUD_TEST, UD_27_IRISH_IDT_DEV, UD_27_IRISH_IDT_TEST, UD_27_IRISH_IDT_TRAIN, UD_27_ITALIAN_ISDT_DEV, UD_27_ITALIAN_ISDT_TEST, UD_27_ITALIAN_ISDT_TRAIN, UD_27_ITALIAN_PARTUT_DEV, UD_27_ITALIAN_PARTUT_TEST, UD_27_ITALIAN_PARTUT_TRAIN, UD_27_ITALIAN_POSTWITA_DEV, UD_27_ITALIAN_POSTWITA_TEST, UD_27_ITALIAN_POSTWITA_TRAIN, UD_27_ITALIAN_PUD_TEST, UD_27_ITALIAN_TWITTIRO_DEV, UD_27_ITALIAN_TWITTIRO_TEST, UD_27_ITALIAN_TWITTIRO_TRAIN, UD_27_ITALIAN_VIT_DEV, UD_27_ITALIAN_VIT_TEST, UD_27_ITALIAN_VIT_TRAIN, UD_27_JAPANESE_BCCWJ_DEV, UD_27_JAPANESE_BCCWJ_TEST, UD_27_JAPANESE_BCCWJ_TRAIN, UD_27_JAPANESE_GSD_DEV, UD_27_JAPANESE_GSD_TEST, UD_27_JAPANESE_GSD_TRAIN, UD_27_JAPANESE_MODERN_TEST, UD_27_JAPANESE_PUD_TEST, UD_27_KARELIAN_KKPP_TEST, UD_27_KAZAKH_KTB_TEST, UD_27_KAZAKH_KTB_TRAIN, UD_27_KHUNSARI_AHA_TEST, UD_27_KOMI_PERMYAK_UH_TEST, UD_27_KOMI_ZYRIAN_IKDP_TEST, UD_27_KOMI_ZYRIAN_LATTICE_TEST, UD_27_KOREAN_GSD_DEV, UD_27_KOREAN_GSD_TEST, UD_27_KOREAN_GSD_TRAIN, UD_27_KOREAN_KAIST_DEV, UD_27_KOREAN_KAIST_TEST, UD_27_KOREAN_KAIST_TRAIN, UD_27_KOREAN_PUD_TEST, UD_27_KURMANJI_MG_TEST, UD_27_KURMANJI_MG_TRAIN, UD_27_LATIN_ITTB_DEV, UD_27_LATIN_ITTB_TEST, UD_27_LATIN_ITTB_TRAIN, UD_27_LATIN_LLCT_DEV, UD_27_LATIN_LLCT_TEST, UD_27_LATIN_LLCT_TRAIN, UD_27_LATIN_PERSEUS_TEST, UD_27_LATIN_PERSEUS_TRAIN, UD_27_LATIN_PROIEL_DEV, UD_27_LATIN_PROIEL_TEST, UD_27_LATIN_PROIEL_TRAIN, UD_27_LATVIAN_LVTB_DEV, UD_27_LATVIAN_LVTB_TEST, UD_27_LATVIAN_LVTB_TRAIN, UD_27_LITHUANIAN_ALKSNIS_DEV, UD_27_LITHUANIAN_ALKSNIS_TEST, UD_27_LITHUANIAN_ALKSNIS_TRAIN, UD_27_LITHUANIAN_HSE_DEV, UD_27_LITHUANIAN_HSE_TEST, UD_27_LITHUANIAN_HSE_TRAIN, UD_27_LIVVI_KKPP_TEST, UD_27_LIVVI_KKPP_TRAIN, UD_27_MALTESE_MUDT_DEV, UD_27_MALTESE_MUDT_TEST, UD_27_MALTESE_MUDT_TRAIN, UD_27_MANX_CADHAN_TEST, UD_27_MARATHI_UFAL_DEV, UD_27_MARATHI_UFAL_TEST, UD_27_MARATHI_UFAL_TRAIN, UD_27_MBYA_GUARANI_DOOLEY_TEST, UD_27_MBYA_GUARANI_THOMAS_TEST, UD_27_MOKSHA_JR_TEST, UD_27_MUNDURUKU_TUDET_TEST, UD_27_NAIJA_NSC_DEV, UD_27_NAIJA_NSC_TEST, UD_27_NAIJA_NSC_TRAIN, UD_27_NAYINI_AHA_TEST, UD_27_NORTH_SAMI_GIELLA_TEST, UD_27_NORTH_SAMI_GIELLA_TRAIN, UD_27_NORWEGIAN_BOKMAAL_DEV, UD_27_NORWEGIAN_BOKMAAL_TEST, UD_27_NORWEGIAN_BOKMAAL_TRAIN, UD_27_NORWEGIAN_NYNORSKLIA_DEV, UD_27_NORWEGIAN_NYNORSKLIA_TEST, UD_27_NORWEGIAN_NYNORSKLIA_TRAIN, UD_27_NORWEGIAN_NYNORSK_DEV, UD_27_NORWEGIAN_NYNORSK_TEST, UD_27_NORWEGIAN_NYNORSK_TRAIN, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_27_OLD_FRENCH_SRCMF_DEV, UD_27_OLD_FRENCH_SRCMF_TEST, UD_27_OLD_FRENCH_SRCMF_TRAIN, UD_27_OLD_RUSSIAN_RNC_TEST, UD_27_OLD_RUSSIAN_RNC_TRAIN, UD_27_OLD_RUSSIAN_TOROT_DEV, UD_27_OLD_RUSSIAN_TOROT_TEST, UD_27_OLD_RUSSIAN_TOROT_TRAIN, UD_27_OLD_TURKISH_TONQQ_TEST, UD_27_PERSIAN_PERDT_DEV, UD_27_PERSIAN_PERDT_TEST, UD_27_PERSIAN_PERDT_TRAIN, UD_27_PERSIAN_SERAJI_DEV, UD_27_PERSIAN_SERAJI_TEST, UD_27_PERSIAN_SERAJI_TRAIN, UD_27_POLISH_LFG_DEV, UD_27_POLISH_LFG_TEST, UD_27_POLISH_LFG_TRAIN, UD_27_POLISH_PDB_DEV, UD_27_POLISH_PDB_TEST, UD_27_POLISH_PDB_TRAIN, UD_27_POLISH_PUD_TEST, UD_27_PORTUGUESE_BOSQUE_DEV, UD_27_PORTUGUESE_BOSQUE_TEST, UD_27_PORTUGUESE_BOSQUE_TRAIN, UD_27_PORTUGUESE_GSD_DEV, UD_27_PORTUGUESE_GSD_TEST, UD_27_PORTUGUESE_GSD_TRAIN, UD_27_PORTUGUESE_PUD_TEST, UD_27_ROMANIAN_NONSTANDARD_DEV, UD_27_ROMANIAN_NONSTANDARD_TEST, UD_27_ROMANIAN_NONSTANDARD_TRAIN, UD_27_ROMANIAN_RRT_DEV, UD_27_ROMANIAN_RRT_TEST, UD_27_ROMANIAN_RRT_TRAIN, UD_27_ROMANIAN_SIMONERO_DEV, UD_27_ROMANIAN_SIMONERO_TEST, UD_27_ROMANIAN_SIMONERO_TRAIN, UD_27_RUSSIAN_GSD_DEV, UD_27_RUSSIAN_GSD_TEST, UD_27_RUSSIAN_GSD_TRAIN, UD_27_RUSSIAN_PUD_TEST, UD_27_RUSSIAN_SYNTAGRUS_DEV, UD_27_RUSSIAN_SYNTAGRUS_TEST, UD_27_RUSSIAN_SYNTAGRUS_TRAIN, UD_27_RUSSIAN_TAIGA_DEV, UD_27_RUSSIAN_TAIGA_TEST, UD_27_RUSSIAN_TAIGA_TRAIN, UD_27_SANSKRIT_UFAL_TEST, UD_27_SANSKRIT_VEDIC_TEST, UD_27_SANSKRIT_VEDIC_TRAIN, UD_27_SCOTTISH_GAELIC_ARCOSG_DEV, UD_27_SCOTTISH_GAELIC_ARCOSG_TEST, UD_27_SCOTTISH_GAELIC_ARCOSG_TRAIN, UD_27_SERBIAN_SET_DEV, UD_27_SERBIAN_SET_TEST, UD_27_SERBIAN_SET_TRAIN, UD_27_SKOLT_SAMI_GIELLAGAS_TEST, UD_27_SLOVAK_SNK_DEV, UD_27_SLOVAK_SNK_TEST, UD_27_SLOVAK_SNK_TRAIN, UD_27_SLOVENIAN_SSJ_DEV, UD_27_SLOVENIAN_SSJ_TEST, UD_27_SLOVENIAN_SSJ_TRAIN, UD_27_SLOVENIAN_SST_TEST, UD_27_SLOVENIAN_SST_TRAIN, UD_27_SOI_AHA_TEST, UD_27_SOUTH_LEVANTINE_ARABIC_MADAR_TEST, UD_27_SPANISH_ANCORA_DEV, UD_27_SPANISH_ANCORA_TEST, UD_27_SPANISH_ANCORA_TRAIN, UD_27_SPANISH_GSD_DEV, UD_27_SPANISH_GSD_TEST, UD_27_SPANISH_GSD_TRAIN, UD_27_SPANISH_PUD_TEST, UD_27_SWEDISH_LINES_DEV, UD_27_SWEDISH_LINES_TEST, UD_27_SWEDISH_LINES_TRAIN, UD_27_SWEDISH_PUD_TEST, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_27_SWEDISH_TALBANKEN_DEV, UD_27_SWEDISH_TALBANKEN_TEST, UD_27_SWEDISH_TALBANKEN_TRAIN, UD_27_SWISS_GERMAN_UZH_TEST, UD_27_TAGALOG_TRG_TEST, UD_27_TAGALOG_UGNAYAN_TEST, UD_27_TAMIL_MWTT_TEST, UD_27_TAMIL_TTB_DEV, UD_27_TAMIL_TTB_TEST, UD_27_TAMIL_TTB_TRAIN, UD_27_TELUGU_MTG_DEV, UD_27_TELUGU_MTG_TEST, UD_27_TELUGU_MTG_TRAIN, UD_27_THAI_PUD_TEST, UD_27_TUPINAMBA_TUDET_TEST, UD_27_TURKISH_BOUN_DEV, UD_27_TURKISH_BOUN_TEST, UD_27_TURKISH_BOUN_TRAIN, UD_27_TURKISH_GB_TEST, UD_27_TURKISH_GERMAN_SAGT_DEV, UD_27_TURKISH_GERMAN_SAGT_TEST, UD_27_TURKISH_GERMAN_SAGT_TRAIN, UD_27_TURKISH_IMST_DEV, UD_27_TURKISH_IMST_TEST, UD_27_TURKISH_IMST_TRAIN, UD_27_TURKISH_PUD_TEST, UD_27_UKRAINIAN_IU_DEV, UD_27_UKRAINIAN_IU_TEST, UD_27_UKRAINIAN_IU_TRAIN, UD_27_UPPER_SORBIAN_UFAL_TEST, UD_27_UPPER_SORBIAN_UFAL_TRAIN, UD_27_URDU_UDTB_DEV, UD_27_URDU_UDTB_TEST, UD_27_URDU_UDTB_TRAIN, UD_27_UYGHUR_UDT_DEV, UD_27_UYGHUR_UDT_TEST, UD_27_UYGHUR_UDT_TRAIN, UD_27_VIETNAMESE_VTB_DEV, UD_27_VIETNAMESE_VTB_TEST, UD_27_VIETNAMESE_VTB_TRAIN, UD_27_WARLPIRI_UFAL_TEST, UD_27_WELSH_CCG_TEST, UD_27_WELSH_CCG_TRAIN, UD_27_WOLOF_WTB_DEV, UD_27_WOLOF_WTB_TEST, UD_27_WOLOF_WTB_TRAIN, UD_27_YORUBA_YTB_TEST, _UD_27_HOME, _UD_27_URL, __name__, _list_dir, _path, basename, glob, hanlp, home, len, lstrip, main, open, os, out, prefix, sorted, sp, split, upper, write","glob.glob, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.uncompress, os.path.basename, os.path.isfile, os.path.join, os.rename","_list_dir, main",,"UD_27_AFRIKAANS_AFRIBOOMS_DEV, UD_27_AFRIKAANS_AFRIBOOMS_TEST, UD_27_AFRIKAANS_AFRIBOOMS_TRAIN, UD_27_AKKADIAN_PISANDUB_TEST, UD_27_AKKADIAN_RIAO_TEST, UD_27_AKUNTSU_TUDET_TEST, UD_27_ALBANIAN_TSA_TEST, UD_27_AMHARIC_ATT_TEST, UD_27_ANCIENT_GREEK_PERSEUS_DEV, UD_27_ANCIENT_GREEK_PERSEUS_TEST, UD_27_ANCIENT_GREEK_PERSEUS_TRAIN, UD_27_ANCIENT_GREEK_PROIEL_DEV, UD_27_ANCIENT_GREEK_PROIEL_TEST, UD_27_ANCIENT_GREEK_PROIEL_TRAIN, UD_27_APURINA_UFPA_TEST, UD_27_ARABIC_NYUAD_DEV, UD_27_ARABIC_NYUAD_TEST, UD_27_ARABIC_NYUAD_TRAIN, UD_27_ARABIC_PADT_DEV, UD_27_ARABIC_PADT_TEST, UD_27_ARABIC_PADT_TRAIN, UD_27_ARABIC_PUD_TEST, UD_27_ARMENIAN_ARMTDP_DEV, UD_27_ARMENIAN_ARMTDP_TEST, UD_27_ARMENIAN_ARMTDP_TRAIN, UD_27_ASSYRIAN_AS_TEST, UD_27_BAMBARA_CRB_TEST, UD_27_BASQUE_BDT_DEV, UD_27_BASQUE_BDT_TEST, UD_27_BASQUE_BDT_TRAIN, UD_27_BELARUSIAN_HSE_DEV, UD_27_BELARUSIAN_HSE_TEST, UD_27_BELARUSIAN_HSE_TRAIN, UD_27_BHOJPURI_BHTB_TEST, UD_27_BRETON_KEB_TEST, UD_27_BULGARIAN_BTB_DEV, UD_27_BULGARIAN_BTB_TEST, UD_27_BULGARIAN_BTB_TRAIN, UD_27_BURYAT_BDT_TEST, UD_27_BURYAT_BDT_TRAIN, UD_27_CANTONESE_HK_TEST, UD_27_CATALAN_ANCORA_DEV, UD_27_CATALAN_ANCORA_TEST, UD_27_CATALAN_ANCORA_TRAIN, UD_27_CHINESE_CFL_TEST, UD_27_CHINESE_GSDSIMP_DEV, UD_27_CHINESE_GSDSIMP_TEST, UD_27_CHINESE_GSDSIMP_TRAIN, UD_27_CHINESE_GSD_DEV, UD_27_CHINESE_GSD_TEST, UD_27_CHINESE_GSD_TRAIN, UD_27_CHINESE_HK_TEST, UD_27_CHINESE_PUD_TEST, UD_27_CHUKCHI_HSE_TEST, UD_27_CLASSICAL_CHINESE_KYOTO_DEV, UD_27_CLASSICAL_CHINESE_KYOTO_TEST, UD_27_CLASSICAL_CHINESE_KYOTO_TRAIN, UD_27_COPTIC_SCRIPTORIUM_DEV, UD_27_COPTIC_SCRIPTORIUM_TEST, UD_27_COPTIC_SCRIPTORIUM_TRAIN, UD_27_CROATIAN_SET_DEV, UD_27_CROATIAN_SET_TEST, UD_27_CROATIAN_SET_TRAIN, UD_27_CZECH_CAC_DEV, UD_27_CZECH_CAC_TEST, UD_27_CZECH_CAC_TRAIN, UD_27_CZECH_CLTT_DEV, UD_27_CZECH_CLTT_TEST, UD_27_CZECH_CLTT_TRAIN, UD_27_CZECH_FICTREE_DEV, UD_27_CZECH_FICTREE_TEST, UD_27_CZECH_FICTREE_TRAIN, UD_27_CZECH_PDT_DEV, UD_27_CZECH_PDT_TEST, UD_27_CZECH_PDT_TRAIN, UD_27_CZECH_PUD_TEST, UD_27_DANISH_DDT_DEV, UD_27_DANISH_DDT_TEST, UD_27_DANISH_DDT_TRAIN, UD_27_DUTCH_ALPINO_DEV, UD_27_DUTCH_ALPINO_TEST, UD_27_DUTCH_ALPINO_TRAIN, UD_27_DUTCH_LASSYSMALL_DEV, UD_27_DUTCH_LASSYSMALL_TEST, UD_27_DUTCH_LASSYSMALL_TRAIN, UD_27_ENGLISH_ESL_DEV, UD_27_ENGLISH_ESL_TEST, UD_27_ENGLISH_ESL_TRAIN, UD_27_ENGLISH_EWT_DEV, UD_27_ENGLISH_EWT_TEST, UD_27_ENGLISH_EWT_TRAIN, UD_27_ENGLISH_GUMREDDIT_DEV, UD_27_ENGLISH_GUMREDDIT_TEST, UD_27_ENGLISH_GUMREDDIT_TRAIN, UD_27_ENGLISH_GUM_DEV, UD_27_ENGLISH_GUM_TEST, UD_27_ENGLISH_GUM_TRAIN, UD_27_ENGLISH_LINES_DEV, UD_27_ENGLISH_LINES_TEST, UD_27_ENGLISH_LINES_TRAIN, UD_27_ENGLISH_PARTUT_DEV, UD_27_ENGLISH_PARTUT_TEST, UD_27_ENGLISH_PARTUT_TRAIN, UD_27_ENGLISH_PRONOUNS_TEST, UD_27_ENGLISH_PUD_TEST, UD_27_ERZYA_JR_TEST, UD_27_ESTONIAN_EDT_DEV, UD_27_ESTONIAN_EDT_TEST, UD_27_ESTONIAN_EDT_TRAIN, UD_27_ESTONIAN_EWT_DEV, UD_27_ESTONIAN_EWT_TEST, UD_27_ESTONIAN_EWT_TRAIN, UD_27_FAROESE_FARPAHC_DEV, UD_27_FAROESE_FARPAHC_TEST, UD_27_FAROESE_FARPAHC_TRAIN, UD_27_FAROESE_OFT_TEST, UD_27_FINNISH_FTB_DEV, UD_27_FINNISH_FTB_TEST, UD_27_FINNISH_FTB_TRAIN, UD_27_FINNISH_OOD_TEST, UD_27_FINNISH_PUD_TEST, UD_27_FINNISH_TDT_DEV, UD_27_FINNISH_TDT_TEST, UD_27_FINNISH_TDT_TRAIN, UD_27_FRENCH_FQB_TEST, UD_27_FRENCH_FTB_DEV, UD_27_FRENCH_FTB_TEST, UD_27_FRENCH_FTB_TRAIN, UD_27_FRENCH_GSD_DEV, UD_27_FRENCH_GSD_TEST, UD_27_FRENCH_GSD_TRAIN, UD_27_FRENCH_PARTUT_DEV, UD_27_FRENCH_PARTUT_TEST, UD_27_FRENCH_PARTUT_TRAIN, UD_27_FRENCH_PUD_TEST, UD_27_FRENCH_SEQUOIA_DEV, UD_27_FRENCH_SEQUOIA_TEST, UD_27_FRENCH_SEQUOIA_TRAIN, UD_27_FRENCH_SPOKEN_DEV, UD_27_FRENCH_SPOKEN_TEST, UD_27_FRENCH_SPOKEN_TRAIN, UD_27_GALICIAN_CTG_DEV, UD_27_GALICIAN_CTG_TEST, UD_27_GALICIAN_CTG_TRAIN, UD_27_GALICIAN_TREEGAL_TEST, UD_27_GALICIAN_TREEGAL_TRAIN, UD_27_GERMAN_GSD_DEV, UD_27_GERMAN_GSD_TEST, UD_27_GERMAN_GSD_TRAIN, UD_27_GERMAN_HDT_DEV, UD_27_GERMAN_HDT_TEST, UD_27_GERMAN_HDT_TRAIN, UD_27_GERMAN_LIT_TEST, UD_27_GERMAN_PUD_TEST, UD_27_GOTHIC_PROIEL_DEV, UD_27_GOTHIC_PROIEL_TEST, UD_27_GOTHIC_PROIEL_TRAIN, UD_27_GREEK_GDT_DEV, UD_27_GREEK_GDT_TEST, UD_27_GREEK_GDT_TRAIN, UD_27_HEBREW_HTB_DEV, UD_27_HEBREW_HTB_TEST, UD_27_HEBREW_HTB_TRAIN, UD_27_HINDI_ENGLISH_HIENCS_DEV, UD_27_HINDI_ENGLISH_HIENCS_TEST, UD_27_HINDI_ENGLISH_HIENCS_TRAIN, UD_27_HINDI_HDTB_DEV, UD_27_HINDI_HDTB_TEST, UD_27_HINDI_HDTB_TRAIN, UD_27_HINDI_PUD_TEST, UD_27_HUNGARIAN_SZEGED_DEV, UD_27_HUNGARIAN_SZEGED_TEST, UD_27_HUNGARIAN_SZEGED_TRAIN, UD_27_ICELANDIC_ICEPAHC_DEV, UD_27_ICELANDIC_ICEPAHC_TEST, UD_27_ICELANDIC_ICEPAHC_TRAIN, UD_27_ICELANDIC_PUD_TEST, UD_27_INDONESIAN_CSUI_TEST, UD_27_INDONESIAN_CSUI_TRAIN, UD_27_INDONESIAN_GSD_DEV, UD_27_INDONESIAN_GSD_TEST, UD_27_INDONESIAN_GSD_TRAIN, UD_27_INDONESIAN_PUD_TEST, UD_27_IRISH_IDT_DEV, UD_27_IRISH_IDT_TEST, UD_27_IRISH_IDT_TRAIN, UD_27_ITALIAN_ISDT_DEV, UD_27_ITALIAN_ISDT_TEST, UD_27_ITALIAN_ISDT_TRAIN, UD_27_ITALIAN_PARTUT_DEV, UD_27_ITALIAN_PARTUT_TEST, UD_27_ITALIAN_PARTUT_TRAIN, UD_27_ITALIAN_POSTWITA_DEV, UD_27_ITALIAN_POSTWITA_TEST, UD_27_ITALIAN_POSTWITA_TRAIN, UD_27_ITALIAN_PUD_TEST, UD_27_ITALIAN_TWITTIRO_DEV, UD_27_ITALIAN_TWITTIRO_TEST, UD_27_ITALIAN_TWITTIRO_TRAIN, UD_27_ITALIAN_VIT_DEV, UD_27_ITALIAN_VIT_TEST, UD_27_ITALIAN_VIT_TRAIN, UD_27_JAPANESE_BCCWJ_DEV, UD_27_JAPANESE_BCCWJ_TEST, UD_27_JAPANESE_BCCWJ_TRAIN, UD_27_JAPANESE_GSD_DEV, UD_27_JAPANESE_GSD_TEST, UD_27_JAPANESE_GSD_TRAIN, UD_27_JAPANESE_MODERN_TEST, UD_27_JAPANESE_PUD_TEST, UD_27_KARELIAN_KKPP_TEST, UD_27_KAZAKH_KTB_TEST, UD_27_KAZAKH_KTB_TRAIN, UD_27_KHUNSARI_AHA_TEST, UD_27_KOMI_PERMYAK_UH_TEST, UD_27_KOMI_ZYRIAN_IKDP_TEST, UD_27_KOMI_ZYRIAN_LATTICE_TEST, UD_27_KOREAN_GSD_DEV, UD_27_KOREAN_GSD_TEST, UD_27_KOREAN_GSD_TRAIN, UD_27_KOREAN_KAIST_DEV, UD_27_KOREAN_KAIST_TEST, UD_27_KOREAN_KAIST_TRAIN, UD_27_KOREAN_PUD_TEST, UD_27_KURMANJI_MG_TEST, UD_27_KURMANJI_MG_TRAIN, UD_27_LATIN_ITTB_DEV, UD_27_LATIN_ITTB_TEST, UD_27_LATIN_ITTB_TRAIN, UD_27_LATIN_LLCT_DEV, UD_27_LATIN_LLCT_TEST, UD_27_LATIN_LLCT_TRAIN, UD_27_LATIN_PERSEUS_TEST, UD_27_LATIN_PERSEUS_TRAIN, UD_27_LATIN_PROIEL_DEV, UD_27_LATIN_PROIEL_TEST, UD_27_LATIN_PROIEL_TRAIN, UD_27_LATVIAN_LVTB_DEV, UD_27_LATVIAN_LVTB_TEST, UD_27_LATVIAN_LVTB_TRAIN, UD_27_LITHUANIAN_ALKSNIS_DEV, UD_27_LITHUANIAN_ALKSNIS_TEST, UD_27_LITHUANIAN_ALKSNIS_TRAIN, UD_27_LITHUANIAN_HSE_DEV, UD_27_LITHUANIAN_HSE_TEST, UD_27_LITHUANIAN_HSE_TRAIN, UD_27_LIVVI_KKPP_TEST, UD_27_LIVVI_KKPP_TRAIN, UD_27_MALTESE_MUDT_DEV, UD_27_MALTESE_MUDT_TEST, UD_27_MALTESE_MUDT_TRAIN, UD_27_MANX_CADHAN_TEST, UD_27_MARATHI_UFAL_DEV, UD_27_MARATHI_UFAL_TEST, UD_27_MARATHI_UFAL_TRAIN, UD_27_MBYA_GUARANI_DOOLEY_TEST, UD_27_MBYA_GUARANI_THOMAS_TEST, UD_27_MOKSHA_JR_TEST, UD_27_MUNDURUKU_TUDET_TEST, UD_27_NAIJA_NSC_DEV, UD_27_NAIJA_NSC_TEST, UD_27_NAIJA_NSC_TRAIN, UD_27_NAYINI_AHA_TEST, UD_27_NORTH_SAMI_GIELLA_TEST, UD_27_NORTH_SAMI_GIELLA_TRAIN, UD_27_NORWEGIAN_BOKMAAL_DEV, UD_27_NORWEGIAN_BOKMAAL_TEST, UD_27_NORWEGIAN_BOKMAAL_TRAIN, UD_27_NORWEGIAN_NYNORSKLIA_DEV, UD_27_NORWEGIAN_NYNORSKLIA_TEST, UD_27_NORWEGIAN_NYNORSKLIA_TRAIN, UD_27_NORWEGIAN_NYNORSK_DEV, UD_27_NORWEGIAN_NYNORSK_TEST, UD_27_NORWEGIAN_NYNORSK_TRAIN, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_DEV, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_TEST, UD_27_OLD_CHURCH_SLAVONIC_PROIEL_TRAIN, UD_27_OLD_FRENCH_SRCMF_DEV, UD_27_OLD_FRENCH_SRCMF_TEST, UD_27_OLD_FRENCH_SRCMF_TRAIN, UD_27_OLD_RUSSIAN_RNC_TEST, UD_27_OLD_RUSSIAN_RNC_TRAIN, UD_27_OLD_RUSSIAN_TOROT_DEV, UD_27_OLD_RUSSIAN_TOROT_TEST, UD_27_OLD_RUSSIAN_TOROT_TRAIN, UD_27_OLD_TURKISH_TONQQ_TEST, UD_27_PERSIAN_PERDT_DEV, UD_27_PERSIAN_PERDT_TEST, UD_27_PERSIAN_PERDT_TRAIN, UD_27_PERSIAN_SERAJI_DEV, UD_27_PERSIAN_SERAJI_TEST, UD_27_PERSIAN_SERAJI_TRAIN, UD_27_POLISH_LFG_DEV, UD_27_POLISH_LFG_TEST, UD_27_POLISH_LFG_TRAIN, UD_27_POLISH_PDB_DEV, UD_27_POLISH_PDB_TEST, UD_27_POLISH_PDB_TRAIN, UD_27_POLISH_PUD_TEST, UD_27_PORTUGUESE_BOSQUE_DEV, UD_27_PORTUGUESE_BOSQUE_TEST, UD_27_PORTUGUESE_BOSQUE_TRAIN, UD_27_PORTUGUESE_GSD_DEV, UD_27_PORTUGUESE_GSD_TEST, UD_27_PORTUGUESE_GSD_TRAIN, UD_27_PORTUGUESE_PUD_TEST, UD_27_ROMANIAN_NONSTANDARD_DEV, UD_27_ROMANIAN_NONSTANDARD_TEST, UD_27_ROMANIAN_NONSTANDARD_TRAIN, UD_27_ROMANIAN_RRT_DEV, UD_27_ROMANIAN_RRT_TEST, UD_27_ROMANIAN_RRT_TRAIN, UD_27_ROMANIAN_SIMONERO_DEV, UD_27_ROMANIAN_SIMONERO_TEST, UD_27_ROMANIAN_SIMONERO_TRAIN, UD_27_RUSSIAN_GSD_DEV, UD_27_RUSSIAN_GSD_TEST, UD_27_RUSSIAN_GSD_TRAIN, UD_27_RUSSIAN_PUD_TEST, UD_27_RUSSIAN_SYNTAGRUS_DEV, UD_27_RUSSIAN_SYNTAGRUS_TEST, UD_27_RUSSIAN_SYNTAGRUS_TRAIN, UD_27_RUSSIAN_TAIGA_DEV, UD_27_RUSSIAN_TAIGA_TEST, UD_27_RUSSIAN_TAIGA_TRAIN, UD_27_SANSKRIT_UFAL_TEST, UD_27_SANSKRIT_VEDIC_TEST, UD_27_SANSKRIT_VEDIC_TRAIN, UD_27_SCOTTISH_GAELIC_ARCOSG_DEV, UD_27_SCOTTISH_GAELIC_ARCOSG_TEST, UD_27_SCOTTISH_GAELIC_ARCOSG_TRAIN, UD_27_SERBIAN_SET_DEV, UD_27_SERBIAN_SET_TEST, UD_27_SERBIAN_SET_TRAIN, UD_27_SKOLT_SAMI_GIELLAGAS_TEST, UD_27_SLOVAK_SNK_DEV, UD_27_SLOVAK_SNK_TEST, UD_27_SLOVAK_SNK_TRAIN, UD_27_SLOVENIAN_SSJ_DEV, UD_27_SLOVENIAN_SSJ_TEST, UD_27_SLOVENIAN_SSJ_TRAIN, UD_27_SLOVENIAN_SST_TEST, UD_27_SLOVENIAN_SST_TRAIN, UD_27_SOI_AHA_TEST, UD_27_SOUTH_LEVANTINE_ARABIC_MADAR_TEST, UD_27_SPANISH_ANCORA_DEV, UD_27_SPANISH_ANCORA_TEST, UD_27_SPANISH_ANCORA_TRAIN, UD_27_SPANISH_GSD_DEV, UD_27_SPANISH_GSD_TEST, UD_27_SPANISH_GSD_TRAIN, UD_27_SPANISH_PUD_TEST, UD_27_SWEDISH_LINES_DEV, UD_27_SWEDISH_LINES_TEST, UD_27_SWEDISH_LINES_TRAIN, UD_27_SWEDISH_PUD_TEST, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_DEV, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_TEST, UD_27_SWEDISH_SIGN_LANGUAGE_SSLC_TRAIN, UD_27_SWEDISH_TALBANKEN_DEV, UD_27_SWEDISH_TALBANKEN_TEST, UD_27_SWEDISH_TALBANKEN_TRAIN, UD_27_SWISS_GERMAN_UZH_TEST, UD_27_TAGALOG_TRG_TEST, UD_27_TAGALOG_UGNAYAN_TEST, UD_27_TAMIL_MWTT_TEST, UD_27_TAMIL_TTB_DEV, UD_27_TAMIL_TTB_TEST, UD_27_TAMIL_TTB_TRAIN, UD_27_TELUGU_MTG_DEV, UD_27_TELUGU_MTG_TEST, UD_27_TELUGU_MTG_TRAIN, UD_27_THAI_PUD_TEST, UD_27_TUPINAMBA_TUDET_TEST, UD_27_TURKISH_BOUN_DEV, UD_27_TURKISH_BOUN_TEST, UD_27_TURKISH_BOUN_TRAIN, UD_27_TURKISH_GB_TEST, UD_27_TURKISH_GERMAN_SAGT_DEV, UD_27_TURKISH_GERMAN_SAGT_TEST, UD_27_TURKISH_GERMAN_SAGT_TRAIN, UD_27_TURKISH_IMST_DEV, UD_27_TURKISH_IMST_TEST, UD_27_TURKISH_IMST_TRAIN, UD_27_TURKISH_PUD_TEST, UD_27_UKRAINIAN_IU_DEV, UD_27_UKRAINIAN_IU_TEST, UD_27_UKRAINIAN_IU_TRAIN, UD_27_UPPER_SORBIAN_UFAL_TEST, UD_27_UPPER_SORBIAN_UFAL_TRAIN, UD_27_URDU_UDTB_DEV, UD_27_URDU_UDTB_TEST, UD_27_URDU_UDTB_TRAIN, UD_27_UYGHUR_UDT_DEV, UD_27_UYGHUR_UDT_TEST, UD_27_UYGHUR_UDT_TRAIN, UD_27_VIETNAMESE_VTB_DEV, UD_27_VIETNAMESE_VTB_TEST, UD_27_VIETNAMESE_VTB_TRAIN, UD_27_WARLPIRI_UFAL_TEST, UD_27_WELSH_CCG_TEST, UD_27_WELSH_CCG_TRAIN, UD_27_WOLOF_WTB_DEV, UD_27_WOLOF_WTB_TEST, UD_27_WOLOF_WTB_TRAIN, UD_27_YORUBA_YTB_TEST, _UD_27_HOME, _UD_27_URL, _path, basename, name, out, path, prefix, sp, split",,",  ,  + "",  = ,  set of , "", ""
, #ud-treebanks-v2.7/, -, .""
, .conllu, .zip, /, /*, /ud-treebanks-v2.7/UD_*, UD_, UD_27 dev set of AFRIKAANS_AFRIBOOMS., UD_27 dev set of ANCIENT_GREEK_PERSEUS., UD_27 dev set of ANCIENT_GREEK_PROIEL., UD_27 dev set of ARABIC_NYUAD., UD_27 dev set of ARABIC_PADT., UD_27 dev set of ARMENIAN_ARMTDP., UD_27 dev set of BASQUE_BDT., UD_27 dev set of BELARUSIAN_HSE., UD_27 dev set of BULGARIAN_BTB., UD_27 dev set of CATALAN_ANCORA., UD_27 dev set of CHINESE_GSD., UD_27 dev set of CHINESE_GSDSIMP., UD_27 dev set of CLASSICAL_CHINESE_KYOTO., UD_27 dev set of COPTIC_SCRIPTORIUM., UD_27 dev set of CROATIAN_SET., UD_27 dev set of CZECH_CAC., UD_27 dev set of CZECH_CLTT., UD_27 dev set of CZECH_FICTREE., UD_27 dev set of CZECH_PDT., UD_27 dev set of DANISH_DDT., UD_27 dev set of DUTCH_ALPINO., UD_27 dev set of DUTCH_LASSYSMALL., UD_27 dev set of ENGLISH_ESL., UD_27 dev set of ENGLISH_EWT., UD_27 dev set of ENGLISH_GUM., UD_27 dev set of ENGLISH_GUMREDDIT., UD_27 dev set of ENGLISH_LINES., UD_27 dev set of ENGLISH_PARTUT., UD_27 dev set of ESTONIAN_EDT., UD_27 dev set of ESTONIAN_EWT., UD_27 dev set of FAROESE_FARPAHC., UD_27 dev set of FINNISH_FTB., UD_27 dev set of FINNISH_TDT., UD_27 dev set of FRENCH_FTB., UD_27 dev set of FRENCH_GSD., UD_27 dev set of FRENCH_PARTUT., UD_27 dev set of FRENCH_SEQUOIA., UD_27 dev set of FRENCH_SPOKEN., UD_27 dev set of GALICIAN_CTG., UD_27 dev set of GERMAN_GSD., UD_27 dev set of GERMAN_HDT., UD_27 dev set of GOTHIC_PROIEL., UD_27 dev set of GREEK_GDT., UD_27 dev set of HEBREW_HTB., UD_27 dev set of HINDI_ENGLISH_HIENCS., UD_27 dev set of HINDI_HDTB., UD_27 dev set of HUNGARIAN_SZEGED., UD_27 dev set of ICELANDIC_ICEPAHC., UD_27 dev set of INDONESIAN_GSD., UD_27 dev set of IRISH_IDT., UD_27 dev set of ITALIAN_ISDT., UD_27 dev set of ITALIAN_PARTUT., UD_27 dev set of ITALIAN_POSTWITA., UD_27 dev set of ITALIAN_TWITTIRO., UD_27 dev set of ITALIAN_VIT., UD_27 dev set of JAPANESE_BCCWJ., UD_27 dev set of JAPANESE_GSD., UD_27 dev set of KOREAN_GSD., UD_27 dev set of KOREAN_KAIST., UD_27 dev set of LATIN_ITTB., UD_27 dev set of LATIN_LLCT., UD_27 dev set of LATIN_PROIEL., UD_27 dev set of LATVIAN_LVTB., UD_27 dev set of LITHUANIAN_ALKSNIS., UD_27 dev set of LITHUANIAN_HSE., UD_27 dev set of MALTESE_MUDT., UD_27 dev set of MARATHI_UFAL., UD_27 dev set of NAIJA_NSC., UD_27 dev set of NORWEGIAN_BOKMAAL., UD_27 dev set of NORWEGIAN_NYNORSK., UD_27 dev set of NORWEGIAN_NYNORSKLIA., UD_27 dev set of OLD_CHURCH_SLAVONIC_PROIEL., UD_27 dev set of OLD_FRENCH_SRCMF., UD_27 dev set of OLD_RUSSIAN_TOROT., UD_27 dev set of PERSIAN_PERDT., UD_27 dev set of PERSIAN_SERAJI., UD_27 dev set of POLISH_LFG., UD_27 dev set of POLISH_PDB., UD_27 dev set of PORTUGUESE_BOSQUE., UD_27 dev set of PORTUGUESE_GSD., UD_27 dev set of ROMANIAN_NONSTANDARD., UD_27 dev set of ROMANIAN_RRT., UD_27 dev set of ROMANIAN_SIMONERO., UD_27 dev set of RUSSIAN_GSD., UD_27 dev set of RUSSIAN_SYNTAGRUS., UD_27 dev set of RUSSIAN_TAIGA., UD_27 dev set of SCOTTISH_GAELIC_ARCOSG., UD_27 dev set of SERBIAN_SET., UD_27 dev set of SLOVAK_SNK., UD_27 dev set of SLOVENIAN_SSJ., UD_27 dev set of SPANISH_ANCORA., UD_27 dev set of SPANISH_GSD., UD_27 dev set of SWEDISH_LINES., UD_27 dev set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_27 dev set of SWEDISH_TALBANKEN., UD_27 dev set of TAMIL_TTB., UD_27 dev set of TELUGU_MTG., UD_27 dev set of TURKISH_BOUN., UD_27 dev set of TURKISH_GERMAN_SAGT., UD_27 dev set of TURKISH_IMST., UD_27 dev set of UKRAINIAN_IU., UD_27 dev set of URDU_UDTB., UD_27 dev set of UYGHUR_UDT., UD_27 dev set of VIETNAMESE_VTB., UD_27 dev set of WOLOF_WTB., UD_27 test set of AFRIKAANS_AFRIBOOMS., UD_27 test set of AKKADIAN_PISANDUB., UD_27 test set of AKKADIAN_RIAO., UD_27 test set of AKUNTSU_TUDET., UD_27 test set of ALBANIAN_TSA., UD_27 test set of AMHARIC_ATT., UD_27 test set of ANCIENT_GREEK_PERSEUS., UD_27 test set of ANCIENT_GREEK_PROIEL., UD_27 test set of APURINA_UFPA., UD_27 test set of ARABIC_NYUAD., UD_27 test set of ARABIC_PADT., UD_27 test set of ARABIC_PUD., UD_27 test set of ARMENIAN_ARMTDP., UD_27 test set of ASSYRIAN_AS., UD_27 test set of BAMBARA_CRB., UD_27 test set of BASQUE_BDT., UD_27 test set of BELARUSIAN_HSE., UD_27 test set of BHOJPURI_BHTB., UD_27 test set of BRETON_KEB., UD_27 test set of BULGARIAN_BTB., UD_27 test set of BURYAT_BDT., UD_27 test set of CANTONESE_HK., UD_27 test set of CATALAN_ANCORA., UD_27 test set of CHINESE_CFL., UD_27 test set of CHINESE_GSD., UD_27 test set of CHINESE_GSDSIMP., UD_27 test set of CHINESE_HK., UD_27 test set of CHINESE_PUD., UD_27 test set of CHUKCHI_HSE., UD_27 test set of CLASSICAL_CHINESE_KYOTO., UD_27 test set of COPTIC_SCRIPTORIUM., UD_27 test set of CROATIAN_SET., UD_27 test set of CZECH_CAC., UD_27 test set of CZECH_CLTT., UD_27 test set of CZECH_FICTREE., UD_27 test set of CZECH_PDT., UD_27 test set of CZECH_PUD., UD_27 test set of DANISH_DDT., UD_27 test set of DUTCH_ALPINO., UD_27 test set of DUTCH_LASSYSMALL., UD_27 test set of ENGLISH_ESL., UD_27 test set of ENGLISH_EWT., UD_27 test set of ENGLISH_GUM., UD_27 test set of ENGLISH_GUMREDDIT., UD_27 test set of ENGLISH_LINES., UD_27 test set of ENGLISH_PARTUT., UD_27 test set of ENGLISH_PRONOUNS., UD_27 test set of ENGLISH_PUD., UD_27 test set of ERZYA_JR., UD_27 test set of ESTONIAN_EDT., UD_27 test set of ESTONIAN_EWT., UD_27 test set of FAROESE_FARPAHC., UD_27 test set of FAROESE_OFT., UD_27 test set of FINNISH_FTB., UD_27 test set of FINNISH_OOD., UD_27 test set of FINNISH_PUD., UD_27 test set of FINNISH_TDT., UD_27 test set of FRENCH_FQB., UD_27 test set of FRENCH_FTB., UD_27 test set of FRENCH_GSD., UD_27 test set of FRENCH_PARTUT., UD_27 test set of FRENCH_PUD., UD_27 test set of FRENCH_SEQUOIA., UD_27 test set of FRENCH_SPOKEN., UD_27 test set of GALICIAN_CTG., UD_27 test set of GALICIAN_TREEGAL., UD_27 test set of GERMAN_GSD., UD_27 test set of GERMAN_HDT., UD_27 test set of GERMAN_LIT., UD_27 test set of GERMAN_PUD., UD_27 test set of GOTHIC_PROIEL., UD_27 test set of GREEK_GDT., UD_27 test set of HEBREW_HTB., UD_27 test set of HINDI_ENGLISH_HIENCS., UD_27 test set of HINDI_HDTB., UD_27 test set of HINDI_PUD., UD_27 test set of HUNGARIAN_SZEGED., UD_27 test set of ICELANDIC_ICEPAHC., UD_27 test set of ICELANDIC_PUD., UD_27 test set of INDONESIAN_CSUI., UD_27 test set of INDONESIAN_GSD., UD_27 test set of INDONESIAN_PUD., UD_27 test set of IRISH_IDT., UD_27 test set of ITALIAN_ISDT., UD_27 test set of ITALIAN_PARTUT., UD_27 test set of ITALIAN_POSTWITA., UD_27 test set of ITALIAN_PUD., UD_27 test set of ITALIAN_TWITTIRO., UD_27 test set of ITALIAN_VIT., UD_27 test set of JAPANESE_BCCWJ., UD_27 test set of JAPANESE_GSD., UD_27 test set of JAPANESE_MODERN., UD_27 test set of JAPANESE_PUD., UD_27 test set of KARELIAN_KKPP., UD_27 test set of KAZAKH_KTB., UD_27 test set of KHUNSARI_AHA., UD_27 test set of KOMI_PERMYAK_UH., UD_27 test set of KOMI_ZYRIAN_IKDP., UD_27 test set of KOMI_ZYRIAN_LATTICE., UD_27 test set of KOREAN_GSD., UD_27 test set of KOREAN_KAIST., UD_27 test set of KOREAN_PUD., UD_27 test set of KURMANJI_MG., UD_27 test set of LATIN_ITTB., UD_27 test set of LATIN_LLCT., UD_27 test set of LATIN_PERSEUS., UD_27 test set of LATIN_PROIEL., UD_27 test set of LATVIAN_LVTB., UD_27 test set of LITHUANIAN_ALKSNIS., UD_27 test set of LITHUANIAN_HSE., UD_27 test set of LIVVI_KKPP., UD_27 test set of MALTESE_MUDT., UD_27 test set of MANX_CADHAN., UD_27 test set of MARATHI_UFAL., UD_27 test set of MBYA_GUARANI_DOOLEY., UD_27 test set of MBYA_GUARANI_THOMAS., UD_27 test set of MOKSHA_JR., UD_27 test set of MUNDURUKU_TUDET., UD_27 test set of NAIJA_NSC., UD_27 test set of NAYINI_AHA., UD_27 test set of NORTH_SAMI_GIELLA., UD_27 test set of NORWEGIAN_BOKMAAL., UD_27 test set of NORWEGIAN_NYNORSK., UD_27 test set of NORWEGIAN_NYNORSKLIA., UD_27 test set of OLD_CHURCH_SLAVONIC_PROIEL., UD_27 test set of OLD_FRENCH_SRCMF., UD_27 test set of OLD_RUSSIAN_RNC., UD_27 test set of OLD_RUSSIAN_TOROT., UD_27 test set of OLD_TURKISH_TONQQ., UD_27 test set of PERSIAN_PERDT., UD_27 test set of PERSIAN_SERAJI., UD_27 test set of POLISH_LFG., UD_27 test set of POLISH_PDB., UD_27 test set of POLISH_PUD., UD_27 test set of PORTUGUESE_BOSQUE., UD_27 test set of PORTUGUESE_GSD., UD_27 test set of PORTUGUESE_PUD., UD_27 test set of ROMANIAN_NONSTANDARD., UD_27 test set of ROMANIAN_RRT., UD_27 test set of ROMANIAN_SIMONERO., UD_27 test set of RUSSIAN_GSD., UD_27 test set of RUSSIAN_PUD., UD_27 test set of RUSSIAN_SYNTAGRUS., UD_27 test set of RUSSIAN_TAIGA., UD_27 test set of SANSKRIT_UFAL., UD_27 test set of SANSKRIT_VEDIC., UD_27 test set of SCOTTISH_GAELIC_ARCOSG., UD_27 test set of SERBIAN_SET., UD_27 test set of SKOLT_SAMI_GIELLAGAS., UD_27 test set of SLOVAK_SNK., UD_27 test set of SLOVENIAN_SSJ., UD_27 test set of SLOVENIAN_SST., UD_27 test set of SOI_AHA., UD_27 test set of SOUTH_LEVANTINE_ARABIC_MADAR., UD_27 test set of SPANISH_ANCORA., UD_27 test set of SPANISH_GSD., UD_27 test set of SPANISH_PUD., UD_27 test set of SWEDISH_LINES., UD_27 test set of SWEDISH_PUD., UD_27 test set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_27 test set of SWEDISH_TALBANKEN., UD_27 test set of SWISS_GERMAN_UZH., UD_27 test set of TAGALOG_TRG., UD_27 test set of TAGALOG_UGNAYAN., UD_27 test set of TAMIL_MWTT., UD_27 test set of TAMIL_TTB., UD_27 test set of TELUGU_MTG., UD_27 test set of THAI_PUD., UD_27 test set of TUPINAMBA_TUDET., UD_27 test set of TURKISH_BOUN., UD_27 test set of TURKISH_GB., UD_27 test set of TURKISH_GERMAN_SAGT., UD_27 test set of TURKISH_IMST., UD_27 test set of TURKISH_PUD., UD_27 test set of UKRAINIAN_IU., UD_27 test set of UPPER_SORBIAN_UFAL., UD_27 test set of URDU_UDTB., UD_27 test set of UYGHUR_UDT., UD_27 test set of VIETNAMESE_VTB., UD_27 test set of WARLPIRI_UFAL., UD_27 test set of WELSH_CCG., UD_27 test set of WOLOF_WTB., UD_27 test set of YORUBA_YTB., UD_27 train set of AFRIKAANS_AFRIBOOMS., UD_27 train set of ANCIENT_GREEK_PERSEUS., UD_27 train set of ANCIENT_GREEK_PROIEL., UD_27 train set of ARABIC_NYUAD., UD_27 train set of ARABIC_PADT., UD_27 train set of ARMENIAN_ARMTDP., UD_27 train set of BASQUE_BDT., UD_27 train set of BELARUSIAN_HSE., UD_27 train set of BULGARIAN_BTB., UD_27 train set of BURYAT_BDT., UD_27 train set of CATALAN_ANCORA., UD_27 train set of CHINESE_GSD., UD_27 train set of CHINESE_GSDSIMP., UD_27 train set of CLASSICAL_CHINESE_KYOTO., UD_27 train set of COPTIC_SCRIPTORIUM., UD_27 train set of CROATIAN_SET., UD_27 train set of CZECH_CAC., UD_27 train set of CZECH_CLTT., UD_27 train set of CZECH_FICTREE., UD_27 train set of CZECH_PDT., UD_27 train set of DANISH_DDT., UD_27 train set of DUTCH_ALPINO., UD_27 train set of DUTCH_LASSYSMALL., UD_27 train set of ENGLISH_ESL., UD_27 train set of ENGLISH_EWT., UD_27 train set of ENGLISH_GUM., UD_27 train set of ENGLISH_GUMREDDIT., UD_27 train set of ENGLISH_LINES., UD_27 train set of ENGLISH_PARTUT., UD_27 train set of ESTONIAN_EDT., UD_27 train set of ESTONIAN_EWT., UD_27 train set of FAROESE_FARPAHC., UD_27 train set of FINNISH_FTB., UD_27 train set of FINNISH_TDT., UD_27 train set of FRENCH_FTB., UD_27 train set of FRENCH_GSD., UD_27 train set of FRENCH_PARTUT., UD_27 train set of FRENCH_SEQUOIA., UD_27 train set of FRENCH_SPOKEN., UD_27 train set of GALICIAN_CTG., UD_27 train set of GALICIAN_TREEGAL., UD_27 train set of GERMAN_GSD., UD_27 train set of GERMAN_HDT., UD_27 train set of GOTHIC_PROIEL., UD_27 train set of GREEK_GDT., UD_27 train set of HEBREW_HTB., UD_27 train set of HINDI_ENGLISH_HIENCS., UD_27 train set of HINDI_HDTB., UD_27 train set of HUNGARIAN_SZEGED., UD_27 train set of ICELANDIC_ICEPAHC., UD_27 train set of INDONESIAN_CSUI., UD_27 train set of INDONESIAN_GSD., UD_27 train set of IRISH_IDT., UD_27 train set of ITALIAN_ISDT., UD_27 train set of ITALIAN_PARTUT., UD_27 train set of ITALIAN_POSTWITA., UD_27 train set of ITALIAN_TWITTIRO., UD_27 train set of ITALIAN_VIT., UD_27 train set of JAPANESE_BCCWJ., UD_27 train set of JAPANESE_GSD., UD_27 train set of KAZAKH_KTB., UD_27 train set of KOREAN_GSD., UD_27 train set of KOREAN_KAIST., UD_27 train set of KURMANJI_MG., UD_27 train set of LATIN_ITTB., UD_27 train set of LATIN_LLCT., UD_27 train set of LATIN_PERSEUS., UD_27 train set of LATIN_PROIEL., UD_27 train set of LATVIAN_LVTB., UD_27 train set of LITHUANIAN_ALKSNIS., UD_27 train set of LITHUANIAN_HSE., UD_27 train set of LIVVI_KKPP., UD_27 train set of MALTESE_MUDT., UD_27 train set of MARATHI_UFAL., UD_27 train set of NAIJA_NSC., UD_27 train set of NORTH_SAMI_GIELLA., UD_27 train set of NORWEGIAN_BOKMAAL., UD_27 train set of NORWEGIAN_NYNORSK., UD_27 train set of NORWEGIAN_NYNORSKLIA., UD_27 train set of OLD_CHURCH_SLAVONIC_PROIEL., UD_27 train set of OLD_FRENCH_SRCMF., UD_27 train set of OLD_RUSSIAN_RNC., UD_27 train set of OLD_RUSSIAN_TOROT., UD_27 train set of PERSIAN_PERDT., UD_27 train set of PERSIAN_SERAJI., UD_27 train set of POLISH_LFG., UD_27 train set of POLISH_PDB., UD_27 train set of PORTUGUESE_BOSQUE., UD_27 train set of PORTUGUESE_GSD., UD_27 train set of ROMANIAN_NONSTANDARD., UD_27 train set of ROMANIAN_RRT., UD_27 train set of ROMANIAN_SIMONERO., UD_27 train set of RUSSIAN_GSD., UD_27 train set of RUSSIAN_SYNTAGRUS., UD_27 train set of RUSSIAN_TAIGA., UD_27 train set of SANSKRIT_VEDIC., UD_27 train set of SCOTTISH_GAELIC_ARCOSG., UD_27 train set of SERBIAN_SET., UD_27 train set of SLOVAK_SNK., UD_27 train set of SLOVENIAN_SSJ., UD_27 train set of SLOVENIAN_SST., UD_27 train set of SPANISH_ANCORA., UD_27 train set of SPANISH_GSD., UD_27 train set of SWEDISH_LINES., UD_27 train set of SWEDISH_SIGN_LANGUAGE_SSLC., UD_27 train set of SWEDISH_TALBANKEN., UD_27 train set of TAMIL_TTB., UD_27 train set of TELUGU_MTG., UD_27 train set of TURKISH_BOUN., UD_27 train set of TURKISH_GERMAN_SAGT., UD_27 train set of TURKISH_IMST., UD_27 train set of UKRAINIAN_IU., UD_27 train set of UPPER_SORBIAN_UFAL., UD_27 train set of URDU_UDTB., UD_27 train set of UYGHUR_UDT., UD_27 train set of VIETNAMESE_VTB., UD_27 train set of WELSH_CCG., UD_27 train set of WOLOF_WTB., UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu, UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu, UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu, UD_Akkadian-PISANDUB/akk_pisandub-ud-test.conllu, UD_Akkadian-RIAO/akk_riao-ud-test.conllu, UD_Akuntsu-TuDeT/aqz_tudet-ud-test.conllu, UD_Albanian-TSA/sq_tsa-ud-test.conllu, UD_Amharic-ATT/am_att-ud-test.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-dev.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-test.conllu, UD_Ancient_Greek-PROIEL/grc_proiel-ud-train.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-dev.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-test.conllu, UD_Ancient_Greek-Perseus/grc_perseus-ud-train.conllu, UD_Apurina-UFPA/apu_ufpa-ud-test.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-dev.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-test.conllu, UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu, UD_Arabic-PADT/ar_padt-ud-dev.conllu, UD_Arabic-PADT/ar_padt-ud-test.conllu, UD_Arabic-PADT/ar_padt-ud-train.conllu, UD_Arabic-PUD/ar_pud-ud-test.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-dev.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-test.conllu, UD_Armenian-ArmTDP/hy_armtdp-ud-train.conllu, UD_Assyrian-AS/aii_as-ud-test.conllu, UD_Bambara-CRB/bm_crb-ud-test.conllu, UD_Basque-BDT/eu_bdt-ud-dev.conllu, UD_Basque-BDT/eu_bdt-ud-test.conllu, UD_Basque-BDT/eu_bdt-ud-train.conllu, UD_Belarusian-HSE/be_hse-ud-dev.conllu, UD_Belarusian-HSE/be_hse-ud-test.conllu, UD_Belarusian-HSE/be_hse-ud-train.conllu, UD_Bhojpuri-BHTB/bho_bhtb-ud-test.conllu, UD_Breton-KEB/br_keb-ud-test.conllu, UD_Bulgarian-BTB/bg_btb-ud-dev.conllu, UD_Bulgarian-BTB/bg_btb-ud-test.conllu, UD_Bulgarian-BTB/bg_btb-ud-train.conllu, UD_Buryat-BDT/bxr_bdt-ud-test.conllu, UD_Buryat-BDT/bxr_bdt-ud-train.conllu, UD_Cantonese-HK/yue_hk-ud-test.conllu, UD_Catalan-AnCora/ca_ancora-ud-dev.conllu, UD_Catalan-AnCora/ca_ancora-ud-test.conllu, UD_Catalan-AnCora/ca_ancora-ud-train.conllu, UD_Chinese-CFL/zh_cfl-ud-test.conllu, UD_Chinese-GSD/zh_gsd-ud-dev.conllu, UD_Chinese-GSD/zh_gsd-ud-test.conllu, UD_Chinese-GSD/zh_gsd-ud-train.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-dev.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-test.conllu, UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu, UD_Chinese-HK/zh_hk-ud-test.conllu, UD_Chinese-PUD/zh_pud-ud-test.conllu, UD_Chukchi-HSE/ckt_hse-ud-test.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-dev.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-test.conllu, UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-train.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-dev.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-test.conllu, UD_Coptic-Scriptorium/cop_scriptorium-ud-train.conllu, UD_Croatian-SET/hr_set-ud-dev.conllu, UD_Croatian-SET/hr_set-ud-test.conllu, UD_Croatian-SET/hr_set-ud-train.conllu, UD_Czech-CAC/cs_cac-ud-dev.conllu, UD_Czech-CAC/cs_cac-ud-test.conllu, UD_Czech-CAC/cs_cac-ud-train.conllu, UD_Czech-CLTT/cs_cltt-ud-dev.conllu, UD_Czech-CLTT/cs_cltt-ud-test.conllu, UD_Czech-CLTT/cs_cltt-ud-train.conllu, UD_Czech-FicTree/cs_fictree-ud-dev.conllu, UD_Czech-FicTree/cs_fictree-ud-test.conllu, UD_Czech-FicTree/cs_fictree-ud-train.conllu, UD_Czech-PDT/cs_pdt-ud-dev.conllu, UD_Czech-PDT/cs_pdt-ud-test.conllu, UD_Czech-PDT/cs_pdt-ud-train.conllu, UD_Czech-PUD/cs_pud-ud-test.conllu, UD_Danish-DDT/da_ddt-ud-dev.conllu, UD_Danish-DDT/da_ddt-ud-test.conllu, UD_Danish-DDT/da_ddt-ud-train.conllu, UD_Dutch-Alpino/nl_alpino-ud-dev.conllu, UD_Dutch-Alpino/nl_alpino-ud-test.conllu, UD_Dutch-Alpino/nl_alpino-ud-train.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-dev.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-test.conllu, UD_Dutch-LassySmall/nl_lassysmall-ud-train.conllu, UD_English-ESL/en_esl-ud-dev.conllu, UD_English-ESL/en_esl-ud-test.conllu, UD_English-ESL/en_esl-ud-train.conllu, UD_English-EWT/en_ewt-ud-dev.conllu, UD_English-EWT/en_ewt-ud-test.conllu, UD_English-EWT/en_ewt-ud-train.conllu, UD_English-GUM/en_gum-ud-dev.conllu, UD_English-GUM/en_gum-ud-test.conllu, UD_English-GUM/en_gum-ud-train.conllu, UD_English-GUMReddit/en_gumreddit-ud-dev.conllu, UD_English-GUMReddit/en_gumreddit-ud-test.conllu, UD_English-GUMReddit/en_gumreddit-ud-train.conllu, UD_English-LinES/en_lines-ud-dev.conllu, UD_English-LinES/en_lines-ud-test.conllu, UD_English-LinES/en_lines-ud-train.conllu, UD_English-PUD/en_pud-ud-test.conllu, UD_English-ParTUT/en_partut-ud-dev.conllu, UD_English-ParTUT/en_partut-ud-test.conllu, UD_English-ParTUT/en_partut-ud-train.conllu, UD_English-Pronouns/en_pronouns-ud-test.conllu, UD_Erzya-JR/myv_jr-ud-test.conllu, UD_Estonian-EDT/et_edt-ud-dev.conllu, UD_Estonian-EDT/et_edt-ud-test.conllu, UD_Estonian-EDT/et_edt-ud-train.conllu, UD_Estonian-EWT/et_ewt-ud-dev.conllu, UD_Estonian-EWT/et_ewt-ud-test.conllu, UD_Estonian-EWT/et_ewt-ud-train.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-dev.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-test.conllu, UD_Faroese-FarPaHC/fo_farpahc-ud-train.conllu, UD_Faroese-OFT/fo_oft-ud-test.conllu, UD_Finnish-FTB/fi_ftb-ud-dev.conllu, UD_Finnish-FTB/fi_ftb-ud-test.conllu, UD_Finnish-FTB/fi_ftb-ud-train.conllu, UD_Finnish-OOD/fi_ood-ud-test.conllu, UD_Finnish-PUD/fi_pud-ud-test.conllu, UD_Finnish-TDT/fi_tdt-ud-dev.conllu, UD_Finnish-TDT/fi_tdt-ud-test.conllu, UD_Finnish-TDT/fi_tdt-ud-train.conllu, UD_French-FQB/fr_fqb-ud-test.conllu, UD_French-FTB/fr_ftb-ud-dev.conllu, UD_French-FTB/fr_ftb-ud-test.conllu, UD_French-FTB/fr_ftb-ud-train.conllu, UD_French-GSD/fr_gsd-ud-dev.conllu, UD_French-GSD/fr_gsd-ud-test.conllu, UD_French-GSD/fr_gsd-ud-train.conllu, UD_French-PUD/fr_pud-ud-test.conllu, UD_French-ParTUT/fr_partut-ud-dev.conllu, UD_French-ParTUT/fr_partut-ud-test.conllu, UD_French-ParTUT/fr_partut-ud-train.conllu, UD_French-Sequoia/fr_sequoia-ud-dev.conllu, UD_French-Sequoia/fr_sequoia-ud-test.conllu, UD_French-Sequoia/fr_sequoia-ud-train.conllu, UD_French-Spoken/fr_spoken-ud-dev.conllu, UD_French-Spoken/fr_spoken-ud-test.conllu, UD_French-Spoken/fr_spoken-ud-train.conllu, UD_Galician-CTG/gl_ctg-ud-dev.conllu, UD_Galician-CTG/gl_ctg-ud-test.conllu, UD_Galician-CTG/gl_ctg-ud-train.conllu, UD_Galician-TreeGal/gl_treegal-ud-test.conllu, UD_Galician-TreeGal/gl_treegal-ud-train.conllu, UD_German-GSD/de_gsd-ud-dev.conllu, UD_German-GSD/de_gsd-ud-test.conllu, UD_German-GSD/de_gsd-ud-train.conllu, UD_German-HDT/de_hdt-ud-dev.conllu, UD_German-HDT/de_hdt-ud-test.conllu, UD_German-HDT/de_hdt-ud-train.conllu, UD_German-LIT/de_lit-ud-test.conllu, UD_German-PUD/de_pud-ud-test.conllu, UD_Gothic-PROIEL/got_proiel-ud-dev.conllu, UD_Gothic-PROIEL/got_proiel-ud-test.conllu, UD_Gothic-PROIEL/got_proiel-ud-train.conllu, UD_Greek-GDT/el_gdt-ud-dev.conllu, UD_Greek-GDT/el_gdt-ud-test.conllu, UD_Greek-GDT/el_gdt-ud-train.conllu, UD_Hebrew-HTB/he_htb-ud-dev.conllu, UD_Hebrew-HTB/he_htb-ud-test.conllu, UD_Hebrew-HTB/he_htb-ud-train.conllu, UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu, UD_Hindi-HDTB/hi_hdtb-ud-test.conllu, UD_Hindi-HDTB/hi_hdtb-ud-train.conllu, UD_Hindi-PUD/hi_pud-ud-test.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-dev.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-test.conllu, UD_Hindi_English-HIENCS/qhe_hiencs-ud-train.conllu, UD_Hungarian-Szeged/hu_szeged-ud-dev.conllu, UD_Hungarian-Szeged/hu_szeged-ud-test.conllu, UD_Hungarian-Szeged/hu_szeged-ud-train.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-dev.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-test.conllu, UD_Icelandic-IcePaHC/is_icepahc-ud-train.conllu, UD_Icelandic-PUD/is_pud-ud-test.conllu, UD_Indonesian-CSUI/id_csui-ud-test.conllu, UD_Indonesian-CSUI/id_csui-ud-train.conllu, UD_Indonesian-GSD/id_gsd-ud-dev.conllu, UD_Indonesian-GSD/id_gsd-ud-test.conllu, UD_Indonesian-GSD/id_gsd-ud-train.conllu, UD_Indonesian-PUD/id_pud-ud-test.conllu, UD_Irish-IDT/ga_idt-ud-dev.conllu, UD_Irish-IDT/ga_idt-ud-test.conllu, UD_Irish-IDT/ga_idt-ud-train.conllu, UD_Italian-ISDT/it_isdt-ud-dev.conllu, UD_Italian-ISDT/it_isdt-ud-test.conllu, UD_Italian-ISDT/it_isdt-ud-train.conllu, UD_Italian-PUD/it_pud-ud-test.conllu, UD_Italian-ParTUT/it_partut-ud-dev.conllu, UD_Italian-ParTUT/it_partut-ud-test.conllu, UD_Italian-ParTUT/it_partut-ud-train.conllu, UD_Italian-PoSTWITA/it_postwita-ud-dev.conllu, UD_Italian-PoSTWITA/it_postwita-ud-test.conllu, UD_Italian-PoSTWITA/it_postwita-ud-train.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-dev.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-test.conllu, UD_Italian-TWITTIRO/it_twittiro-ud-train.conllu, UD_Italian-VIT/it_vit-ud-dev.conllu, UD_Italian-VIT/it_vit-ud-test.conllu, UD_Italian-VIT/it_vit-ud-train.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-dev.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-test.conllu, UD_Japanese-BCCWJ/ja_bccwj-ud-train.conllu, UD_Japanese-GSD/ja_gsd-ud-dev.conllu, UD_Japanese-GSD/ja_gsd-ud-test.conllu, UD_Japanese-GSD/ja_gsd-ud-train.conllu, UD_Japanese-Modern/ja_modern-ud-test.conllu, UD_Japanese-PUD/ja_pud-ud-test.conllu, UD_Karelian-KKPP/krl_kkpp-ud-test.conllu, UD_Kazakh-KTB/kk_ktb-ud-test.conllu, UD_Kazakh-KTB/kk_ktb-ud-train.conllu, UD_Khunsari-AHA/kfm_aha-ud-test.conllu, UD_Komi_Permyak-UH/koi_uh-ud-test.conllu, UD_Komi_Zyrian-IKDP/kpv_ikdp-ud-test.conllu, UD_Komi_Zyrian-Lattice/kpv_lattice-ud-test.conllu, UD_Korean-GSD/ko_gsd-ud-dev.conllu, UD_Korean-GSD/ko_gsd-ud-test.conllu, UD_Korean-GSD/ko_gsd-ud-train.conllu, UD_Korean-Kaist/ko_kaist-ud-dev.conllu, UD_Korean-Kaist/ko_kaist-ud-test.conllu, UD_Korean-Kaist/ko_kaist-ud-train.conllu, UD_Korean-PUD/ko_pud-ud-test.conllu, UD_Kurmanji-MG/kmr_mg-ud-test.conllu, UD_Kurmanji-MG/kmr_mg-ud-train.conllu, UD_Latin-ITTB/la_ittb-ud-dev.conllu, UD_Latin-ITTB/la_ittb-ud-test.conllu, UD_Latin-ITTB/la_ittb-ud-train.conllu, UD_Latin-LLCT/la_llct-ud-dev.conllu, UD_Latin-LLCT/la_llct-ud-test.conllu, UD_Latin-LLCT/la_llct-ud-train.conllu, UD_Latin-PROIEL/la_proiel-ud-dev.conllu, UD_Latin-PROIEL/la_proiel-ud-test.conllu, UD_Latin-PROIEL/la_proiel-ud-train.conllu, UD_Latin-Perseus/la_perseus-ud-test.conllu, UD_Latin-Perseus/la_perseus-ud-train.conllu, UD_Latvian-LVTB/lv_lvtb-ud-dev.conllu, UD_Latvian-LVTB/lv_lvtb-ud-test.conllu, UD_Latvian-LVTB/lv_lvtb-ud-train.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-dev.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-test.conllu, UD_Lithuanian-ALKSNIS/lt_alksnis-ud-train.conllu, UD_Lithuanian-HSE/lt_hse-ud-dev.conllu, UD_Lithuanian-HSE/lt_hse-ud-test.conllu, UD_Lithuanian-HSE/lt_hse-ud-train.conllu, UD_Livvi-KKPP/olo_kkpp-ud-test.conllu, UD_Livvi-KKPP/olo_kkpp-ud-train.conllu, UD_Maltese-MUDT/mt_mudt-ud-dev.conllu, UD_Maltese-MUDT/mt_mudt-ud-test.conllu, UD_Maltese-MUDT/mt_mudt-ud-train.conllu, UD_Manx-Cadhan/gv_cadhan-ud-test.conllu, UD_Marathi-UFAL/mr_ufal-ud-dev.conllu, UD_Marathi-UFAL/mr_ufal-ud-test.conllu, UD_Marathi-UFAL/mr_ufal-ud-train.conllu, UD_Mbya_Guarani-Dooley/gun_dooley-ud-test.conllu, UD_Mbya_Guarani-Thomas/gun_thomas-ud-test.conllu, UD_Moksha-JR/mdf_jr-ud-test.conllu, UD_Munduruku-TuDeT/myu_tudet-ud-test.conllu, UD_Naija-NSC/pcm_nsc-ud-dev.conllu, UD_Naija-NSC/pcm_nsc-ud-test.conllu, UD_Naija-NSC/pcm_nsc-ud-train.conllu, UD_Nayini-AHA/nyq_aha-ud-test.conllu, UD_North_Sami-Giella/sme_giella-ud-test.conllu, UD_North_Sami-Giella/sme_giella-ud-train.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-dev.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-test.conllu, UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-dev.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-test.conllu, UD_Norwegian-Nynorsk/no_nynorsk-ud-train.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-dev.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-test.conllu, UD_Norwegian-NynorskLIA/no_nynorsklia-ud-train.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-dev.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-test.conllu, UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-train.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-dev.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-test.conllu, UD_Old_French-SRCMF/fro_srcmf-ud-train.conllu, UD_Old_Russian-RNC/orv_rnc-ud-test.conllu, UD_Old_Russian-RNC/orv_rnc-ud-train.conllu, UD_Old_Russian-TOROT/orv_torot-ud-dev.conllu, UD_Old_Russian-TOROT/orv_torot-ud-test.conllu, UD_Old_Russian-TOROT/orv_torot-ud-train.conllu, UD_Old_Turkish-Tonqq/otk_tonqq-ud-test.conllu, UD_Persian-PerDT/fa_perdt-ud-dev.conllu, UD_Persian-PerDT/fa_perdt-ud-test.conllu, UD_Persian-PerDT/fa_perdt-ud-train.conllu, UD_Persian-Seraji/fa_seraji-ud-dev.conllu, UD_Persian-Seraji/fa_seraji-ud-test.conllu, UD_Persian-Seraji/fa_seraji-ud-train.conllu, UD_Polish-LFG/pl_lfg-ud-dev.conllu, UD_Polish-LFG/pl_lfg-ud-test.conllu, UD_Polish-LFG/pl_lfg-ud-train.conllu, UD_Polish-PDB/pl_pdb-ud-dev.conllu, UD_Polish-PDB/pl_pdb-ud-test.conllu, UD_Polish-PDB/pl_pdb-ud-train.conllu, UD_Polish-PUD/pl_pud-ud-test.conllu, UD_Portuguese-Bosque/pt_bosque-ud-dev.conllu, UD_Portuguese-Bosque/pt_bosque-ud-test.conllu, UD_Portuguese-Bosque/pt_bosque-ud-train.conllu, UD_Portuguese-GSD/pt_gsd-ud-dev.conllu, UD_Portuguese-GSD/pt_gsd-ud-test.conllu, UD_Portuguese-GSD/pt_gsd-ud-train.conllu, UD_Portuguese-PUD/pt_pud-ud-test.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-dev.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-test.conllu, UD_Romanian-Nonstandard/ro_nonstandard-ud-train.conllu, UD_Romanian-RRT/ro_rrt-ud-dev.conllu, UD_Romanian-RRT/ro_rrt-ud-test.conllu, UD_Romanian-RRT/ro_rrt-ud-train.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-dev.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-test.conllu, UD_Romanian-SiMoNERo/ro_simonero-ud-train.conllu, UD_Russian-GSD/ru_gsd-ud-dev.conllu, UD_Russian-GSD/ru_gsd-ud-test.conllu, UD_Russian-GSD/ru_gsd-ud-train.conllu, UD_Russian-PUD/ru_pud-ud-test.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu, UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu, UD_Russian-Taiga/ru_taiga-ud-dev.conllu, UD_Russian-Taiga/ru_taiga-ud-test.conllu, UD_Russian-Taiga/ru_taiga-ud-train.conllu, UD_Sanskrit-UFAL/sa_ufal-ud-test.conllu, UD_Sanskrit-Vedic/sa_vedic-ud-test.conllu, UD_Sanskrit-Vedic/sa_vedic-ud-train.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-dev.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-test.conllu, UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-train.conllu, UD_Serbian-SET/sr_set-ud-dev.conllu, UD_Serbian-SET/sr_set-ud-test.conllu, UD_Serbian-SET/sr_set-ud-train.conllu, UD_Skolt_Sami-Giellagas/sms_giellagas-ud-test.conllu, UD_Slovak-SNK/sk_snk-ud-dev.conllu, UD_Slovak-SNK/sk_snk-ud-test.conllu, UD_Slovak-SNK/sk_snk-ud-train.conllu, UD_Slovenian-SSJ/sl_ssj-ud-dev.conllu, UD_Slovenian-SSJ/sl_ssj-ud-test.conllu, UD_Slovenian-SSJ/sl_ssj-ud-train.conllu, UD_Slovenian-SST/sl_sst-ud-test.conllu, UD_Slovenian-SST/sl_sst-ud-train.conllu, UD_Soi-AHA/soj_aha-ud-test.conllu, UD_South_Levantine_Arabic-MADAR/ajp_madar-ud-test.conllu, UD_Spanish-AnCora/es_ancora-ud-dev.conllu, UD_Spanish-AnCora/es_ancora-ud-test.conllu, UD_Spanish-AnCora/es_ancora-ud-train.conllu, UD_Spanish-GSD/es_gsd-ud-dev.conllu, UD_Spanish-GSD/es_gsd-ud-test.conllu, UD_Spanish-GSD/es_gsd-ud-train.conllu, UD_Spanish-PUD/es_pud-ud-test.conllu, UD_Swedish-LinES/sv_lines-ud-dev.conllu, UD_Swedish-LinES/sv_lines-ud-test.conllu, UD_Swedish-LinES/sv_lines-ud-train.conllu, UD_Swedish-PUD/sv_pud-ud-test.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-dev.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-test.conllu, UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-dev.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-test.conllu, UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-train.conllu, UD_Swiss_German-UZH/gsw_uzh-ud-test.conllu, UD_Tagalog-TRG/tl_trg-ud-test.conllu, UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu, UD_Tamil-MWTT/ta_mwtt-ud-test.conllu, UD_Tamil-TTB/ta_ttb-ud-dev.conllu, UD_Tamil-TTB/ta_ttb-ud-test.conllu, UD_Tamil-TTB/ta_ttb-ud-train.conllu, UD_Telugu-MTG/te_mtg-ud-dev.conllu, UD_Telugu-MTG/te_mtg-ud-test.conllu, UD_Telugu-MTG/te_mtg-ud-train.conllu, UD_Thai-PUD/th_pud-ud-test.conllu, UD_Tupinamba-TuDeT/tpn_tudet-ud-test.conllu, UD_Turkish-BOUN/tr_boun-ud-dev.conllu, UD_Turkish-BOUN/tr_boun-ud-test.conllu, UD_Turkish-BOUN/tr_boun-ud-train.conllu, UD_Turkish-GB/tr_gb-ud-test.conllu, UD_Turkish-IMST/tr_imst-ud-dev.conllu, UD_Turkish-IMST/tr_imst-ud-test.conllu, UD_Turkish-IMST/tr_imst-ud-train.conllu, UD_Turkish-PUD/tr_pud-ud-test.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-dev.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-test.conllu, UD_Turkish_German-SAGT/qtd_sagt-ud-train.conllu, UD_Ukrainian-IU/uk_iu-ud-dev.conllu, UD_Ukrainian-IU/uk_iu-ud-test.conllu, UD_Ukrainian-IU/uk_iu-ud-train.conllu, UD_Upper_Sorbian-UFAL/hsb_ufal-ud-test.conllu, UD_Upper_Sorbian-UFAL/hsb_ufal-ud-train.conllu, UD_Urdu-UDTB/ur_udtb-ud-dev.conllu, UD_Urdu-UDTB/ur_udtb-ud-test.conllu, UD_Urdu-UDTB/ur_udtb-ud-train.conllu, UD_Uyghur-UDT/ug_udt-ud-dev.conllu, UD_Uyghur-UDT/ug_udt-ud-test.conllu, UD_Uyghur-UDT/ug_udt-ud-train.conllu, UD_Vietnamese-VTB/vi_vtb-ud-dev.conllu, UD_Vietnamese-VTB/vi_vtb-ud-test.conllu, UD_Vietnamese-VTB/vi_vtb-ud-train.conllu, UD_Warlpiri-UFAL/wbp_ufal-ud-test.conllu, UD_Welsh-CCG/cy_ccg-ud-test.conllu, UD_Welsh-CCG/cy_ccg-ud-train.conllu, UD_Wolof-WTB/wo_wtb-ud-dev.conllu, UD_Wolof-WTB/wo_wtb-ud-test.conllu, UD_Wolof-WTB/wo_wtb-ud-train.conllu, UD_Yoruba-YTB/yo_ytb-ud-test.conllu, _, _HOME, _UD_27_HOME, __main__, a, dev, https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3424/allzip, test, train, ud-treebanks-v2.7.tgz, ud27.py",0,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-07 21:03, # noinspection PyShadowingNames, #ud-treebanks-v2.7/'",
https://github.com/hankcs/HanLP,ud27m.py,0,0.0,6,37.5,7,43.75,0,0.0,3,18.75,0,0.0,0,0,4,0,,"UD_27_MULTILINGUAL_DEV, UD_27_MULTILINGUAL_TEST, UD_27_MULTILINGUAL_TRAIN, _UD_27_MULTILINGUAL_HOME, hanlp, os","hanlp.datasets.parsing.ud.concat_treebanks, hanlp.datasets.parsing.ud.ud27._UD_27_HOME, os.path.join",,,"UD_27_MULTILINGUAL_DEV, UD_27_MULTILINGUAL_TEST, UD_27_MULTILINGUAL_TRAIN, _UD_27_MULTILINGUAL_HOME",,"2.7, Dev set of multilingual UD_27 obtained by concatenating all dev sets., Test set of multilingual UD_27 obtained by concatenating all test sets., Training set of multilingual UD_27 obtained by concatenating all training sets., dev.conllu, test.conllu, train.conllu",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-21 20:39",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,19,59.38,10,31.25,0,0.0,3,9.38,0,0.0,1,0,9,0,,"concat_treebanks, dev, hanlp, home, len, list, name, open, os, output_dir, read, shutil, test, train, treebanks, ud_home, version, write, zip","hanlp.components.parsers.ud.udify_util.get_ud_treebank_files, hanlp.utils.io_util.get_resource, hanlp.utils.log_util.flash, os.makedirs, os.path.abspath, os.path.isdir, os.path.join, os.path.pardir, shutil.copyfileobj",concat_treebanks,,"dev, name, output_dir, read, test, train, treebanks, ud_home, write",,",  [blink][yellow]...[/yellow][/blink],  treebanks into , Concatenating , dev.conllu, r, test.conllu, train.conllu, ud-multilingual-v, w",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-07 21:45",
https://github.com/hankcs/HanLP,json_ner.py,0,0.0,67,69.07,19,19.59,5,5.15,6,6.19,0,0.0,6,1,28,2,,"JsonNERDataset, __init__, add, adjusted_ner, append, be, be_set, cache, cells, convert_conll03_to_json, deduplicated_srl, dict, doc, doc_key, doc_level_offset, each, erase, file_path, filename, filepath, generate_idx, get, hanlp, hanlp_common, iobes_tags_to_spans, isinstance, json, label, len, line, list, load_file, log, ner, ner_type, new_doc, num_docs, num_sentences, num_tokens_in_doc, offset, open, os, out, output_path, prune_ner_tagset, pruned_tag, reader, role, sample, self, sentence, span_end, span_start, span_util, split, str, strip, super, tag, tagset, transform, typing, unpack_ner, utils, write, x, zip","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.TimingFileIterator, hanlp.utils.io_util.read_tsv_as_sents, hanlp_common.constant.NULL, json.dump, json.loads, os.path.basename, os.path.splitext, typing.Callable, typing.Dict, typing.List, typing.Union","__init__, convert_conll03_to_json, load_file, new_doc, prune_ner_tagset, unpack_ner",JsonNERDataset,"adjusted_ner, be, be_set, cells, dataset, deduplicated_srl, doc, doc_key, each, filename, label, line, ner, ner_type, num_docs, num_sentences, num_tokens_in_doc, offset, out, output_path, pruned_tag, reader, role, sentence, span_end, span_start, tag, x","A dataset for ``.jsonlines`` format NER corpora.

Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler.
    doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
    tagset: Optional tagset to prune entities outside of this tagset from datasets., Load ``.jsonlines`` NER corpus. Samples of this corpus can be found using the following scripts.

.. highlight:: python
.. code-block:: python

    import json
    from hanlp_common.document import Document
    from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
    from hanlp.utils.io_util import get_resource

    with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
        for line in src:
            doc = json.loads(line)
            print(Document(doc))
            break

Args:
    filepath: ``.jsonlines`` NER corpus.","
,  ,  documents, ,  sentences [blink][yellow]...[/yellow][/blink], -, -DOCSTART-, .json, A dataset for ``.jsonlines`` format NER corpora.

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.
            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
            tagset: Optional tagset to prune entities outside of this tagset from datasets.
        , Load ``.jsonlines`` NER corpus. Samples of this corpus can be found using the following scripts.

        .. highlight:: python
        .. code-block:: python

            import json
            from hanlp_common.document import Document
            from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
            from hanlp.utils.io_util import get_resource

            with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
                for line in src:
                    doc = json.loads(line)
                    print(Document(doc))
                    break

        Args:
            filepath: ``.jsonlines`` NER corpus.
        , O, begin_offset, doc_key, end_offset, label, ner, sentences, tag, token, w","0, 1, 2, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-21 16:26, # It's necessary to create a null label when there is no NER in the sentence for the sake of padding., A dataset for ``.jsonlines`` format NER corpora.

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.
doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
tagset: Optional tagset to prune entities outside of this tagset from datasets.
, Load ``.jsonlines`` NER corpus. Samples of this corpus can be found using the following scripts.

.. highlight:: python
.. code-block:: python

import json
from hanlp_common.document import Document
from hanlp.datasets.srl.ontonotes5.chinese import ONTONOTES5_CONLL12_CHINESE_DEV
from hanlp.utils.io_util import get_resource

with open(get_resource(ONTONOTES5_CONLL12_CHINESE_DEV)) as src:
for line in src:
doc = json.loads(line)
print(Document(doc))
break

Args:
filepath: ``.jsonlines`` NER corpus.
",
https://github.com/hankcs/HanLP,tsv.py,0,0.0,19,52.78,4,11.11,3,8.33,10,27.78,0,0.0,2,1,4,2,,"TSVTaggingDataset, __init__, cache, char_level, end, filepath, generate_idx, hanlp, hard_constraint, len, load_file, max_seq_len, self, sent_delimiter, short_sents, start, super, transform, typing","hanlp.common.dataset.TransformableDataset, hanlp.utils.io_util.generate_words_tags_from_tsv, hanlp.utils.io_util.get_resource, hanlp.utils.string_util.split_long_sentence_into, typing.Callable, typing.List, typing.Union","__init__, load_file",TSVTaggingDataset,"end, filepath, short_sents, start","Args:
    data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
    transform: Predefined transform(s).
    cache: ``True`` to enable caching, so that transforms won't be called twice.
    generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
        samples are re-ordered by a sampler.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level, which is never the case for
        lemmatization.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    kwargs: Not used., Load a ``.tsv`` file. A ``.tsv`` file for tagging is defined as a tab separated text file, where non-empty
lines have two columns for token and tag respectively, empty lines mark the end of sentences.

Args:
    filepath: Path to a ``.tsv`` tagging file.

.. highlight:: bash
.. code-block:: bash

    $ head eng.train.tsv
    -DOCSTART-      O

    EU      S-ORG
    rejects O
    German  S-MISC
    call    O
    to      O
    boycott O
    British S-MISC
    lamb    O","

        Args:
            data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
            transform: Predefined transform(s).
            cache: ``True`` to enable caching, so that transforms won't be called twice.
            generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
                samples are re-ordered by a sampler.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level, which is never the case for
                lemmatization.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            kwargs: Not used.
        , Load a ``.tsv`` file. A ``.tsv`` file for tagging is defined as a tab separated text file, where non-empty
        lines have two columns for token and tag respectively, empty lines mark the end of sentences.

        Args:
            filepath: Path to a ``.tsv`` tagging file.

        .. highlight:: bash
        .. code-block:: bash

            $ head eng.train.tsv
            -DOCSTART-      O

            EU      S-ORG
            rejects O
            German  S-MISC
            call    O
            to      O
            boycott O
            British S-MISC
            lamb    O

        , tag, token","0, False, None","

Args:
data: The local or remote path to a dataset, or a list of samples where each sample is a dict.
transform: Predefined transform(s).
cache: ``True`` to enable caching, so that transforms won't be called twice.
generate_idx: Create a :const:`~hanlp_common.constants.IDX` field for each sample to store its order in dataset. Useful for prediction when
samples are re-ordered by a sampler.
max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
be split here.
char_level: Whether the sequence length is measured at char level, which is never the case for
lemmatization.
hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
in a sentence, it will be split at a token anyway.
kwargs: Not used.
, #     print(f'\rRead instances {idx // 1000}k', end=''), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-24 23:09, # idx += 1, # idx = 0, # if idx % 1000 == 0:, # print('\r', end=''), Load a ``.tsv`` file. A ``.tsv`` file for tagging is defined as a tab separated text file, where non-empty
lines have two columns for token and tag respectively, empty lines mark the end of sentences.

Args:
filepath: Path to a ``.tsv`` tagging file.

.. highlight:: bash
.. code-block:: bash

$ head eng.train.tsv
-DOCSTART-      O

EU      S-ORG
rejects O
German  S-MISC
call    O
to      O
boycott O
British S-MISC
lamb    O

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:04",
https://github.com/hankcs/HanLP,lm_dataset.py,0,0.0,72,73.47,14,14.29,9,9.18,3,3.06,0,0.0,10,1,21,0,,"LanguageModelDataset, __init__, __iter__, _debug_load_cache, _fp, _read_chunk, append, append_transform, available_tokenizers, batch_size, cache, contiguous, data, eos, erase, estimate_num_batches, filecache, filepath, fp, from_bytes, hanlp, hanlp_common, id, ids, int, io, isinstance, j, keys, len, length, line, lines, load_file, load_json, lock, log, max_seq_len, mutable, num_tokens, offset, open, os, out, property, range, read, safety, sample, save_json, seek, self, seq_len, src, staticmethod, str, strip, super, targets, to_bytes, tokenizer, tokens, torch, training, transform, transform_sample, transpose_, typing, valid, vocab, vocab_path, write","hanlp.common.dataset.TransformSequentialDataset, hanlp.common.transform.AppendEOS, hanlp.common.transform.FieldToIndex, hanlp.common.transform.ToChar, hanlp.common.transform.WhitespaceTokenizer, hanlp.common.vocab.Vocab, hanlp.utils.io_util.TimingFileIterator, hanlp.utils.io_util.file_cache, hanlp.utils.io_util.get_resource, os.path.getsize, os.path.isfile, os.path.splitext, torch.LongTensor, typing.Callable, typing.List, typing.Union","__init__, __iter__, _debug_load_cache, _read_chunk, estimate_num_batches, filecache, load_file, max_seq_len, vocab, vocab_path",LanguageModelDataset,"available_tokenizers, batch_size, data, f, fp, i, id, ids, j, line, lines, max_seq_len, out, safety, sample, seq_len, src, targets, tokens, valid, vocab",,"
,  ,  not supported, available options: , .vocab.json, M lines
, M tokens, , char, little, rb, text, token, token_id, wb, whitespace","0, 1, 10, 1000000, 2, 4, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-05 21:42",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:04",
https://github.com/hankcs/HanLP,nn_eos.py,0,0.0,8,25.81,14,45.16,0,0.0,9,29.03,0,0.0,0,0,8,0,,"EUROPARL_V7_DE_EN_EN_SENTENCES_DEV, EUROPARL_V7_DE_EN_EN_SENTENCES_TEST, EUROPARL_V7_DE_EN_EN_SENTENCES_TRAIN, SETIMES2_EN_HR_HR_SENTENCES_DEV, SETIMES2_EN_HR_HR_SENTENCES_TEST, SETIMES2_EN_HR_HR_SENTENCES_TRAIN, _EUROPARL_V7_DE_EN_EN_SENTENCES_HOME, _SETIMES2_EN_HR_SENTENCES_HOME",,,,"EUROPARL_V7_DE_EN_EN_SENTENCES_DEV, EUROPARL_V7_DE_EN_EN_SENTENCES_TEST, EUROPARL_V7_DE_EN_EN_SENTENCES_TRAIN, SETIMES2_EN_HR_HR_SENTENCES_DEV, SETIMES2_EN_HR_HR_SENTENCES_TEST, SETIMES2_EN_HR_HR_SENTENCES_TRAIN, _EUROPARL_V7_DE_EN_EN_SENTENCES_HOME, _SETIMES2_EN_HR_SENTENCES_HOME",,"#SETIMES2.en-hr.hr.sentences.dev, #SETIMES2.en-hr.hr.sentences.test, #SETIMES2.en-hr.hr.sentences.train, #europarl-v7.de-en.en.sentences.dev, #europarl-v7.de-en.en.sentences.test, #europarl-v7.de-en.en.sentences.train, Dev set of Europarl corpus (:cite:`koehn2005europarl`)., Dev set of SETimes corpus., Test set of Europarl corpus (:cite:`koehn2005europarl`)., Test set of SETimes corpus., Training set of Europarl corpus (:cite:`koehn2005europarl`)., Training set of SETimes corpus., http://schweter.eu/cloud/nn_eos/europarl-v7.de-en.en.sentences.tar.xz, https://schweter.eu/cloud/nn_eos/SETIMES2.en-hr.sentences.tar.xz",,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-24 22:51, #SETIMES2.en-hr.hr.sentences.test', #SETIMES2.en-hr.hr.sentences.train', #europarl-v7.de-en.en.sentences.dev', Dev set of Europarl corpus (:cite:`koehn2005europarl`).
EUROPARL_V7_DE_EN_EN_SENTENCES_TEST = _EUROPARL_V7_DE_EN_EN_SENTENCES_HOME + '#europarl-v7.de-en.en.sentences.test'
Test set of Europarl corpus (:cite:`koehn2005europarl`)., Test set of SETimes corpus.
_EUROPARL_V7_DE_EN_EN_SENTENCES_HOME = 'http://schweter.eu/cloud/nn_eos/europarl-v7.de-en.en.sentences.tar.xz'
EUROPARL_V7_DE_EN_EN_SENTENCES_TRAIN = _EUROPARL_V7_DE_EN_EN_SENTENCES_HOME + '#europarl-v7.de-en.en.sentences.train'
Training set of Europarl corpus (:cite:`koehn2005europarl`)., Training set of SETimes corpus.
SETIMES2_EN_HR_HR_SENTENCES_DEV = _SETIMES2_EN_HR_SENTENCES_HOME + '#SETIMES2.en-hr.hr.sentences.dev'
Dev set of SETimes corpus.",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:03",
https://github.com/hankcs/HanLP,conll12coref.py,0,0.0,47,68.12,5,7.25,4,5.8,13,18.84,0,0.0,4,2,13,1,,"CONLL12CorefDataset, Ontonotes, _Ontonotes, __init__, _conll_rows_to_sentence, append, cache, clusters, collections, conll_rows, coref_spans, dataset_document_iterator, document, end, erase, file_path, filepath, gold_clusters, hanlp, int, len, line, list, load_file, log, max_sentences, max_span_width, ontonotes_reader, open_file, os, remove_singleton_clusters, self, sentence, sentences, span_id, start, startswith, str, strip, super, text_to_instance, total_tokens, transform, typed_span, typing, values, words","collections.defaultdict, hanlp.common.dataset.TransformableDataset, hanlp.datasets.srl.loaders.ontonotes_loader.Ontonotes, hanlp.datasets.srl.loaders.ontonotes_loader.OntonotesSentence, hanlp.datasets.srl.loaders.ontonotes_loader.make_coref_instance, hanlp.utils.io_util.TimingFileIterator, os.path.basename, typing.Callable, typing.DefaultDict, typing.Iterator, typing.List, typing.Optional, typing.Tuple, typing.Union","__init__, dataset_document_iterator, load_file, text_to_instance","CONLL12CorefDataset, Ontonotes","clusters, conll_rows, document, end, line, ontonotes_reader, open_file, sentence, sentences, span_id, start, total_tokens, typed_span","An iterator over CONLL formatted files which yields documents, regardless
of the number of document annotations in a particular file. This is useful
for conll data which has been preprocessed, such as the preprocessing which
takes place for the 2012 CONLL Coreference Resolution task.

Args:
  file_path: str: 

Returns:",", #, #end document, An iterator over CONLL formatted files which yields documents, regardless
        of the number of document annotations in a particular file. This is useful
        for conll data which has been preprocessed, such as the preprocessing which
        takes place for the 2012 CONLL Coreference Resolution task.

        Args:
          file_path: str: 

        Returns:

        , Loading ","0, 10, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Collect any stragglers or files which might not, # Coref annotations are on a _per sentence_, # Date: 2020-07-04 15:33, # Non-empty line. Collect the annotation., # basis, so we need to adjust them to be relative, # have the '#end document' format for the end of the file., # to the length of the document., # type: ignore, #""):, #end document""):, An iterator over CONLL formatted files which yields documents, regardless
of the number of document annotations in a particular file. This is useful
for conll data which has been preprocessed, such as the preprocessing which
takes place for the 2012 CONLL Coreference Resolution task.

Args:
file_path: str:

Returns:

",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-12-28 19:03",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-20 17:35",
https://github.com/hankcs/HanLP,fasttext_classifier.py,0,0.0,44,73.33,8,13.33,4,6.67,4,6.67,0,0.0,5,1,15,1,,"FastTextClassifier, __init__, _model, _strip_prefix, append, batch_labels, batch_probs, config, config_path, dict, fasttext, filepath, flat, get, get_labels, hanlp, hanlp_common, isinstance, k, label, labels, len, list, load, max_len, model_path, num_labels, os, predict, prob, probs, property, replace, result, results, save_dir, self, staticmethod, super, sys, tolist, topk, typing, zip","fasttext.FastText._FastText, fasttext.load_model, hanlp.__version__, hanlp.common.component.Component, hanlp.utils.io_util.get_resource, hanlp.utils.io_util.stdout_redirected, hanlp_common.io.load_json, hanlp_common.reflection.classpath_of, hanlp_common.structure.SerializableDict, os.devnull, os.path.isfile, os.path.join, sys.stderr, typing.List, typing.Union","__init__, _strip_prefix, labels, load, predict",FastTextClassifier,"batch_labels, batch_probs, config_path, filepath, flat, k, labels, model_path, num_labels, prob, probs, result, results, text, topk","Classify text.

Args:
    text: A document or a list of documents.
    topk: ``True`` or ``int`` to return the top-k labels.
    prob: Return also probabilities.
    max_len: Strip long document into ``max_len`` characters for faster prediction.
    **kwargs: Not used

Returns:
    Classification results.","
, 
        Classify text.

        Args:
            text: A document or a list of documents.
            topk: ``True`` or ``int`` to return the top-k labels.
            prob: Return also probabilities.
            max_len: Strip long document into ``max_len`` characters for faster prediction.
            **kwargs: Not used

        Returns:
            Classification results.
        ,  , __label__, classpath, config.json, hanlp_version, model_path","0, False, None, True","
Classify text.

Args:
text: A document or a list of documents.
topk: ``True`` or ``int`` to return the top-k labels.
prob: Return also probabilities.
max_len: Strip long document into ``max_len`` characters for faster prediction.
**kwargs: Not used

Returns:
Classification results.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-09-28 13:31",
https://github.com/hankcs/HanLP,transformer_classifier.py,0,0.0,158,70.22,37,16.44,15,6.67,15,6.67,0,0.0,20,3,42,2,,"CrossEntropyLoss, Dropout, Linear, Module, TransformerClassificationModel, TransformerClassifier, TransformerComponent, ValueError, __init__, abc, adam_epsilon, any, append, append_transform, argmax, attention_mask, average_subwords, backward, batch, batch_max_tokens, batch_sampler, batch_size, best_epoch, best_metric, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_transformer, build_vocabs, cache, classifier, close, columns, compute_loss, config, criterion, current, dataloader, dev, dev_score, device, dropout, elapsed, elapsed_human, encoder, enumerate, epoch, epochs, eval, evaluate_dataloader, execute_training_loop, feed_batch, filename, fit, fit_dataloader, flat, float, forward, from_pretrained, get, get_metric, hanlp, hanlp_common, headers, hidden_dropout_prob, hidden_size, idx, idx_to_token, info, input_ids, int, isinstance, item, items, key, label, label_ids, label_key, label_vocab, labels, lambda_scheduler, layer, len, lens, list, lock, log, logger, logging, logits, loss, lr, max_seq_length, model, mutable, num_labels, num_samples, num_training_steps, on_config_ready, open, output, pred, predict, property, purge_cache, range, ratio_width, report, reset, ret_raw_hidden_states, sample, samples, save_dir, save_weights, scalar_mix, self, seq_length, sequence_output, shuffle, size, sorting, step, str, summary, super, tag, target, teacher, text_a_key, text_b_key, timer, token_type_ids, tolist, torch, total, total_loss, total_time_human, train, training, transformer, transformer_layers, transformer_lr, transformer_tokenizer, trn, truncate_long_sequences, tuple, typing, update_metric, vocabs, warmup_steps, weight_decay, word_dropout, write, zero_grad, zip","abc.ABC, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSampler, hanlp.common.dataset.TableDataset, hanlp.common.dataset.TransformableDataset, hanlp.common.torch_component.TorchComponent, hanlp.common.vocab.Vocab, hanlp.components.distillation.schedulers.LinearTeacherAnnealingScheduler, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.layers.transformers.encoder.TransformerEncoder, hanlp.layers.transformers.pt_imports.AutoTokenizer, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.pt_imports.BertTokenizer, hanlp.layers.transformers.pt_imports.PreTrainedModel, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.layers.transformers.utils.transformer_sliding_window, hanlp.metrics.accuracy.CategoricalAccuracy, hanlp.transform.transformer_tokenizer.TransformerTextTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.isdebugging, hanlp_common.util.merge_dict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.LongTensor, torch.Tensor, torch.nn, torch.nn.Module, torch.utils.data.DataLoader, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_transformer, build_vocabs, compute_loss, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, forward, label_vocab, on_config_ready, predict, update_metric","TransformerClassificationModel, TransformerClassifier, TransformerComponent","batch, batch_sampler, batch_size, best_epoch, best_metric, columns, criterion, data, dataloader, dataset, dev_score, epoch, flat, idx, key, label, label_ids, labels, lambda_scheduler, lens, logits, loss, model, num_samples, num_training_steps, output, pred, ratio_width, report, sample, samples, scheduler, seq_length, sequence_output, tag, target, timer, total_loss, transformer, transformer_layers, transformer_lr, vocab","A classifier using transformer as encoder.

Args:
    **kwargs: Passed to config., The base class for transorfmer based components. If offers methods to build transformer tokenizers
, optimizers and models.

Args:
    **kwargs: Passed to config.",", 	, 
,  ,  / ,  The base class for transorfmer based components. If offers methods to build transformer tokenizers
        , optimizers and models.

        Args:
            **kwargs: Passed to config.
        ,  [red]saved[/red],  acc: ,  samples/sec, , , .0f, .2%, .4f, /, :[/yellow], =, A classifier using transformer as encoder.

        Args:
            **kwargs: Passed to config.
        , Guess [bold][blue], Pre-processing and caching dataset [blink][yellow]...[/yellow][/blink], Unsupported data , Wrong dataset format, [/blue], [/blue][/bold] according to the headers of training dataset: [blue], [yellow]Epoch , albert_chinese, attention_mask, gradient_accumulation, input_ids, label_id, label_key, loss: , max, text_a_key, text_b_key, token_type_ids, transformer_layers, w","0, 0.1, 1, 1000, 1e-06, 1e-08, 2, 3, 32, 5.0, 512, 5e-05, False, None, True"," The base class for transorfmer based components. If offers methods to build transformer tokenizers
, optimizers and models.

Args:
**kwargs: Passed to config.
, #     dataset.append_transform(self.config.transform), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-08 16:31, # config.hidden_dropout_prob = self.config.hidden_dropout_prob, # config.num_labels = len(self.vocabs.label), # config: PretrainedConfig = AutoConfig.from_pretrained(transformer), # hidden_dropout_prob=0.0,, # if self.config.transform:, # noinspection PyMethodOverriding, # text_a text_b pred gold, # transform=None,, # truncated_normal_(model.classifier.weight, mean=0.02, std=0.05), A classifier using transformer as encoder.

Args:
**kwargs: Passed to config.
",
https://github.com/hankcs/HanLP,transformer_classifier_hf.py,0,0.0,67,81.71,4,4.88,6,7.32,5,6.1,0,0.0,13,1,23,1,,"NotImplementedError, TransformerClassifierHF, __init__, _tokenizer, append, batch, batch_labels, batch_probs, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, dataloader, device, evaluate_dataloader, execute_training_loop, extend, fit_dataloader, flat, from_pretrained, hanlp, hanlp_common, id2label, isinstance, items, kwargs, labels, len, lens, list, load_vocabs, load_weights, loader, logging, logits, max_position_embeddings, model, pad_token_id, predict, prob, probs, property, result, results, sample, sampler, sampler_builder, save_dir, self, shuffle, softmax, sort, sorted, str, super, text, tolist, topk, torch, transformers, typing, vocabs, x, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.dataset.TableDataset, hanlp.common.torch_component.TorchComponent, hanlp_common.constant.IDX, hanlp_common.util.reorder, hanlp_common.util.split_dict, logging.Logger, torch.nn.Module, torch.utils.data.DataLoader, transformers.AutoModelForSequenceClassification, transformers.AutoTokenizer, transformers.PreTrainedTokenizer, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, evaluate_dataloader, execute_training_loop, fit_dataloader, labels, load_vocabs, load_weights, predict",TransformerClassifierHF,"batch, batch_labels, batch_probs, dataloader, dataset, flat, id2label, k, labels, lens, loader, logits, order, p, prob, probs, result, results, sample, sampler, text, topk, x","Classify text.

Args:
    text: A document or a list of documents.
    topk: ``True`` or ``int`` to return the top-k labels.
    prob: Return also probabilities.
    max_len: Strip long document into ``max_len`` characters for faster prediction.
    **kwargs: Not used

Returns:
    Classification results.","
        Classify text.

        Args:
            text: A document or a list of documents.
            topk: ``True`` or ``int`` to return the top-k labels.
            prob: Return also probabilities.
            max_len: Strip long document into ``max_len`` characters for faster prediction.
            **kwargs: Not used

        Returns:
            Classification results.
        , input_ids, model.pt, vocabs.json","0, 1, 32, False, None, True","
Classify text.

Args:
text: A document or a list of documents.
topk: ``True`` or ``int`` to return the top-k labels.
prob: Return also probabilities.
max_len: Strip long document into ``max_len`` characters for faster prediction.
**kwargs: Not used

Returns:
Classification results.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2023-02-17 17:54, # noinspection PyTypeChecker",
https://github.com/hankcs/HanLP,transformer_classifier_tf.py,0,0.0,113,67.66,26,15.57,12,7.19,16,9.58,0,0.0,15,2,40,0,,"TransformerClassifierTF, TransformerTextTransform, X, Y, Y_gold, Y_pred, Y_to_outputs, __init__, _y_id_to_str, attention_mask, batch, batch_size, bert_text_transform, bool, build_loss, build_metrics, build_model, build_optimizer, build_vocab, config, convert_ids_to_tokens, convert_tokens_to_ids, correct, create_types_shapes_values, delimiter, dict, diff, enumerate, epochs, evaluate_output, exit, fatal, feature, fit, get, gold, hanlp, hanlp_common, hasattr, idx, idx_to_token, input, input_is_single_sample, inputs, inputs_to_samples, isinstance, label, label_vocab, len, lookup, loss, lr_config, map_x, map_y, mapped, math, max_length, metric, model, multi_label, mutable, num_batches, num_features, numpy, one_hots, optimizer, out, outputs, pad_token, predict_on_batch, preds, print, replace, safe_pad_token, score, segment_ids, self, shapes, skip_header, startswith, super, tensorflow, text_a, text_b, token_ids, tokenize, tokenizer, tokens, tokens_a, tokens_b, total, train_examples, train_steps, transform, transformers, trn_data, tst_data, tuple, types, typing, use_amp, v, values, warmup_steps, warmup_steps_per_epoch, warmup_steps_ratio, write, x, x_columns, x_to_idx, y_column, y_to_idx, zip","hanlp.common.keras_component.KerasComponent, hanlp.layers.transformers.loader_tf.build_transformer, hanlp.optimizers.adamw.create_optimizer, hanlp.transform.table_tf.TableTransform, hanlp.utils.log_util.logger, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_locals_kwargs, math.ceil, tf.Tensor, tf.TensorSpec, tf.argmax, tf.cast, tf.dtypes.int32, tf.equal, tf.int32, tf.keras.Model, tf.keras.losses.BinaryCrossentropy, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.losses.loss, tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.SparseCategoricalAccuracy, tf.keras.mixed_precision.experimental.LossScaleOptimizer, tf.keras.utils.serialize_keras_object, tf.map_fn, tf.one_hot, tf.reduce_sum, tf.string, transformers.tokenization_utils.PreTrainedTokenizer, typing.Any, typing.Iterable, typing.Tuple, typing.Union","Y_to_outputs, __init__, _y_id_to_str, build_loss, build_metrics, build_model, build_optimizer, build_vocab, create_types_shapes_values, evaluate_output, fit, input_is_single_sample, inputs_to_samples, x_to_idx, y_to_idx","TransformerClassifierTF, TransformerTextTransform","X, Y, Y_gold, Y_pred, attention_mask, batch, bert_text_transform, correct, diff, feature, idx, label, loss, lr_config, mapped, max_length, metric, model, num_features, one_hots, opt, outputs, pad_token, preds, score, segment_ids, shapes, text_a, text_b, token_ids, tokenizer, tokens, tokens_a, tokens_b, total, train_examples, types, v, values, warmup_steps_per_epoch",,", {}/{} {}: {:.2f},  ,  ##,  inconsistent with current , =, Error with input length {} vs {}, Must specify loss as an instance in tf.keras.losses, Numbers of features , [CLS], [PAD], [SEP], _, accuracy, adamw, auto, binary_accuracy, config, decay_schedule_fn, dynamic, get_config, learning_rate, map_x should always be set to True, multi_label, sentence	pred	gold
, {}	{}	{}
","0, 0.1, 1, 100, 128, 2, 3, 32, 5e-05, False, None, True","#     f'Input tokens {tokens} exceed the max sequence length of {max_length - 2}. ', #     f'The exceeded part will be truncated and ignored. ', #     f'You are recommended to split your long text into several sentences within ', #     f'{max_length - 2} tokens beforehand.'), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 13:19, # Loss to be BinaryCrossentropy for multi-label:, # Prediction to be Y > 0:, # logger.warning(, # loss scaling is currently required when using mixed precision, # need to change index to binary vector, # noinspection PyMethodOverriding, # opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08), # opt = tfa.optimizers.AdamW(learning_rate=3e-5, epsilon=1e-08, weight_decay=0.01), ##', '')  # fix sub-word generated by BERT tagger",
https://github.com/hankcs/HanLP,transformer_regression_hf.py,0,0.0,51,77.27,4,6.06,6,9.09,5,7.58,0,0.0,12,1,12,1,,"NotImplementedError, TransformerRegressionHF, __init__, _tokenizer, batch, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, dataloader, device, evaluate_dataloader, execute_training_loop, extend, fit_dataloader, flat, from_pretrained, hanlp, hanlp_common, isinstance, kwargs, len, lens, load_vocabs, load_weights, loader, logging, logits, max_position_embeddings, model, pad_token_id, predict, results, sample, sampler, sampler_builder, save_dir, self, shuffle, squeeze, str, super, text, tolist, torch, transformers, typing, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.dataset.TableDataset, hanlp.common.torch_component.TorchComponent, hanlp_common.constant.IDX, hanlp_common.util.reorder, hanlp_common.util.split_dict, logging.Logger, torch.nn.Module, torch.utils.data.DataLoader, transformers.AutoModelForSequenceClassification, transformers.AutoTokenizer, transformers.PreTrainedTokenizer, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, evaluate_dataloader, execute_training_loop, fit_dataloader, load_vocabs, load_weights, predict",TransformerRegressionHF,"batch, dataloader, dataset, flat, lens, loader, logits, order, results, sample, sampler, text","Classify text.

Args:
    text: A document or a list of documents.
    topk: ``True`` or ``int`` to return the top-k labels.
    prob: Return also probabilities.
    max_len: Strip long document into ``max_len`` characters for faster prediction.
    **kwargs: Not used

Returns:
    Classification results.","
        Classify text.

        Args:
            text: A document or a list of documents.
            topk: ``True`` or ``int`` to return the top-k labels.
            prob: Return also probabilities.
            max_len: Strip long document into ``max_len`` characters for faster prediction.
            **kwargs: Not used

        Returns:
            Classification results.
        , input_ids, model.pt, vocabs.json","0, 1, 32, False, None, True","
Classify text.

Args:
text: A document or a list of documents.
topk: ``True`` or ``int`` to return the top-k labels.
prob: Return also probabilities.
max_len: Strip long document into ``max_len`` characters for faster prediction.
**kwargs: Not used

Returns:
Classification results.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2023-02-17 17:54, # noinspection PyTypeChecker",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-11-10 13:18",
https://github.com/hankcs/HanLP,distillable_component.py,0,0.0,24,61.54,10,25.64,1,2.56,4,10.26,0,0.0,3,1,7,0,,"DistillableComponent, _savable_config, abc, batch_size, build_teacher, config, copy, distill, epochs, from_name, get, hanlp, hanlp_common, isinstance, kd_criterion, load_path, property, self, str, super, teacher, temperature_scheduler, update, vocabs","abc.ABC, copy.copy, hanlp.common.torch_component.TorchComponent, hanlp.components.distillation.losses.KnowledgeDistillationLoss, hanlp.components.distillation.schedulers.TemperatureScheduler, hanlp.load, hanlp.utils.torch_util.cuda_devices, hanlp_common.util.merge_locals_kwargs","_savable_config, build_teacher, distill",DistillableComponent,"batch_size, config, devices, epochs, kd_criterion, teacher, temperature_scheduler",,"__class__, batch_size, config, devices, epochs, flsw, kd_ce_loss, kwargs, self, teacher",None,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-17 20:30, # noinspection PyMethodMayBeStatic,PyTypeChecker",
https://github.com/hankcs/HanLP,losses.py,0,0.0,66,62.26,13,12.26,7,6.6,20,18.87,0,0.0,13,1,28,11,,"KnowledgeDistillationLoss, __call__, __init__, __name__, _loss, args, att_ce_loss, att_ce_mean_loss, att_mse_loss, att_mse_sum_loss, attention_S, attention_S_select, attention_T, attention_T_select, beta_logits_S, beta_logits_T, cls_S, cls_T, cos_loss, dim, fsp_loss, getattr, gram_S, gram_T, hanlp_common, hid_mse_loss, isinstance, kd_ce_loss, kd_mse_loss, kwargs, len, lengths, logits_S, logits_T, loss, mask, mean, mmd_loss, name, new, normed_cls_S, normed_cls_T, p_T, pkd_loss, probs_T, probs_T_select, self, size, state_S, state_S_0, state_S_1, state_T, state_T_0, state_T_1, sum, super, sys, target, temperature, thismodule, to, torch, transpose, unsqueeze, valid_count, view","F.cosine_embedding_loss, F.log_softmax, F.mse_loss, F.softmax, hanlp_common.configurable.AutoConfigurable, sys.modules, torch.Tensor, torch.bmm, torch.masked_select, torch.norm, torch.pow, torch.where, torch.zeros_like","__call__, __init__, att_ce_loss, att_ce_mean_loss, att_mse_loss, att_mse_sum_loss, cos_loss, fsp_loss, hid_mse_loss, kd_ce_loss, kd_mse_loss, mmd_loss, pkd_loss",KnowledgeDistillationLoss,"attention_S, attention_S_select, attention_T, attention_T_select, beta_logits_S, beta_logits_T, cls_S, cls_T, gram_S, gram_T, lengths, loss, mask, normed_cls_S, normed_cls_T, p_T, probs_T, probs_T_select, state_S, state_S_0, state_S_1, state_T, state_T_0, state_T_1, target, temperature, thismodule, valid_count","* Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*), * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
* If the shape is (*batch_size*, *num_heads*, *length*, *length*), averages over dimension `num_heads` and then computes cross-entropy loss between the two matrics.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.tensor mask:     tensor of shape  (*batch_size*, *length*), * Calculates the mse loss between `attention_S` and `attention_T`.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor mask: tensor of shape  (*batch_size*, *length*), * Calculates the mse loss between `attention_S` and `attention_T`. 
* If the the shape is (*batch_size*, *num_heads*, *length*, *length*), sums along the `num_heads` dimension and then calcuates the mse loss between the two matrices.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*), * Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*), * Computes normalized vector mse loss at position 0 along `length` dimension. This is the loss used in BERT-PKD, see `Patient Knowledge Distillation for BERT Model Compression <https://arxiv.org/abs/1908.09355>`_.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param mask: not used., * Computes the cosine similarity loss between the inputs. This is the loss used in DistilBERT, see `DistilBERT <https://arxiv.org/abs/1910.01108>`_
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*), * Takes in two lists of matrices `state_S` and `state_T`. Each list contains 2 matrices of the shape (*batch_size*, *length*, *hidden_size*). `hidden_size` of matrices in `State_S` doesn't need to be the same as that of `state_T`. Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *length*, *length*) ) and the ones in B ( with the resulting shape (*batch_size*, *length*, *length*) ), then computes the mse loss between the similarity matrices:

.. math::

        loss = mean((S_{1} \cdot S_{2}^T - T_{1} \cdot T_{2}^T)^2)

* It is a Variant of the NST loss in `Like What You Like: Knowledge Distill via Neuron Selectivity Transfer <https://arxiv.org/abs/1707.01219>`_
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)

Example in `intermediate_matches`::

    intermediate_matches = [
    {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'nst', 'weight' : 1},
    ...], * Takes in two lists of matrics `state_S` and `state_T`. Each list contains two matrices of the shape (*batch_size*, *length*, *hidden_size*). Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ) and the ones in B ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ), then computes the mse loss between the similarity matrices:

.. math::

    loss = mean((S_{1}^T \cdot S_{2} - T_{1}^T \cdot T_{2})^2)

* It is a Variant of FSP loss in `A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning <http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf>`_.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)

Example in `intermediate_matches`::

    intermediate_matches = [
    {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'fsp', 'weight' : 1, 'proj':['linear',384,768]},
    ...], Calculate the cross entropy between logits_S and logits_T

:param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,), Calculate the mse loss between logits_S and logits_T

:param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)","

    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
    
    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)
    , 
    * Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
    * If the shape is (*batch_size*, *num_heads*, *length*, *length*), averages over dimension `num_heads` and then computes cross-entropy loss between the two matrics.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
    
    :param torch.tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
    :param torch.tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
    :param torch.tensor mask:     tensor of shape  (*batch_size*, *length*)
    , 
    * Calculates the mse loss between `attention_S` and `attention_T`.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
    :param torch.Tensor mask: tensor of shape  (*batch_size*, *length*)
    , 
    * Calculates the mse loss between `attention_S` and `attention_T`. 
    * If the the shape is (*batch_size*, *num_heads*, *length*, *length*), sums along the `num_heads` dimension and then calcuates the mse loss between the two matrices.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

    :param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
    :param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
    :param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)
    , 
    * Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)
    , 
    * Computes normalized vector mse loss at position 0 along `length` dimension. This is the loss used in BERT-PKD, see `Patient Knowledge Distillation for BERT Model Compression <https://arxiv.org/abs/1908.09355>`_.
    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param mask: not used.
    , 
    * Computes the cosine similarity loss between the inputs. This is the loss used in DistilBERT, see `DistilBERT <https://arxiv.org/abs/1910.01108>`_
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

    :param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)
    , 
    * Takes in two lists of matrices `state_S` and `state_T`. Each list contains 2 matrices of the shape (*batch_size*, *length*, *hidden_size*). `hidden_size` of matrices in `State_S` doesn't need to be the same as that of `state_T`. Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *length*, *length*) ) and the ones in B ( with the resulting shape (*batch_size*, *length*, *length*) ), then computes the mse loss between the similarity matrices:
    
    .. math::

            loss = mean((S_{1} \cdot S_{2}^T - T_{1} \cdot T_{2}^T)^2)

    * It is a Variant of the NST loss in `Like What You Like: Knowledge Distill via Neuron Selectivity Transfer <https://arxiv.org/abs/1707.01219>`_
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)

    Example in `intermediate_matches`::

        intermediate_matches = [
        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'nst', 'weight' : 1},
        ...]
    , 
    * Takes in two lists of matrics `state_S` and `state_T`. Each list contains two matrices of the shape (*batch_size*, *length*, *hidden_size*). Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ) and the ones in B ( with the resulting shape (*batch_size*, *hidden_size*, *hidden_size*) ), then computes the mse loss between the similarity matrices:

    .. math::

        loss = mean((S_{1}^T \cdot S_{2} - T_{1}^T \cdot T_{2})^2)

    * It is a Variant of FSP loss in `A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning <http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf>`_.
    * If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
    * If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

    :param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
    :param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)

    Example in `intermediate_matches`::

        intermediate_matches = [
        {'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'fsp', 'weight' : 1, 'proj':['linear',384,768]},
        ...]
    , 
    Calculate the cross entropy between logits_S and logits_T

    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)
    , 
    Calculate the mse loss between logits_S and logits_T

    :param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
    :param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
    :param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)
    , mean, none","0, 0.001, 1, 2, 4, None, True","

* Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)
, 
* Calculates the cross-entropy loss between `attention_S` and `attention_T`, where softmax is to applied on ``dim=-1``.
* If the shape is (*batch_size*, *num_heads*, *length*, *length*), averages over dimension `num_heads` and then computes cross-entropy loss between the two matrics.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.tensor mask:     tensor of shape  (*batch_size*, *length*)
, 
* Calculates the mse loss between `attention_S` and `attention_T`.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*)
:param torch.Tensor mask: tensor of shape  (*batch_size*, *length*)
, 
* Calculates the mse loss between `attention_S` and `attention_T`.
* If the the shape is (*batch_size*, *num_heads*, *length*, *length*), sums along the `num_heads` dimension and then calcuates the mse loss between the two matrices.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.Tensor logits_S: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.Tensor logits_T: tensor of shape  (*batch_size*, *num_heads*, *length*, *length*) or (*batch_size*, *length*, *length*)
:param torch.Tensor mask:     tensor of shape  (*batch_size*, *length*)
, 
* Calculates the mse loss between `state_S` and `state_T`, which are the hidden state of the models.
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)
, 
* Computes normalized vector mse loss at position 0 along `length` dimension. This is the loss used in BERT-PKD, see `Patient Knowledge Distillation for BERT Model Compression <https://arxiv.org/abs/1908.09355>`_.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param mask: not used.
, 
* Computes the cosine similarity loss between the inputs. This is the loss used in DistilBERT, see `DistilBERT <https://arxiv.org/abs/1910.01108>`_
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.
* If the hidden sizes of student and teacher are different, 'proj' option is required in `inetermediate_matches` to match the dimensions.

:param torch.Tensor state_S: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor state_T: tensor of shape  (*batch_size*, *length*, *hidden_size*)
:param torch.Tensor mask:    tensor of shape  (*batch_size*, *length*)
, 
Calculate the cross entropy between logits_S and logits_T

:param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)
, 
Calculate the mse loss between logits_S and logits_T

:param logits_S: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param logits_T: Tensor of shape (batch_size, length, num_labels) or (batch_size, num_labels)
:param temperature: A float or a tensor of shape (batch_size, length) or (batch_size,)
, 
if mask is None:
state_S_0 = state_S[0]  # (batch_size , length, hidden_dim)
state_S_1 = state_S[1]  # (batch_size,  length, hidden_dim)
state_T_0 = state_T[0]
state_T_1 = state_T[1]
gram_S = torch.bmm(state_S_0.transpose(1, 2), state_S_1) / state_S_1.size(
1)  # (batch_size, hidden_dim, hidden_dim)
gram_T = torch.bmm(state_T_0.transpose(1, 2), state_T_1) / state_T_1.size(1)
else:
mask = mask.to(state_S[0]).unsqueeze(-1)
lengths = mask.sum(dim=1, keepdim=True)
state_S_0 = state_S[0] * mask
state_S_1 = state_S[1] * mask
state_T_0 = state_T[0] * mask
state_T_1 = state_T[1] * mask
gram_S = torch.bmm(state_S_0.transpose(1, 2), state_S_1) / lengths
gram_T = torch.bmm(state_T_0.transpose(1, 2), state_T_1) / lengths
loss = F.mse_loss(gram_S, gram_T)
return loss


def mmd_loss(state_S, state_T, mask=None):
r'''
* Takes in two lists of matrices `state_S` and `state_T`. Each list contains 2 matrices of the shape (*batch_size*, *length*, *hidden_size*). `hidden_size` of matrices in `State_S` doesn't need to be the same as that of `state_T`. Computes the similarity matrix between the two matrices in `state_S` ( with the resulting shape (*batch_size*, *length*, *length*) ) and the ones in B ( with the resulting shape (*batch_size*, *length*, *length*) ), then computes the mse loss between the similarity matrices:

.. math::

loss = mean((S_{1} \cdot S_{2}^T - T_{1} \cdot T_{2}^T)^2)

* It is a Variant of the NST loss in `Like What You Like: Knowledge Distill via Neuron Selectivity Transfer <https://arxiv.org/abs/1707.01219>`_
* If the `inputs_mask` is given, masks the positions where ``input_mask==0``.

:param torch.tensor state_S: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor state_T: list of two tensors, each tensor is of the shape  (*batch_size*, *length*, *hidden_size*)
:param torch.tensor mask:    tensor of the shape  (*batch_size*, *length*)

Example in `intermediate_matches`::

intermediate_matches = [
{'layer_T':[0,0], 'layer_S':[0,0], 'feature':'hidden','loss': 'nst', 'weight' : 1},
...]
, # (batch_size , length, hidden_dim_S), # (batch_size , length, hidden_dim_T), # (batch_size, hidden_dim), # (batch_size, length, length), # (bs * select, dim), # (bs, len, len), # (bs, num_of_heads, len), # (bs,len,dim), # Adopted from https://github.com/airaria/TextBrewer, # Apache License Version 2.0",
https://github.com/hankcs/HanLP,schedulers.py,0,0.0,51,71.83,8,11.27,7,9.86,5,7.04,0,0.0,13,6,6,2,,"ConstantScheduler, CwsmScheduler, FlswScheduler, FunctionalScheduler, LinearTeacherAnnealingScheduler, NotImplementedError, TemperatureScheduler, __call__, __float__, __init__, _current_training_steps, _num_training_steps, _scheduler_func, abc, base_temperature, beta, classes, constant_temperature_scheduler, cwsm_temperature_scheduler, cwsm_temperature_scheduler_builder, detach, eps, flsw_temperature_scheduler, flsw_temperature_scheduler_builder, forward, from_name, gamma, hanlp_common, int, keys, linear_decay_weight_scheduler, linear_growth_weight_scheduler, list, logits_S, logits_T, max, mean, name, num_training_steps, object, scheduler_func, self, staticmethod, step, super, tau, torch, v, v_max, w, x","abc.ABC, abc.abstractmethod, hanlp_common.configurable.AutoConfigurable, torch.no_grad, torch.norm, torch.pow, torch.softmax","__call__, __float__, __init__, constant_temperature_scheduler, cwsm_temperature_scheduler, cwsm_temperature_scheduler_builder, flsw_temperature_scheduler, flsw_temperature_scheduler_builder, forward, from_name, linear_decay_weight_scheduler, linear_growth_weight_scheduler, step","ConstantScheduler, CwsmScheduler, FlswScheduler, FunctionalScheduler, LinearTeacherAnnealingScheduler, TemperatureScheduler","classes, t, tau, v, v_max, w","Remember to detach logits_S , adapted from arXiv:1911.07471","
    Remember to detach logits_S 
    , 
    adapted from arXiv:1911.07471
    , ., . Expect one from , Unsupported temperature scheduler , constant, cwsm, flsw","0, 0.0001, 0.001, 1, 8, None, True","
Remember to detach logits_S
, 
adapted from arXiv:1911.07471
, # Adopted from https://github.com/airaria/TextBrewer, # Apache License Version 2.0, # x is between 0 and 1",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-17 20:29",
https://github.com/hankcs/HanLP,ngram.py,0,0.0,134,76.57,21,12.0,14,8.0,6,3.43,0,0.0,19,2,38,2,,"Adam, Dropout, Embedding, GRU, LSTM, Linear, Module, NgramSentenceBoundaryDetectionModel, NgramSentenceBoundaryDetector, NotImplementedError, __init__, add, append, backward, batch, batch_size, best_epoch, best_metric, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, build_vocabs, cache, char, char_counter, char_min_freq, char_vocab_size, collections, compute_loss, config, corpus, criterion, current, dataloader, dense, dev, dev_score, device, dict, doc_id_, dropout, each, elapsed_human, embdding_size, embed, embedding_size, enumerate, eos_chars, eos_prediction, epoch, epochs, eval, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, flat, flatten_parameters, forward, grad_norm, hanlp, hanlp_common, has_cache, info, int, isinstance, item, items, len, list, lock, log, logger, logging, logits, loss, lower, lr, max, model, nonzero, nonzero_offsets, offset, offset_, optimizer, output, outputs, parameters, predict, prediction, prev_offset, purge_cache, range, ratio_width, report, report_metrics, reset, reset_metrics, rnn, rnn_bidirectional, rnn_layers, rnn_size, rnn_type, samples, save_dir, save_weights, score, self, sents_per_document, shuffle, squeeze, step, str, strip, summary, super, timer, torch, total, total_loss, total_time_human, train, trn, typing, update_metrics, vocabs, window, window_size, x, zero_grad, zip","collections.Counter, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.torch_component.TorchComponent, hanlp.common.vocab.Vocab, hanlp.datasets.eos.eos.SentenceBoundaryDetectionDataset, hanlp.metrics.f1.F1, hanlp.utils.time_util.CountdownTimer, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.FloatTensor, torch.Tensor, torch.nn, torch.nn.BCEWithLogitsLoss, torch.nn.Module, torch.nn.utils.clip_grad_norm_, torch.optim, torch.utils.data.DataLoader, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, build_vocabs, compute_loss, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, forward, nonzero_offsets, predict, report_metrics, reset_metrics, update_metrics","NgramSentenceBoundaryDetectionModel, NgramSentenceBoundaryDetector","batch, best_epoch, best_metric, char_counter, char_min_freq, corpus, data, dataloader, dataset, dev_score, doc_id_, each, eos_chars, eos_prediction, epoch, flat, has_cache, logits, loss, model, offset, offset_, optimizer, output, outputs, prediction, prev_offset, ratio_width, report, rnn_type, samples, sents_per_document, timer, total_loss, vocab, window, window_size, x","A sentence boundary detector using ngram as features and LSTM as encoder (:cite:`Schweter:Ahmed:2019`).
It predicts whether a punctuation marks an ``EOS``.

.. Note::
    This component won't work on text without the punctuations defined in its config. It's always
    recommended to understand how it works before using it. The predefined punctuations can be listed by the
    following codes.

    >>> print(eos.config.eos_chars)

Args:
    **kwargs: Passed to config., Sentence split.

Args:
    data: A paragraph or a list of paragraphs.
    batch_size: Number of samples per batch.
    strip: Strip out blank characters at the head and tail of each sentence.

Returns:
    A list of sentences or a list of lists of sentences."," ,  / ,  [red]saved[/red], ', ' has to be one of [LSTM, GRU], .4f, /, :[/yellow], A sentence boundary detector using ngram as features and LSTM as encoder (:cite:`Schweter:Ahmed:2019`).
        It predicts whether a punctuation marks an ``EOS``.

        .. Note::
            This component won't work on text without the punctuations defined in its config. It's always
            recommended to understand how it works before using it. The predefined punctuations can be listed by the
            following codes.

            >>> print(eos.config.eos_chars)

        Args:
            **kwargs: Passed to config.
        , LSTM, Sentence split.

        Args:
            data: A paragraph or a list of paragraphs.
            batch_size: Number of samples per batch.
            strip: Strip out blank characters at the head and tail of each sentence.

        Returns:
            A list of sentences or a list of lists of sentences.
        , [yellow]Epoch , char, char_id, doc_id_, gru, label_id, loss: , lstm, offset_, sum","0, 0.0, 0.001, 0.2, 1, 128, 2, 200, 256, 32, 5, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-26 20:19, # noinspection PyMethodOverriding, A sentence boundary detector using ngram as features and LSTM as encoder (:cite:`Schweter:Ahmed:2019`).
It predicts whether a punctuation marks an ``EOS``.

.. Note::
This component won't work on text without the punctuations defined in its config. It's always
recommended to understand how it works before using it. The predefined punctuations can be listed by the
following codes.

>>> print(eos.config.eos_chars)

Args:
**kwargs: Passed to config.
, Sentence split.

Args:
data: A paragraph or a list of paragraphs.
batch_size: Number of samples per batch.
strip: Strip out blank characters at the head and tail of each sentence.

Returns:
A list of sentences or a list of lists of sentences.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-26 20:19",
https://github.com/hankcs/HanLP,mlm.py,0,0.0,72,80.9,7,7.87,7,7.87,3,3.37,0,0.0,15,2,20,0,,"MaskedLanguageModel, MaskedLanguageModelDataset, NotImplementedError, __init__, all_special_ids, any, append, batch, batch_size, br, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, convert_ids_to_tokens, dataloader, device, dict, each, evaluate_dataloader, execute_training_loop, extend, fit_dataloader, flat, from_pretrained, hanlp, hanlp_common, index, indices, input_ids, input_is_flat, isinstance, kwargs, len, lens, load_config, load_file, load_vocabs, load_weights, log, logging, logits, mask, mask_token_id, masked_logits, masked_sents, math, model, num_masks, offset, orders, outputs, predict, probs, results, save_dir, self, size, str, sum, super, tokenizer, tolist, topk, torch, transformer, transformers, typing, verbose, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSampler, hanlp.common.dataset.TransformableDataset, hanlp.common.torch_component.TorchComponent, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.transform.transformer_tokenizer.TransformerTextTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.reorder, logging.Logger, math.inf, torch.nn.Module, torch.nn.functional.softmax, torch.utils.data.DataLoader, transformers.AutoModelForMaskedLM, transformers.tokenization_utils.PreTrainedTokenizer, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, evaluate_dataloader, execute_training_loop, fit_dataloader, input_is_flat, load_config, load_file, load_vocabs, load_weights, predict","MaskedLanguageModel, MaskedLanguageModelDataset","batch, br, dataloader, dataset, each, flat, index, indices, input_ids, lens, mask, masked_logits, masked_sents, num_masks, offset, orders, outputs, probs, results, verbose",,"Preprocessing and caching samples [blink][yellow]...[/yellow][/blink], config.json, model.pt, token, token_attention_mask, token_input_ids, vocabs.json","0, 1, 10, 32, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-01-29 21:07",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-01-29 21:07",
https://github.com/hankcs/HanLP,multi_task_learning.py,0,0.0,274,65.87,85,20.43,14,3.37,43,10.34,0,0.0,38,3,117,3,,"AdamW, MultiTaskDataLoader, MultiTaskLearning, MultiTaskModel, NotImplementedError, TypeError, Z, __call__, __delitem__, __getitem__, __init__, __iter__, __len__, __repr__, __setattr__, _close_dataloader, _computation_graph, _data, _encode, _lr, _resolve_task_name, _task_topological_order, _tasks_in_topological_order, _x, adam_epsilon, add, any, append, average_subwords, backward, batch, batch_size, best_epoch, best_metric, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, build_samples, build_tokenizer, build_transform, cache, cells, close, cls_is_bos, collections, computation_graph, compute_loss, config, copy, criterion, cstr, current, dataloader, dataloaders, dataset, debug, decay_parameters, decode_output, decoder, decoder_grad_norm, decoder_optimizer, decoder_optimizers, decoder_parameters, decoder_scheduler, decoders, dependencies, dependency, dev, dev_data, dev_score, device, discard, elapsed_human, encoder, encoder_grad_norm, encoder_lr, encoder_optimizer, encoder_parameters, encoder_scheduler, encoder_size, encoder_transform, endswith, enumerate, epoch, epochs, eval, eval_trn, evaluate, evaluate_dataloader, execute_training_loop, extend, extra_transform, feed_batch, filename, finalize_document, first_task, first_task_name, fit, fit_dataloader, flat, flatten_target_tasks, float, from_iterable, functools, get, get_linear_schedule_with_warmup, get_output_dim, grad_norm, gradient_accumulation, group, group_id, hanlp, hanlp_common, hasattr, headings, hidden, hidden_states, history, idx, index, info, input, input_is_flat, insert, int, isinstance, item, items, iterators, itertools, key, keys, len, length_transform, lengths_to_mask, list, load_vocabs, load_weights, log, logger, logging, longest, loss, lr, mask, matched_children, matrix, min_epochs, model, model_, module, name, named_parameters, next, no_decay, no_decay_by_lr, no_decay_parameters, num_training_steps, numpy, object, on_config_ready, optimizer, os, output, output_dict, output_key, output_per_task, output_spans, pad_data, parallelize, parameter_groups, parameters, patience, please_read_the_doc_ok, pop, pow, predict, predict_task, prediction, prediction_to_result, prefetch, prefix_matched, property, ps, range, ratio_width, raw_hidden, remove, replace, report, report_metrics, repr, reset, reset_metrics, resolve_tasks, resolved_dependencies, resolved_tasks, results, ret_raw_hidden_states, rets, rows, run_transform, samples, sampling_weights, save_dir, save_vocabs, save_weights, scalar_mix, scalar_mixes, score, self, sep_is_eos, separate_optimizer, shuffle, sizes, skip_tasks, sorted, span_per_sent, spans, splitlines, startswith, staticmethod, step, sum, super, target_tasks, target_topological_order, task_dataloader, task_lr, task_name, task_names, task_topological_order, tasks, tasks_in_topological_order, tasks_need_custom_eval, tau, timer, token_per_sent, tokenizer, tokens, toposort, torch, torch_util, total, total_loss, total_size, total_time_human, train, training, transform, transform_batch, transformer_module, transformers, trn, trn_data, tst, tuple, type, typing, update, update_metrics, use_raw_hidden_states, utils, value, values, vocabs, warmup_steps, weight, weight_decay, zero_grad, zip","collections.defaultdict, copy.copy, functools.partial, hanlp.common.dataset.CachedDataLoader, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.PrefetchDataLoader, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.transform.TransformList, hanlp.components.mtl.tasks.Task, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbedding, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.transformers.utils.pick_tensor_for_each_token, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.clip_grad_norm, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.constant.IDX, hanlp_common.document.Document, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.prefix_match, hanlp_common.util.reorder, hanlp_common.util.topological_sort, hanlp_common.visualization.markdown_table, itertools.chain, itertools.cycle, logging.Logger, np.random.choice, os.getpid, toposort.toposort, torch.FloatTensor, torch.Tensor, torch.argmax, torch.device, torch.long, torch.nn.Module, torch.nn.ModuleDict, torch.nn.utils.clip_grad_norm_, torch.no_grad, torch.tensor, torch.utils.data.DataLoader, transformers.optimization, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Optional, typing.Tuple, typing.Union","__call__, __delitem__, __getitem__, __init__, __iter__, __len__, __repr__, __setattr__, _close_dataloader, _encode, _resolve_task_name, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, build_transform, compute_loss, decode_output, evaluate, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, items, load_vocabs, on_config_ready, parallelize, predict, predict_task, report_metrics, reset_metrics, resolve_tasks, sampling_weights, save_vocabs, sizes, update_metrics","MultiTaskDataLoader, MultiTaskLearning, MultiTaskModel","Z, _data, _lr, average_subwords, batch, batch_size, best_epoch, best_metric, cells, cls_is_bos, computation_graph, config, data, dataloader, decay_parameters, decoder, decoder_optimizer, decoder_optimizers, decoder_parameters, decoder_scheduler, decoders, dependencies, dependency, dev_data, dev_score, device, doc, encoder, encoder_optimizer, encoder_parameters, encoder_scheduler, encoder_size, encoder_transform, epoch, epochs, extra_transform, first_task, first_task_name, flat, flatten_target_tasks, group, group_id, h, headings, hidden, hidden_states, history, idx, item, iterators, key, length_transform, longest, loss, mask, matched_children, matrix, metric, metrics, min_epochs, model, name, no_decay, no_decay_by_lr, no_decay_parameters, num_training_steps, order, output, output_dict, output_per_task, output_spans, parameter_groups, patience, please_read_the_doc_ok, prediction, prefix_matched, ps, ratio_width, raw_hidden, report, resolved_dependencies, results, rets, rows, samples, sampling_weights, scalar_mix, scalar_mixes, sep_is_eos, skip_tasks, span_per_sent, spans, table, target_tasks, target_topological_order, task, task_dataloader, task_lr, task_name, task_names, task_topological_order, tasks, tasks_in_topological_order, tasks_need_custom_eval, timer, token_per_sent, tokens, total_loss, total_size, training, transform, transformer_module, trn_data, tst, use_raw_hidden_states, v, weight","A multi-task learning (MTL) framework. It shares the same encoder across multiple decoders. These decoders
can have dependencies on each other which will be properly handled during decoding. To integrate a component
into this MTL framework, a component needs to implement the :class:`~hanlp.components.mtl.tasks.Task` interface.

This framework mostly follows the architecture of :cite:`clark-etal-2019-bam` and :cite:`he-choi-2021-stem`, with additional scalar mix
tricks (:cite:`kondratyuk-straka-2019-75`) allowing each task to attend to any subset of layers. We also
experimented with knowledge distillation on single tasks, the performance gain was nonsignificant on a large
dataset. In the near future, we have no plan to invest more efforts in distillation, since most datasets HanLP
uses are relatively large, and our hardware is relatively powerful.

Args:
    **kwargs: Arguments passed to config., Delete a task (and every resource it owns) from this component.

Args:
    task_name: The name of the task to be deleted.

Examples:
    >>> del mtl['dep']  # Delete dep from MTL, Predict on data.

Args:
    data: A sentence or a list of sentences.
    tasks: The tasks to predict.
    skip_tasks: The tasks to skip.
    resolved_tasks: The resolved tasks to override ``tasks`` and ``skip_tasks``.
    **kwargs: Not used.

Returns:
    A :class:`~hanlp_common.document.Document`.","
,  ,  (,  / ,  = ...,  A multi-task learning (MTL) framework. It shares the same encoder across multiple decoders. These decoders
        can have dependencies on each other which will be properly handled during decoding. To integrate a component
        into this MTL framework, a component needs to implement the :class:`~hanlp.components.mtl.tasks.Task` interface.

        This framework mostly follows the architecture of :cite:`clark-etal-2019-bam` and :cite:`he-choi-2021-stem`, with additional scalar mix
        tricks (:cite:`kondratyuk-straka-2019-75`) allowing each task to attend to any subset of layers. We also
        experimented with knowledge distillation on single tasks, the performance gain was nonsignificant on a large
        dataset. In the near future, we have no plan to invest more efforts in distillation, since most datasets HanLP
        uses are relatively large, and our hardware is relatively powerful.

        Args:
            **kwargs: Arguments passed to config.
        ,  Maybe you are looking for one of its tasks: ,  [red]saved[/red],  ^,  early stop,  to return a decoder.,  to return a metric., ""]., #batches, #epoch, #scaled, %batches, %scaled, ), *, -, -cache.pt, ., . Check your dependencies definition: , . For example, HanLP["", .2%, .2f, .4f, /, :[/yellow], Delete a task (and every resource it owns) from this component.

        Args:
            task_name: The name of the task to be deleted.

        Examples:
            >>> del mtl['dep']  # Delete dep from MTL

        , LayerNorm.bias, LayerNorm.weight, No prefix matching for , No task to perform due to `tasks = , Parallelization is not implemented yet., Please implement `build_metric` of , Please implement `build_model` of , Predict on data.

        Args:
            data: A sentence or a list of sentences.
            tasks: The tasks to predict.
            skip_tasks: The tasks to skip.
            resolved_tasks: The resolved tasks to override ``tasks`` and ``skip_tasks``.
            **kwargs: Not used.

        Returns:
            A :class:`~hanlp_common.document.Document`.
        , Restoring best model saved [red], Samples Distribution, This MTL component has no , [/blue] dataset for [cyan], [/cyan] ..., [/red] epochs ago, [/red][/bold], [/yellow] Building [blue], [/yellow][/bold], [bold][red], [bold][yellow], [yellow], [yellow]Epoch , _, __class__, `., bias, close, dev, dict, grad_norm, gradient_accumulation, hidden, kwargs, loss: , lr, mask, output, output_spans, params, prediction, raw_hidden, self, task, tasks, tasks_need_custom_eval, tok, token, token_length, token_token_span, transform, trn, tst, vocabs.json, weight_decay, |","0, 0.0, 0.001, 0.1, 0.5, 0.8, 1, 1e-08, 2, 5.0, 5e-05, False, None, True"," A multi-task learning (MTL) framework. It shares the same encoder across multiple decoders. These decoders
can have dependencies on each other which will be properly handled during decoding. To integrate a component
into this MTL framework, a component needs to implement the :class:`~hanlp.components.mtl.tasks.Task` interface.

This framework mostly follows the architecture of :cite:`clark-etal-2019-bam` and :cite:`he-choi-2021-stem`, with additional scalar mix
tricks (:cite:`kondratyuk-straka-2019-75`) allowing each task to attend to any subset of layers. We also
experimented with knowledge distillation on single tasks, the performance gain was nonsignificant on a large
dataset. In the near future, we have no plan to invest more efforts in distillation, since most datasets HanLP
uses are relatively large, and our hardware is relatively powerful.

Args:
**kwargs: Arguments passed to config.
, #                 f'samples out of {size_before}.'), #     # noinspection PyTypeChecker, #     logger.info(f'Pruned [yellow]{num_pruned} ({num_pruned / size_before:.1%})[/yellow] ', #     num_pruned = size_before - size_after, #     return list(doc.values())[0], #     size_after = len(task_dataset), #     size_before = len(task_dataset), #     task_dataset.prune(prune), #     task_dataset: TransformDataset = task_dataloader.dataset, # -*- coding:utf-8 -*-, # Activate scalar mix starting from 0-th layer, # Adjust Tokenizer according to task config, # Allow task to perform finalization on document, # Author: hankcs, # Date: 2020-06-20 19:55, # If any task enables scalar_mix, hidden_states will be a 4d tensor, # If no tok in doc, use raw input as tok, # If the task doesn't need cls while h has cls, remove cls, # If there is only one field, don't bother to wrap it, # Now build the dataloaders and execute tasks, # Now we decide which tasks to perform and their orders, # Override the tokenizer config of the 1st task, # Put results into doc in the order of tasks, # Run each task group in order, # Run the first task, let it make the initial batch for the successors, # Sort target tasks within the same group in a defined order, # The offsets start with 0 while [CLS] is zero, # This method is only called during training or evaluation but not prediction, # We are kind of hard coding here. If the first task is a tokenizer,, # We can cache this order, # We could parallelize this in the future, # if len(doc) == 1:, # if prune:, # noinspection PyAttributeOutsideInit, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, # prune: Callable = None,, # self.iterators = dict((k, iter(v)) for k, v in dataloaders.items()), # we need to convert the hidden and mask to token level, #batches', '%batches', '#scaled', '%scaled', '#epoch'], Delete a task (and every resource it owns) from this component.

Args:
task_name: The name of the task to be deleted.

Examples:
>>> del mtl['dep']  # Delete dep from MTL

, Predict on data.

Args:
data: A sentence or a list of sentences.
tasks: The tasks to predict.
skip_tasks: The tasks to skip.
resolved_tasks: The resolved tasks to override ``tasks`` and ``skip_tasks``.
**kwargs: Not used.

Returns:
A :class:`~hanlp_common.document.Document`.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-20 19:54",
https://github.com/hankcs/HanLP,ner_tf.py,0,0.0,44,64.71,3,4.41,16,23.53,5,7.35,0,0.0,5,6,4,0,,"IOBES_NamedEntityRecognizer, IOBES_Transform, IOBES_TransformerTransform, NgramConvNamedEntityRecognizerTF, RNNNamedEntityRecognizerTF, TransformerNamedEntityRecognizerTF, X, Y, Y_to_outputs, __init__, abc, batch, batch_size, build_loss, dev_data, dict, dropout_embed, dropout_hidden, embedding_trainable, epochs, filters, fit, gold, hanlp, hanlp_common, inputs, int, kernel_size, logger, ngram_embed, predict_batch, save_dir, str, super, tags, tensorflow, trn_data, typing, verbose, weight_norm, window_size, word_embed, words, zip","abc.ABC, hanlp.common.keras_component.KerasComponent, hanlp.common.transform_tf.Transform, hanlp.components.taggers.ngram_conv.ngram_conv_tagger.NgramConvTaggerTF, hanlp.components.taggers.rnn_tagger_tf.RNNTaggerTF, hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF, hanlp.components.taggers.transformers.transformer_transform_tf.TransformerTransform, hanlp.metrics.chunking.sequence_labeling.iobes_to_span, hanlp_common.util.merge_locals_kwargs, tf.Tensor, tf.keras.losses.Loss, tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.optimizers.Optimizer, typing.Any, typing.Iterable, typing.Tuple, typing.Union","Y_to_outputs, __init__, build_loss, fit, predict_batch","IOBES_NamedEntityRecognizer, IOBES_Transform, IOBES_TransformerTransform, NgramConvNamedEntityRecognizerTF, RNNNamedEntityRecognizerTF, TransformerNamedEntityRecognizerTF","loss, tags, transform, words",,"adam, adamw, f1","0, 0.2, 1.0, 100, 128, 1e-08, 20, 200, 3, 32, 4, 50, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-09-14 20:33, # assert kwargs.get('run_eagerly', True), 'This component can only run eagerly', # kwargs['run_eagerly'] = True",
https://github.com/hankcs/HanLP,rnn_ner.py,0,0.0,56,68.29,11,13.41,11,13.41,4,4.88,0,0.0,8,1,9,1,,"RNNNamedEntityRecognizer, ValueError, __init__, _id_to_tags, all, b, batch, batch_size, bio_tags_to_spans, bioul_tags_to_spans, build_metric, config, criterion, data, decode_output, entities, enumerate, es, evaluate_dataloader, filename, fit, float, hanlp, hanlp_common, info, int, iobes_tags_to_spans, isinstance, join, len, logger, logits, loss, mask, outputs, predict, predict_data, ratio_width, result, save_config, save_dir, self, span_util, super, tagging_scheme, token_delimiter, token_key, tokens, tolist, torch, typing, update_metrics, utils, vocabs, x, zip","hanlp.components.taggers.rnn_tagger.RNNTagger, hanlp.metrics.chunking.conlleval.SpanF1, hanlp_common.util.merge_locals_kwargs, torch.Tensor, typing.Any","__init__, build_metric, evaluate_dataloader, fit, predict, predict_data, save_config, update_metrics",RNNNamedEntityRecognizer,"b, entities, es, logits, loss, outputs, tagging_scheme, tokens, x","An old-school RNN tagger using word2vec or fasttext embeddings.

Args:
    **kwargs: Predefined config.",",  , An old-school RNN tagger using word2vec or fasttext embeddings.

        Args:
            **kwargs: Predefined config.
        , BIO, BIOUL, IOBES, Unrecognized tag scheme , adam, config.json, tag, token","0.001, 0.5, 1, 10, 100, 2, 256, 50, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-12 18:00, An old-school RNN tagger using word2vec or fasttext embeddings.

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,transformer_ner.py,0,0.0,70,64.81,22,20.37,12,11.11,4,3.7,0,0.0,14,1,25,2,,"TransformerNamedEntityRecognizer, __init__, append, append_transform, batch, batch_ner, batch_tags, begin, build_dataset, build_metric, build_vocabs, config, decode_output, decorate_spans, delimiter_in_entity, dict_blacklist, dict_whitelist, dictionary, end, entity, fit, float, functools, generate_prediction_filename, get, gold, hanlp, hanlp_common, hanlp_trie, info, input_is_flat, int, isinstance, join, len, list, logger, logits, mask, matches, merge_types, merged_entities, model, ner_per_sent, output, pred, prediction, prediction_to_human, property, pruned, range, replace, save_dir, self, sents, setter, spans, spans_per_sent, start, super, tag_to_span, token_key, tokenize, tokens, trn, tst_data, typing, update_metrics, vocabs, zip","functools.partial, hanlp.common.dataset.SamplerBuilder, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, hanlp.datasets.ner.loaders.json_ner.prune_ner_tagset, hanlp.metrics.chunking.sequence_labeling.get_entities, hanlp.metrics.f1.F1, hanlp.utils.string_util.guess_delimiter, hanlp_common.util.merge_locals_kwargs, hanlp_trie.DictInterface, hanlp_trie.TrieDict, typing.Any, typing.Dict, typing.List, typing.Set, typing.Union","__init__, build_dataset, build_metric, build_vocabs, decode_output, decorate_spans, dict_blacklist, dict_whitelist, fit, generate_prediction_filename, input_is_flat, prediction_to_human, tag_to_span, update_metrics",TransformerNamedEntityRecognizer,"batch_ner, begin, dataset, delimiter_in_entity, dict_blacklist, dict_whitelist, dictionary, end, entities, entity, gold, matches, merge_types, merged_entities, ner_per_sent, output, pred, prediction, pruned, sents, spans, spans_per_sent, start, tagset, tokens","A simple tagger using transformers and a linear layer with an optional CRF
(:cite:`lafferty2001conditional`) layer for
NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.
During decoding, it performs longest-prefix-matching of these words to override the prediction from
underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.

.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
    do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

Args:
    **kwargs: Not used., Fit component to training set.

Args:
    trn_data: Training set.
    dev_data: Development set.
    save_dir: The directory to save trained component.
    transformer: An identifier of a pre-trained transformer.
    delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining
        tokens during decoding.
    merge_types: The types of consecutive entities to be merged.
    average_subwords: ``True`` to average subword representations.
    word_dropout: Dropout rate to randomly replace a subword with MASK.
    hidden_dropout: Dropout rate applied to hidden states.
    layer_dropout: Randomly zero out hidden states of a transformer layer.
    scalar_mix: Layer attention.
    grad_norm: Gradient norm for clipping.
    lr: Learning rate for decoder.
    transformer_lr: Learning for encoder.
    adam_epsilon: The epsilon to use in Adam.
    weight_decay: The weight decay to use.
    warmup_steps: The number of warmup steps.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden
        states from the main encoder as input.
    reduction: The loss reduction used in aggregating losses.
    batch_size: The number of samples in a batch.
    sampler_builder: The builder to build sampler, which will override batch_size.
    epochs: The number of epochs to train.
    tagset: Optional tagset to prune entities outside of this tagset from datasets.
    token_key: The key to tokens in dataset.
    max_seq_len: The maximum sequence length. Sequence longer than this will be handled by sliding
        window.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level, which is never the case for
        lemmatization.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    transform: An optional transform to be applied to samples. Usually a character normalization transform is
        passed in.
    devices: Devices this component will live on.
    logger: Any :class:`logging.Logger` instance.
    seed: Random seed to reproduce this training.
    **kwargs: Not used.

Returns:
    The best metrics on training set."," , ""[/blue]. If not, specify `delimiter_in_entity` in `fit()`, .tsv, .txt, A simple tagger using transformers and a linear layer with an optional CRF
        (:cite:`lafferty2001conditional`) layer for
        NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.
        During decoding, it performs longest-prefix-matching of these words to override the prediction from
        underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.

        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

        Args:
            **kwargs: Not used.
        , B-, BM, E-, Fit component to training set.

        Args:
            trn_data: Training set.
            dev_data: Development set.
            save_dir: The directory to save trained component.
            transformer: An identifier of a pre-trained transformer.
            delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining
                tokens during decoding.
            merge_types: The types of consecutive entities to be merged.
            average_subwords: ``True`` to average subword representations.
            word_dropout: Dropout rate to randomly replace a subword with MASK.
            hidden_dropout: Dropout rate applied to hidden states.
            layer_dropout: Randomly zero out hidden states of a transformer layer.
            scalar_mix: Layer attention.
            grad_norm: Gradient norm for clipping.
            lr: Learning rate for decoder.
            transformer_lr: Learning for encoder.
            adam_epsilon: The epsilon to use in Adam.
            weight_decay: The weight decay to use.
            warmup_steps: The number of warmup steps.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden
                states from the main encoder as input.
            reduction: The loss reduction used in aggregating losses.
            batch_size: The number of samples in a batch.
            sampler_builder: The builder to build sampler, which will override batch_size.
            epochs: The number of epochs to train.
            tagset: Optional tagset to prune entities outside of this tagset from datasets.
            token_key: The key to tokens in dataset.
            max_seq_len: The maximum sequence length. Sequence longer than this will be handled by sliding
                window.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level, which is never the case for
                lemmatization.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            transform: An optional transform to be applied to samples. Usually a character normalization transform is
                passed in.
            devices: Devices this component will live on.
            logger: Any :class:`logging.Logger` instance.
            seed: Random seed to reproduce this training.
            **kwargs: Not used.

        Returns:
            The best metrics on training set.
        , Guess the delimiter between tokens in named entity could be [blue]"", I-, ME, S-, _, delimiter_in_entity, dict_blacklist, dict_whitelist, merge_types, sum, tag, tagset, token","0, 0.1, 0.2, 1, 1e-08, 2, 3, 32, 5.0, 5e-05, False, None","
super().__init__(**kwargs)

def build_metric(self, **kwargs):
return F1()

# noinspection PyMethodOverriding
def update_metrics(self, metric, logits, y, mask, batch, prediction):
for p, g in zip(prediction, self.tag_to_span(batch['tag'], batch)):
pred = set(p)
gold = set(g)
metric(pred, gold)

# noinspection PyMethodOverriding
def decode_output(self, logits, mask, batch, model=None):
output = super().decode_output(logits, mask, batch, model)
prediction = super().prediction_to_human(output, self.vocabs['tag'].idx_to_token, batch)
return self.tag_to_span(prediction, batch)

def tag_to_span(self, batch_tags, batch):
spans = []
sents = batch[self.config.token_key]
dict_whitelist = self.dict_whitelist
dict_blacklist = self.dict_blacklist
merge_types = self.config.get('merge_types', None)
for tags, tokens in zip(batch_tags, sents):
entities = get_entities(tags)
if dict_whitelist:
matches = dict_whitelist.tokenize(tokens)
if matches:
# Fix O E-LOC O like predictions
entities = get_entities(tags)
for label, start, end in entities:
if end - start == 1:
tags[start] = 'S-' + label
else:
tags[start] = 'B-' + label
for i in range(start + 1, end - 1):
tags[i] = 'I-' + label
tags[end - 1] = 'E-' + label
for start, end, label in matches:
if (not tags[start][0] in 'ME') and (not tags[end - 1][0] in 'BM'):
if end - start == 1:
tags[start] = 'S-' + label
else:
tags[start] = 'B-' + label
for i in range(start + 1, end - 1):
tags[i] = 'I-' + label
tags[end - 1] = 'E-' + label
entities = get_entities(tags)
if merge_types and len(entities) > 1:
merged_entities = []
begin = 0
for i in range(1, len(entities)):
if entities[begin][0] != entities[i][0] or entities[i - 1][2] != entities[i][1] \
or entities[i][0] not in merge_types:
merged_entities.append((entities[begin][0], entities[begin][1], entities[i - 1][2]))
begin = i
merged_entities.append((entities[begin][0], entities[begin][1], entities[-1][2]))
entities = merged_entities

if dict_blacklist:
pruned = []
delimiter_in_entity = self.config.get('delimiter_in_entity', ' ')
for label, start, end in entities:
entity = delimiter_in_entity.join(tokens[start:end])
if entity not in dict_blacklist:
pruned.append((label, start, end))
entities = pruned
spans.append(entities)
return spans

def decorate_spans(self, spans, batch):
batch_ner = []
delimiter_in_entity = self.config.get('delimiter_in_entity', ' ')
for spans_per_sent, tokens in zip(spans, batch.get(f'{self.config.token_key}_', batch[self.config.token_key])):
ner_per_sent = []
for label, start, end in spans_per_sent:
ner_per_sent.append((delimiter_in_entity.join(tokens[start:end]), label, start, end))
batch_ner.append(ner_per_sent)
return batch_ner

def generate_prediction_filename(self, tst_data, save_dir):
return super().generate_prediction_filename(tst_data.replace('.tsv', '.txt'), save_dir)

def prediction_to_human(self, pred, vocab, batch):
return self.decorate_spans(pred, batch)

def input_is_flat(self, tokens):
return tokens and isinstance(tokens, list) and isinstance(tokens[0], str)

def fit(self, trn_data, dev_data, save_dir, transformer,
delimiter_in_entity=None,
merge_types: List[str] = None,
average_subwords=False,
word_dropout: float = 0.2,
hidden_dropout=None,
layer_dropout=0,
scalar_mix=None,
grad_norm=5.0,
lr=5e-5,
transformer_lr=None,
adam_epsilon=1e-8,
weight_decay=0,
warmup_steps=0.1,
crf=False,
secondary_encoder=None,
reduction='sum',
batch_size=32,
sampler_builder: SamplerBuilder = None,
epochs=3,
tagset=None,
token_key='token',
max_seq_len=None,
sent_delimiter=None,
char_level=False,
hard_constraint=False,
transform=None,
logger=None,
seed=None,
devices: Union[float, int, List[int]] = None,
**kwargs):
Fit component to training set., # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-07 11:08",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-21 17:22",
https://github.com/hankcs/HanLP,alg.py,0,0.0,144,59.5,14,5.79,8,3.31,76,31.4,0,0.0,14,0,92,10,,"alg, any, append, arc_preds, argmax, as_strided, assigned, backtrack, bad, batch_size, centroids, chuliu_edmonds, cky, cl, cl_path, cl_span, clusters, complete, connect, contiguous, contract, contracted, cr, cr_path, cr_span, cycle, cycle_head, cycle_heads, cycle_root, d, decode_dep, deps, detach, device, dfn, diagonal, dim, dists, eisner, eisner2o, enumerate, eq, equal, flag, gt, hanlp_common, head, heads, il, il0, il_path, il_span, ilr, index_fill, index_fill_, ir, ir0, ir_path, ir_span, istree, k, kmeans, len, length, lens, list, low, ltree, mask, max, max_it, min, mst, multiroot, n_roots, ne, new_tensor, new_zeros, next, noncycle, none, numel, offset, old, onstack, out_tensor, p_i, p_s, p_span, pad, padding_value, permute, pop, preds, range, root, roots, rtree, s_arc, s_best, s_c, s_cycle, s_dep, s_head, s_i, s_root, s_s, s_sib, s_span, s_tree, scores, seq, seq_len, sequence, shape, size, slice, slr, slr_path, slr_span, sorted, split, squeeze, stack, starts, stride, stripe, subtree, sum, tarjan, tensors, timestep, to, tolist, torch, total_length, tree, trees, unique, unsqueeze, view, x, y, zip","hanlp_common.conll.isprojective, torch.abs_, torch.arange, torch.cat, torch.float, torch.full_like, torch.long, torch.ones, torch.randperm, torch.tensor, torch.where","backtrack, chuliu_edmonds, cky, connect, contract, decode_dep, eisner, eisner2o, istree, kmeans, mst, pad, stripe, tarjan",,"alg, arc_preds, assigned, b, bad, batch_size, c, centroids, cl, cl_path, cl_span, clusters, contracted, cr, cr_path, cr_span, cycle, cycle_head, cycle_heads, cycle_root, d, deps, dfn, dists, f, head, heads, il, il0, il_path, il_span, ilr, ir, ir0, ir_path, ir_span, k, length, lens, low, ltree, mask, n, n_roots, noncycle, none, numel, old, onstack, out_tensor, p, p_c, p_i, p_s, p_span, preds, r, root, roots, rtree, s, s_arc, s_best, s_c, s_cycle, s_dep, s_head, s_i, s_root, s_s, s_sib, s_span, s_tree, scores, seq, seq_len, sequence, size, slr, slr_path, slr_span, split, stack, starts, stride, subtree, t, timestep, tree, trees, x, y","Args:
    s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all possible arcs.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask for covering the unpadded tokens.
    tree (bool):
        If ``True``, ensures to output well-formed trees. Default: ``False``.
    proj (bool):
        If ``True``, ensures to output projective trees. Default: ``False``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        Predicted arcs and labels of shape ``[batch_size, seq_len]``., Checks if the arcs form an valid dependency tree.

Args:
    sequence (list[int]):
        A list of head indices.
    proj (bool):
        If ``True``, requires the tree to be projective. Default: ``False``.
    multiroot (bool):
        If ``False``, requires the tree to contain only a single root. Default: ``True``.

Returns:
    ``True`` if the arcs form an valid tree, ``False`` otherwise.

Examples:
    >>> istree([3, 0, 0, 3], multiroot=True)
    True
    >>> istree([3, 0, 0, 3], proj=True)
    False, ChuLiu/Edmonds algorithm for non-projective decoding.

Some code is borrowed from `tdozat's implementation`_.
Descriptions of notations and formulas can be found in
`Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

Notes:
    The algorithm does not guarantee to parse a single-root tree.

References:
    - Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan Hajic. 2005.
      `Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

Args:
    s (~torch.Tensor): ``[seq_len, seq_len]``.
        Scores of all dependent-head pairs.

Returns:
    ~torch.Tensor:
        A tensor with shape ``[seq_len]`` for the resulting non-projective parse tree.

.. _tdozat's implementation:
    https://github.com/tdozat/Parser-v3
.. _Non-projective Dependency Parsing using Spanning Tree Algorithms:
    https://www.aclweb.org/anthology/H05-1066/, First-order Eisner algorithm for projective decoding.

References:
    - Ryan McDonald, Koby Crammer and Fernando Pereira. 2005.
      `Online Large-Margin Training of Dependency Parsers`_.

Args:
    scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all dependent-head pairs.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask to avoid parsing over padding tokens.
        The first column serving as pseudo words for roots should be ``False``.

Returns:
    ~torch.Tensor:
        A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

Examples:
    >>> scores = torch.tensor([[[-13.5026, -18.3700, -13.0033, -16.6809],
                                [-36.5235, -28.6344, -28.4696, -31.6750],
                                [ -2.9084,  -7.4825,  -1.4861,  -6.8709],
                                [-29.4880, -27.6905, -26.1498, -27.0233]]])
    >>> mask = torch.tensor([[False,  True,  True,  True]])
    >>> eisner(scores, mask)
    tensor([[0, 2, 0, 2]])

.. _Online Large-Margin Training of Dependency Parsers:
    https://www.aclweb.org/anthology/P05-1012/, KMeans algorithm for clustering the sentences by length.

Args:
    x (list[int]):
        The list of sentence lengths.
    k (int):
        The number of clusters.
        This is an approximate value. The final number of clusters can be less or equal to `k`.
    max_it (int):
        Maximum number of iterations.
        If centroids does not converge after several iterations, the algorithm will be early stopped.

Returns:
    list[float], list[list[int]]:
        The first list contains average lengths of sentences in each cluster.
        The second is the list of clusters holding indices of data points.

Examples:
    >>> x = torch.randint(10,20,(10,)).tolist()
    >>> x
    [15, 10, 17, 11, 18, 13, 17, 19, 18, 14]
    >>> centroids, clusters = kmeans(x, 3)
    >>> centroids
    [10.5, 14.0, 17.799999237060547]
    >>> clusters
    [[1, 3], [0, 5, 9], [2, 4, 6, 7, 8]], MST algorithm for decoding non-pojective trees.
This is a wrapper for ChuLiu/Edmonds algorithm.

The algorithm first runs ChuLiu/Edmonds to parse a tree and then have a check of multi-roots,
If ``multiroot=True`` and there indeed exist multi-roots, the algorithm seeks to find
best single-root trees by iterating all possible single-root trees parsed by ChuLiu/Edmonds.
Otherwise the resulting trees are directly taken as the final outputs.

Args:
    scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all dependent-head pairs.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask to avoid parsing over padding tokens.
        The first column serving as pseudo words for roots should be ``False``.
    muliroot (bool):
        Ensures to parse a single-root tree If ``False``.

Returns:
    ~torch.Tensor:
        A tensor with shape ``[batch_size, seq_len]`` for the resulting non-projective parse trees.

Examples:
    >>> scores = torch.tensor([[[-11.9436, -13.1464,  -6.4789, -13.8917],
                                [-60.6957, -60.2866, -48.6457, -63.8125],
                                [-38.1747, -49.9296, -45.2733, -49.5571],
                                [-19.7504, -23.9066,  -9.9139, -16.2088]]])
    >>> scores[:, 0, 1:] = float('-inf')
    >>> scores.diagonal(0, 1, 2)[1:].fill_(float('-inf'))
    >>> mask = torch.tensor([[False,  True,  True,  True]])
    >>> mst(scores, mask)
    tensor([[0, 2, 0, 2]]), Second-order Eisner algorithm for projective decoding.
This is an extension of the first-order one that further incorporates sibling scores into tree scoring.

References:
    - Ryan McDonald and Fernando Pereira. 2006.
      `Online Learning of Approximate Dependency Parsing Algorithms`_.

Args:
    scores (~torch.Tensor, ~torch.Tensor):
        A tuple of two tensors representing the first-order and second-order scores repectively.
        The first (``[batch_size, seq_len, seq_len]``) holds scores of all dependent-head pairs.
        The second (``[batch_size, seq_len, seq_len, seq_len]``) holds scores of all dependent-head-sibling triples.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask to avoid parsing over padding tokens.
        The first column serving as pseudo words for roots should be ``False``.

Returns:
    ~torch.Tensor:
        A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

Examples:
    >>> s_arc = torch.tensor([[[ -2.8092,  -7.9104,  -0.9414,  -5.4360],
                               [-10.3494,  -7.9298,  -3.6929,  -7.3985],
                               [  1.1815,  -3.8291,   2.3166,  -2.7183],
                               [ -3.9776,  -3.9063,  -1.6762,  -3.1861]]])
    >>> s_sib = torch.tensor([[[[ 0.4719,  0.4154,  1.1333,  0.6946],
                                [ 1.1252,  1.3043,  2.1128,  1.4621],
                                [ 0.5974,  0.5635,  1.0115,  0.7550],
                                [ 1.1174,  1.3794,  2.2567,  1.4043]],
                               [[-2.1480, -4.1830, -2.5519, -1.8020],
                                [-1.2496, -1.7859, -0.0665, -0.4938],
                                [-2.6171, -4.0142, -2.9428, -2.2121],
                                [-0.5166, -1.0925,  0.5190,  0.1371]],
                               [[ 0.5827, -1.2499, -0.0648, -0.0497],
                                [ 1.4695,  0.3522,  1.5614,  1.0236],
                                [ 0.4647, -0.7996, -0.3801,  0.0046],
                                [ 1.5611,  0.3875,  1.8285,  1.0766]],
                               [[-1.3053, -2.9423, -1.5779, -1.2142],
                                [-0.1908, -0.9699,  0.3085,  0.1061],
                                [-1.6783, -2.8199, -1.8853, -1.5653],
                                [ 0.3629, -0.3488,  0.9011,  0.5674]]]])
    >>> mask = torch.tensor([[False,  True,  True,  True]])
    >>> eisner2o((s_arc, s_sib), mask)
    tensor([[0, 2, 0, 2]])

.. _Online Learning of Approximate Dependency Parsing Algorithms:
    https://www.aclweb.org/anthology/E06-1011/, Tarjan algorithm for finding Strongly Connected Components (SCCs) of a graph.

Args:
    sequence (list):
        List of head indices.

Yields:
    A list of indices that make up a SCC. All self-loops are ignored.

Examples:
    >>> next(tarjan([2, 5, 0, 3, 1]))  # (1 -> 5 -> 2 -> 1) is a cycle
    [2, 5, 1], The implementation of `Cocke-Kasami-Younger`_ (CKY) algorithm to parse constituency trees.

References:
    - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.
      `Fast and Accurate Neural CRF Constituency Parsing`_.

Args:
    scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all candidate constituents.
    mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
        The mask to avoid parsing over padding tokens.
        For each square matrix in a batch, the positions except upper triangular part should be masked out.

Returns:
    Sequences of factorized predicted bracketed trees that are traversed in pre-order.

Examples:
    >>> scores = torch.tensor([[[ 2.5659,  1.4253, -2.5272,  3.3011],
                                [ 1.3687, -0.5869,  1.0011,  3.3020],
                                [ 1.2297,  0.4862,  1.1975,  2.5387],
                                [-0.0511, -1.2541, -0.7577,  0.2659]]])
    >>> mask = torch.tensor([[[False,  True,  True,  True],
                              [False, False,  True,  True],
                              [False, False, False,  True],
                              [False, False, False, False]]])
    >>> cky(scores, mask)
    [[(0, 3), (0, 1), (1, 3), (1, 2), (2, 3)]]

.. _Cocke-Kasami-Younger:
    https://en.wikipedia.org/wiki/CYK_algorithm
.. _Fast and Accurate Neural CRF Constituency Parsing:
    https://www.ijcai.org/Proceedings/2020/560/, r'''Returns a diagonal stripe of the tensor.

Args:
  x: Tensor
  n: int
  w: int
  offset: tuple (Default value = (0)
  dim: int (Default value = 1)
  Example: 
  0): 

Returns:

>>> x = torch.arange(25).view(5, 5)
>>> x
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [20, 21, 22, 23, 24]])
>>> stripe(x, 2, 3, (1, 1))
tensor([[ 6,  7,  8],
        [12, 13, 14]])
>>> stripe(x, 2, 3, dim=0)
tensor([[ 0,  5, 10],
        [ 6, 11, 16]])","
    Args:
        s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
            Scores of all possible arcs.
        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
            The mask for covering the unpadded tokens.
        tree (bool):
            If ``True``, ensures to output well-formed trees. Default: ``False``.
        proj (bool):
            If ``True``, ensures to output projective trees. Default: ``False``.

    Returns:
        ~torch.Tensor, ~torch.Tensor:
            Predicted arcs and labels of shape ``[batch_size, seq_len]``.
    , 
    Checks if the arcs form an valid dependency tree.

    Args:
        sequence (list[int]):
            A list of head indices.
        proj (bool):
            If ``True``, requires the tree to be projective. Default: ``False``.
        multiroot (bool):
            If ``False``, requires the tree to contain only a single root. Default: ``True``.

    Returns:
        ``True`` if the arcs form an valid tree, ``False`` otherwise.

    Examples:
        >>> istree([3, 0, 0, 3], multiroot=True)
        True
        >>> istree([3, 0, 0, 3], proj=True)
        False
    , 
    ChuLiu/Edmonds algorithm for non-projective decoding.

    Some code is borrowed from `tdozat's implementation`_.
    Descriptions of notations and formulas can be found in
    `Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

    Notes:
        The algorithm does not guarantee to parse a single-root tree.

    References:
        - Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan Hajic. 2005.
          `Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

    Args:
        s (~torch.Tensor): ``[seq_len, seq_len]``.
            Scores of all dependent-head pairs.

    Returns:
        ~torch.Tensor:
            A tensor with shape ``[seq_len]`` for the resulting non-projective parse tree.

    .. _tdozat's implementation:
        https://github.com/tdozat/Parser-v3
    .. _Non-projective Dependency Parsing using Spanning Tree Algorithms:
        https://www.aclweb.org/anthology/H05-1066/
    , 
    First-order Eisner algorithm for projective decoding.

    References:
        - Ryan McDonald, Koby Crammer and Fernando Pereira. 2005.
          `Online Large-Margin Training of Dependency Parsers`_.

    Args:
        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
            Scores of all dependent-head pairs.
        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
            The mask to avoid parsing over padding tokens.
            The first column serving as pseudo words for roots should be ``False``.

    Returns:
        ~torch.Tensor:
            A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

    Examples:
        >>> scores = torch.tensor([[[-13.5026, -18.3700, -13.0033, -16.6809],
                                    [-36.5235, -28.6344, -28.4696, -31.6750],
                                    [ -2.9084,  -7.4825,  -1.4861,  -6.8709],
                                    [-29.4880, -27.6905, -26.1498, -27.0233]]])
        >>> mask = torch.tensor([[False,  True,  True,  True]])
        >>> eisner(scores, mask)
        tensor([[0, 2, 0, 2]])

    .. _Online Large-Margin Training of Dependency Parsers:
        https://www.aclweb.org/anthology/P05-1012/
    , 
    KMeans algorithm for clustering the sentences by length.

    Args:
        x (list[int]):
            The list of sentence lengths.
        k (int):
            The number of clusters.
            This is an approximate value. The final number of clusters can be less or equal to `k`.
        max_it (int):
            Maximum number of iterations.
            If centroids does not converge after several iterations, the algorithm will be early stopped.

    Returns:
        list[float], list[list[int]]:
            The first list contains average lengths of sentences in each cluster.
            The second is the list of clusters holding indices of data points.

    Examples:
        >>> x = torch.randint(10,20,(10,)).tolist()
        >>> x
        [15, 10, 17, 11, 18, 13, 17, 19, 18, 14]
        >>> centroids, clusters = kmeans(x, 3)
        >>> centroids
        [10.5, 14.0, 17.799999237060547]
        >>> clusters
        [[1, 3], [0, 5, 9], [2, 4, 6, 7, 8]]
    , 
    MST algorithm for decoding non-pojective trees.
    This is a wrapper for ChuLiu/Edmonds algorithm.

    The algorithm first runs ChuLiu/Edmonds to parse a tree and then have a check of multi-roots,
    If ``multiroot=True`` and there indeed exist multi-roots, the algorithm seeks to find
    best single-root trees by iterating all possible single-root trees parsed by ChuLiu/Edmonds.
    Otherwise the resulting trees are directly taken as the final outputs.

    Args:
        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
            Scores of all dependent-head pairs.
        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
            The mask to avoid parsing over padding tokens.
            The first column serving as pseudo words for roots should be ``False``.
        muliroot (bool):
            Ensures to parse a single-root tree If ``False``.

    Returns:
        ~torch.Tensor:
            A tensor with shape ``[batch_size, seq_len]`` for the resulting non-projective parse trees.

    Examples:
        >>> scores = torch.tensor([[[-11.9436, -13.1464,  -6.4789, -13.8917],
                                    [-60.6957, -60.2866, -48.6457, -63.8125],
                                    [-38.1747, -49.9296, -45.2733, -49.5571],
                                    [-19.7504, -23.9066,  -9.9139, -16.2088]]])
        >>> scores[:, 0, 1:] = float('-inf')
        >>> scores.diagonal(0, 1, 2)[1:].fill_(float('-inf'))
        >>> mask = torch.tensor([[False,  True,  True,  True]])
        >>> mst(scores, mask)
        tensor([[0, 2, 0, 2]])
    , 
    Second-order Eisner algorithm for projective decoding.
    This is an extension of the first-order one that further incorporates sibling scores into tree scoring.

    References:
        - Ryan McDonald and Fernando Pereira. 2006.
          `Online Learning of Approximate Dependency Parsing Algorithms`_.

    Args:
        scores (~torch.Tensor, ~torch.Tensor):
            A tuple of two tensors representing the first-order and second-order scores repectively.
            The first (``[batch_size, seq_len, seq_len]``) holds scores of all dependent-head pairs.
            The second (``[batch_size, seq_len, seq_len, seq_len]``) holds scores of all dependent-head-sibling triples.
        mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
            The mask to avoid parsing over padding tokens.
            The first column serving as pseudo words for roots should be ``False``.

    Returns:
        ~torch.Tensor:
            A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

    Examples:
        >>> s_arc = torch.tensor([[[ -2.8092,  -7.9104,  -0.9414,  -5.4360],
                                   [-10.3494,  -7.9298,  -3.6929,  -7.3985],
                                   [  1.1815,  -3.8291,   2.3166,  -2.7183],
                                   [ -3.9776,  -3.9063,  -1.6762,  -3.1861]]])
        >>> s_sib = torch.tensor([[[[ 0.4719,  0.4154,  1.1333,  0.6946],
                                    [ 1.1252,  1.3043,  2.1128,  1.4621],
                                    [ 0.5974,  0.5635,  1.0115,  0.7550],
                                    [ 1.1174,  1.3794,  2.2567,  1.4043]],
                                   [[-2.1480, -4.1830, -2.5519, -1.8020],
                                    [-1.2496, -1.7859, -0.0665, -0.4938],
                                    [-2.6171, -4.0142, -2.9428, -2.2121],
                                    [-0.5166, -1.0925,  0.5190,  0.1371]],
                                   [[ 0.5827, -1.2499, -0.0648, -0.0497],
                                    [ 1.4695,  0.3522,  1.5614,  1.0236],
                                    [ 0.4647, -0.7996, -0.3801,  0.0046],
                                    [ 1.5611,  0.3875,  1.8285,  1.0766]],
                                   [[-1.3053, -2.9423, -1.5779, -1.2142],
                                    [-0.1908, -0.9699,  0.3085,  0.1061],
                                    [-1.6783, -2.8199, -1.8853, -1.5653],
                                    [ 0.3629, -0.3488,  0.9011,  0.5674]]]])
        >>> mask = torch.tensor([[False,  True,  True,  True]])
        >>> eisner2o((s_arc, s_sib), mask)
        tensor([[0, 2, 0, 2]])

    .. _Online Learning of Approximate Dependency Parsing Algorithms:
        https://www.aclweb.org/anthology/E06-1011/
    , 
    Tarjan algorithm for finding Strongly Connected Components (SCCs) of a graph.

    Args:
        sequence (list):
            List of head indices.

    Yields:
        A list of indices that make up a SCC. All self-loops are ignored.

    Examples:
        >>> next(tarjan([2, 5, 0, 3, 1]))  # (1 -> 5 -> 2 -> 1) is a cycle
        [2, 5, 1]
    , 
    The implementation of `Cocke-Kasami-Younger`_ (CKY) algorithm to parse constituency trees.

    References:
        - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.
          `Fast and Accurate Neural CRF Constituency Parsing`_.

    Args:
        scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
            Scores of all candidate constituents.
        mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
            The mask to avoid parsing over padding tokens.
            For each square matrix in a batch, the positions except upper triangular part should be masked out.

    Returns:
        Sequences of factorized predicted bracketed trees that are traversed in pre-order.

    Examples:
        >>> scores = torch.tensor([[[ 2.5659,  1.4253, -2.5272,  3.3011],
                                    [ 1.3687, -0.5869,  1.0011,  3.3020],
                                    [ 1.2297,  0.4862,  1.1975,  2.5387],
                                    [-0.0511, -1.2541, -0.7577,  0.2659]]])
        >>> mask = torch.tensor([[[False,  True,  True,  True],
                                  [False, False,  True,  True],
                                  [False, False, False,  True],
                                  [False, False, False, False]]])
        >>> cky(scores, mask)
        [[(0, 3), (0, 1), (1, 3), (1, 2), (2, 3)]]

    .. _Cocke-Kasami-Younger:
        https://en.wikipedia.org/wiki/CYK_algorithm
    .. _Fast and Accurate Neural CRF Constituency Parsing:
        https://www.ijcai.org/Proceedings/2020/560/
    , -inf, c, i, r'''Returns a diagonal stripe of the tensor.

    Args:
      x: Tensor
      n: int
      w: int
      offset: tuple (Default value = (0)
      dim: int (Default value = 1)
      Example: 
      0): 

    Returns:

    >>> x = torch.arange(25).view(5, 5)
    >>> x
    tensor([[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24]])
    >>> stripe(x, 2, 3, (1, 1))
    tensor([[ 6,  7,  8],
            [12, 13, 14]])
    >>> stripe(x, 2, 3, dim=0)
    tensor([[ 0,  5, 10],
            [ 6, 11, 16]])
    , s","0, 1, 2, 3, 32, False, None, True","

# the number of clusters must not be greater than the number of datapoints
x, k = torch.tensor(x, dtype=torch.float), min(len(x), k)
# collect unique datapoints
d = x.unique()
# initialize k centroids randomly
c = d[torch.randperm(len(d))[:k]]
# assign each datapoint to the cluster with the closest centroid
dists, y = torch.abs_(x.unsqueeze(-1) - c).min(-1)

for _ in range(max_it):
# if an empty cluster is encountered,
# choose the farthest datapoint from the biggest cluster and move that the empty one
mask = torch.arange(k).unsqueeze(-1).eq(y)
none = torch.where(~mask.any(-1))[0].tolist()
while len(none) > 0:
for i in none:
# the biggest cluster
b = torch.where(mask[mask.sum(-1).argmax()])[0]
# the datapoint farthest from the centroid of cluster b
f = dists[b].argmax()
# update the assigned cluster of f
y[b[f]] = i
# re-calculate the mask
mask = torch.arange(k).unsqueeze(-1).eq(y)
none = torch.where(~mask.any(-1))[0].tolist()
# update the centroids
c, old = (x * mask).sum(-1) / mask.sum(-1), c
# re-assign all datapoints to clusters
dists, y = torch.abs_(x.unsqueeze(-1) - c).min(-1)
# stop iteration early if the centroids converge
if c.equal(old):
break
# assign all datapoints to the new-generated clusters
# the empty ones are discarded
assigned = y.unique().tolist()
# get the centroids of the assigned clusters
centroids = c[assigned].tolist()
# map all values of datapoints to buckets
clusters = [torch.where(y.eq(i))[0].tolist() for i in assigned]

return centroids, clusters


def eisner(scores, mask):
r""""""
First-order Eisner algorithm for projective decoding.

References:
- Ryan McDonald, Koby Crammer and Fernando Pereira. 2005.
`Online Large-Margin Training of Dependency Parsers`_.

Args:
scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
Scores of all dependent-head pairs.
mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
The mask to avoid parsing over padding tokens.
The first column serving as pseudo words for roots should be ``False``.

Returns:
~torch.Tensor:
A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

Examples:
>>> scores = torch.tensor([[[-13.5026, -18.3700, -13.0033, -16.6809],
[-36.5235, -28.6344, -28.4696, -31.6750],
[ -2.9084,  -7.4825,  -1.4861,  -6.8709],
[-29.4880, -27.6905, -26.1498, -27.0233]]])
>>> mask = torch.tensor([[False,  True,  True,  True]])
>>> eisner(scores, mask)
tensor([[0, 2, 0, 2]])

.. _Online Large-Margin Training of Dependency Parsers:
https://www.aclweb.org/anthology/P05-1012/
, 

batch_size, seq_len, _ = scores.shape
scores = scores.detach().cpu().unbind()

preds = []
for i, length in enumerate(mask.sum(1).tolist()):
s = scores[i][:length + 1, :length + 1]
tree = chuliu_edmonds(s)
roots = torch.where(tree[1:].eq(0))[0] + 1
if not multiroot and len(roots) > 1:
s_root = s[:, 0]
s_best = float('-inf')
s = s.index_fill(1, torch.tensor(0), float('-inf'))
for root in roots:
s[:, 0] = float('-inf')
s[root, 0] = s_root[root]
t = chuliu_edmonds(s)
s_tree = s[1:].gather(1, t[1:].unsqueeze(-1)).sum()
if s_tree > s_best:
s_best, tree = s_tree, t
preds.append(tree)

return pad(preds, total_length=seq_len).to(mask.device)


def eisner2o(scores, mask):
r""""""
Second-order Eisner algorithm for projective decoding.
This is an extension of the first-order one that further incorporates sibling scores into tree scoring.

References:
- Ryan McDonald and Fernando Pereira. 2006.
`Online Learning of Approximate Dependency Parsing Algorithms`_.

Args:
scores (~torch.Tensor, ~torch.Tensor):
A tuple of two tensors representing the first-order and second-order scores repectively.
The first (``[batch_size, seq_len, seq_len]``) holds scores of all dependent-head pairs.
The second (``[batch_size, seq_len, seq_len, seq_len]``) holds scores of all dependent-head-sibling triples.
mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
The mask to avoid parsing over padding tokens.
The first column serving as pseudo words for roots should be ``False``.

Returns:
~torch.Tensor:
A tensor with shape ``[batch_size, seq_len]`` for the resulting projective parse trees.

Examples:
>>> s_arc = torch.tensor([[[ -2.8092,  -7.9104,  -0.9414,  -5.4360],
[-10.3494,  -7.9298,  -3.6929,  -7.3985],
[  1.1815,  -3.8291,   2.3166,  -2.7183],
[ -3.9776,  -3.9063,  -1.6762,  -3.1861]]])
>>> s_sib = torch.tensor([[[[ 0.4719,  0.4154,  1.1333,  0.6946],
[ 1.1252,  1.3043,  2.1128,  1.4621],
[ 0.5974,  0.5635,  1.0115,  0.7550],
[ 1.1174,  1.3794,  2.2567,  1.4043]],
[[-2.1480, -4.1830, -2.5519, -1.8020],
[-1.2496, -1.7859, -0.0665, -0.4938],
[-2.6171, -4.0142, -2.9428, -2.2121],
[-0.5166, -1.0925,  0.5190,  0.1371]],
[[ 0.5827, -1.2499, -0.0648, -0.0497],
[ 1.4695,  0.3522,  1.5614,  1.0236],
[ 0.4647, -0.7996, -0.3801,  0.0046],
[ 1.5611,  0.3875,  1.8285,  1.0766]],
[[-1.3053, -2.9423, -1.5779, -1.2142],
[-0.1908, -0.9699,  0.3085,  0.1061],
[-1.6783, -2.8199, -1.8853, -1.5653],
[ 0.3629, -0.3488,  0.9011,  0.5674]]]])
>>> mask = torch.tensor([[False,  True,  True,  True]])
>>> eisner2o((s_arc, s_sib), mask)
tensor([[0, 2, 0, 2]])

.. _Online Learning of Approximate Dependency Parsing Algorithms:
https://www.aclweb.org/anthology/E06-1011/
, 

lens = mask[:, 0].sum(-1)
scores = scores.permute(1, 2, 0)
seq_len, seq_len, batch_size = scores.shape
s = scores.new_zeros(seq_len, seq_len, batch_size)
p = scores.new_zeros(seq_len, seq_len, batch_size).long()

for w in range(1, seq_len):
n = seq_len - w
starts = p.new_tensor(range(n)).unsqueeze(0)

if w == 1:
s.diagonal(w).copy_(scores.diagonal(w))
continue
# [n, w, batch_size]
s_span = stripe(s, n, w - 1, (0, 1)) + stripe(s, n, w - 1, (1, w), 0)
# [batch_size, n, w]
s_span = s_span.permute(2, 0, 1)
# [batch_size, n]
s_span, p_span = s_span.max(-1)
s.diagonal(w).copy_(s_span + scores.diagonal(w))
p.diagonal(w).copy_(p_span + starts + 1)

def backtrack(p, i, j):
if j == i + 1:
return [(i, j)]
split = p[i][j]
ltree = backtrack(p, i, split)
rtree = backtrack(p, split, j)
return [(i, j)] + ltree + rtree

p = p.permute(2, 0, 1).tolist()
trees = [backtrack(p[i], 0, length) if length else [] for i, length in enumerate(lens.tolist())]

return trees


def istree(sequence, proj=False, multiroot=False):
r""""""
Checks if the arcs form an valid dependency tree.

Args:
sequence (list[int]):
A list of head indices.
proj (bool):
If ``True``, requires the tree to be projective. Default: ``False``.
multiroot (bool):
If ``False``, requires the tree to contain only a single root. Default: ``True``.

Returns:
``True`` if the arcs form an valid tree, ``False`` otherwise.

Examples:
>>> istree([3, 0, 0, 3], multiroot=True)
True
>>> istree([3, 0, 0, 3], proj=True)
False
, 

sequence = [-1] + sequence
# record the search order, i.e., the timestep
dfn = [-1] * len(sequence)
# record the the smallest timestep in a SCC
low = [-1] * len(sequence)
# push the visited into the stack
stack, onstack = [], [False] * len(sequence)

def connect(i, timestep):
dfn[i] = low[i] = timestep[0]
timestep[0] += 1
stack.append(i)
onstack[i] = True

for j, head in enumerate(sequence):
if head != i:
continue
if dfn[j] == -1:
yield from connect(j, timestep)
low[i] = min(low[i], low[j])
elif onstack[j]:
low[i] = min(low[i], dfn[j])

# a SCC is completed
if low[i] == dfn[i]:
cycle = [stack.pop()]
while cycle[-1] != i:
onstack[cycle[-1]] = False
cycle.append(stack.pop())
onstack[i] = False
# ignore the self-loop
if len(cycle) > 1:
yield cycle

timestep = [0]
for i in range(len(sequence)):
if dfn[i] == -1:
yield from connect(i, timestep)


def chuliu_edmonds(s):
r""""""
ChuLiu/Edmonds algorithm for non-projective decoding.

Some code is borrowed from `tdozat's implementation`_.
Descriptions of notations and formulas can be found in
`Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

Notes:
The algorithm does not guarantee to parse a single-root tree.

References:
- Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan Hajic. 2005.
`Non-projective Dependency Parsing using Spanning Tree Algorithms`_.

Args:
s (~torch.Tensor): ``[seq_len, seq_len]``.
Scores of all dependent-head pairs.

Returns:
~torch.Tensor:
A tensor with shape ``[seq_len]`` for the resulting non-projective parse tree.

.. _tdozat's implementation:
https://github.com/tdozat/Parser-v3
.. _Non-projective Dependency Parsing using Spanning Tree Algorithms:
https://www.aclweb.org/anthology/H05-1066/
, #, #                                                    a(v) is the predecessor of v in cycle, #                                                    s(cycle) = sum(s(a(v)->v)), #               C(i->i) + C(j->i+1)), #               C(j->j) + C(i->j-1)), #           + s(i->j), #           + s(j->i), # (1 -> 5 -> 2 -> 1) is a cycle, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # C(i->j) = max(I(i->r) + C(r->j)), i < r <= j, # C(j->i) = max(C(r->i) + I(j->r)), i <= r < j, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # I(i->j) = max(C(i->r) + C(j->r+1) + s(i->j)), i <= r < j, # I(i->j) = max(I(i->r) + S(i->r, j), i < r < j |, # I(j->i) = max(C(i->r) + C(j->r+1) + s(j->i)), i <= r < j, # I(j->i) = max(I(j->r) + S(j->r, i)), i < r < j |, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # S(i, j) = max(C(i->r) + C(j->r+1)), i <= r < j, # S(j, i) = max(C(i->r) + C(j->r+1)), i <= r < j, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # [batch_size, n, w], # [n, 1, batch_size], # [n, w, batch_size], # [seq_len, seq_len, batch_size], # [seq_len, seq_len, seq_len, batch_size], # add the nodes to the new tree, # add the nodes to the tree, # break the cycle and add the root of the cycle to the tree, # calculate the scores of contracted graph, # calculate the scores of cycle's potential dependents, # calculate the scores of cycle's potential heads, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # disable multi words to modify the root, # exclude head of cycle from y, # find the best cycle head for each noncycle dependent, # find the best noncycle head for each cycle dependent, # fix the root of the cycle, # fix the subtree with heads coming from the cycle, # fix the subtree with no heads coming from the cycle, # from span (0, w) to span (n, n+w) given width w, # furnished to do so, subject to the following conditions:, # heads of cycle in original tree, # if the tree has no cycles, then it is a MST, # il0[0] are set to zeros since the scores of the complete spans starting from 0 are always -inf, # ilr = C(i->r) + C(j->r+1), # in the Software without restriction, including without limitation the rights, # indices of cycle in the original tree, # indices of noncycle in the original tree, # keep track of the endpoints of the edges into and out of cycle for reconstruction later, # len(y) denotes heads coming from the cycle, # n denotes the number of spans to iterate,, # of this software and associated documentation files (the ""Software""), to deal, # prevent self-loops, # return the cycle finded by tarjan algorithm lazily, # s(c->x) = max(s(x'->x)), x in noncycle and x' in cycle, # s(x->c) = max(s(x'->x) - s(a(x')->x') + s(cycle)), x in noncycle and x' in cycle, # scores of cycle in original tree, # select heads with highest scores, # set the contracted graph scores of cycle's potential dependents, # set the contracted graph scores of cycle's potential heads, # the end position of each sentence in a batch, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell, # y is the contracted tree, r'''Returns a diagonal stripe of the tensor.

Args:
x: Tensor
n: int
w: int
offset: tuple (Default value = (0)
dim: int (Default value = 1)
Example:
0):

Returns:

>>> x = torch.arange(25).view(5, 5)
>>> x
tensor([[ 0,  1,  2,  3,  4],
[ 5,  6,  7,  8,  9],
[10, 11, 12, 13, 14],
[15, 16, 17, 18, 19],
[20, 21, 22, 23, 24]])
>>> stripe(x, 2, 3, (1, 1))
tensor([[ 6,  7,  8],
[12, 13, 14]])
>>> stripe(x, 2, 3, dim=0)
tensor([[ 0,  5, 10],
[ 6, 11, 16]])
",
https://github.com/hankcs/HanLP,alg_tf.py,0,0.0,86,69.35,9,7.26,5,4.03,24,19.35,0,0.0,14,1,47,6,,"I, SCC, SCCs, Tarjan, _SCCs, __init__, _edges, _indices, _lowlinks, _onstack, _vertices, add, append, arange, assigned, biggest, centroids, change, changed_cycle, clusters, collections, cycle, dep, dependents, dims, dists, edges, ensure_tree, enumerate, farthest, head, index, indices, int, isinstance, k, kmeans, len, length, lens, list, mask, new_head, new_head_probs, new_heads, new_rel_preds, new_rel_probs, new_root, new_root_probs, node, non_heads, nonzero, numpy, old, old_head, old_head_probs, old_heads, parse_preds, parse_probs, pop, prediction, property, randperm, rel_argmax, rel_preds, rel_probs, root, root_probs, roots, seed, self, set, strongconnect, tarjan, tensorflow, to_visit, tokens, tokens_to_keep, tolist, total, typing, update, v, vertices, view, x","collections.defaultdict, np.arange, np.argmax, np.argmin, np.array, np.eye, np.repeat, np.where, tf.Tensor, tf.abs, tf.argmax, tf.argmin, tf.cast, tf.constant, tf.expand_dims, tf.float32, tf.gather, tf.gather_nd, tf.int32, tf.random.shuffle, tf.range, tf.reduce_all, tf.reduce_any, tf.reduce_sum, tf.reshape, tf.shape, tf.squeeze, tf.stack, tf.tensor_scatter_nd_update, tf.transpose, tf.unique, tf.unique_with_counts, tf.where, typing.List","SCCs, __init__, arange, edges, indices, kmeans, nonzero, randperm, rel_argmax, strongconnect, tarjan, tolist, vertices, view",Tarjan,"I, SCC, assigned, biggest, centroids, change, changed_cycle, clusters, cycle, dep, dependents, dists, f, farthest, head, index, indices, lens, mask, new_head, new_head_probs, new_heads, new_rel_preds, new_rel_probs, new_root, new_root_probs, node, non_heads, old, old_head, old_head_probs, old_heads, parse_preds, parse_probs, rel_preds, root_probs, roots, stack, t, tarjan, to_visit, tokens, total, v, w, x, y","Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

Args:
  parse_probs(NDArray): seq_len x seq_len, the probability of arcs
  length(NDArray): sentence length including ROOT
  tokens_to_keep(NDArray): mask matrix
  ensure_tree:  (Default value = True)

Returns:, Args:
  v: 
  index: 
  stack: 

Returns:, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, Fix the relation prediction by heuristic rules

Args:
  rel_probs(NDArray): seq_len x rel_size
  length: real sentence length
  ensure_tree:  (Default value = True)
  root: 

Returns:, Parameters
----------
prediction : numpy.ndarray
    a predicted dependency tree where prediction[dep_idx] = head_idx
tokens : numpy.ndarray
    the tokens we care about (i.e. exclude _GO, _EOS, and _PAD), See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

Args:
  x(list): Lengths of sentences
  k(int): 
  seed:  (Default value = None)

Returns:","

        Args:
          v: 
          index: 
          stack: 

        Returns:

        , 

        Parameters
        ----------
        prediction : numpy.ndarray
            a predicted dependency tree where prediction[dep_idx] = head_idx
        tokens : numpy.ndarray
            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)
        ,  clusters,  datapoints to , Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

    Args:
      parse_probs(NDArray): seq_len x seq_len, the probability of arcs
      length(NDArray): sentence length including ROOT
      tokens_to_keep(NDArray): mask matrix
      ensure_tree:  (Default value = True)

    Returns:

    
    , Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, Fix the relation prediction by heuristic rules

    Args:
      rel_probs(NDArray): seq_len x rel_size
      length: real sentence length
      ensure_tree:  (Default value = True)
      root: 

    Returns:

    
    , See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

    Args:
      x(list): Lengths of sentences
      k(int): 
      seed:  (Default value = None)

    Returns:

    
    , unable to assign ","0, 1, False, None, True","

self._indices[v] = index
self._lowlinks[v] = index
index += 1
stack.append(v)
self._onstack[v] = True
for w in self.edges[v]:
if w not in self.indices:
self.strongconnect(w, index, stack)
self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])
elif self._onstack[w]:
self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])

if self._lowlinks[v] == self._indices[v]:
self._SCCs.append(set())
while stack[-1] != v:
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
return

# ======================
@property
def edges(self):
return self._edges

@property
def vertices(self):
return self._vertices

@property
def indices(self):
return self._indices

@property
def SCCs(self):
return self._SCCs


def tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):
Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py, 
if ensure_tree:
I = np.eye(len(tokens_to_keep))
# block loops and pad heads
parse_probs = parse_probs * tokens_to_keep * (1 - I)
parse_preds = np.argmax(parse_probs, axis=1)
tokens = np.arange(1, length)
roots = np.where(parse_preds[tokens] == 0)[0] + 1
# ensure at least one root
if len(roots) < 1:
# The current root probabilities
root_probs = parse_probs[tokens, 0]
# The current head probabilities
old_head_probs = parse_probs[tokens, parse_preds[tokens]]
# Get new potential root probabilities
new_root_probs = root_probs / old_head_probs
# Select the most probable root
new_root = tokens[np.argmax(new_root_probs)]
# Make the change
parse_preds[new_root] = 0
# ensure at most one root
elif len(roots) > 1:
# The probabilities of the current heads
root_probs = parse_probs[roots, 0]
# Set the probability of depending on the root zero
parse_probs[roots, 0] = 0
# Get new potential heads and their probabilities
new_heads = np.argmax(parse_probs[roots][:, tokens], axis=1) + 1
new_head_probs = parse_probs[roots, new_heads] / root_probs
# Select the most probable root
new_root = roots[np.argmin(new_head_probs)]
# Make the change
parse_preds[roots] = new_heads
parse_preds[new_root] = 0
# remove cycles
tarjan = Tarjan(parse_preds, tokens)
for SCC in tarjan.SCCs:
if len(SCC) > 1:
dependents = set()
to_visit = set(SCC)
while len(to_visit) > 0:
node = to_visit.pop()
if not node in dependents:
dependents.add(node)
to_visit.update(tarjan.edges[node])
# The indices of the nodes that participate in the cycle
cycle = np.array(list(SCC))
# The probabilities of the current heads
old_heads = parse_preds[cycle]
old_head_probs = parse_probs[cycle, old_heads]
# Set the probability of depending on a non-head to zero
non_heads = np.array(list(dependents))
parse_probs[np.repeat(cycle, len(non_heads)), np.repeat([non_heads], len(cycle), axis=0).flatten()] = 0
# Get new potential heads and their probabilities
new_heads = np.argmax(parse_probs[cycle][:, tokens], axis=1) + 1
new_head_probs = parse_probs[cycle, new_heads] / old_head_probs
# Select the most probable change
change = np.argmax(new_head_probs)
changed_cycle = cycle[change]
old_head = old_heads[change]
new_head = new_heads[change]
# Make the change
parse_preds[changed_cycle] = new_head
tarjan.edges[new_head].add(changed_cycle)
tarjan.edges[old_head].remove(changed_cycle)
return parse_preds
else:
# block and pad heads
parse_probs = parse_probs * tokens_to_keep
parse_preds = np.argmax(parse_probs, axis=1)
return parse_preds


def rel_argmax(rel_probs, length, root, ensure_tree=True):
Fix the relation prediction by heuristic rules, 
self._edges = defaultdict(set)
self._vertices = set((0,))
for dep, head in enumerate(prediction[tokens]):
self._vertices.add(dep + 1)
self._edges[head].add(dep + 1)
self._indices = {}
self._lowlinks = {}
self._onstack = defaultdict(lambda: False)
self._SCCs = []

index = 0
stack = []
for v in self.vertices:
if v not in self.indices:
self.strongconnect(v, index, stack)

# =============================================================
def strongconnect(self, v, index, stack):
, # ***************************************************************, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 19:49, # Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser, # and move that the empty one, # assign all datapoints to the new-generated clusters, # assign labels to each datapoint based on centroids, # calculate the sum of the values of the same datapoints, # choose the farthest datapoint from the biggest cluster, # count the frequency of each datapoint, # get the centroids of the assigned clusters, # if an empty cluster is encountered,, # initialize k centroids randomly, # make sure number of datapoints is greater than that of clusters, # map all values of datapoints to buckets, # re-assign all datapoints to clusters, # update the centroids, # without considering the empty ones, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph

def __init__(self, prediction, tokens):
, See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

Args:
x(list): Lengths of sentences
k(int):
seed:  (Default value = None)

Returns:


",
https://github.com/hankcs/HanLP,biaffine_parser_tf.py,0,0.0,215,70.03,51,16.61,27,8.79,14,4.56,0,0.0,28,4,64,0,,"BiaffineDependencyParserTF, BiaffineSemanticDependencyParserTF, BiaffineTransformerDependencyParserTF, BiaffineTransformerSemanticDependencyParser, OSError, TypeError, ValueError, XY_to_inputs_outputs, Y, __init__, _apply_grads, _build_metrics, _init_config, _values, accum_grads, append, apply_gradients, arc, arc_loss, arc_pred, arc_preds, arc_probs, arc_scores, arcs, assign, astype, batch, batch_size, bos_index, build_callbacks, build_loss, build_model, build_optimizer, build_progbar, build_train_dataset, build_transformer, build_vocab, clip, clipnorm, close, compile_model, config, cpos_vocab, dataset, decay, decay_steps, decode, dev_data, dict, early_stopping_patience, embed_dropout, endswith, enumerate, epoch, epochs, epsilon, evaluate, evaluate_batch, evaluate_dataset, ext, feats, file_to_dataset, filename, fit, float, form_vocab, fp16, from_json_file, from_pretrained, functional, get, get_loss, get_weights, getattr, grad, gradient, grads, hanlp, hanlp_common, hasattr, head_probs, heads, idx, input_path, inputs, inputs_to_dataset, isinstance, iter, learning_rate, learning_rate_transformer, len, length, list, load_weights, logger, logging, logs, loss, lr, lstm_dropout, mask, math, metrics, min_freq, mlp_dropout, model, mu, n_buckets, n_embed, n_feats, n_lstm_hidden, n_lstm_layers, n_mlp_arc, n_mlp_rel, n_rels, n_words, name, next, nu, num_samples_in, numpy, on_batch_begin, on_batch_end, on_epoch_begin, on_epoch_end, on_test_batch_begin, on_test_batch_end, on_test_begin, on_test_end, on_train_begin, on_train_end, open, optimizer, optimizer_transformer, os, output, output_dim, outputs, pad_idx, pad_index, params, patience, policy, predict_batch, pretrained, pretrained_embed, progbar, property, punct, puncts, range, read_value, rel, rel_loss, rel_pred, rel_preds, rel_prob, rel_scores, rel_vocab, rels, reset_states, ret_scores, root_rel_idx, root_rel_onehot, run_eagerly, run_metrics, sample_data, samples_per_batch, save_dir, scheduler, scores, self, set_model, set_params, set_weights, startswith, steps_per_epoch, stop_training, str, sum, super, tape, tensorflow, to_dict, to_functional, tokenizer, tokens, train_batch, train_examples, train_loop, train_steps, train_steps_per_epoch, trainable, trainable_variables, training, transform, transformer, transformer_config, tree, trn_data, tst_data, tuple, tv, typing, unk_idx, unk_index, update, v, verbose, warm_up, warmup_steps, weight_decay_rate, words, write, zip","hanlp.common.keras_component.KerasComponent, hanlp.components.parsers.alg_tf.tarjan, hanlp.components.parsers.biaffine_tf.model.BiaffineModelTF, hanlp.components.parsers.parse_alg.adjust_root_score, hanlp.components.parsers.parse_alg.chu_liu_edmonds, hanlp.components.parsers.parse_alg.unique_root, hanlp.layers.embeddings.util_tf.build_embedding, hanlp.layers.transformers.loader_tf.build_transformer, hanlp.layers.transformers.tf_imports.AlbertConfig, hanlp.layers.transformers.tf_imports.AutoTokenizer, hanlp.layers.transformers.tf_imports.BertTokenizer, hanlp.layers.transformers.tf_imports.BertTokenizerFast, hanlp.layers.transformers.tf_imports.PreTrainedTokenizer, hanlp.layers.transformers.tf_imports.TFAutoModel, hanlp.layers.transformers.tf_imports.TFAutoModelWithLMHead, hanlp.layers.transformers.tf_imports.TFBertModel, hanlp.layers.transformers.tf_imports.TFPreTrainedModel, hanlp.layers.transformers.utils_tf.build_adamw_optimizer, hanlp.metrics.parsing.labeled_f1_tf.LabeledF1TF, hanlp.metrics.parsing.labeled_score.LabeledScore, hanlp.optimizers.adamw.optimization.AdamTF, hanlp.transform.conll_tf.CoNLL_DEP_Transform, hanlp.transform.conll_tf.CoNLL_SDP_Transform, hanlp.transform.conll_tf.CoNLL_Transformer_Transform, hanlp_common.util.merge_locals_kwargs, logging.Logger, math.ceil, np.arange, np.argmax, np.expand_dims, np.eye, np.int64, np.where, np.zeros_like, os.path.join, os.path.splitext, tf.GradientTape, tf.Variable, tf.argmax, tf.constant, tf.expand_dims, tf.gather, tf.gather_nd, tf.int64, tf.keras.Model, tf.keras.callbacks.Callback, tf.keras.callbacks.ProgbarLogger, tf.keras.layers.Embedding, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.mixed_precision.experimental.Policy, tf.keras.mixed_precision.experimental.set_policy, tf.keras.optimizers.schedules.ExponentialDecay, tf.losses.BinaryCrossentropy, tf.math.count_nonzero, tf.nn.softmax, tf.not_equal, tf.range, tf.reduce_all, tf.shape, tf.squeeze, tf.stack, tf.tile, tf.transpose, tf.zeros_like, typing.List","__init__, _apply_grads, _build_metrics, _init_config, build_callbacks, build_loss, build_model, build_optimizer, build_progbar, build_train_dataset, build_transformer, build_vocab, compile_model, decode, evaluate, evaluate_batch, evaluate_dataset, fit, get_loss, load_weights, num_samples_in, on_epoch_end, on_train_begin, predict_batch, run_metrics, sample_data, train_batch, train_loop","BiaffineDependencyParserTF, BiaffineSemanticDependencyParserTF, BiaffineTransformerDependencyParserTF, BiaffineTransformerSemanticDependencyParser","Y, accum_grads, arc, arc_loss, arc_pred, arc_preds, arc_probs, arc_scores, arcs, batch_size, callbacks, dataset, epoch, epochs, ext, feats, grad, grads, head_probs, heads, idx, learning_rate_transformer, length, logs, loss, m, mask, metric, metrics, model, optimizer, optimizer_transformer, output, outputs, params, policy, pretrained, rel, rel_loss, rel_pred, rel_preds, rel_prob, rel_scores, rels, root, root_rel_idx, root_rel_onehot, scheduler, scores, steps_per_epoch, tape, tokenizer, tokens, train_examples, train_steps, train_steps_per_epoch, trainable_variables, transform, transformer, trn_data, tv, v, warmup_steps, words",,"

, AutoModelWithLMHead, HanLP, Hello, LAS, LF, NN, UAS, UF, Unknown identifier , Unknown tree algorithm , accum_grads, adam, adamw, albert, albert_chinese, albert_config.json, binary_crossentropy, chinese-roberta, epochs, float16, fp16, good, is, learning_rate_transformer, loss, mask_p, metrics, mixed_float16, model.h5, mst, optimizer_transformer, params, sparse_categorical_crossentropy, steps, stop_training, tarjan, trained_samples, transformer_variable_names, trn_data, use_amp, use_pos, utf-8, val_, val_loss, verbose, vocab_chinese.txt, w, warmup_steps_ratio, world, zh","0, 0.0, 0.002, 0.33, 0.75, 0.9, 1, 100, 150, 1e-08, 1e-12, 2, 256, 3, 3000, 32, 4, 400, 5, 5.0, 500, 5000, 50000, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-22 12:47, # because we are customizing batching, # evaluate on dev, # ignore all punctuation if not specified, # keras will use cuda lstm when config.lstm_dropout is 0, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, # print('Apply grads after', self.params['trained_samples'], 'samples'), # progbar: tf.keras.callbacks.ProgbarLogger = callbacks[-1], # rels = rel_argmax(rel_prob, length, root_rel_idx), # tf.config.optimizer.set_experimental_options({""auto_mixed_precision"": True}), # use a normal adam for biaffine",
https://github.com/hankcs/HanLP,chu_liu_edmonds.py,0,0.0,64,55.65,5,4.35,8,6.96,38,33.04,0,0.0,3,0,42,2,,"ValueError, _, _find_cycle, add, added, append, argmax, bool, child, chu_liu_edmonds, considered_representatives, current_nodes, cycle, cycle_rep, cycle_representative, cycle_weight, decode_mst, energy, enumerate, final_edges, float, found, has_cycle, has_labels, head_type, heads, in_edge, in_edge_weight, index, input_shape, int, items, key_node, label_id_matrix, length, list, max, max_length, max_score, ndim, new_score, next_node, node, node1, node2, node_in_cycle, numpy, old_input, old_output, original, original_score_matrix, out_edge, out_edge_weight, parent, parents, previous, range, representatives, score, score_matrix, set, shape, this_cycle, typing","numpy.array, numpy.int32, numpy.ndarray, numpy.ones, numpy.zeros, typing.Dict, typing.List, typing.Set, typing.Tuple","_find_cycle, chu_liu_edmonds, decode_mst",,"_, added, child, considered_representatives, current_nodes, cycle, cycle_rep, cycle_representative, cycle_weight, energy, final_edges, found, has_cycle, head_type, heads, in_edge, in_edge_weight, index, input_shape, key_node, label_id_matrix, max_length, max_score, new_score, next_node, node, node1, node2, node_in_cycle, old_input, old_output, original, original_score_matrix, out_edge, out_edge_weight, parent, parents, previous, representatives, score, score_matrix, this_cycle","Applies the chu-liu-edmonds algorithm recursively
to a graph with edge weights defined by score_matrix.

Note that this function operates in place, so variables
will be modified.

# Parameters

length : `int`, required.
    The number of nodes.
score_matrix : `numpy.ndarray`, required.
    The score matrix representing the scores for pairs
    of nodes.
current_nodes : `List[bool]`, required.
    The nodes which are representatives in the graph.
    A representative at it's most basic represents a node,
    but as the algorithm progresses, individual nodes will
    represent collapsed cycles in the graph.
final_edges : `Dict[int, int]`, required.
    An empty dictionary which will be populated with the
    nodes which are connected in the maximum spanning tree.
old_input : `numpy.ndarray`, required.
old_output : `numpy.ndarray`, required.
representatives : `List[Set[int]]`, required.
    A list containing the nodes that a particular node
    is representing at this iteration in the graph.

# Returns

Nothing - all variables are modified in place.

Args:
  length: int: 
  score_matrix: numpy.ndarray: 
  current_nodes: List[bool]: 
  final_edges: Dict[int: 
  int]: 
  old_input: numpy.ndarray: 
  old_output: numpy.ndarray: 
  representatives: List[Set[int]]: 

Returns:, Note: Counter to typical intuition, this function decodes the _maximum_
spanning tree.

Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for
maximum spanning arborescences on graphs.

Adopted from https://github.com/allenai/allennlp/blob/master/allennlp/nn/chu_liu_edmonds.py
which is licensed under the Apache License 2.0

# Parameters

energy : `numpy.ndarray`, required.
    A tensor with shape (num_labels, timesteps, timesteps)
    containing the energy of each edge. If has_labels is `False`,
    the tensor should have shape (timesteps, timesteps) instead.
length : `int`, required.
    The length of this sequence, as the energy may have come
    from a padded batch.
has_labels : `bool`, optional, (default = True)
    Whether the graph has labels or not.

Args:
  energy: numpy.ndarray: 
  length: int: 
  has_labels: bool:  (Default value = True)

Returns:","-inf, Applies the chu-liu-edmonds algorithm recursively
    to a graph with edge weights defined by score_matrix.
    
    Note that this function operates in place, so variables
    will be modified.
    
    # Parameters
    
    length : `int`, required.
        The number of nodes.
    score_matrix : `numpy.ndarray`, required.
        The score matrix representing the scores for pairs
        of nodes.
    current_nodes : `List[bool]`, required.
        The nodes which are representatives in the graph.
        A representative at it's most basic represents a node,
        but as the algorithm progresses, individual nodes will
        represent collapsed cycles in the graph.
    final_edges : `Dict[int, int]`, required.
        An empty dictionary which will be populated with the
        nodes which are connected in the maximum spanning tree.
    old_input : `numpy.ndarray`, required.
    old_output : `numpy.ndarray`, required.
    representatives : `List[Set[int]]`, required.
        A list containing the nodes that a particular node
        is representing at this iteration in the graph.
    
    # Returns
    
    Nothing - all variables are modified in place.

    Args:
      length: int: 
      score_matrix: numpy.ndarray: 
      current_nodes: List[bool]: 
      final_edges: Dict[int: 
      int]: 
      old_input: numpy.ndarray: 
      old_output: numpy.ndarray: 
      representatives: List[Set[int]]: 

    Returns:

    , Note: Counter to typical intuition, this function decodes the _maximum_
    spanning tree.
    
    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for
    maximum spanning arborescences on graphs.
    
    Adopted from https://github.com/allenai/allennlp/blob/master/allennlp/nn/chu_liu_edmonds.py
    which is licensed under the Apache License 2.0
    
    # Parameters
    
    energy : `numpy.ndarray`, required.
        A tensor with shape (num_labels, timesteps, timesteps)
        containing the energy of each edge. If has_labels is `False`,
        the tensor should have shape (timesteps, timesteps) instead.
    length : `int`, required.
        The length of this sequence, as the energy may have come
        from a padded batch.
    has_labels : `bool`, optional, (default = True)
        Whether the graph has labels or not.

    Args:
      energy: numpy.ndarray: 
      length: int: 
      has_labels: bool:  (Default value = True)

    Returns:

    , The dimension of the energy array is not equal to 2., The dimension of the energy array is not equal to 3.","0, 0.0, 1, 2, 3, False, None, True","# Add the new edge score to the cycle weight, # Adopted from https://github.com/allenai/allennlp under Apache Licence 2.0., # Changed the packaging., # Check if this solution has a cycle., # Expansion stage., # Find the weight of the cycle., # For each node in the graph, find the maximum weight incoming, # For the next recursive iteration, we want to consider the cycle as a, # From here until the recursive call is the contraction stage of the algorithm., # If there are no cycles, find all edges and return., # If we see a node we've already processed,, # Initialize a new possible cycle., # Otherwise, we have a cycle so we need to remove an edge., # Our energy matrix might have been batched -, # Set the initial graph to be the greedy best one., # The main algorithm operates inplace., # We need to consider at least one, # and outgoing edge into the cycle., # and subtract the edge we're considering removing., # check each node in cycle, if one of its representatives, # considered in the next iteration. We also keep track of which, # cycle (first node is arbitrary), set all the other nodes not be, # don't redo nodes we've already, # get best label for each edge., # get original score matrix, # here we clip it to contain only non padded tokens., # initialize score matrix to original score matrix, # is a key in the final_edges, it is the one we need., # node in the cycle, arbitrarily choose, # processing would have been in that cycle., # representatives we are considering this iteration because we need, # single node. Here we collapse the cycle into the first node in the, # the first., # them below to check if we're done., # visited or aren't considering., # we can stop, because the node we are, Applies the chu-liu-edmonds algorithm recursively
to a graph with edge weights defined by score_matrix.

Note that this function operates in place, so variables
will be modified.

# Parameters

length : `int`, required.
The number of nodes.
score_matrix : `numpy.ndarray`, required.
The score matrix representing the scores for pairs
of nodes.
current_nodes : `List[bool]`, required.
The nodes which are representatives in the graph.
A representative at it's most basic represents a node,
but as the algorithm progresses, individual nodes will
represent collapsed cycles in the graph.
final_edges : `Dict[int, int]`, required.
An empty dictionary which will be populated with the
nodes which are connected in the maximum spanning tree.
old_input : `numpy.ndarray`, required.
old_output : `numpy.ndarray`, required.
representatives : `List[Set[int]]`, required.
A list containing the nodes that a particular node
is representing at this iteration in the graph.

# Returns

Nothing - all variables are modified in place.

Args:
length: int:
score_matrix: numpy.ndarray:
current_nodes: List[bool]:
final_edges: Dict[int:
int]:
old_input: numpy.ndarray:
old_output: numpy.ndarray:
representatives: List[Set[int]]:

Returns:

, Note: Counter to typical intuition, this function decodes the _maximum_
spanning tree.

Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for
maximum spanning arborescences on graphs.

Adopted from https://github.com/allenai/allennlp/blob/master/allennlp/nn/chu_liu_edmonds.py
which is licensed under the Apache License 2.0

# Parameters

energy : `numpy.ndarray`, required.
A tensor with shape (num_labels, timesteps, timesteps)
containing the energy of each edge. If has_labels is `False`,
the tensor should have shape (timesteps, timesteps) instead.
length : `int`, required.
The length of this sequence, as the energy may have come
from a padded batch.
has_labels : `bool`, optional, (default = True)
Whether the graph has labels or not.

Args:
energy: numpy.ndarray:
length: int:
has_labels: bool:  (Default value = True)

Returns:

",
https://github.com/hankcs/HanLP,conll.py,0,0.0,34,57.63,12,20.34,8,13.56,5,8.47,0,0.0,2,0,13,0,,"ValueError, append, arrows, cells, close, collapse_enhanced_empty_nodes, collapsed, deprel, endswith, enhanced_collapse_empty_nodes, enumerate, exception, filepath, float, hanlp, head, id, idx, int, isinstance, line, list, open, read_conll, sent, sorted, split, src, startswith, str, strip, typing, underline_to_none, x","hanlp.utils.io_util.TimingFileIterator, hanlp.utils.io_util.get_resource, hanlp.utils.log_util.logger, typing.Union","collapse_enhanced_empty_nodes, read_conll",,"arrows, cells, collapsed, deprel, enhanced_collapse_empty_nodes, filepath, head, id, idx, line, sent, src, x",,"	, 
, #, -, ., .conllu, :, >, Wrong CoNLL format , _, utf-8, |","0, 1, 6, 7, 8, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 15:37, # sent[-1][1] += cells[1], #'):",
https://github.com/hankcs/HanLP,parse_alg.py,0,0.0,117,87.31,5,3.73,6,4.48,6,4.48,0,0.0,18,2,53,4,,"I, SCC, SCCs, StopIteration, T, Tarjan, UnionFind, _SCCs, __init__, _edges, _indices, _lowlinks, _onstack, _vertices, add, add_secondary_arcs_by_preds, add_secondary_arcs_by_scores, adjust_root_score, adjust_root_score_then_add_secondary_arcs, any, append, arc_pred, arc_preds, arc_probs, arc_scores, change, changed_cycle, chu_liu_edmonds, collections, cycle, dep, dependents, dfs, dh, edges, end, ensure_tree, enumerate, find, float, fringe, graph, hanlp, head, height, index, indices, isinstance, len, length, list, mask, min, mst_then_greedy, ndim, new_head, new_head_probs, new_heads, new_root, new_root_probs, next, next_state, node, non_heads, num_root, numpy, object, old_head, old_head_probs, old_heads, parent, parse_graph, parse_preds, parse_probs, path, pop, prediction, property, range, rel, rel_idx, rel_pred, rel_preds, rel_probs, rel_scores, root, root_probs, root_rel_idx, roots, same, scipy, sdh, self, set, sigmoid, sorted, span_util, stack, start, state, strongconnect, sum, super, tarjan, to_visit, tokens, tokens_to_keep, tree, unique_root, unite, update, utils, v, vertices, w, x, y","collections.defaultdict, hanlp.components.parsers.chu_liu_edmonds.decode_mst, np.arange, np.argmax, np.argmin, np.argwhere, np.array, np.expand_dims, np.eye, np.max, np.min, np.ndarray, np.repeat, np.where, scipy.special.expit, scipy.special.softmax","SCCs, __init__, add_secondary_arcs_by_preds, add_secondary_arcs_by_scores, adjust_root_score, adjust_root_score_then_add_secondary_arcs, chu_liu_edmonds, dfs, edges, find, indices, mst_then_greedy, same, strongconnect, tarjan, unique_root, unite, vertices","Tarjan, UnionFind","I, SCC, arc_pred, arc_preds, arc_probs, arc_scores, change, changed_cycle, cycle, dep, dependents, dh, fringe, graph, head, index, length, mask, new_head, new_head_probs, new_heads, new_root, new_root_probs, next_state, node, non_heads, num_root, old_head, old_head_probs, old_heads, parse_graph, parse_preds, parse_probs, path, rel, rel_pred, rel_probs, rel_scores, root, root_probs, roots, sdh, stack, state, tarjan, to_visit, tokens, tokens_to_keep, tree, v, w, x, y","Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

Args:
  parse_probs(NDArray): seq_len x seq_len, the probability of arcs
  length(NDArray): sentence length including ROOT
  tokens_to_keep(NDArray): mask matrix
  ensure_tree:  (Default value = True)

Returns:, Args:
  v: 
  index: 
  stack: 

Returns:, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, Parameters
----------
prediction : numpy.ndarray
    a predicted dependency tree where prediction[dep_idx] = head_idx
tokens : numpy.ndarray
    the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)","

        Args:
          v: 
          index: 
          stack: 

        Returns:

        , 

        Parameters
        ----------
        prediction : numpy.ndarray
            a predicted dependency tree where prediction[dep_idx] = head_idx
        tokens : numpy.ndarray
            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)
        , Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

    Args:
      parse_probs(NDArray): seq_len x seq_len, the probability of arcs
      length(NDArray): sentence length including ROOT
      tokens_to_keep(NDArray): mask matrix
      ensure_tree:  (Default value = True)

    Returns:

    
    , Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, inf","0, 1, 1000, False, None, True","

self._indices[v] = index
self._lowlinks[v] = index
index += 1
stack.append(v)
self._onstack[v] = True
for w in self.edges[v]:
if w not in self.indices:
self.strongconnect(w, index, stack)
self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])
elif self._onstack[w]:
self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])

if self._lowlinks[v] == self._indices[v]:
self._SCCs.append(set())
while stack[-1] != v:
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
return

# ======================
@property
def edges(self):
return self._edges

@property
def vertices(self):
return self._vertices

@property
def indices(self):
return self._indices

@property
def SCCs(self):
return self._SCCs


class UnionFind(object):

def __init__(self, n) -> None:
super().__init__()
self.parent = [x for x in range(n)]
self.height = [0] * n

def find(self, x):
if self.parent[x] == x:
return x
self.parent[x] = self.find(self.parent[x])
return self.parent[x]

def unite(self, x, y):
x = self.find(x)
y = self.find(y)
if x == y:
return
if self.height[x] < self.height[y]:
self.parent[x] = y
else:
self.parent[y] = x
if self.height[x] == self.height[y]:
self.height[x] += 1

def same(self, x, y):
return self.find(x) == self.find(y)


def tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):
Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py, 
self._edges = defaultdict(set)
self._vertices = set((0,))
for dep, head in enumerate(prediction[tokens]):
self._vertices.add(dep + 1)
self._edges[head].add(dep + 1)
self._indices = {}
self._lowlinks = {}
self._onstack = defaultdict(lambda: False)
self._SCCs = []

index = 0
stack = []
for v in self.vertices:
if v not in self.indices:
self.strongconnect(v, index, stack)

# =============================================================
def strongconnect(self, v, index, stack):
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-04-02 23:20, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph

def __init__(self, prediction, tokens):
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-22 12:46",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-22 20:50",
https://github.com/hankcs/HanLP,transformer_sts.py,0,0.0,112,70.89,26,16.46,14,8.86,6,3.8,0,0.0,15,1,25,2,,"TransformerSemanticTextualSimilarity, __init__, _tokenizer, adam_epsilon, backward, base_model, batch, batch_size, best_epoch, best_metric, bool, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, criterion, current, dataloader, decode, delimiter, dev, device, elapsed_human, epoch, epochs, eval, evaluate_dataloader, execute_training_loop, extend, feed_batch, fit, fit_dataloader, flat, float, from_config, from_pretrained, get, grad_norm, gradient_accumulation, hanlp, hanlp_common, history, info, int, isinstance, item, len, lens, list, load_weights, log, logger, logging, logits, loss, lr, max, max_score, max_seq_len, min, min_score, model, num_training_steps, on_config_ready, open, orders, output_dict, pad_token_id, parameters, predict, prediction, predictions, range, ratio_width, report, report_metrics, reset, sampler_builder, samples, save_dir, save_weights, scores, self, sent_a_col, sent_b_col, shuffle, similarity_col, split, squeeze, step, super, timer, tokenizer, tolist, torch, total, total_loss, total_time_human, train, training, transformer_lr, transformers, trn, typing, warmup_steps, weight_decay, write, zero_grad, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.datasets.sts.stsb.SemanticTextualSimilarityDataset, hanlp.layers.transformers.pt_imports.AutoConfig_, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.metrics.spearman_correlation.SpearmanCorrelation, hanlp.transform.transformer_tokenizer.TransformerTextTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, torch.nn.Module, torch.nn.utils.clip_grad_norm_, torch.no_grad, torch.utils.data.DataLoader, transformers.AutoModelForSequenceClassification, transformers.modeling_outputs.SequenceClassifierOutput, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, decode, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, on_config_ready, predict, report_metrics",TransformerSemanticTextualSimilarity,"batch, best_epoch, best_metric, config, data, dataloader, dataset, epoch, flat, history, lens, loss, model, num_training_steps, orders, output, output_dict, prediction, predictions, report, sampler_builder, samples, scores, timer, total_loss","A simple Semantic Textual Similarity (STS) baseline which fine-tunes a transformer with a regression layer on
top of it.

Args:
    **kwargs: Predefined config., Predict the similarity between sentence pairs.

Args:
    data: Sentence pairs.
    batch_size: The number of samples in a batch.
    **kwargs: Not used.

Returns:
    Similarities between sentences.",", 	, 
, 
        A simple Semantic Textual Similarity (STS) baseline which fine-tunes a transformer with a regression layer on
        top of it.

        Args:
            **kwargs: Predefined config.
        ,  ,  / ,  Predict the similarity between sentence pairs.

        Args:
            data: Sentence pairs.
            batch_size: The number of samples in a batch.
            **kwargs: Not used.

        Returns:
            Similarities between sentences.
        ,  [red]saved[/red],  epochs ago,  saved , .4f, /, :[/yellow], Restored the best model with , [yellow]Epoch , attention_mask, auto, input_ids, loss, loss: , sent_a, sent_b, similarity, token_type_ids, trn, w","0, 0.0, 0.001, 0.1, 1, 1.0, 128, 1e-08, 3, 32, 5e-05, False, None, True","
A simple Semantic Textual Similarity (STS) baseline which fine-tunes a transformer with a regression layer on
top of it.

Args:
**kwargs: Predefined config.
,  Predict the similarity between sentence pairs.

Args:
data: Sentence pairs.
batch_size: The number of samples in a batch.
**kwargs: Not used.

Returns:
Similarities between sentences.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-20 17:03, # noinspection PyMethodOverriding",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-05-20 17:02",
https://github.com/hankcs/HanLP,cnn_tagger_tf.py,0,0.0,75,71.43,9,8.57,12,11.43,9,8.57,0,0.0,8,3,24,0,,"CNNTaggerTF, CNNTaggingModel, WindowTokenTransform, X, X_to_inputs, __init__, _keras_mask, abc, append, build_model, call, concat, config, conv, conv2d, conv2d_dropout, create_types_shapes_values, dense, dropout, dtype, embed, embed_dropout, embeds, enumerate, feature, features, file_to_samples, filters, first_token, fit, gold, hanlp, hs, idx_to_token, index, input_shape, inputs, inputs_to_samples, int, kernels, kwargs, len, locals, map_x, mask, model, ngrams, num_tags, pad_token, padding_values, pop, property, range, self, shape, shape_h, shapes, str, super, tag_vocab, tags, tensorflow, transform, trn_path, types, typing, update, values, window_radius, window_size, word, word_vocab, words, x, xs","abc.ABC, hanlp.common.vocab_tf.VocabTF, hanlp.components.taggers.tagger_tf.TaggerComponent, hanlp.layers.embeddings.util_tf.build_embedding, hanlp.transform.tsv_tf.TSVTaggingTransform, tf.Tensor, tf.keras.Model, tf.keras.layers.Concatenate, tf.keras.layers.Conv2D, tf.keras.layers.Dense, tf.keras.layers.Dropout, tf.keras.losses.Loss, tf.keras.models.Model, tf.keras.optimizers.Optimizer, tf.reshape, tf.shape, tf.string, typing.Any, typing.Iterable, typing.List, typing.Tuple, typing.Union","X_to_inputs, __init__, build_model, call, create_types_shapes_values, fit, input_shape, inputs_to_samples","CNNTaggerTF, CNNTaggingModel, WindowTokenTransform","conv, embed, embeds, feature, features, h, hs, index, mask, model, ngrams, o, shape_h, shapes, tags, transform, types, values, window_radius, window_size, word, words, x, xs",,"__class__, accuracy, adam, bos{}, channels_last, eos+{}, kwargs, same, self","0, 0.3, 1, 100, 2, 200, 3, 4, 5, False, None, True","#     print(inputs), #     return tf.zeros_like(), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-31 13:52, # if inputs.shape_h[0] is None:, # model.build((None, None, 3)), # noinspection PyMethodOverriding, # refine the type",
https://github.com/hankcs/HanLP,pos_tf.py,0,0.0,3,50.0,0,0.0,0,0.0,3,50.0,0,0.0,0,2,0,0,,"CNNPartOfSpeechTaggerTF, RNNPartOfSpeechTaggerTF, hanlp","hanlp.components.taggers.cnn_tagger_tf.CNNTaggerTF, hanlp.components.taggers.rnn_tagger_tf.RNNTaggerTF",,"CNNPartOfSpeechTaggerTF, RNNPartOfSpeechTaggerTF",,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-05 23:05",
https://github.com/hankcs/HanLP,rnn_tagger.py,0,0.0,99,68.75,23,15.97,13,9.03,9,6.25,0,0.0,13,1,30,1,,"RNNTagger, __init__, _convert_embed, _id_to_tags, anneal_factor, anneal_patience, append, backward, batch, batch_size, build_criterion, build_dataloader, build_dataset, build_model, build_scheduler, build_vocabs, clip_grad_norm_, compute_loss, config, crf, criterion, decode_output, dev, dev_metric, device, drop, each, elapsed_average_human, elapsed_human, enumerate, epoch, epochs, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, float, from_config, hanlp, hanlp_common, ids, idx, info, insert, isinstance, item, len, load_weights, lock, log, logger, logging, loss, mask, max_e, max_metric, metric, model, module, mutable, optimizer, out, output_dim, param_groups, parameters, patience, prediction, range, ratio_width, report_patience, rnn_hidden, rnn_input, sample, sampler, save_dir, save_weights, score, self, shuffle, step, stop, summary, super, tag, timer, token_embed, token_key, torch, total_loss, train, transform, trn, update_metrics, utils, vocabs, write_output, x, zero_grad","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSampler, hanlp.common.dataset.TransformableDataset, hanlp.common.transform.EmbeddingNamedTransform, hanlp.common.vocab.Vocab, hanlp.components.taggers.rnn.rnntaggingmodel.RNNTaggingModel, hanlp.components.taggers.tagger.Tagger, hanlp.datasets.ner.loaders.tsv.TSVTaggingDataset, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.util.build_word2vec_with_vocab, hanlp.utils.time_util.CountdownTimer, hanlp_common.configurable.Configurable, hanlp_common.util.merge_dict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.nn, torch.nn.Module, torch.optim.lr_scheduler.ReduceLROnPlateau, torch.utils.data.DataLoader","__init__, _convert_embed, _id_to_tags, build_dataloader, build_dataset, build_model, build_scheduler, build_vocabs, execute_training_loop, feed_batch, fit, fit_dataloader, write_output",RNNTagger,"batch, criterion, dataset, dev_metric, each, embed, epoch, idx, loss, mask, max_e, max_metric, model, out, patience, prediction, ratio_width, report_patience, sample, sampler, scheduler, stop, timer, token_embed, total_loss, transform, vocab, vocabs, x, y","An old-school tagger using non-contextualized embeddings and RNNs as context layer.

Args:
    **kwargs: Predefined config."," ,  / ,  at epoch ,  elapsed, average time of each epoch is ,  lr: , .2%, .4f, /, :[/yellow], An old-school tagger using non-contextualized embeddings and RNNs as context layer.

        Args:
            **kwargs: Predefined config.
        , Max score of dev is , Patience: , [red]Saved[/red] , [yellow]Epoch , _id, adam, embed, loss: , lr, max, tag, tag_id, token","0, 0.001, 0.5, 1, 10, 100, 2, 256, 5.0, 50, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Before building vocabs, let embeddings submit their vocabs, some embeddings will possibly opt out as their, # Date: 2020-05-20 13:12, # Vocabs built, now add all transforms to the pipeline. Be careful about redundant ones., # noinspection PyMethodOverriding, # save the model if it is the best so far, # transforms are not relevant to vocabs, An old-school tagger using non-contextualized embeddings and RNNs as context layer.

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,rnn_tagger_tf.py,0,0.0,41,61.19,10,14.93,9,13.43,7,10.45,0,0.0,9,1,7,0,,"RNNTaggerTF, __init__, _keras_mask, add, batch_size, build, build_loss, build_model, build_transform, char_vocab, embedding_layer, filename, fit, get_layer, hanlp, hanlp_common, layers, len, map_x, model, output_shape, pad_token, predict, property, rnn_input_dropout, rnn_output_dropout, rnn_units, sample, sample_data, save_dir, save_weights, self, sents, super, tag_vocab, tensorflow, trainable, transform, truncated_model, typing, word_vocab","hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.components.taggers.tagger_tf.TaggerComponent, hanlp.layers.embeddings.util_tf.build_embedding, hanlp.layers.embeddings.util_tf.embeddings_require_char_input, hanlp.layers.embeddings.util_tf.embeddings_require_string_input, hanlp.transform.tsv_tf.TSVTaggingTransform, hanlp_common.util.merge_locals_kwargs, tf.constant, tf.keras.Model, tf.keras.Sequential, tf.keras.layers.Bidirectional, tf.keras.layers.Dense, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.layers.LSTM, tf.keras.layers.TimeDistributed, tf.keras.losses.Loss, tf.keras.losses.Reduction.SUM, tf.keras.losses.SparseCategoricalCrossentropy, tf.keras.optimizers.Optimizer, tf.not_equal, typing.List, typing.Union","__init__, build_loss, build_model, build_transform, fit, predict, sample_data, save_weights, tag_vocab",RNNTaggerTF,"embedding_layer, embeddings, loss, model, sample, transform, truncated_model",,"accuracy, adam, bilstm, dense, hello, model.h5, rnn_input_dropout, rnn_output_dropout, this, world","0, 0.2, 1, 100, 20, 32, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-09-14 20:30, # Instead, in order to instantiate and build your model, `call` your model on real tensor data (of the, # You cannot build your model by calling `build` if your layers do not support float type inputs., # correct dtype)., # remove the pre-trained embedding",
https://github.com/hankcs/HanLP,tagger.py,0,0.0,134,62.91,34,15.96,7,3.29,7,3.29,31,14.55,18,1,45,1,,"Adam, CrossEntropyLoss, DataParallel, Tagger, ValueError, _d, abc, append, argmax, batch, batch_size, begin, best_epoch, best_metric, build_criterion, build_dataloader, build_metric, build_optimizer, build_samples, close, compute_loss, config, crf, criterion, dataloader, decode, decode_output, decoder, dev, dev_metric, device, dict_tags, each, elapsed_average_human, elapsed_human, end, enumerate, epoch, epochs, eta_human, eval, eval_trn, evaluate_dataloader, execute_training_loop, extend, feed_batch, fit_dataloader, flat, float, forward, get, gs, hanlp, hanlp_common, hanlp_trie, history, id, id_to_tags, ids, idx, idx_to_token, info, input_is_flat, int, isinstance, items, k, kwargs, label, len, lens, list, load_weights, log, logger, logging, logits, loss, lr, mask, model, model_from_config, module, open, optimizer, orders, out, output, outputs, parameters, patience, pred, pred_ids, predict, predict_data, prediction, prediction_to_human, property, ps, range, ratio_width, reduction, report, reset, sampler_builder, samples, save_dir, save_weights, self, sent, sents, setter, tag, tagging_scheme, tags, timer, token_key, tokenize, tokens, tolist, torch, total_time_human, trn, type, typing, update, update_metrics, v, vocab, vocabs, warnings, write, write_prediction, zip","abc.ABC, abc.abstractmethod, hanlp.common.structure.History, hanlp.components.distillation.distillable_component.DistillableComponent, hanlp.components.taggers.util.guess_tagging_scheme, hanlp.layers.crf.crf.CRF, hanlp.metrics.accuracy.CategoricalAccuracy, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.reorder, hanlp_trie.DictInterface, hanlp_trie.TrieDict, hanlp_trie.dictionary.TupleTrieDict, logging.Logger, torch.LongTensor, torch.Tensor, torch.nn, torch.no_grad, torch.optim, torch.optim.SGD, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.List, typing.Sequence, typing.TextIO, typing.Tuple, typing.Union, warnings.warn","build_criterion, build_metric, build_optimizer, build_samples, compute_loss, decode_output, dict_tags, evaluate_dataloader, execute_training_loop, feed_batch, id_to_tags, input_is_flat, predict, predict_data, prediction_to_human, tagging_scheme, update_metrics, write_prediction",Tagger,"_d, batch, batch_size, begin, best_epoch, best_metric, criterion, dataloader, dev_metric, dict_tags, dictionary, each, end, epoch, flat, gs, history, id, idx, k, label, logits, loss, mask, metric, model, orders, out, output, outputs, pred, pred_ids, prediction, ps, report, samples, sent, sents, tagging_scheme, tags, timer, tokens, v, vocab, y","A custom dictionary to override predicted tags by performing longest-prefix-matching.

Examples:
    >>> pos.dict_tags = {'HanLP': 'state-of-the-art-tool'} # Force 'HanLP' to be 'state-of-the-art-tool'
    >>> tagger(""HanLP为生产环境带来次世代最先进的多语种NLP技术。"")
        # HanLP/state-of-the-art-tool 为/P 生产/NN 环境/NN 带来/VV 次世代/NN 最/AD 先进/VA 的/DEC 多语种/NN NLP/NR 技术/NN 。/PU
    >>> pos.dict_tags = {('的', '希望'): ('补语成分', '名词'), '希望': '动词'} # Conditional matching
    >>> tagger(""我的希望是希望张晚霞的背影被晚霞映红。"")
        # 我/PN 的/补语成分 希望/名词 是/VC 希望/动词 张晚霞/NR 的/DEG 背影/NN 被/LB 晚霞/NN 映红/VV 。/PU","	, 
,  ,  (,  / ,  A custom dictionary to override predicted tags by performing longest-prefix-matching.

        Examples:
            >>> pos.dict_tags = {'HanLP': 'state-of-the-art-tool'} # Force 'HanLP' to be 'state-of-the-art-tool'
            >>> tagger(""HanLP为生产环境带来次世代最先进的多语种NLP技术。"")
                # HanLP/state-of-the-art-tool 为/P 生产/NN 环境/NN 带来/VV 次世代/NN 最/AD 先进/VA 的/DEC 多语种/NN NLP/NR 技术/NN 。/PU
            >>> pos.dict_tags = {('的', '希望'): ('补语成分', '名词'), '希望': '动词'} # Conditional matching
            >>> tagger(""我的希望是希望张晚霞的背影被晚霞映红。"")
                # 我/PN 的/补语成分 希望/名词 是/VC 希望/动词 张晚霞/NR 的/DEG 背影/NN 被/LB 晚霞/NN 映红/VV 。/PU
        ,  ETA: ,  [red](saved)[/red],  at epoch ,  early stop,  elapsed,  might be IOB1 or IOB2 but we are using IOB2 by default. Please set tagging_scheme=""IOB1"" or tagging_scheme=""BIO"" to get rid of this warning., ), ., .4f, :[/yellow], Average time of each epoch is , BIO, DataParallel not supported when CRF is used, Expected dictionary to be `dict` but got , Max score of dev is , The tag scheme for , [yellow]Epoch , _, adam, batch_size, crf, dict_tags, loss: , mean, sgd, tag, tag_id, w","0, 1, 32, 5, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Conditional matching, # Date: 2020-08-11 12:19, # Force 'HanLP' to be 'state-of-the-art-tool', # HanLP/state-of-the-art-tool 为/P 生产/NN 环境/NN 带来/VV 次世代/NN 最/AD 先进/VA 的/DEC 多语种/NN NLP/NR 技术/NN 。/PU, # 我/PN 的/补语成分 希望/名词 是/VC 希望/动词 张晚霞/NR 的/DEG 背影/NN 被/LB 晚霞/NN 映红/VV 。/PU","'动词', '名词', '希望', '的', '补语成分', HanLP/state-of-the-art-tool 为/P 生产/NN 环境/NN 带来/VV 次世代/NN 最/AD 先进/VA 的/DEC 多语种/NN NLP/NR 技术/NN 。/PU, HanLP为生产环境带来次世代最先进的多语种NLP技术。, 。/PU, 为/P, 先进/VA, 多语种/NN, 希望/动词, 希望/名词, 带来/VV, 张晚霞/NR, 我/PN, 我/PN 的/补语成分 希望/名词 是/VC 希望/动词 张晚霞/NR 的/DEG 背影/NN 被/LB 晚霞/NN 映红/VV 。/PU, 我的希望是希望张晚霞的背影被晚霞映红。, 技术/NN, 映红/VV, 是/VC, 晚霞/NN, 最/AD, 次世代/NN, 环境/NN, 生产/NN, 的/DEC, 的/DEG, 的/补语成分, 背影/NN, 被/LB"
https://github.com/hankcs/HanLP,tagger_tf.py,0,0.0,25,67.57,7,18.92,2,5.41,3,8.11,0,0.0,2,1,2,0,,"TaggerComponent, abc, add, build_loss, build_metrics, config, crf, debug, dtype, get, hanlp, hasattr, isinstance, kwargs, len, logger, logging, loss, model, run_eagerly, self, super, tag_vocab, tensorflow, transform","abc.ABC, hanlp.common.keras_component.KerasComponent, hanlp.layers.crf.crf_layer_tf.CRF, hanlp.layers.crf.crf_layer_tf.CRFLoss, hanlp.layers.crf.crf_layer_tf.CRFWrapper, hanlp.metrics.chunking.iobes_tf.IOBES_F1_TF, logging.Logger, tf.keras.models.Sequential","build_loss, build_metrics",TaggerComponent,"crf, loss",,"ChunkingF1 runs only under eager mode, set run_eagerly=True to remove this warning, Name your tag vocab tag_vocab in your transform or override build_metrics, crf, f1, run_eagerly, should create model before build loss, tag_vocab","None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-25 21:49",
https://github.com/hankcs/HanLP,util.py,0,0.0,15,51.72,9,31.03,2,6.9,3,10.34,0,0.0,2,0,3,0,,"dict, enumerate, guess_allowed_transitions, guess_tagging_scheme, hanlp, int, labels, list, replace, scheme, set, split, str, tagset, typing","hanlp.utils.span_util.allowed_transitions, typing.List, typing.Tuple","guess_allowed_transitions, guess_tagging_scheme",,"labels, scheme, tagset",,"-, BIO, BIOUL, BMES, E-, IOBES, L-, S-, U-","0, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-01 00:31",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-08-28 15:39",
https://github.com/hankcs/HanLP,multi_criteria_cws_transformer.py,0,0.0,48,62.34,12,15.58,14,18.18,3,3.9,0,0.0,10,1,10,1,,"MultiCriteriaTransformerTaggingTokenizer, __init__, append, batch, build_dataset, build_metric, build_samples, build_vocabs, config, convert_tokens_to_ids, criteria_token_map, dict, enumerate, feed_batch, fit, float, functools, gold, hanlp, hanlp_common, ids, info, int, keys, last_transform, len, list, logger, mask, next, on_config_ready, pred, prediction, range, sample, samples, self, str, super, tag_to_span, transformer_tokenizer, transforms, trn, typing, unk_token_id, unused_tokens, update_metrics, zip","functools.partial, hanlp.common.dataset.SamplerBuilder, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, hanlp.components.tokenizers.transformer.TransformerTaggingTokenizer, hanlp.datasets.tokenization.loaders.multi_criteria_cws.mcws_dataset.MultiCriteriaTextTokenizingDataset, hanlp.datasets.tokenization.loaders.multi_criteria_cws.mcws_dataset.append_criteria_token, hanlp.metrics.f1.F1, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, typing.List, typing.Union","__init__, build_dataset, build_metric, build_samples, build_vocabs, feed_batch, fit, last_transform, on_config_ready, update_metrics",MultiCriteriaTransformerTaggingTokenizer,"criteria, gold, ids, mask, metrics, pred, sample, samples, transforms, unused_tokens","Transformer based implementation of ""Effective Neural Solution for Multi-Criteria Word Segmentation""
(:cite:`he2019effective`). It uses an artificial token ``[unused_i]`` instead of ``[SEP]`` in the input_ids to
mark the i-th segmentation criteria.

Args:
    **kwargs: Not used.",". Choose one from , BMES, Transformer based implementation of ""Effective Neural Solution for Multi-Criteria Word Segmentation""
        (:cite:`he2019effective`). It uses an artificial token ``[unused_i]`` instead of ``[SEP]`` in the input_ids to
        mark the i-th segmentation criteria.

        Args:
            **kwargs: Not used.
        , Unsupported criteria , [unused, ], ] = , criteria, criteria[, criteria_token_map, sum, tag","0, 0.1, 0.2, 1, 100, 1e-08, 2, 30, 32, 5, 5.0, 5e-05, False, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-21 19:55",
https://github.com/hankcs/HanLP,tok.py,0,0.0,34,66.67,4,7.84,10,19.61,3,5.88,0,0.0,6,1,8,0,,"RNNTokenizer, _id_to_tags, append_transform, batch, batch_size, build_dataset, build_metric, config, decode_output, enumerate, fit, flat, float, gold, hanlp, hanlp_common, int, isinstance, list, logits, mask, outputs, pred, predict, predict_data, self, sentence, str, super, tags, transform, typing, update_metrics, zip","hanlp.components.taggers.rnn_tagger.RNNTagger, hanlp.datasets.tokenization.loaders.chunking_dataset.ChunkingDataset, hanlp.metrics.chunking.chunking_f1.ChunkingF1, hanlp.utils.span_util.bmes_to_words, hanlp_common.util.merge_locals_kwargs, typing.Any, typing.Callable","build_dataset, build_metric, fit, predict, predict_data, update_metrics",RNNTokenizer,"dataset, flat, gold, outputs, pred, sentence, tags, words",,"adam, char, tag, transform","0, 0.001, 0.5, 10, 100, 2, 256, 50, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-12 13:08",
https://github.com/hankcs/HanLP,tok_tf.py,0,0.0,51,70.83,3,4.17,14,19.44,4,5.56,0,0.0,9,7,7,0,,"BMESTokenizerTF, NgramConvTokenizerTF, NgramConvTokenizerTransform, RNNTokenizerTF, RNNTokenizerTransform, TransformerTokenizerTF, TransformerTokenizerTransform, X_to_inputs, Y, Y_to_outputs, Y_to_tokens, __init__, batch, bool, build_loss, build_metrics, config, dict, evaluate_output_to_file, fit, gold, hanlp, hanlp_common, idx_to_token, input, input_is_single_sample, input_truth_output_to_str, inputs, inputs_to_samples, int, isinstance, len, list, logger, logging, out, outputs, run_eagerly, self, str, super, tag_vocab, tensorflow, text, transform, typing, window_size, write, y_pred, ys, zip","hanlp.common.keras_component.KerasComponent, hanlp.components.taggers.ngram_conv.ngram_conv_tagger.NgramConvTaggerTF, hanlp.components.taggers.ngram_conv.ngram_conv_tagger.NgramTransform, hanlp.components.taggers.rnn_tagger_tf.RNNTaggerTF, hanlp.components.taggers.transformers.transformer_tagger_tf.TransformerTaggerTF, hanlp.components.taggers.transformers.transformer_transform_tf.TransformerTransform, hanlp.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropyOverBatchFirstDim, hanlp.metrics.chunking.bmes_tf.BMES_F1_TF, hanlp.transform.tsv_tf.TSVTaggingTransform, hanlp.transform.txt_tf.TxtBMESFormat, hanlp.transform.txt_tf.TxtFormat, hanlp.transform.txt_tf.bmes_to_words, hanlp.transform.txt_tf.extract_ngram_features_and_tags, hanlp_common.util.merge_locals_kwargs, logging.Logger, tf.Tensor, tf.argmax, tf.keras.losses.Loss, tf.keras.optimizers.Optimizer, typing.Any, typing.Iterable, typing.List, typing.Tuple, typing.Union","Y_to_outputs, Y_to_tokens, __init__, build_loss, build_metrics, evaluate_output_to_file, fit, input_is_single_sample, inputs_to_samples","BMESTokenizerTF, NgramConvTokenizerTF, NgramConvTokenizerTransform, RNNTokenizerTF, RNNTokenizerTransform, TransformerTokenizerTF, TransformerTokenizerTransform","Y, inputs, tags, text, transform, y_pred, ys",,"
, adam, f1","0, 0.2, 1, 100, 2, 20, 200, 3, 32, 4, 50, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-27 14:30, # bigram_only = false",
https://github.com/hankcs/HanLP,transformer.py,0,0.0,91,58.71,25,16.13,13,8.39,9,5.81,17,10.97,17,1,38,4,,"E, M, TransformerTaggingTokenizer, __init__, _d, _tokenizer_transform, add, all, append, batch, batch_tags, batch_tokens, buffer, build_dataset, build_metric, combined, combined_span, config, custom_words, decode_output, delta, dict, dict_combine, dict_force, end, enumerate, extend, feed_batch, fit, float, functools, generate_prediction_filename, get, gold, hanlp, hanlp_common, hanlp_trie, id_to_tags, input_is_flat, insert, int, isinstance, label, last_transform, len, logits, mask, model, offs, offset, offsets, output, output_spans, pred, prediction, prediction_to_human, prev_tag, property, range, raw_text, rebuild_span, replace, results, save_dir, self, setter, spans_per_sent, spans_to_tokens, start, sub_tokens, subs, subtoken_offsets, subtoken_spans, super, tag_to_span, tagging_scheme, text, token_key, tokenize, tokenizer_transform, toks, tolist, torch, transformer_tokenizer, tst_data, typing, update, update_metrics, write, write_prediction, zip","functools.partial, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.TransformList, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, hanlp.datasets.tokenization.loaders.txt.TextTokenizingDataset, hanlp.datasets.tokenization.loaders.txt.generate_tags_for_subtokens, hanlp.metrics.f1.F1, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.span_util.bmes_to_spans, hanlp.utils.string_util.possible_tokenization, hanlp_common.util.merge_locals_kwargs, hanlp_trie.DictInterface, hanlp_trie.TrieDict, hanlp_trie.dictionary.TupleTrieDict, torch.Tensor, typing.Any, typing.Dict, typing.List, typing.Set, typing.TextIO, typing.Union","__init__, build_dataset, build_metric, decode_output, dict_combine, dict_force, feed_batch, fit, generate_prediction_filename, input_is_flat, last_transform, prediction_to_human, spans_to_tokens, tag_to_span, tokenizer_transform, update_metrics, write_prediction",TransformerTaggingTokenizer,"E, M, S, _d, batch_tokens, buffer, combined, combined_span, custom_words, delta, dict_combine, dictionary, end, gold, label, mask, offs, offset, offsets, output, output_spans, pred, prediction, prev_tag, r, raw_text, results, spans, spans_per_sent, start, sub_tokens, subs, subtoken_offsets, subtoken_spans, tag, text, tokens, toks","A tokenizer using transformer tagger for span prediction. It features with 2 high performance dictionaries
to handle edge cases in real application.

- ``dict_force``: High priority dictionary performs longest-prefix-matching on input text which takes higher
  priority over model predictions.
- ``dict_combine``: Low priority dictionary performs longest-prefix-matching on model predictions then
  combines them.

.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
    do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

It also supports outputting the span of each token by setting ``config.output_spans = True``.

Args:
    **kwargs: Predefined config., Args:
    trn_data: Training set.
    dev_data: Development set.
    save_dir: The directory to save trained component.
    transformer: An identifier of a pre-trained transformer.
    average_subwords: ``True`` to average subword representations.
    word_dropout: Dropout rate to randomly replace a subword with MASK.
    hidden_dropout: Dropout rate applied to hidden states.
    layer_dropout: Randomly zero out hidden states of a transformer layer.
    scalar_mix: Layer attention.
    grad_norm: Gradient norm for clipping.
    transformer_grad_norm: Gradient norm for clipping transformer gradient.
    lr: Learning rate for decoder.
    transformer_lr: Learning for encoder.
    transformer_layers: The number of bottom layers to use.
    gradient_accumulation: Number of batches per update.
    adam_epsilon: The epsilon to use in Adam.
    weight_decay: The weight decay to use.
    warmup_steps: The number of warmup steps.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    reduction: The loss reduction used in aggregating losses.
    batch_size: The number of samples in a batch.
    sampler_builder: The builder to build sampler, which will override batch_size.
    epochs: The number of epochs to train.
    patience: The number of patience epochs before early stopping.
    token_key: The key to tokens in dataset.
    tagging_scheme: Either ``BMES`` or ``BI``.
    delimiter: Delimiter between tokens used to split a line in the corpus.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    transform: An optional transform to be applied to samples. Usually a character normalization transform is
        passed in.
    devices: Devices this component will live on.
    logger: Any :class:`logging.Logger` instance.
    seed: Random seed to reproduce this training.
    **kwargs: Not used.

Returns:
    Best metrics on dev set., The high priority dictionary which perform longest-prefix-matching on inputs to split them into two subsets:

1. spans containing no keywords, which are then fed into tokenizer for further tokenization.
2. keywords, which will be outputed without furthur tokenization.

.. Caution::
    Longest-prefix-matching **NEVER** guarantee the presence of any keywords. Abuse of
    ``dict_force`` can lead to low quality results. For more details, refer to
    `this book <http://nlp.hankcs.com/book.php>`_.

Examples:
    >>> tok.dict_force = {'和服', '服务行业'} # Force '和服' and '服务行业' by longest-prefix-matching
    >>> tok(""商品和服务行业"")
        ['商品', '和服', '务行业']
    >>> tok.dict_force = {'和服务': ['和', '服务']} # Force '和服务' to be tokenized as ['和', '服务']
    >>> tok(""商品和服务行业"")
        ['商品', '和', '服务', '行业'], The low priority dictionary which perform longest-prefix-matching on model predictions and combing them.

Examples:
    >>> tok.dict_combine = {'和服', '服务行业'}
    >>> tok(""商品和服务行业"") # '和服' is not in the original results ['商品', '和', '服务']. '服务', '行业' are combined to '服务行业'
        ['商品', '和', '服务行业']",", 
, 

        Args:
            trn_data: Training set.
            dev_data: Development set.
            save_dir: The directory to save trained component.
            transformer: An identifier of a pre-trained transformer.
            average_subwords: ``True`` to average subword representations.
            word_dropout: Dropout rate to randomly replace a subword with MASK.
            hidden_dropout: Dropout rate applied to hidden states.
            layer_dropout: Randomly zero out hidden states of a transformer layer.
            scalar_mix: Layer attention.
            grad_norm: Gradient norm for clipping.
            transformer_grad_norm: Gradient norm for clipping transformer gradient.
            lr: Learning rate for decoder.
            transformer_lr: Learning for encoder.
            transformer_layers: The number of bottom layers to use.
            gradient_accumulation: Number of batches per update.
            adam_epsilon: The epsilon to use in Adam.
            weight_decay: The weight decay to use.
            warmup_steps: The number of warmup steps.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            reduction: The loss reduction used in aggregating losses.
            batch_size: The number of samples in a batch.
            sampler_builder: The builder to build sampler, which will override batch_size.
            epochs: The number of epochs to train.
            patience: The number of patience epochs before early stopping.
            token_key: The key to tokens in dataset.
            tagging_scheme: Either ``BMES`` or ``BI``.
            delimiter: Delimiter between tokens used to split a line in the corpus.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            transform: An optional transform to be applied to samples. Usually a character normalization transform is
                passed in.
            devices: Devices this component will live on.
            logger: Any :class:`logging.Logger` instance.
            seed: Random seed to reproduce this training.
            **kwargs: Not used.

        Returns:
            Best metrics on dev set.
        ,  ,  A tokenizer using transformer tagger for span prediction. It features with 2 high performance dictionaries
        to handle edge cases in real application.

        - ``dict_force``: High priority dictionary performs longest-prefix-matching on input text which takes higher
          priority over model predictions.
        - ``dict_combine``: Low priority dictionary performs longest-prefix-matching on model predictions then
          combines them.

        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

        It also supports outputting the span of each token by setting ``config.output_spans = True``.

        Args:
            **kwargs: Predefined config.
        ,  The high priority dictionary which perform longest-prefix-matching on inputs to split them into two subsets:

        1. spans containing no keywords, which are then fed into tokenizer for further tokenization.
        2. keywords, which will be outputed without furthur tokenization.

        .. Caution::
            Longest-prefix-matching **NEVER** guarantee the presence of any keywords. Abuse of
            ``dict_force`` can lead to low quality results. For more details, refer to
            `this book <http://nlp.hankcs.com/book.php>`_.

        Examples:
            >>> tok.dict_force = {'和服', '服务行业'} # Force '和服' and '服务行业' by longest-prefix-matching
            >>> tok(""商品和服务行业"")
                ['商品', '和服', '务行业']
            >>> tok.dict_force = {'和服务': ['和', '服务']} # Force '和服务' to be tokenized as ['和', '服务']
            >>> tok(""商品和服务行业"")
                ['商品', '和', '服务', '行业']
        ,  The low priority dictionary which perform longest-prefix-matching on model predictions and combing them.

        Examples:
            >>> tok.dict_combine = {'和服', '服务行业'}
            >>> tok(""商品和服务行业"") # '和服' is not in the original results ['商品', '和', '服务']. '服务', '行业' are combined to '服务行业'
                ['商品', '和', '服务行业']

        , .tsv, .txt, B, BMES, E, I, M, S, custom_words, dict_combine, dict_force, output_spans, sum, tag, token, token_, token_subtoken_offsets, token_subtoken_offsets_group","0, 0.1, 0.2, 1, 1e-08, 30, 32, 5, 5.0, 5e-05, False, None, True","
return self.config.get('dict_combine', None)

@dict_combine.setter
def dict_combine(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):
if dictionary is not None and not isinstance(dictionary, DictInterface):
if all(isinstance(k, str) for k in dictionary):
dictionary = TrieDict(dictionary)
else:
_d = set()
for k in dictionary:
if isinstance(k, str):
_d.update(possible_tokenization(k))
else:
_d.add(k)
dictionary = TupleTrieDict(_d)
self.config.dict_combine = dictionary

def build_metric(self, **kwargs):
return F1()

# noinspection PyMethodOverriding
def update_metrics(self, metric, logits, y, mask, batch, prediction):
for p, g in zip(prediction, self.tag_to_span(batch['tag'], batch)):
pred = set(p)
gold = set(g)
metric(pred, gold)

def decode_output(self, logits, mask, batch, model=None):
output = super().decode_output(logits, mask, batch, model)
if isinstance(output, torch.Tensor):
output = output.tolist()
prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
return self.tag_to_span(prediction, batch)

def tag_to_span(self, batch_tags, batch: dict):
spans = []
if 'custom_words' in batch:
if self.config.tagging_scheme == 'BMES':
S = 'S'
M = 'M'
E = 'E'
else:
S = 'B'
M = 'I'
E = 'I'
for tags, custom_words in zip(batch_tags, batch['custom_words']):
# [batch['raw_token'][0][x[0]:x[1]] for x in subwords]
if custom_words:
for start, end, label in custom_words:
if end - start == 1:
tags[start] = S
else:
tags[start] = 'B'
tags[end - 1] = E
for i in range(start + 1, end - 1):
tags[i] = M
if end < len(tags):
tags[end] = 'B'
if 'token_subtoken_offsets_group' not in batch:  # only check prediction on raw text for now
# Check cases that a single char gets split into multiple subtokens, e.g., ‥ -> . + .
for tags, subtoken_offsets in zip(batch_tags, batch['token_subtoken_offsets']):
offset = -1  # BERT produces 'ᄒ', '##ᅡ', '##ᆫ' for '한' and they share the same span
prev_tag = None
for i, (tag, (b, e)) in enumerate(zip(tags, subtoken_offsets)):
if b < offset:
if prev_tag == 'S':
tags[i - 1] = 'B'
elif prev_tag == 'E':
tags[i - 1] = 'M'
tags[i] = tag = 'M'
offset = e
prev_tag = tag
for tags in batch_tags:
spans.append(bmes_to_spans(tags))
return spans

def write_prediction(self, prediction, batch, output: TextIO):
batch_tokens = self.spans_to_tokens(prediction, batch)
for tokens in batch_tokens:
output.write(' '.join(tokens))
output.write('\n')

@property
def tokenizer_transform(self):
if not self._tokenizer_transform:
self._tokenizer_transform = TransformerSequenceTokenizer(self.transformer_tokenizer,
self.config.token_key,
ret_subtokens=True,
ret_subtokens_group=True,
ret_token_span=False,
dict_force=self.dict_force)
return self._tokenizer_transform

def spans_to_tokens(self, spans, batch, rebuild_span=False):
batch_tokens = []
dict_combine = self.dict_combine
raw_text = batch.get('token_', None)  # Use raw text to rebuild the token according to its offset
for b, (spans_per_sent, sub_tokens) in enumerate(zip(spans, batch[self.config.token_key])):
if raw_text:  # This will restore iPhone X as a whole
text = raw_text[b]
offsets = batch['token_subtoken_offsets'][b]
tokens = [text[offsets[b][0]:offsets[e - 1][-1]] for b, e in spans_per_sent]
else:  # This will merge iPhone X into iPhoneX
tokens = [''.join(sub_tokens[span[0]:span[1]]) for span in spans_per_sent]
if dict_combine:
buffer = []
offset = 0
delta = 0
for start, end, label in dict_combine.tokenize(tokens):
if offset < start:
buffer.extend(tokens[offset:start])
if raw_text:
# noinspection PyUnboundLocalVariable
combined = text[offsets[spans_per_sent[start - delta][0]][0]:
offsets[spans_per_sent[end - delta - 1][1] - 1][1]]
else:
combined = ''.join(tokens[start:end])
buffer.append(combined)
offset = end
if rebuild_span:
start -= delta
end -= delta
combined_span = (spans_per_sent[start][0], spans_per_sent[end - 1][1])
del spans_per_sent[start:end]
delta += end - start - 1
spans_per_sent.insert(start, combined_span)
if offset < len(tokens):
buffer.extend(tokens[offset:])
tokens = buffer
batch_tokens.append(tokens)
return batch_tokens

def generate_prediction_filename(self, tst_data, save_dir):
return super().generate_prediction_filename(tst_data.replace('.tsv', '.txt'), save_dir)

def prediction_to_human(self, pred, vocab, batch, rebuild_span=False):
output_spans = self.config.get('output_spans', None)
tokens = self.spans_to_tokens(pred, batch, rebuild_span or output_spans)
if output_spans:
subtoken_spans = batch['token_subtoken_offsets']
results = []
for toks, offs, subs in zip(tokens, pred, subtoken_spans):
r = []
results.append(r)
for t, (b, e) in zip(toks, offs):
r.append([t, subs[b][0], subs[e - 1][-1]])
return results
return tokens

def input_is_flat(self, tokens):
return isinstance(tokens, str)

def build_dataset(self, data, **kwargs):
return TextTokenizingDataset(data, **kwargs)

def last_transform(self):
return TransformList(functools.partial(generate_tags_for_subtokens, tagging_scheme=self.config.tagging_scheme),
super().last_transform())

def fit(self, trn_data, dev_data, save_dir, transformer, average_subwords=False, word_dropout: float = 0.2,
hidden_dropout=None, layer_dropout=0, scalar_mix=None, grad_norm=5.0,
transformer_grad_norm=None, lr=5e-5, eval_trn=True,
transformer_lr=None, transformer_layers=None, gradient_accumulation=1,
adam_epsilon=1e-8, weight_decay=0, warmup_steps=0.1, crf=False, reduction='sum',
batch_size=32, sampler_builder: SamplerBuilder = None, epochs=30, patience=5, token_key=None,
tagging_scheme='BMES', delimiter=None,
max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False, transform=None, logger=None,
devices: Union[float, int, List[int]] = None, **kwargs):
, 
return self.config.get('dict_force', None)

@dict_force.setter
def dict_force(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):
if dictionary is not None and not isinstance(dictionary, DictInterface):
dictionary = TrieDict(dictionary)
self.config.dict_force = dictionary
self.tokenizer_transform.dict = dictionary

@property
def dict_combine(self) -> DictInterface:
 The low priority dictionary which perform longest-prefix-matching on model predictions and combing them.,  A tokenizer using transformer tagger for span prediction. It features with 2 high performance dictionaries
to handle edge cases in real application.

- ``dict_force``: High priority dictionary performs longest-prefix-matching on input text which takes higher
priority over model predictions.
- ``dict_combine``: Low priority dictionary performs longest-prefix-matching on model predictions then
combines them.

.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

It also supports outputting the span of each token by setting ``config.output_spans = True``.

Args:
**kwargs: Predefined config.
, # '和服' is not in the original results ['商品', '和', '服务']. '服务', '行业' are combined to '服务行业', # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-11 02:48, # Force '和服' and '服务行业' by longest-prefix-matching, # Force '和服务' to be tokenized as ['和', '服务']","
return self.config.get('dict_combine', None)

@dict_combine.setter
def dict_combine(self, dictionary: Union[DictInterface, Union[Dict[str, Any], Set[str]]]):
if dictionary is not None and not isinstance(dictionary, DictInterface):
if all(isinstance(k, str) for k in dictionary):
dictionary = TrieDict(dictionary)
else:
_d = set()
for k in dictionary:
if isinstance(k, str):
_d.update(possible_tokenization(k))
else:
_d.add(k)
dictionary = TupleTrieDict(_d)
self.config.dict_combine = dictionary

def build_metric(self, **kwargs):
return F1()

# noinspection PyMethodOverriding
def update_metrics(self, metric, logits, y, mask, batch, prediction):
for p, g in zip(prediction, self.tag_to_span(batch['tag'], batch)):
pred = set(p)
gold = set(g)
metric(pred, gold)

def decode_output(self, logits, mask, batch, model=None):
output = super().decode_output(logits, mask, batch, model)
if isinstance(output, torch.Tensor):
output = output.tolist()
prediction = self.id_to_tags(output, [len(x) for x in batch['token']])
return self.tag_to_span(prediction, batch)

def tag_to_span(self, batch_tags, batch: dict):
spans = []
if 'custom_words' in batch:
if self.config.tagging_scheme == 'BMES':
S = 'S'
M = 'M'
E = 'E'
else:
S = 'B'
M = 'I'
E = 'I'
for tags, custom_words in zip(batch_tags, batch['custom_words']):
# [batch['raw_token'][0][x[0]:x[1]] for x in subwords]
if custom_words:
for start, end, label in custom_words:
if end - start == 1:
tags[start] = S
else:
tags[start] = 'B'
tags[end - 1] = E
for i in range(start + 1, end - 1):
tags[i] = M
if end < len(tags):
tags[end] = 'B'
if 'token_subtoken_offsets_group' not in batch:  # only check prediction on raw text for now
# Check cases that a single char gets split into multiple subtokens, e.g., ‥ -> . + .
for tags, subtoken_offsets in zip(batch_tags, batch['token_subtoken_offsets']):
offset = -1  # BERT produces 'ᄒ', '##ᅡ', '##ᆫ' for '한' and they share the same span
prev_tag = None
for i, (tag, (b, e)) in enumerate(zip(tags, subtoken_offsets)):
if b < offset:
if prev_tag == 'S':
tags[i - 1] = 'B'
elif prev_tag == 'E':
tags[i - 1] = 'M'
tags[i] = tag = 'M'
offset = e
prev_tag = tag
for tags in batch_tags:
spans.append(bmes_to_spans(tags))
return spans

def write_prediction(self, prediction, batch, output: TextIO):
batch_tokens = self.spans_to_tokens(prediction, batch)
for tokens in batch_tokens:
output.write(' '.join(tokens))
output.write('\n')

@property
def tokenizer_transform(self):
if not self._tokenizer_transform:
self._tokenizer_transform = TransformerSequenceTokenizer(self.transformer_tokenizer,
self.config.token_key,
ret_subtokens=True,
ret_subtokens_group=True,
ret_token_span=False,
dict_force=self.dict_force)
return self._tokenizer_transform

def spans_to_tokens(self, spans, batch, rebuild_span=False):
batch_tokens = []
dict_combine = self.dict_combine
raw_text = batch.get('token_', None)  # Use raw text to rebuild the token according to its offset
for b, (spans_per_sent, sub_tokens) in enumerate(zip(spans, batch[self.config.token_key])):
if raw_text:  # This will restore iPhone X as a whole
text = raw_text[b]
offsets = batch['token_subtoken_offsets'][b]
tokens = [text[offsets[b][0]:offsets[e - 1][-1]] for b, e in spans_per_sent]
else:  # This will merge iPhone X into iPhoneX
tokens = [''.join(sub_tokens[span[0]:span[1]]) for span in spans_per_sent]
if dict_combine:
buffer = []
offset = 0
delta = 0
for start, end, label in dict_combine.tokenize(tokens):
if offset < start:
buffer.extend(tokens[offset:start])
if raw_text:
# noinspection PyUnboundLocalVariable
combined = text[offsets[spans_per_sent[start - delta][0]][0]:
offsets[spans_per_sent[end - delta - 1][1] - 1][1]]
else:
combined = ''.join(tokens[start:end])
buffer.append(combined)
offset = end
if rebuild_span:
start -= delta
end -= delta
combined_span = (spans_per_sent[start][0], spans_per_sent[end - 1][1])
del spans_per_sent[start:end]
delta += end - start - 1
spans_per_sent.insert(start, combined_span)
if offset < len(tokens):
buffer.extend(tokens[offset:])
tokens = buffer
batch_tokens.append(tokens)
return batch_tokens

def generate_prediction_filename(self, tst_data, save_dir):
return super().generate_prediction_filename(tst_data.replace('.tsv', '.txt'), save_dir)

def prediction_to_human(self, pred, vocab, batch, rebuild_span=False):
output_spans = self.config.get('output_spans', None)
tokens = self.spans_to_tokens(pred, batch, rebuild_span or output_spans)
if output_spans:
subtoken_spans = batch['token_subtoken_offsets']
results = []
for toks, offs, subs in zip(tokens, pred, subtoken_spans):
r = []
results.append(r)
for t, (b, e) in zip(toks, offs):
r.append([t, subs[b][0], subs[e - 1][-1]])
return results
return tokens

def input_is_flat(self, tokens):
return isinstance(tokens, str)

def build_dataset(self, data, **kwargs):
return TextTokenizingDataset(data, **kwargs)

def last_transform(self):
return TransformList(functools.partial(generate_tags_for_subtokens, tagging_scheme=self.config.tagging_scheme),
super().last_transform())

def fit(self, trn_data, dev_data, save_dir, transformer, average_subwords=False, word_dropout: float = 0.2,
hidden_dropout=None, layer_dropout=0, scalar_mix=None, grad_norm=5.0,
transformer_grad_norm=None, lr=5e-5, eval_trn=True,
transformer_lr=None, transformer_layers=None, gradient_accumulation=1,
adam_epsilon=1e-8, weight_decay=0, warmup_steps=0.1, crf=False, reduction='sum',
batch_size=32, sampler_builder: SamplerBuilder = None, epochs=30, patience=5, token_key=None,
tagging_scheme='BMES', delimiter=None,
max_seq_len=None, sent_delimiter=None, char_level=False, hard_constraint=False, transform=None, logger=None,
devices: Union[float, int, List[int]] = None, **kwargs):
, '务行业'], '和', '和服', '和服' is not in the original results ['商品', '和', '服务']. '服务', '行业' are combined to '服务行业', '和服务', '服务', '服务'], '服务行业', '服务行业'], '行业', '行业'], Force '和服' and '服务行业' by longest-prefix-matching, Force '和服务' to be tokenized as ['和', '服务'], ['和', ['商品', 商品和服务行业"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-11 02:48",
https://github.com/hankcs/HanLP,ngram_conv_tagger.py,0,0.0,90,75.63,11,9.24,12,10.08,6,5.04,0,0.0,11,3,38,0,,"NgramConvTaggerTF, NgramConvTaggingModel, NgramTransform, X, X_to_inputs, Y, __init__, _keras_mask, append, build_model, call, chars, config, conv, conv_norm, create_conv1d, create_types_shapes_values, data, defaults, dense, dict, dropout_embed, dropout_hidden, embed_input, embeds, enumerate, extend, file_to_samples, filter, filters, filters_v, filters_w, first_token, fit, fv, fw, get, gold, hanlp, hanlp_common, hasattr, hidden_output, ids, idx, input, input_truth_output_to_str, inputs, inputs_to_samples, int, isinstance, kernel_size, layers, len, logits, lookup, map_word_feature, map_x, map_y, mask, mask_float, name, ngram_embed, ngram_size, ngram_vocab, ngrams, num_samples, num_tags, output, pad_token, safe_pad_token, self, shapes, super, tag_vocab, tags, tensorflow, transform, trn_path, tuple, types, typing, update, vec_dim, vocabs, window_size, word_embed, word_vocab, x_to_idx, y_to_idx, zip","hanlp.common.vocab_tf.VocabTF, hanlp.components.taggers.tagger_tf.TaggerComponent, hanlp.layers.embeddings.util_tf.build_embedding, hanlp.layers.weight_normalization.WeightNormalization, hanlp.transform.tsv_tf.TSVTaggingTransform, hanlp.transform.txt_tf.bmes_to_words, hanlp.transform.txt_tf.extract_ngram_features, hanlp_common.structure.SerializableDict, hanlp_common.util.merge_locals_kwargs, tf.Tensor, tf.concat, tf.dtypes.cast, tf.expand_dims, tf.float32, tf.keras.Model, tf.keras.layers.Conv1D, tf.keras.layers.Dense, tf.keras.layers.Dropout, tf.keras.layers.Embedding, tf.keras.losses.Loss, tf.keras.models.Model, tf.keras.optimizers.Optimizer, tf.nn.sigmoid, tf.string, typing.Any, typing.Iterable, typing.List, typing.Optional, typing.Tuple, typing.Union","X_to_inputs, __init__, build_model, call, create_conv1d, create_types_shapes_values, fit, input_truth_output_to_str, inputs_to_samples, x_to_idx, y_to_idx","NgramConvTaggerTF, NgramConvTaggingModel, NgramTransform","X, Y, chars, conv, conv_norm, data, defaults, embed_input, embeds, features, filter, fv, fw, hidden_output, ids, idx, logits, mask, mask_float, model, ngram_embed, ngram_size, ngram_vocab, ngrams, num_samples, shapes, tag_vocab, tags, transform, types, v, vec_dim, vocabs, w, window_size, word_embed, word_vocab, words",," , Conv1Dv_{}, Conv1Dw_{}, NgramConvTaggingModel can only run eagerly, _norm, accuracy, adam, map_x, ngram_embed, run_eagerly, same","0, 0.2, 1, 100, 2, 200, 3, 4, 50, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-10-25 00:04, # Mask paddings., # dirty hack, # self.concat = tf.keras.layers.Concatenate(axis=2)",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 22:18",
https://github.com/hankcs/HanLP,rnntaggingmodel.py,0,0.0,31,47.69,0,0.0,7,10.77,27,41.54,0,0.0,3,1,5,0,,"RNNTaggingModel, __init__, batch, crf, crf_constraints, dim, drop, embed, embed_to_rnn, embedding_dim, forward, gt, hanlp, int, isinstance, lens, mask, n_embed, n_out, out, reset_parameters, rnn_hidden, rnn_input, self, sum, super, tolist, torch, typing, weight, word_lstm","hanlp.layers.crf.crf.CRF, nn.Dropout, nn.Embedding, nn.LSTM, nn.Linear, nn.Module, nn.init.xavier_uniform_, torch.Tensor, torch.nn.utils.rnn.pack_sequence, torch.nn.utils.rnn.pad_packed_sequence, torch.split, typing.Union","__init__, forward, reset_parameters",RNNTaggingModel,"lens, mask, n_embed, rnn_input, x",,,"0, 0.5, 1, 2, 3, None, True","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # get outputs from embedding layers, # get the mask and lengths of given batch, # in the Software without restriction, including without limitation the rights, # init Linear, # of this software and associated documentation files (the ""Software""), to deal, # self.drop = LockedDropout(drop), # self.drop = SharedDropout(drop), # the CRF layer, # the embedding layer, # the output layer, # the word-lstm layer, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-19 15:41",
https://github.com/hankcs/HanLP,metrics_tf.py,0,0.0,12,66.67,1,5.56,2,11.11,3,16.67,0,0.0,2,1,1,0,,"Accuracy, __init__, dtype, mask_value, name, sample_weight, self, super, tensorflow, update_state, y_pred, y_true","tf.keras.metrics.SparseCategoricalAccuracy, tf.not_equal","__init__, update_state",Accuracy,sample_weight,,sparse_categorical_accuracy,"0, None","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-30 16:33",
https://github.com/hankcs/HanLP,transformer_tagger.py,0,0.0,133,72.28,27,14.67,14,7.61,10,5.43,0,0.0,14,2,39,2,,"Linear, Module, TransformerTagger, TransformerTaggingModel, __init__, _lambda, _step, _tokenizer_transform, append_transform, backward, batch, batch_size, bias, build, build_dataloader, build_dataset, build_model, build_transformer, build_vocabs, classifier, compute_distill_loss, compute_loss, config, crf, criterion, data, decode_output, device, distill, each, encoder, enumerate, eval_trn, extra_embeddings, feature_size, features, feed_batch, finetune, fit, fit_dataloader, float, forward, get, get_output_dim, gradient_accumulation, hanlp, hanlp_common, hidden_size, history, idx, info, input_ids, int, isinstance, item, iter, kd_criterion, kd_loss, keys, lambda_scheduler, last_transform, lens, list, load_state, load_state_dict, lock, log, logger, logging, logits_S, logits_T, loss, max, max_seq_len, metric, missing_params, model, model_state, module, mutable, next, num_labels, num_training_steps, optimizer, out, out_S, out_T, output_key, prediction, property, prune, purge_cache, ratio_width, report, safe_state, sampler, sampler_builder, scheduler, secondary_encoder, self, set_unk_as_safe_unk, shuffle, size, sorted, state_dict, step, summary, super, tag, teacher, temperature, temperature_scheduler, timer, token_key, token_span, token_type_ids, tokenizer_transform, torch, total_loss, train, training, transform, transformer, transformer_grad_norm, transformer_tokenizer, transforms, trn, typing, update_metrics, vocabs, weight, x, zero_grad","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.dataset.TransformableDataset, hanlp.common.structure.History, hanlp.common.transform.FieldLength, hanlp.common.transform.TransformList, hanlp.common.vocab.Vocab, hanlp.components.classifiers.transformer_classifier.TransformerComponent, hanlp.components.taggers.tagger.Tagger, hanlp.datasets.ner.loaders.tsv.TSVTaggingDataset, hanlp.layers.crf.crf.CRF, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.embeddings.embedding.EmbeddingDim, hanlp.layers.transformers.encoder.TransformerEncoder, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.clip_grad_norm, hanlp.utils.torch_util.filter_state_dict_safely, hanlp.utils.torch_util.lengths_to_mask, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.LongTensor, torch.cat, torch.nn, torch.nn.Module, torch.no_grad, torch.utils.data.DataLoader, typing.List, typing.Union","__init__, _step, build_dataloader, build_dataset, build_model, build_vocabs, compute_distill_loss, distill, feed_batch, fit, fit_dataloader, forward, last_transform, tokenizer_transform","TransformerTagger, TransformerTaggingModel","_lambda, args, batch, dataset, each, embed, feature_size, features, idx, input_ids, kd_loss, lambda_scheduler, lens, load_state, logits_S, logits_T, loss, mask, max_seq_len, missing_params, model, model_state, n, optimizer, out, out_T, prediction, report, safe_state, sampler, scheduler, temperature, timer, token_key, token_span, total_loss, transforms, x, y","A shallow tagging model use transformer as decoder.
Args:
    encoder: A pretrained transformer.
    num_labels: Size of tagset.
    crf: True to enable CRF.
    extra_embeddings: Extra embeddings which will be concatenated to the encoder outputs., A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
any tagging tasks including PoS tagging and many others.

Args:
    **kwargs: Not used.",", 
        A shallow tagging model use transformer as decoder.
        Args:
            encoder: A pretrained transformer.
            num_labels: Size of tagset.
            crf: True to enable CRF.
            extra_embeddings: Extra embeddings which will be concatenated to the encoder outputs.
        ,  , ), , , ., .4f, A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
        any tagging tasks including PoS tagging and many others.

        Args:
            **kwargs: Not used.
        , Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: , Guess [bold][blue]token_key=, The following parameters were missing from the checkpoint: , [/blue], [/blue][/bold] according to the training dataset: [blue], _input_ids, _length, _token_type_ids, char_level, delimiter, flsw, hard_constraint, loss: , max_seq_len, secondary_encoder, sent_delimiter, sum, tag, tag_id","0, 0.1, 0.2, 1, 1e-06, 2, 3, 32, 5, 5.0, 5e-05, False, None, True","
A shallow tagging model use transformer as decoder.
Args:
encoder: A pretrained transformer.
num_labels: Size of tagset.
crf: True to enable CRF.
extra_embeddings: Extra embeddings which will be concatenated to the encoder outputs.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-06-15 20:55, # noinspection PyAbstractClass, # noinspection PyCallingNonCallable, # noinspection PyMethodOverriding, # noinspection PyNoneFunctionAssignment, # noinspection PyUnresolvedReferences, A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
any tagging tasks including PoS tagging and many others.

Args:
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,transformer_tagger_tf.py,0,0.0,48,70.59,2,2.94,10,14.71,8,11.76,0,0.0,9,2,7,0,,"TransformerTaggerTF, TransformerTaggingModel, __init__, batch_size, build_loss, build_model, build_optimizer, build_vocab, call, callbacks, clipnorm, config, dev_data, dev_steps, epochs, epsilon, fit, hanlp, hanlp_common, history, inputs, learning_rate, len, load_transform, logger, mask, math, max_seq_length, model, save_dir, self, super, tag_vocab, tensorflow, tokenizer, train_examples, train_loop, train_steps, train_steps_per_epoch, training, transform, transformer, trn_data, use_amp, warmup_steps, warmup_steps_per_epoch, warmup_steps_ratio, weight_decay_rate","hanlp.common.transform_tf.Transform, hanlp.components.taggers.tagger_tf.TaggerComponent, hanlp.components.taggers.transformers.transformer_transform_tf.TransformerTransform, hanlp.layers.transformers.loader_tf.build_transformer, hanlp.layers.transformers.utils_tf.build_adamw_optimizer, hanlp.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropyOverBatchFirstDim, hanlp_common.util.merge_locals_kwargs, math.ceil, tf.keras.Model","__init__, build_loss, build_model, build_optimizer, build_vocab, call, fit, load_transform, train_loop","TransformerTaggerTF, TransformerTaggingModel","history, model, opt, tokenizer, train_examples, transform, warmup_steps_per_epoch",,"accuracy, adamw","0, 1.0, 128, 1e-08, 3, 32, 5e-05, False, None, True","#     (i, 0 if i == 0 else 1) for i in range(len(self.transform.tag_vocab))), # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 13:55, # class_weight=dict(, # mask out padding labels, # noinspection PyMethodOverriding, # type:tf.keras.callbacks.History",
https://github.com/hankcs/HanLP,transformer_transform_tf.py,0,0.0,73,71.57,12,11.76,8,7.84,9,8.82,0,0.0,11,1,26,0,,"NotImplementedError, TransformerTransform, Y, Y_to_outputs, __init__, _tokenizer, _vocab, batch, bool, cls_token, config, convert_tokens_to_ids, create_types_shapes_values, file_to_inputs, fit, float, get, gold, hanlp, hanlp_common, hasattr, idx_to_token, input, input_ids, input_is_single_sample, input_mask, inputs, inputs_to_samples, isinstance, kwargs, label_ids, label_mask, len, lock_vocabs, map_x, map_y, max_seq_length, num_samples, offset, pad, pad_idx, pad_label_idx, pad_token, print, property, roberta, sample, segment_ids, self, sep_token, setter, shapes, special_token_ids, super, tag_vocab, tags, tensorflow, tid, token, token_to_idx, tokenizer, trn_path, types, typing, unk, unk_token, update, values, vocab, words, x_to_idx, xlnet, y_to_idx","hanlp.common.transform_tf.Transform, hanlp.common.vocab_tf.VocabTF, hanlp.layers.transformers.utils_tf.convert_examples_to_features, hanlp.transform.tsv_tf.TsvTaggingFormat, hanlp_common.structure.SerializableDict, tf.Tensor, tf.argmax, tf.constant, tf.int32, typing.Iterable, typing.List, typing.Tuple, typing.Union","Y_to_outputs, __init__, create_types_shapes_values, fit, input_is_single_sample, inputs_to_samples, lock_vocabs, max_seq_length, tokenizer, x_to_idx, y_to_idx",TransformerTransform,"Y, cls_token, input_ids, input_mask, label_ids, label_mask, max_seq_length, num_samples, offset, pad_label_idx, pad_token, roberta, sample, segment_ids, sep_token, shapes, tags, tid, token, tokenizer, types, unk_token, values, vocab, words, xlnet",,"-inf, <pad>, <unk>, Need the batch to know actual length of Y, [CLS], [PAD], [SEP], [UNK], _vocab, max_seq_length, transformers has its own tagger, not need to convert idx for x, transformers has its own tagger, not need to convert idx for y","0, 1, 128, 2, 4, False, None, True","# (input_ids, input_mask, segment_ids), label_ids, # -*- coding:utf-8 -*-, # -2 for special tokens [CLS] and [SEP], # Author: hankcs, # Date: 2019-12-29 15:14, # English albert use <pad> instead of [PAD], # pad on the left for xlnet, # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805, # xlnet has a cls token at the end",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-29 13:57",
https://github.com/hankcs/HanLP,baffine_tagging.py,0,0.0,35,70.0,0,0.0,9,18.0,6,12.0,0,0.0,2,2,6,0,,"BiaffineTaggingDecoder, Module, SpanBIOSemanticRoleLabelingModel, __init__, batch, bias, crf, decoder, embed, encoder, forward, get_output_dim, hanlp, hidden_size, init, int, mask, math, mlp_dropout, mlp_rel_d, mlp_rel_h, n_mlp_rel, n_rels, num_labels, rel_attn, rel_d, rel_h, s_rel, self, size, super, torch, uniform_, weight, x","hanlp.components.parsers.biaffine.biaffine.Biaffine, hanlp.components.parsers.biaffine.mlp.MLP, hanlp.layers.crf.crf.CRF, math.sqrt, torch.Tensor, torch.nn","__init__, forward","BiaffineTaggingDecoder, SpanBIOSemanticRoleLabelingModel","bias, hidden_size, rel_d, rel_h, s_rel, x",,,"0, 0.2, 1, 2, 3, 300, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-04 13:59, # [batch_size, seq_len, seq_len, n_rels], # get arc and rel scores from the bilinear attention, # noinspection PyUnusedLocal",
https://github.com/hankcs/HanLP,span_bio.py,0,0.0,148,73.63,28,13.93,15,7.46,10,4.98,0,0.0,23,1,49,1,,"CrossEntropyLoss, SpanBIOSemanticRoleLabeler, __init__, _step, adam_epsilon, append, argmax, arguments, backward, batch, batch_size, best_epoch, best_metric, bisect, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, compute_loss, compute_mask, config, copy, crf, criterion, dataloader, decode, decode_output, decoder, delimiter, dev, dev_metric, device, dict, elapsed_average_human, elapsed_human, embed, encoder, end, enumerate, epoch, epochs, eta_human, eval, eval_trn, evaluate_dataloader, execute_training_loop, extend, feed_batch, fit, fit_dataloader, flat, flatten, float, forward, get, gradient_accumulation, hanlp, hanlp_common, history, idx, info, input_is_flat, insert, int, isinstance, item, join, lens, load_weights, lock, log, logger, logging, loss, loss_reduction, lr, mask2d, mask3d, matrix, max_seq_len, mlp_dropout, model, module, mutable, n_mlp_rel, naive_decode, num_training_steps, offset, pas, patience, pred, predict, prediction, prediction_to_result, range, ratio_width, report, reset, result, results, sample, sampler, sampler_builder, save_dir, save_weights, self, sent, shuffle, size, srl, srl_per_token, start, step, sum, summary, super, tags_per_token, timer, token, token_index, tokens, tolist, torch, total, total_loss, total_time_human, train, training, transform, transformer_lr, transforms, transpose, trn, typing, unsqueeze_, update, update_metrics, vocabs, warmup_steps, weight_decay, zero_grad, zip","F.log_softmax, bisect.bisect, copy.copy, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.dataset.TransformableDataset, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.vocab.Vocab, hanlp.components.srl.span_bio.baffine_tagging.SpanBIOSemanticRoleLabelingModel, hanlp.datasets.srl.loaders.conll2012.CoNLL2012SRLBIODataset, hanlp.layers.crf.crf.CRF, hanlp.layers.embeddings.contextual_word_embedding.find_transformer, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.metrics.chunking.sequence_labeling.get_entities, hanlp.metrics.f1.F1, hanlp.utils.string_util.guess_delimiter, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.clip_grad_norm, hanlp.utils.torch_util.lengths_to_mask, hanlp_common.constant.IDX, hanlp_common.constant.PRED, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, torch.nn, torch.nn.DataParallel, torch.nn.Module, torch.no_grad, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.List, typing.Union","__init__, _step, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, compute_loss, compute_mask, decode_output, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, input_is_flat, naive_decode, predict, prediction_to_result, update_metrics",SpanBIOSemanticRoleLabeler,"arguments, batch, best_epoch, best_metric, criterion, data, dataloader, dataset, decoder, delimiter, dev_metric, end, epoch, flat, history, idx, lens, loss, mask2d, mask3d, matrix, max_seq_len, model, num_training_steps, offset, order, pas, patience, pred, prediction, report, result, results, sample, sampler, sent, srl, srl_per_token, start, tags_per_token, timer, token, token_index, tokens, total_loss, transformer, transformer_lr, transforms, vocab","A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

Args:
    **kwargs: Predefined config."," ,  (,  / ,  ETA: ,  [red](saved)[/red],  at epoch ,  early stop,  elapsed, ""[/blue]. If not, specify `delimiter` in `fit()`, ), .4f, :[/yellow], A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
        predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

        Args:
            **kwargs: Predefined config.
        , Average time of each epoch is , Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: , Guess the delimiter between tokens could be [blue]"", Max score of dev is , [yellow]Epoch , delimiter, doc_level_offset, loss: , mean, srl, srl_id, srl_set, token, token_input_ids, token_length","0, 0.0001, 0.001, 0.1, 0.2, 0.5, 1, 1e-08, 2, 30, 300, 32, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # C-ARGM-FRQ appears only in test set, # Date: 2020-06-22 20:54, # assert results == naive, # naive = self.naive_decode(pred, mask, batch, decoder), # noinspection PyCallByClass, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-04 13:59",
https://github.com/hankcs/HanLP,highway_variational_lstm.py,0,0.0,79,85.87,1,1.09,6,6.52,6,6.52,0,0.0,8,2,30,1,,"HighwayBiLSTM, StackedHighwayBiLSTM, _, __init__, _forward_brnn, _forward_rnn, append, b_dropout, b_hidden_dropout, b_initial, b_layer_initial, b_states, batch_first, batch_size, bcells, bidirectional, blayer_c_n, blayer_h_n, blayer_output, c_n, c_next, cell, data, drop_masks, dropout_in, dropout_out, expand, f_dropout, f_hidden_dropout, f_initial, f_layer_initial, f_states, fcells, forward, h_n, h_next, hidden_drop, hidden_mask, hidden_size, highway_gates, hx, initial, initializer, initializer_1d, input, input_size, input_tensor, layer, layer_c_n, layer_h_n, layer_initial, layer_input_size, layer_output, len, list, lstm_project_layer, masks, max_time, new, num_directions, num_layers, output, outputs, range, reset_dropout_layer, reset_parameters, reset_state, reverse, reversed, self, size, staticmethod, super, time, torch, training, transpose, view, zip","F.dropout, init.xavier_uniform_, layer.DropoutLayer, layer.HighwayLSTMCell, layer.VariationalLSTMCell, layer.bias, layer.weight, nn.Linear, nn.Module, nn.ModuleList, nn.Parameter, nn.ParameterList, torch.Tensor, torch.autograd.Variable, torch.cat, torch.sigmoid, torch.stack, torch.unsqueeze","__init__, _forward_brnn, _forward_rnn, forward, initializer_1d, reset_dropout_layer, reset_parameters, reset_state","HighwayBiLSTM, StackedHighwayBiLSTM","_, b_layer_initial, b_states, batch_size, blayer_c_n, blayer_h_n, blayer_output, c_n, c_next, f_layer_initial, f_states, h_n, h_next, hidden_drop, hidden_mask, highway_gates, hx, initial, input, input_tensor, layer_c_n, layer_h_n, layer_initial, layer_input_size, layer_output, masks, max_time, output, outputs, time",A module that runs multiple steps of HighwayBiLSTM.,A module that runs multiple steps of HighwayBiLSTM.,"0, 1, 2, False, None, True","# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL, # Highway, # expand: -1 means not expand that dimension, # reset the dropout each batch forward, # transpose: return the transpose matrix, A module that runs multiple steps of HighwayBiLSTM.

def __init__(self, input_size, hidden_size, num_layers=1, batch_first=False, bidirectional=False, dropout_in=0,
dropout_out=0):
super(HighwayBiLSTM, self).__init__()
self.input_size = input_size
self.hidden_size = hidden_size
self.num_layers = num_layers
self.batch_first = batch_first
self.bidirectional = bidirectional
self.dropout_in = dropout_in
self.dropout_out = dropout_out
self.num_directions = 2 if bidirectional else 1

self.fcells, self.f_dropout, self.f_hidden_dropout = [], [], []
self.bcells, self.b_dropout, self.b_hidden_dropout = [], [], []
for layer in range(num_layers):
layer_input_size = input_size if layer == 0 else hidden_size
self.fcells.append(HighwayLSTMCell(input_size=layer_input_size, hidden_size=hidden_size))
self.f_dropout.append(DropoutLayer(hidden_size, self.dropout_out))
self.f_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))
if self.bidirectional:
self.bcells.append(HighwayLSTMCell(input_size=hidden_size, hidden_size=hidden_size))
self.b_dropout.append(DropoutLayer(hidden_size, self.dropout_out))
self.b_hidden_dropout.append(DropoutLayer(hidden_size, self.dropout_out))
self.fcells, self.bcells = nn.ModuleList(self.fcells), nn.ModuleList(self.bcells)
self.f_dropout, self.b_dropout = nn.ModuleList(self.f_dropout), nn.ModuleList(self.b_dropout)

def reset_dropout_layer(self, batch_size):
for layer in range(self.num_layers):
self.f_dropout[layer].reset_dropout_mask(batch_size)
if self.bidirectional:
self.b_dropout[layer].reset_dropout_mask(batch_size)

@staticmethod
def _forward_rnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):
max_time = input.size(0)
output = []
hx = initial
for time in range(max_time):
h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)
hx = (h_next, c_next)
output.append(h_next)
output = torch.stack(output, 0)
return output, hx

@staticmethod
def _forward_brnn(cell, gate, input, masks, initial, drop_masks=None, hidden_drop=None):
max_time = input.size(0)
output = []
hx = initial
for time in reversed(list(range(max_time))):
h_next, c_next = cell(input[time], mask=masks[time], hx=hx, dropout=drop_masks)
hx = (h_next, c_next)
output.append(h_next)
output.reverse()
output = torch.stack(output, 0)
return output, hx

def forward(self, input, masks, initial=None):
if self.batch_first:
input = input.transpose(0, 1)  # transpose: return the transpose matrix
masks = torch.unsqueeze(masks.transpose(0, 1), dim=2)
max_time, batch_size, _ = input.size()

self.reset_dropout_layer(batch_size)  # reset the dropout each batch forward

masks = masks.expand(-1, -1, self.hidden_size)  # expand: -1 means not expand that dimension
if initial is None:
initial = Variable(input.data.new(batch_size, self.hidden_size).zero_())
initial = (initial, initial)  # h0, c0

h_n, c_n = [], []
for layer in range(self.num_layers):
# hidden_mask, hidden_drop = None, None
hidden_mask, hidden_drop = self.f_dropout[layer], self.f_hidden_dropout[layer]
layer_output, (layer_h_n, layer_c_n) = HighwayBiLSTM._forward_rnn(cell=self.fcells[layer], \
gate=None, input=input, masks=masks,
initial=initial, \
drop_masks=hidden_mask,
hidden_drop=hidden_drop)
h_n.append(layer_h_n)
c_n.append(layer_c_n)
if self.bidirectional:
hidden_mask, hidden_drop = self.b_dropout[layer], self.b_hidden_dropout[layer]
blayer_output, (blayer_h_n, blayer_c_n) = HighwayBiLSTM._forward_brnn(cell=self.bcells[layer], \
gate=None, input=layer_output,
masks=masks, initial=initial, \
drop_masks=hidden_mask,
hidden_drop=hidden_drop)
h_n.append(blayer_h_n)
c_n.append(blayer_c_n)

input = blayer_output if self.bidirectional else layer_output

h_n, c_n = torch.stack(h_n, 0), torch.stack(c_n, 0)
if self.batch_first:
input = input.transpose(1, 0)  # transpose: return the transpose matrix
return input, (h_n, c_n)


class StackedHighwayBiLSTM(nn.Module):
A module that runs multiple steps of HighwayBiLSTM.",
https://github.com/hankcs/HanLP,inference_utils.py,0,0.0,108,55.1,24,12.24,14,7.14,50,25.51,0,0.0,7,0,64,2,,"_, _CORE_ARGS, _decode_non_overlapping_spans, _dp_decode_non_overlapping_spans, _update_state, append, arg_end, arg_ends, arg_id, arg_labels, arg_spans, arg_start, arg_starts, args_list, best_state, bool, config, core_state, decode_spans, delta, end, ends, enforce_srl_constraint, enumerate, f, flags, float, get, get_predicted_clusters, greedy_decode, i, int, items, j, k, keys, label, label_str, labels, labels_inv, len, list, max_len, mention, mention_to_predicted, new_args_list, new_spans, num_args, num_preds, num_roles, num_sentences, num_suppressed_args, numpy, pc, pointers, pred_id, pred_spans, pred_to_args, predicates, predict_dict, predict_dict_arg_ends_, predict_dict_arg_starts_, predict_dict_num_args_, predict_dict_num_preds_, predict_dict_predicates_, predict_dict_srl_scores_, predicted_antecedent, predicted_antecedents, predicted_cluster, predicted_clusters, predicted_index, predicted_spans, predictions, r0, r0_str, r_str, range, role, role_states, rs, rs0, rs1, score, scores, sentence_lengths, set, shape, sorted, span_ends, span_labels, span_scores, span_starts, spans, spans_list, srl_decode, srl_labels_inv, start, starts, states, t0, t1, t_states, top_span_ends, top_span_starts, tuple, u_constraint, use_gold_predicates, zip","np.argmax, np.int64, np.max, np.zeros","_decode_non_overlapping_spans, _dp_decode_non_overlapping_spans, _update_state, decode_spans, get_predicted_clusters, greedy_decode, srl_decode",,"_, _CORE_ARGS, arg_end, arg_ends, arg_labels, arg_spans, arg_start, arg_starts, args_list, best_state, core_state, end, f, flags, i, j, k, label, label_str, labels, max_len, mention, mention_to_predicted, new_args_list, new_spans, num_args, num_preds, num_roles, num_sentences, num_suppressed_args, pc, pointers, pred_id, pred_spans, pred_to_args, predicates, predict_dict_arg_ends_, predict_dict_arg_starts_, predict_dict_num_args_, predict_dict_num_preds_, predict_dict_predicates_, predict_dict_srl_scores_, predicted_antecedent, predicted_cluster, predicted_clusters, predicted_index, predicted_spans, predictions, r0, r0_str, r_str, role_states, rs, rs0, score, scores, span_labels, spans, spans_list, start, states, t, t0, t_states","Args:
  span_starts: [num_candidates,]
  span_scores: [num_candidates, num_labels]
  span_ends: 
  labels_inv: 

Returns:, Greedy decoding for SRL predicate-argument structures.

Args:
  predict_dict: Dictionary of name to numpy arrays.
  srl_labels_inv: SRL label id to string name.
  suppress_overlap: Whether to greedily suppress overlapping arguments for the same predicate.

Returns:","

    Args:
      span_starts: [num_candidates,]
      span_scores: [num_candidates, num_labels]
      span_ends: 
      labels_inv: 

    Returns:

    
    , A0, A1, A2, A3, A4, A5, AA, ARG0, ARG1, ARG2, ARG3, ARG4, ARG5, ARGA, Greedy decoding for SRL predicate-argument structures.

    Args:
      predict_dict: Dictionary of name to numpy arrays.
      srl_labels_inv: SRL label id to string name.
      suppress_overlap: Whether to greedily suppress overlapping arguments for the same predicate.

    Returns:

    
    , V, arg_ends, arg_labels, arg_starts, num_args, num_preds, predicates, srl_scores","0, 0.1, 1, 128, 16, 2, 3, 32, 4, 64, 8, False, None, True","

Args:
span_starts: [num_candidates,]
span_scores: [num_candidates, num_labels]
span_ends:
labels_inv:

Returns:


, #     continue, #     predictions[""srl""][i][predict_dict[""predicates""][i]] = [], # , score)), # A dictionary from id to list of binary core-arg states., # A dictionary from states to (arg_id, role, prev_t, prev_rs), # Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL, # Backtrack to decode., # Decode sentence-level tasks., # Enumerate explored states., # If label is not null., # If none of the tokens has been covered:, # Inference functions for the SRL model., # Locally best role assignment., # Map from predicates to a list of labeled spans., # Only add predicate if it has any argument., # Predicate will not overlap with arguments either., # Sentence-level predictions., # Skip invalid span., # Sort arguments by highest score first., # Strictly better to incorporate a dummy span if it has the highest local score., # The extra dummy score should be same for all states, so we can safely skip arguments overlap, # This one, # To avoid warnings in the eval script., # True; this one, # Update states if best role is not a core arg., # [arg_start, arg_end, arg_span_id], # [num_arg, num_roles], # [num_candidates], # collect the state which is before the current span, # decode the predictions., # dummy score, # false, # for each predicate id, exec the decode process, # for each sentences, # for each state, # if label not in [""V"", ""C-V""]:, # if predict_dict[""No_arg""] is True:, # labels_inv[r0] == ""O"", # print start, end, i, r_str, core_state, rs, # skip the span contains the predicate, # sort according to the span start index, # sorted arg_starts and arg_ends and srl_scores ? should be??? enforce_srl_constraint = False, # the number of the candidate argument spans, # the number of the candidate predicates, # the pointers store, # the set type in the value in the state dict, # update the state, # with the predicate., Greedy decoding for SRL predicate-argument structures.

Args:
predict_dict: Dictionary of name to numpy arrays.
srl_labels_inv: SRL label id to string name.
suppress_overlap: Whether to greedily suppress overlapping arguments for the same predicate.

Returns:


",
https://github.com/hankcs/HanLP,layer.py,0,0.0,144,75.0,18,9.38,18,9.38,12,6.25,0,0.0,9,8,61,2,,"Biaffine, DropoutLayer, DropoutLayer3D, HighwayLSTMCell, I, LayerNorm, NonLinear, Q, Q2, QTQmI, T, ValueError, VariationalLSTM, VariationalLSTMCell, __class__, __getattr__, __init__, __name__, __repr__, _activate, _all_weights, _c, _forward_brnn, _forward_rnn, _h, _x, activation, affine, append, astype, batch_first, batch_size, bcells, beta, biaffine, bias, bidirectional, blayer_c_n, blayer_h_n, blayer_output, c_n, c_next, callable, cell, chunk, contiguous, copy_, cpu, cuda, data, device, dim1, dim2, dot, drop_mask, drop_masks, dropout, dropout_in, dropout_out, dropout_rate, eps, expand, fcells, features, format, forward, gamma, get_tensor_np, h_next, hanlp, hidden_mask, hidden_size, hx, in1_features, in2_features, input, input1, input2, input_mask, input_size, int, j, layer, layer_c_n, layer_h_n, layer_input_size, layer_output, layer_params, len1, len2, length, linear, linear_hh, linear_ih, linear_input_size, linear_output_size, list, loss, lr, mask, masks, max_time, mean, name, named_parameters, new, num_directions, num_layers, numpy, orthonormal_initializer, out_features, output, output_size, param, param_names, preact, print, range, reset_dropout_mask, reset_parameters, reverse, reversed, self, setattr, size, staticmethod, std, str, success, suffix, sum, super, time, to, torch, training, transpose, tries, type, view, weight, weight_hh, weight_ih, zip","F.sigmoid, F.tanh, hanlp.components.srl.span_rank.util.block_orth_normal_initializer, nn.LSTMCell, nn.Linear, nn.Module, nn.Parameter, nn.init.constant, nn.init.constant_, nn.init.normal, nn.init.orthogonal, nn.init.xavier_uniform_, nn.init.zeros_, np.abs, np.eye, np.float32, np.isfinite, np.max, np.random.randn, np.sqrt, np.sum, np.transpose, torch.FloatTensor, torch.Tensor, torch.autograd.Variable, torch.bernoulli, torch.bmm, torch.cat, torch.cuda.is_available, torch.mul, torch.nn.init.xavier_uniform_, torch.ones, torch.sigmoid, torch.stack, torch.tanh, torch.transpose, torch.unsqueeze, torch.zeros","__init__, __repr__, _forward_brnn, _forward_rnn, forward, get_tensor_np, orthonormal_initializer, reset_dropout_mask, reset_parameters","Biaffine, DropoutLayer, DropoutLayer3D, HighwayLSTMCell, LayerNorm, NonLinear, VariationalLSTM, VariationalLSTMCell","I, Q, Q2, QTQmI, _c, _h, _x, affine, batch_size, biaffine, blayer_c_n, blayer_h_n, blayer_output, c, c_n, c_next, dim1, dim2, eps, h, h_n, h_next, hidden_mask, hx, i, initial, input, input1, input2, input_mask, input_size, j, k, layer, layer_c_n, layer_h_n, layer_input_size, layer_output, layer_params, len1, len2, loss, lr, masks, max_time, mean, name, ones, output, param, param_names, preact, std, success, suffix, time, tries, weight, weight_hh, weight_ih, y","A module that runs multiple steps of LSTM., adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/linalg.py

Args:
  output_size: 
  input_size: 

Returns:",",  (, ), , in2_features=, , out_features=, A module that runs multiple steps of LSTM., Orthogonal pretrainer failed, using non-orthogonal random matrix, Orthogonal pretrainer loss: %.2e, _reverse, activation must be callable: type={}, adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/linalg.py

    Args:
      output_size: 
      input_size: 

    Returns:

    
    , bias, bias_hh_l{}{}, bias_ih_l{}{}, in1_features=, weight, weight_hh_l{}{}, weight_ih_l{}{}","0, 0.0, 0.01, 0.05, 0.1, 1, 1.0, 10, 100, 1000000.0, 1e-08, 2, 3, 5, 6, False, None, True","# + (1.0 - mask) * _c, # + (1.0 - mask) * _h, # Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL, # compute the x, # eval, # nn.init.constant(self.linear_hh.weight, 1.0), # nn.init.constant(self.linear_ih.weight, 1.0), # reset all the param in the MyLSTMCell, # this kind of implementation is too tedious, # torch.bmm: Performs a batch matrix-matrix product of matrices stored in batch1 and batch2., # view: Returns a new tensor with the same data as the self tensor but of a different size., adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/linalg.py

Args:
output_size:
input_size:

Returns:


",
https://github.com/hankcs/HanLP,span_rank.py,0,0.0,181,68.82,45,17.11,22,8.37,14,5.32,1,0.38,23,1,67,1,,"SpanRankingSemanticRoleLabeler, __init__, _get_transformer, _step, adam_epsilon, add, append, append_transform, argmax, argument_span, argument_spans, backward, batch, batch_max_tokens, batch_size, bel, best_epoch, best_metric, bisect, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_vocabs, config, confusion_matrix, conll_f1, conll_precision, conll_recall, context_layer, criterion, current, dataloader, decode_output, dev, dev_score, device, dict, doc_level_offset, each, elapsed_human, embed, end_to_end, end_to_end_f1, enumerate, epoch, epochs, erase, eval, evaluate_dataloader, exclusive_offset, execute_training_loop, extend, f1, feed_batch, fit, fit_dataloader, flat, fmt, format_dict_to_results, generate_idx, get, getattr, gold, grad_norm, gradient_accumulation, hanlp, hanlp_common, headings, idx, idx_to_label, info, input_is_flat, insert, int, isinstance, item, items, j, join, keys, label_confusions, label_first, labels, len, linear_scheduler, lock, log, logger, logging, loss, lr, matrix, max, max_seq_len, model, model_, module, mutable, num_training_steps, numel, official, output, output_dict, outputs, pal, pal_list, parameters, precision, pred, predicate, predicate_f1, predicate_index, predicate_indices, predict, prediction, purge_cache, range, ratio_width, recall, report, report_metrics, reset, reset_metrics, results, reversed, row, sample, sampler, samples, save_dir, save_weights, score, scores, self, sentences, shuffle, sorted, splitlines, srl_label, srl_mask, srl_per_sentence, srl_scores, staticmethod, step, stop, str, sum, summary, super, timer, token, tokens, top_predicate_indices, top_spans, torch, total, total_loss, total_time_human, train, training, transform, transformer_lr, trn, tuple, typing, unlabeled_f1, unlabeled_precision, unlabeled_recall, unpack, update_metrics, vocabs, warmup_steps, weight_decay, with_argument, with_predicate, x, zero_grad, zip","bisect.bisect, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSampler, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.vocab.Vocab, hanlp.components.srl.span_rank.inference_utils.srl_decode, hanlp.components.srl.span_rank.span_ranking_srl_model.SpanRankingSRLModel, hanlp.components.srl.span_rank.srl_eval_utils.compute_srl_f1, hanlp.datasets.srl.loaders.conll2012.CoNLL2012SRLDataset, hanlp.datasets.srl.loaders.conll2012.filter_v_args, hanlp.datasets.srl.loaders.conll2012.group_pa_by_p, hanlp.datasets.srl.loaders.conll2012.unpack_srl, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.metrics.f1.F1, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, hanlp_common.visualization.markdown_table, logging.Logger, torch.isnan, torch.nn.Module, torch.nn.utils.clip_grad_norm_, torch.no_grad, torch.optim.Adam, torch.optim.lr_scheduler.ReduceLROnPlateau, torch.stack, torch.utils.data.DataLoader, torch.zeros, typing.Any, typing.Callable, typing.Dict, typing.List, typing.Tuple, typing.Union","__init__, _get_transformer, _step, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_vocabs, decode_output, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, format_dict_to_results, input_is_flat, predict, report_metrics, reset_metrics, unpack, update_metrics",SpanRankingSemanticRoleLabeler,"a, args, argument_span, argument_spans, batch, batch_max_tokens, batch_size, bel, best_epoch, best_metric, data, dataloader, dataset, dev_score, each, end_to_end, end_to_end_f1, epoch, flat, gold, gradient_accumulation, headings, idx, idx_to_label, j, labels, loss, matrix, max_seq_len, model, num_training_steps, optimizer, order, output, output_dict, outputs, pal, pal_list, pred, predicate, predicate_f1, predicate_index, predicate_indices, prediction, ratio_width, report, results, row, sample, sampler, samples, scheduler, scores, sentences, srl_mask, srl_per_sentence, srl_scores, table, timer, token, tokens, top_predicate_indices, top_spans, total_loss, transform, transformer, x","An implementation of ""Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling""
(:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.

Args:
    **kwargs: Predefined config."," / ,  [red]saved[/red],  ^,  end_to_end: ,  predicate: , ) [blink][yellow]...[/yellow][/blink], .2%, .4f, /, :[/yellow], <null>, An implementation of ""Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling""
        (:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.

        Args:
            **kwargs: Predefined config.
        , Building vocabs (max sequence length , Confusion Matrix, F1, GOLD↓PRED→, Labeled, Official, PRED, Precision, Recall, Scores, Settings, Unlabeled, [yellow]Epoch , arg_ends, arg_starts, dict, embed, gradient_accumulation, list, loss, loss: , max, predicates, prediction, srl, srl_label, srl_mask, srl_scores, sum, token, token_input_ids, token_length, transformer","0, 0.001, 0.01, 0.1, 0.2, 0.4, 0.5, 0.8, 1, 100, 150, 1e-05, 1e-06, 2, 20, 30, 40, 5.0, 700, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-09 18:13, # Enable cache, # For data parallel, # Use fast decoding during training,, # Use null to indicate no relationship, # a: [(0, 0, 'ARG0')], # embed: torch.nn.Embedding = self.config.embed.module(vocabs=self.vocabs)[0].embed, # noinspection PyMethodOverriding, # noinspection PyProtectedMember, # noinspection PyTypeChecker, # w/ gold pred, some batches do not have PAs at all, resulting in empty scores, An implementation of ""Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling
(:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.

Args:
**kwargs: Predefined config.
",GOLD↓PRED→
https://github.com/hankcs/HanLP,span_ranking_srl_model.py,0,0.0,246,78.1,35,11.11,9,2.86,25,7.94,0,0.0,23,2,126,3,,"Dropout, Embedding, Linear, Module, ModuleList, SpanRankingSRLDecoder, SpanRankingSRLModel, __init__, actual_sent_lengths, append, arg_dropout_layers, arg_emb, arg_ends, arg_scores, arg_span_indices, arg_starts, arg_unary_score_layers, arg_unary_score_projection, args_mask, argument_ratio, batch, batch_index_select, batch_sentences_mask, batch_word_num, batch_word_offset, biaffine, bias, candidate_arg_scores, candidate_ends, candidate_mask, candidate_pred_emb, candidate_pred_ids, candidate_pred_scores, candidate_scores, candidate_span_emb, candidate_span_ids, candidate_span_number, candidate_starts, candidate_width, config, context_embeddings, context_layer, context_layer_output_dim, contextualized_embeddings, contiguous, cpu_flatted_candidte_starts, decode, decoder, dense_labels, dense_srl_labels, device, dropout, dummy_scores, emb, embed, enforce_non_crossing, enumerate, exclusive, exclusive_cumsum, exclusive_sent_lengths, expand, expanded_arg_emb, expanded_arg_ends, expanded_arg_starts, expanded_pred_emb, expanded_predicates, extract_spans, ffnn, ffnn_depth, ffnn_size, flat_pair_emb, flat_srl_scores, flatted_candidate_arg_scores, flatted_candidate_ends, flatted_candidate_mask, flatted_candidate_starts, flatted_context_emb, flatted_context_output, flatted_emb, flatted_span_indices, flatten_emb, flatten_emb_in_sentence, float, forward, gather_4d, get, get_arg_unary_scores, get_batch_topk, get_candidate_spans, get_dense_span_labels, get_output_dim, get_pred_unary_scores, get_span_emb, get_srl_labels, get_srl_loss_mask, get_srl_scores, get_srl_softmax_loss, get_srl_unary_scores, gold_arg_ends, gold_arg_labels, gold_arg_starts, gold_predicates, hanlp, head_indices, head_indices_log_mask, head_scores, hidden_states, highway_variational_lstm, index_select, indices, indices_a, indices_b, indices_c, indices_d, init, initializer, initializer_1d, input, input_tensor, int, item, k, keys, label_space_size, len, lengths_to_mask, lexical_dropout, loss, loss_reduction, mask, masks, max_arg_num, max_arg_width, max_candidate_spans_num_per_sentence, max_num_arg, max_num_output_spans, max_num_pred, max_pred_num, max_sent_length, max_sentence_length, max_spans_num, new_zeros, nn, nonzero, num_args, num_predicted_args, num_predicted_preds, num_preds, num_sentences, num_spans, offset, output, output_span_indices_tensor, pad, pair_emb, pair_emb_list, pair_emb_size, params, parsers, pred_dropout_layers, pred_emb, pred_mask, pred_scores, pred_unary_score_layers, pred_unary_score_projection, predicate_ratio, predicate_scale, predicates, predict_dict, predicted_ends, predicted_indices, predicted_scores, predicted_starts, range, rank, relu, reset_parameters, reshape, result, score, self, sent_lengths, sentence_indices, sentence_indices_2d, size, sort_spans, span_emb, span_emb_feature_list, span_emb_size, span_end_emb, span_ends, span_head_emb, span_indices, span_indices_mask, span_labels, span_parents, span_start_emb, span_starts, span_text_emb, span_width, span_width_emb, span_width_embedding, span_width_feature_size, span_width_index, spans_log_mask, spans_width, sparse_indices, sparse_values, squeeze, srl_dropout_layers, srl_labels, srl_loss, srl_loss_mask, srl_mask, srl_scores, srl_unary_score_input_size, srl_unary_score_layers, srl_unary_score_projection, staticmethod, super, t, text_len, to, top_arg_indices, top_pred_indices, topk, topk_ratio, torch, torch_util, training, typing, unpack, unsqueeze, unsqueezed_arg_emb, unsqueezed_arg_scores, unsqueezed_pred_emb, unsqueezed_pred_scores, update, use_biaffine, use_gold_predicates, utils, view, weight, word_embedding_dim, xavier_uniform_, zip","hanlp.layers.feedforward.FeedForward, hanlp.layers.time_distributed.TimeDistributed, highway_variational_lstm.*, parsers.biaffine.biaffine.Biaffine, torch.Size, torch.Tensor, torch.arange, torch.cat, torch.clamp, torch.cumsum, torch.float, torch.floor, torch.gather, torch.index_select, torch.log, torch.long, torch.max, torch.mean, torch.nn.Module, torch.nn.functional.cross_entropy, torch.ones, torch.sparse.FloatTensor, torch.sparse.LongTensor, torch.stack, torch.zeros, torch.zeros_like, typing.Dict","__init__, batch_index_select, decode, exclusive_cumsum, extract_spans, flatten_emb, flatten_emb_in_sentence, forward, gather_4d, get_arg_unary_scores, get_batch_topk, get_candidate_spans, get_dense_span_labels, get_pred_unary_scores, get_span_emb, get_srl_labels, get_srl_loss_mask, get_srl_scores, get_srl_softmax_loss, get_srl_unary_scores, initializer_1d, reset_parameters, unpack","SpanRankingSRLDecoder, SpanRankingSRLModel","actual_sent_lengths, arg_emb, arg_ends, arg_scores, arg_span_indices, arg_starts, args_mask, batch_word_num, batch_word_offset, candidate_arg_scores, candidate_ends, candidate_mask, candidate_pred_emb, candidate_pred_ids, candidate_pred_scores, candidate_span_emb, candidate_span_ids, candidate_span_number, candidate_starts, candidate_width, context_embeddings, context_layer_output_dim, cpu_flatted_candidte_starts, dense_labels, dense_srl_labels, device, dummy_scores, exclusive_sent_lengths, expanded_arg_emb, expanded_arg_ends, expanded_arg_starts, expanded_pred_emb, expanded_predicates, ffnn, flat_pair_emb, flat_srl_scores, flatted_candidate_arg_scores, flatted_candidate_ends, flatted_candidate_mask, flatted_candidate_starts, flatted_context_output, flatted_emb, flatted_span_indices, flatten_emb, gold_arg_ends, gold_arg_labels, gold_arg_starts, gold_predicates, head_indices, head_indices_log_mask, head_scores, indices, indices_a, indices_b, indices_c, indices_d, input, input_tensor, item, k, keys, loss, mask, masks, max_arg_num, max_arg_width, max_candidate_spans_num_per_sentence, max_num_arg, max_num_output_spans, max_num_pred, max_pred_num, max_sent_length, max_sentence_length, max_spans_num, num_args, num_preds, num_sentences, num_spans, offset, output, output_span_indices_tensor, pair_emb, pair_emb_list, pair_emb_size, pred_emb, pred_mask, pred_scores, predicates, predict_dict, predicted_ends, predicted_indices, predicted_scores, predicted_starts, rank, result, score, sent_lengths, sentence_indices, sentence_indices_2d, span_emb, span_emb_feature_list, span_end_emb, span_head_emb, span_indices, span_indices_mask, span_start_emb, span_text_emb, span_width, span_width_emb, span_width_index, spans_log_mask, spans_width, sparse_indices, sparse_values, srl_labels, srl_loss, srl_loss_mask, srl_mask, srl_scores, top_arg_indices, top_pred_indices, topk, unsqueezed_arg_emb, unsqueezed_arg_scores, unsqueezed_pred_emb, unsqueezed_pred_scores","Args:
  input: input is the sentence lengths tensor.
  exclusive: exclude the last sentence length (Default value = True)
  input(torch.Tensor :): 
  input: torch.Tensor: 

Returns:, Compute span score with FFNN(span embedding)

Args:
  span_emb: tensor of [num_sentences, num_spans, emb_size]
  config: param dropout:
  num_labels: param name:

Returns:, extract the topk span indices

Args:
  candidate_scores: param candidate_starts:
  candidate_ends: param topk: [num_sentences]
  max_sentence_length: param sort_spans:
  enforce_non_crossing: return: indices [num_sentences, max_num_predictions]
  candidate_starts: 
  topk: 
  sort_spans: 

Returns:","

        Args:
          input: input is the sentence lengths tensor.
          exclusive: exclude the last sentence length (Default value = True)
          input(torch.Tensor :): 
          input: torch.Tensor: 

        Returns:

        
        , Compute span score with FFNN(span embedding)

        Args:
          span_emb: tensor of [num_sentences, num_spans, emb_size]
          config: param dropout:
          num_labels: param name:

        Returns:

        
        , Compute the srl loss, Get the answers according to the labels, Get the candidate predicate, Get the span ids, Get the srl scores according to the arg emb and pre emb., Get top arg embeddings, Get top predicate embeddings, Get unary scores and topk of candidate argument spans., ReLU, arg_ends, arg_scores, arg_starts, argument_begin_offset, argument_end_offset, candidate_arg_scores, candidate_ends, candidate_pred_scores, candidate_starts, extract the topk span indices

        Args:
          candidate_scores: param candidate_starts:
          candidate_ends: param topk: [num_sentences]
          max_sentence_length: param sort_spans:
          enforce_non_crossing: return: indices [num_sentences, max_num_predictions]
          candidate_starts: 
          topk: 
          sort_spans: 

        Returns:

        
        , generate candidate spans with argument pruning, generate the span embedding, head features, head_scores, loss, num_args, num_preds, pred_scores, predicate_offset, predicates, srl_label_id, srl_mask, srl_scores, token_length","0, 0.0, 1, 2, 3, 4, False, None, True","

Args:
input: input is the sentence lengths tensor.
exclusive: exclude the last sentence length (Default value = True)
input(torch.Tensor :):
input: torch.Tensor:

Returns:


, 
# num_sentences = candidate_scores.size()[0]
# num_input_spans = candidate_scores.size()[1]
max_num_output_spans = int(torch.max(topk))
indices = [score.topk(k)[1] for score, k in zip(candidate_scores, topk)]
output_span_indices_tensor = [F.pad(item, [0, max_num_output_spans - item.size()[0]], value=item[-1])
for item in indices]
output_span_indices_tensor = torch.stack(output_span_indices_tensor)
return output_span_indices_tensor

def batch_index_select(self, emb, indices):
num_sentences = emb.size()[0]
max_sent_length = emb.size()[1]
flatten_emb = self.flatten_emb(emb)
offset = (torch.arange(0, num_sentences, device=emb.device) * max_sent_length).unsqueeze(1)
return torch.index_select(flatten_emb, 0, (indices + offset).view(-1)) \
.view(indices.size()[0], indices.size()[1], emb.size(-1))

def get_batch_topk(self, candidate_starts: torch.Tensor, candidate_ends, candidate_scores, topk_ratio, text_len,
max_sentence_length, sort_spans=False, enforce_non_crossing=True):
num_sentences = candidate_starts.size()[0]
max_sentence_length = candidate_starts.size()[1]

topk = torch.floor(text_len.to(torch.float) * topk_ratio).to(torch.long)
topk = torch.max(topk, torch.ones(num_sentences, device=candidate_starts.device, dtype=torch.long))

# this part should be implemented with C++
predicted_indices = self.extract_spans(candidate_scores, candidate_starts, candidate_ends, topk,
max_sentence_length, sort_spans, enforce_non_crossing)
predicted_starts = torch.gather(candidate_starts, 1, predicted_indices)
predicted_ends = torch.gather(candidate_ends, 1, predicted_indices)
predicted_scores = torch.gather(candidate_scores, 1, predicted_indices)
return predicted_starts, predicted_ends, predicted_scores, topk, predicted_indices

def get_dense_span_labels(self, span_starts, span_ends, span_labels, max_sentence_length,
span_parents=None):
num_sentences = span_starts.size()[0]
max_spans_num = span_starts.size()[1]

# span_starts = span_starts + 1 - (span_labels > 0).to(torch.long)
span_starts[(span_labels == 0) & (span_starts < max_sentence_length - 1)] += 1  # make start > end
sentence_indices = torch.arange(0, num_sentences, device=span_starts.device).unsqueeze(1).expand(-1,
max_spans_num)

sparse_indices = torch.cat([sentence_indices.unsqueeze(2), span_starts.unsqueeze(2), span_ends.unsqueeze(2)],
dim=2)
if span_parents is not None:  # semantic span predicate offset
sparse_indices = torch.cat([sparse_indices, span_parents.unsqueeze(2)], 2)

rank = 3 if span_parents is None else 4
dense_labels = torch.sparse.LongTensor(sparse_indices.view(num_sentences * max_spans_num, rank).t(),
span_labels.view(-1),
torch.Size([num_sentences] + [max_sentence_length] * (rank - 1))) \
.to_dense()
return dense_labels

@staticmethod
def gather_4d(params, indices):
assert len(params.size()) == 4 and len(indices) == 4
indices_a, indices_b, indices_c, indices_d = indices
result = params[indices_a, indices_b, indices_c, indices_d]
return result

def get_srl_labels(self,
arg_starts,
arg_ends,
predicates,
gold_predicates,
gold_arg_starts,
gold_arg_ends,
gold_arg_labels,
max_sentence_length
):
num_sentences = arg_starts.size()[0]
max_arg_num = arg_starts.size()[1]
max_pred_num = predicates.size()[1]

sentence_indices_2d = torch.arange(0, num_sentences, device=arg_starts.device).unsqueeze(1).unsqueeze(2).expand(
-1, max_arg_num, max_pred_num)
expanded_arg_starts = arg_starts.unsqueeze(2).expand(-1, -1, max_pred_num)
expanded_arg_ends = arg_ends.unsqueeze(2).expand(-1, -1, max_pred_num)
expanded_predicates = predicates.unsqueeze(1).expand(-1, max_arg_num, -1)

dense_srl_labels = self.get_dense_span_labels(gold_arg_starts,
gold_arg_ends,
gold_arg_labels,
max_sentence_length, span_parents=gold_predicates)  # ans
srl_labels = self.gather_4d(dense_srl_labels,
[sentence_indices_2d, expanded_arg_starts, expanded_arg_ends, expanded_predicates])
return srl_labels

def get_srl_unary_scores(self, span_emb):
input = span_emb
for i, ffnn in enumerate(self.srl_unary_score_layers):
input = F.relu(ffnn.forward(input))
input = self.srl_dropout_layers[i].forward(input)
output = self.srl_unary_score_projection.forward(input)
return output

def get_srl_scores(self, arg_emb, pred_emb, arg_scores, pred_scores, num_labels, config, dropout):
num_sentences = arg_emb.size()[0]
num_args = arg_emb.size()[1]  # [batch_size, max_arg_num, arg_emb_size]
num_preds = pred_emb.size()[1]  # [batch_size, max_pred_num, pred_emb_size]

unsqueezed_arg_emb = arg_emb.unsqueeze(2)
unsqueezed_pred_emb = pred_emb.unsqueeze(1)
expanded_arg_emb = unsqueezed_arg_emb.expand(-1, -1, num_preds, -1)
expanded_pred_emb = unsqueezed_pred_emb.expand(-1, num_args, -1, -1)
pair_emb_list = [expanded_arg_emb, expanded_pred_emb]
pair_emb = torch.cat(pair_emb_list, 3)  # concatenate the argument emb and pre emb
pair_emb_size = pair_emb.size()[3]
flat_pair_emb = pair_emb.view(num_sentences * num_args * num_preds, pair_emb_size)
# get unary scores
flat_srl_scores = self.get_srl_unary_scores(flat_pair_emb)
srl_scores = flat_srl_scores.view(num_sentences, num_args, num_preds, flat_srl_scores.size(-1))
if self.config.use_biaffine:
srl_scores += self.biaffine(arg_emb, self.predicate_scale(pred_emb)).permute([0, 2, 3, 1])
unsqueezed_arg_scores, unsqueezed_pred_scores = \
arg_scores.unsqueeze(2).unsqueeze(3), pred_scores.unsqueeze(1).unsqueeze(3)
srl_scores = srl_scores + unsqueezed_arg_scores + unsqueezed_pred_scores
dummy_scores = torch.zeros([num_sentences, num_args, num_preds, 1], device=arg_emb.device)
srl_scores = torch.cat([dummy_scores, srl_scores], 3)
return srl_scores

def get_srl_softmax_loss(self, srl_scores, srl_labels, num_predicted_args, num_predicted_preds):
srl_loss_mask = self.get_srl_loss_mask(srl_scores, num_predicted_args, num_predicted_preds)

loss = torch.nn.functional.cross_entropy(srl_scores[srl_loss_mask], srl_labels[srl_loss_mask],
reduction=self.loss_reduction)
return loss, srl_loss_mask

def get_srl_loss_mask(self, srl_scores, num_predicted_args, num_predicted_preds):
max_num_arg = srl_scores.size()[1]
max_num_pred = srl_scores.size()[2]
# num_predicted_args, 1D tensor; max_num_arg: a int variable means the gold ans's max arg number
args_mask = hanlp.utils.torch_util.lengths_to_mask(num_predicted_args, max_num_arg)
pred_mask = hanlp.utils.torch_util.lengths_to_mask(num_predicted_preds, max_num_pred)
srl_loss_mask = args_mask.unsqueeze(2) & pred_mask.unsqueeze(1)
return srl_loss_mask

def decode(self, contextualized_embeddings, sent_lengths, masks, gold_arg_starts, gold_arg_ends, gold_arg_labels,
gold_predicates):
num_sentences, max_sent_length = masks.size()
device = sent_lengths.device
generate candidate spans with argument pruning, 
input = span_emb
for i, ffnn in enumerate(self.arg_unary_score_layers):
input = F.relu(ffnn.forward(input))
input = self.arg_dropout_layers[i].forward(input)
output = self.arg_unary_score_projection.forward(input)
return output

def get_pred_unary_scores(self, span_emb):
input = span_emb
for i, ffnn in enumerate(self.pred_unary_score_layers):
input = F.relu(ffnn.forward(input))
input = self.pred_dropout_layers[i].forward(input)
output = self.pred_unary_score_projection.forward(input)
return output

def extract_spans(self, candidate_scores, candidate_starts, candidate_ends, topk, max_sentence_length,
sort_spans, enforce_non_crossing):
extract the topk span indices, # [,150], # [num_sentences, max_num_preds, emb], # [num_spans], generate the span width, # candidate_starts [num_sentences, max_sent_length * max_arg_width], # choose the flatted_candidate_starts with the actual existing positions, i.e. exclude the illegal starts, # flatten the lstm output according to the sentence mask, i.e. exclude the illegal (padding) lstm output, # gather slices from embeddings according to indices, # get the span width feature emb, # get the word offset in a batch, # init.xavier_uniform_(self.context_projective_layer.weight), # initializer_1d(self.context_projective_layer.bias, init.xavier_uniform_), # predicate scores, # self.context_projective_layer = nn.Linear(2 * self.lstm_hidden_size, self.config.num_attention_heads), # span scores, # span width feature embedding, # srl scores, # store the span vector representations for span rep., Get the srl scores according to the arg emb and pre emb.
srl_scores = self.get_srl_scores(arg_emb, pred_emb, arg_scores, pred_scores, self.label_space_size, self.config,
self.dropout)  # [num_sentences, max_num_args, max_num_preds, num_labels]
if gold_arg_labels is not None:
Get the answers according to the labels, Get top arg embeddings
arg_span_indices = torch.gather(candidate_span_ids, 1, top_arg_indices)  # [num_sentences, max_num_args]
arg_emb = candidate_span_emb.index_select(0, arg_span_indices.view(-1)).view(
arg_span_indices.size()[0], arg_span_indices.size()[1], -1
)  # [num_sentences, max_num_args, emb]
Get top predicate embeddings, Get unary scores and topk of candidate argument spans.
flatted_candidate_arg_scores = self.get_arg_unary_scores(candidate_span_emb)
candidate_arg_scores = flatted_candidate_arg_scores.index_select(0, candidate_span_ids.view(-1)) \
.view(candidate_span_ids.size()[0], candidate_span_ids.size()[1])
candidate_arg_scores = candidate_arg_scores + spans_log_mask
arg_starts, arg_ends, arg_scores, num_args, top_arg_indices = \
self.get_batch_topk(candidate_starts, candidate_ends, candidate_arg_scores,
self.config.argument_ratio, sent_lengths, max_sent_length,
sort_spans=False, enforce_non_crossing=False)
Get the candidate predicate, generate the span embedding
candidate_span_emb, head_scores, span_head_emb, head_indices, head_indices_log_mask = self.get_span_emb(
flatted_context_output, flatted_candidate_starts, flatted_candidate_ends,
self.config, dropout=self.dropout)
Get the span ids, head features
cpu_flatted_candidte_starts = flatted_candidate_starts
span_indices = torch.arange(0, max_arg_width, device=flatted_context_emb.device).view(1, -1) + \
cpu_flatted_candidte_starts.view(-1, 1)  # For all the i, where i in [begin, ..i, end] for span
# reset the position index to the batch_word_num index with index - 1
span_indices = torch.clamp(span_indices, max=batch_word_num - 1)
num_spans, spans_width = span_indices.size()[0], span_indices.size()[1]
flatted_span_indices = span_indices.view(-1)  # so Huge!!!, column is the span?
# if torch.cuda.is_available():
flatted_span_indices = flatted_span_indices
span_text_emb = flatted_context_emb.index_select(0, flatted_span_indices).view(num_spans, spans_width, -1)
span_indices_mask = hanlp.utils.torch_util.lengths_to_mask(span_width, max_len=max_arg_width)
# project context output to num head
# head_scores = self.context_projective_layer.forward(flatted_context_emb)
# get span attention
# span_attention = head_scores.index_select(0, flatted_span_indices).view(num_spans, spans_width)
# span_attention = torch.add(span_attention, expanded_span_indices_log_mask).unsqueeze(2)  # control the span len
# span_attention = F.softmax(span_attention, dim=1)
span_text_emb = span_text_emb * span_indices_mask.unsqueeze(2).expand(-1, -1, span_text_emb.size()[-1])
span_head_emb = torch.mean(span_text_emb, 1)
span_emb_feature_list.append(span_head_emb)

span_emb = torch.cat(span_emb_feature_list, 1)
return span_emb, None, span_text_emb, span_indices, span_indices_mask

def get_arg_unary_scores(self, span_emb):
Compute span score with FFNN(span embedding)",
https://github.com/hankcs/HanLP,srl_eval_utils.py,0,0.0,120,65.22,35,19.02,9,4.89,20,10.87,0,0.0,9,0,69,3,,"SRLScores, _SRL_CONLL_EVAL_SCRIPT, _calc_f1, a0, a1, append, args, close, codecs, col_labels, collections, compute_span_f1, compute_srl_f1, compute_unlabeled_span_f1, conll_f1, conll_precision, conll_recall, debugging, end, enumerate, evaluate_retrieval, evaluator, evaluators, example, f1, filtered_args, filtered_gold_args, fin, flags, fout, gold, gold_args, gold_data, gold_path, gold_predicates, gold_rels, gold_spans, gold_srl, hanlp, info, is_predicted, items, j, k, keys, label, label_column, label_confusions, labels, len, line, list, matched, max, message, ner_spans, num_predictions, num_sents, num_words, operator, output_filename, prec, precision, pred, pred_ends, pred_id, pred_rels, pred_starts, pred_to_args, predicted_ends, predicted_spans, predicted_starts, prediction, predictions, print, print_sentence_to_conll, print_to_conll, props, range, read_gold_predicates, recall, samples, sent_id, sentence, sentences, set, sorted, sorted_ends, sorted_scores, sorted_starts, span_ends, span_scores, span_starts, split, split_example_for_eval, srl_labels, srl_rels, start, strip, sum, task_name, temp_output, tempfile, text_length, tokens, total_gold, total_matched, total_predicted, total_unlabeled_matched, ul_f1, ul_prec, ul_recall, unlabeled_f1, unlabeled_precision, unlabeled_recall, update, word_offset, words, write, zip","codecs.open, collections.Counter, collections.namedtuple, hanlp.metrics.srl.srlconll.official_conll_05_evaluate, operator.itemgetter, tempfile.NamedTemporaryFile","_calc_f1, compute_span_f1, compute_srl_f1, compute_unlabeled_span_f1, evaluate_retrieval, print_sentence_to_conll, print_to_conll, read_gold_predicates, split_example_for_eval",,"SRLScores, _SRL_CONLL_EVAL_SCRIPT, a0, a1, args, col_labels, conll_f1, conll_precision, conll_recall, end, evaluator, f1, filtered_args, filtered_gold_args, fin, flags, fout, gold, gold_args, gold_path, gold_predicates, gold_rels, info, is_predicted, j, k, label, label_column, label_confusions, line, matched, ner_spans, num_predictions, num_sents, num_words, prec, precision, pred, pred_id, pred_rels, pred_to_args, predicted_ends, predicted_spans, predicted_starts, prediction, props, recall, samples, sent_id, sentence, sentences, sorted_ends, sorted_scores, sorted_starts, srl_rels, start, temp_output, total_gold, total_matched, total_predicted, total_unlabeled_matched, ul_f1, ul_prec, ul_recall, unlabeled_f1, unlabeled_precision, unlabeled_recall, word_offset, words","Evaluation for unlabeled retrieval.

Args:
  gold_spans: Set of tuples of (start, end).
  span_starts: 
  span_ends: 
  span_scores: 
  pred_starts: 
  pred_ends: 
  text_length: 
  evaluators: 
  debugging: (Default value = False)

Returns:, Print a labeled sentence into CoNLL format.

Args:
  fout: 
  tokens: 
  labels: 

Returns:, Split document-based samples into sentence-based samples for evaluation.

Args:
  example: 

Returns:","
, (, (V*), ), *, -, ../run_eval.sh, C-V, Evaluation for unlabeled retrieval.

    Args:
      gold_spans: Set of tuples of (start, end).
      span_starts: 
      span_ends: 
      span_scores: 
      pred_starts: 
      pred_ends: 
      text_length: 
      evaluators: 
      debugging: (Default value = False)

    Returns:

    
    , Gold, P, Predicted, Print a labeled sentence into CoNLL format.

    Args:
      fout: 
      tokens: 
      labels: 

    Returns:

    
    , SRLScores, Split document-based samples into sentence-based samples for evaluation.

    Args:
      example: 

    Returns:

    
    , Unlabeled , V, conll_f1, conll_precision, conll_recall, f1, gold path, label_confusions, num_sents, precision, r, recall, sentences, srl, unlabeled_f1, unlabeled_precision, unlabeled_recall, utf-8, w, {}: Precision: {:.2%} Recall: {:.2%} F1: {:.2%}","0, 1, 100, 15, 2, 3, False, None, True","# ""C-V""]], # ""SRL (unofficial)"", # ""Unlabeled SRL (unofficial)"", # Add unpredicted verb (for predicted SRL)., # Compute unofficial F1 of SRL relations., # Counter of (gold, pred) label pairs., # Evaluate twice with official script., # Evaluation util functions for PropBank SRL., # FIXME: scalar index error, # Prepare to compute official F1., # To make sure CoNLL-eval script count matching predicates as correct., # Unused., # assert i == 0  # For CoNLL-2005, there are always document == sentence., # assert len(sentences) == 1, # if the predicate id is False, # print(""No gold conll_eval data provided. Recreating ...""), # print((""Output temp outoput {}"".format(temp_output))), Evaluation for unlabeled retrieval.

Args:
gold_spans: Set of tuples of (start, end).
span_starts:
span_ends:
span_scores:
pred_starts:
pred_ends:
text_length:
evaluators:
debugging: (Default value = False)

Returns:


, Print a labeled sentence into CoNLL format.

Args:
fout:
tokens:
labels:

Returns:


, Split document-based samples into sentence-based samples for evaluation.

Args:
example:

Returns:


",
https://github.com/hankcs/HanLP,util.py,0,0.0,9,90.0,0,0.0,0,0.0,1,10.0,0,0.0,1,0,4,0,,"append, block_orth_normal_initializer, i, input_size, o, output_size, param, torch, weight","torch.FloatTensor, torch.cat, torch.nn.init.orthogonal_",block_orth_normal_initializer,,"i, o, param, weight",,,,# Adopted from https://github.com/KiroSummer/A_Syntax-aware_MTL_Framework_for_Chinese_SRL,
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-19 22:22",
https://github.com/hankcs/HanLP,biaffine.py,0,0.0,18,36.0,9,18.0,2,4.0,21,42.0,0,0.0,4,1,3,2,,"Biaffine, __class__, __init__, __name__, __repr__, bias_x, bias_y, forward, n_in, n_out, reset_parameters, self, squeeze, super, torch, weight, x, y","nn.Module, nn.Parameter, nn.init.zeros_, torch.Tensor, torch.cat, torch.einsum, torch.ones_like","__init__, __repr__, forward, reset_parameters",Biaffine,"s, x, y","Args:
    x (torch.Tensor): ``[batch_size, seq_len, n_in]``.
    y (torch.Tensor): ``[batch_size, seq_len, n_in]``.

Returns:
    ~torch.Tensor:
        A scoring tensor of shape ``[batch_size, n_out, seq_len, seq_len]``.
        If ``n_out=1``, the dimension for ``n_out`` will be squeezed automatically., Biaffine layer for first-order scoring.

This function has a tensor of weights :math:`W` and bias terms if needed.
The score :math:`s(x, y)` of the vector pair :math:`(x, y)` is computed as :math:`x^T W y`,
in which :math:`x` and :math:`y` can be concatenated with bias terms.

References:
    - Timothy Dozat and Christopher D. Manning. 2017.
      `Deep Biaffine Attention for Neural Dependency Parsing`_.

Args:
    n_in (int):
        The size of the input feature.
    n_out (int):
        The number of output channels.
    bias_x (bool):
        If ``True``, adds a bias term for tensor :math:`x`. Default: ``True``.
    bias_y (bool):
        If ``True``, adds a bias term for tensor :math:`y`. Default: ``True``.

.. _Deep Biaffine Attention for Neural Dependency Parsing:
    https://openreview.net/forum?id=Hk95PK9le","
        Args:
            x (torch.Tensor): ``[batch_size, seq_len, n_in]``.
            y (torch.Tensor): ``[batch_size, seq_len, n_in]``.

        Returns:
            ~torch.Tensor:
                A scoring tensor of shape ``[batch_size, n_out, seq_len, seq_len]``.
                If ``n_out=1``, the dimension for ``n_out`` will be squeezed automatically.
        , 
    Biaffine layer for first-order scoring.

    This function has a tensor of weights :math:`W` and bias terms if needed.
    The score :math:`s(x, y)` of the vector pair :math:`(x, y)` is computed as :math:`x^T W y`,
    in which :math:`x` and :math:`y` can be concatenated with bias terms.

    References:
        - Timothy Dozat and Christopher D. Manning. 2017.
          `Deep Biaffine Attention for Neural Dependency Parsing`_.

    Args:
        n_in (int):
            The size of the input feature.
        n_out (int):
            The number of output channels.
        bias_x (bool):
            If ``True``, adds a bias term for tensor :math:`x`. Default: ``True``.
        bias_y (bool):
            If ``True``, adds a bias term for tensor :math:`y`. Default: ``True``.

    .. _Deep Biaffine Attention for Neural Dependency Parsing:
        https://openreview.net/forum?id=Hk95PK9le
    , (, ), , bias_x=, , bias_y=, , n_out=, bxi,oij,byj->boxy, n_in=","1, True","

def __init__(self, n_in, n_out=1, bias_x=True, bias_y=True):
super().__init__()

self.n_in = n_in
self.n_out = n_out
self.bias_x = bias_x
self.bias_y = bias_y
self.weight = nn.Parameter(torch.Tensor(n_out, n_in + bias_x, n_in + bias_y))

self.reset_parameters()

def __repr__(self):
s = f""n_in={self.n_in}, n_out={self.n_out}""
if self.bias_x:
s += f"", bias_x={self.bias_x}""
if self.bias_y:
s += f"", bias_y={self.bias_y}""

return f""{self.__class__.__name__}({s})""

def reset_parameters(self):
nn.init.zeros_(self.weight)

def forward(self, x, y):
r""""""
Args:
x (torch.Tensor): ``[batch_size, seq_len, n_in]``.
y (torch.Tensor): ``[batch_size, seq_len, n_in]``.

Returns:
~torch.Tensor:
A scoring tensor of shape ``[batch_size, n_out, seq_len, seq_len]``.
If ``n_out=1``, the dimension for ``n_out`` will be squeezed automatically.
, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # [batch_size, n_out, seq_len, seq_len], # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # remove dim 1 if n_out == 1, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,biaffine_2nd_dep.py,0,0.0,122,73.49,13,7.83,21,12.65,10,6.02,0,0.0,16,4,45,0,,"BiaffineJointDecoder, BiaffineSecondaryModel, BiaffineSecondaryParser, BiaffineSeparateDecoder, __init__, append, apply_mlps, arc_attn, arc_attn_2nd, arc_d, arc_h, arc_pred_1st, arc_pred_2nd, arc_preds, arc_scores, arc_scores_1st, arc_scores_2nd, batch, biaffine_decoder, biaffine_decoder_2nd, bos_transform, build_criterion, build_dataset, build_metric, build_vocabs, cell, collect_outputs_extend, compute_loss, compute_mask, config, convert_to_3d_mask, convert_to_3d_puncts, cpu, create_model, criterion, decode, decoder, encoder, enumerate, extend, feats, fit, float, forward, functools, get, get_pad_dict, graph, graphs, hanlp, hanlp_common, hidden_size, hrs, idx, input_ids, int, joint, len, lens, list, logger, loss_1st, loss_2st, mask, mask_1st, mlp_dropout, model, n_mlp_arc, n_mlp_rel, n_rels, n_rels_2nd, output_1st, output_2nd, outputs, pad_rel, pos, predicting, predictions, predictions_to_human, pretrained_embed, puncts, rel_2nd, rel_attn, rel_attn_2nd, rel_d, rel_h, rel_pred_1st, rel_pred_2nd, rel_preds, rel_scores, rel_scores_1st, rel_scores_2nd, rel_vocab, rels, root_rel_idx, s_arc, s_arc_2nd, s_rel, s_rel_2nd, self, sent, sent_len, staticmethod, super, token, token_span, tokens, torch, transformer_tokenizer, transformers, transpose, tree, trees, tuple, typing, unpack_scores, update, update_metric, use_pos, vocabs, words, zip","functools.partial, hanlp.common.transform.TransformList, hanlp.common.vocab.Vocab, hanlp.components.parsers.biaffine.biaffine.Biaffine, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDecoder, hanlp.components.parsers.biaffine.biaffine_model.EncoderWithContextualLayer, hanlp.components.parsers.biaffine.biaffine_sdp.BiaffineSemanticDependencyParser, hanlp.components.parsers.parse_alg.add_secondary_arcs_by_preds, hanlp.datasets.parsing.loaders.conll_dataset.append_bos, hanlp.datasets.parsing.semeval15.merge_head_deprel_with_2nd, hanlp.datasets.parsing.semeval15.unpack_deps_to_head_deprel, hanlp.metrics.mtl.MetricDict, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLUWord, hanlp_common.constant.UNK, hanlp_common.util.merge_locals_kwargs, torch.Tensor, torch.nn.Module, transformers.PreTrainedModel, transformers.PreTrainedTokenizer, typing.Any, typing.List, typing.Union","__init__, build_criterion, build_dataset, build_metric, build_vocabs, collect_outputs_extend, compute_loss, compute_mask, create_model, decode, fit, forward, get_pad_dict, predictions_to_human, unpack_scores, update_metric","BiaffineJointDecoder, BiaffineSecondaryModel, BiaffineSecondaryParser, BiaffineSeparateDecoder","arc_d, arc_h, arc_pred_1st, arc_pred_2nd, arc_preds, arc_scores, arc_scores_1st, arc_scores_2nd, cell, d, deprel, deps, graph, graphs, head, hrs, idx, loss_1st, loss_2st, mask, output_1st, output_2nd, pos, puncts, rel_2nd, rel_d, rel_h, rel_pred_1st, rel_pred_2nd, rel_preds, rel_scores_1st, rel_scores_2nd, rel_vocab, root_rel_idx, s_arc, s_arc_2nd, s_rel, s_rel_2nd, sent, sent_len, token, tokens, transform, tree, trees",,"1st, 2nd, No cycle constraint for evaluation is not implemented yet. If you are interested, welcome to submit a pull request., UPOS, arc_2nd, mask_2nd, no_cycle, outputs, rel, rel_2nd, rel_2nd_id, root, token","0, 0.002, 0.2, 0.33, 0.75, 0.9, 1, 100, 1e-12, 2, 3, 400, 5.0, 500, 5000, 50000, 512, 5e-05, False, None, True","#         pass, #     # Write back to torch Tensor, #     for d, hr in zip(graph):, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-06 13:57, # Remove root, # if not predicting:, # noinspection PyCallByClass, # the Biaffine layers for secondary dep",
https://github.com/hankcs/HanLP,biaffine_dep.py,0,0.0,207,69.46,52,17.45,24,8.05,15,5.03,0,0.0,35,1,76,1,,"BiaffineDependencyParser, CrossEntropyLoss, DataParallel, __init__, _report, _step, append, append_transform, arc_loss, arc_preds, arc_scores, arcs, arcs_per_sent, argmax, backward, batch, batch_max_tokens, batch_size, before_outputs, best_e, best_metric, bos_transform, build, build_criterion, build_data, build_dataloader, build_dataset, build_embeddings, build_metric, build_model, build_optimizer, build_samples, build_tokenizer_transform, build_transformer_tokenizer, build_vocabs, cache_dataset, cell, clear, clip_grad_norm_, clone, close, collect_outputs, collect_outputs_extend, collections, compute_loss, config, counter, create_model, criterion, current, dataloader, decay, decay_steps, decode, dev, dev_metric, device, devices, each, elapsed_average_human, elapsed_human, encoder, enumerate, epoch, epochs, epsilon, eta_human, eval, evaluate_dataloader, execute_training_loop, extend, feats, feed_batch, filename, fit, fit_dataloader, flat, float, fp, freq, freq_words, from_pretrained, gather, get, get_pad_dict, grad_norm, gradient_accumulation, hanlp, hanlp_common, history, idx, info, input_is_flat, int, isinstance, item, items, lens, list, load_weights, lock, log, logger, loss, lr, min, min_freq, model, module, mu, mutable, n_embed, n_feats, n_rels, n_words, nu, num_training_steps, on_config_ready, open, os, output, outputs, pad_index, pad_token, parameters, patience, pos, pos_key, post_outputs, predict, prediction_to_head_rel, predictions, predictions_to_human, pretrained_embed, proj, property, punct, puncts, purge_cache, put, range, ratio_width, rel_loss, rel_preds, rel_scores, rel_vocab, rels, rels_per_sent, report, requires_grad_, reserved_token, result, sample, sampler, sampler_builder, samples, save_dir, save_weights, score, self, sent, sent_len, separate_optimizer, seq, set_unk_as_safe_unk, shuffle, size, step, stop, summary, super, timer, token, token_to_idx, token_vocab, tokens, tolist, torch, total_loss, total_time_human, train, training, transform, transformer, transformer_lr, transformer_optimizer, transformer_scheduler, transformer_tokenizer, tree, trn, tuple, typing, unk, unk_index, unk_token, unsqueeze, update, update_metric, use_pos, utils, vocabs, warmup_steps, weight_decay, words, write, zero_grad, zip","collections.Counter, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.transform.LowerCase, hanlp.common.transform.PunctuationMask, hanlp.common.vocab.Vocab, hanlp.components.parsers.alg.decode_dep, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDependencyModel, hanlp.datasets.parsing.loaders.conll_dataset.CoNLLParsingDataset, hanlp.datasets.parsing.loaders.conll_dataset.append_bos, hanlp.layers.embeddings.util.index_word2vec_with_vocab, hanlp.layers.transformers.pt_imports.AutoModel_, hanlp.layers.transformers.pt_imports.AutoTokenizer_, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.metrics.parsing.attachmentscore.AttachmentScore, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.lengths_to_mask, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLWord, hanlp_common.constant.IDX, hanlp_common.constant.ROOT, hanlp_common.constant.UNK, hanlp_common.util.isdebugging, hanlp_common.util.merge_dict, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, os.path.basename, torch.LongTensor, torch.arange, torch.nn, torch.nn.Module, torch.no_grad, torch.optim.Adam, torch.optim.lr_scheduler.ExponentialLR, torch.utils.data.DataLoader, typing.Any, typing.List, typing.Union","__init__, _report, _step, before_outputs, build_criterion, build_dataloader, build_dataset, build_embeddings, build_metric, build_model, build_optimizer, build_samples, build_tokenizer_transform, build_transformer_tokenizer, build_vocabs, cache_dataset, collect_outputs, collect_outputs_extend, compute_loss, create_model, decode, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, get_pad_dict, input_is_flat, on_config_ready, post_outputs, predict, prediction_to_head_rel, predictions_to_human, update_metric, use_pos",BiaffineDependencyParser,"arc_loss, arc_preds, arc_scores, arcs, arcs_per_sent, batch, batch_max_tokens, batch_size, best_e, best_metric, bos_transform, build_data, cell, config, counter, criterion, data, dataloader, dataset, dev_metric, each, epoch, feats, flat, fp, freq, freq_words, history, idx, lens, loader, loss, mask, metric, model, num_training_steps, optimizer, outputs, pos, pos_key, predictions, pretrained_embed, proj, puncts, ratio_width, rel_loss, rel_preds, rel_scores, rel_vocab, rels, rels_per_sent, report, reserved_token, result, sample, sampler, samples, scheduler, sent, sent_len, seq, timer, token, token_vocab, tokens, total_loss, transform, transformer, transformer_optimizer, transformer_scheduler, transformer_tokenizer, tree, unk, use_pos, vocab, words","Biaffine dependency parsing (:cite:`dozat:17a`).
        ",", 

,  ,  (,  ([red]saved[/red]),  / ,  ETA: ,  at epoch ,  elapsed,  epochs, early stop., ), .2%, .4f, /, :[/yellow], Average time of each epoch is , Biaffine dependency parsing (:cite:`dozat:17a`).
        , CPOS, FORM, LAS has stopped improving for , Max score of dev is , Predictions saved in [underline][yellow], Preprocessing and caching samples [blink][yellow]...[/yellow][/blink], UPOS, [/yellow][/underline], [yellow]Epoch , arc, batch_max_tokens, feat, grad_norm, input_ids, loss: , lowercase, max_sequence_length, min_freq, pad_rel, pos, pos_id, pretrained_embed, proj, punct_mask, rel, rel_id, sent_length, token, token_id, transform, unk, use_pos, vocab building [blink][yellow]...[/yellow][/blink], w, zeros","0, 0.002, 0.1, 0.2, 0.33, 0.75, 0.9, 1, 100, 1000, 1e-08, 1e-12, 2, 3, 400, 5.0, 500, 5000, 50000, 512, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-08 20:51, # RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation, # Skip the ROOT, # Some relation in dev set is OOV, # We have to build tokenizer before building the dataloader and model, # ignore all punctuation if not specified, # ignore the first token of each sentence, # logger.info(f""{'Dev' + ' ' * ratio_width} loss: {loss:.4f} {dev_metric}""), # noinspection PyMethodOverriding, # noinspection PyUnboundLocalVariable, # save the model if it is the best so far, # train one epoch and update the parameters, Biaffine dependency parsing (:cite:`dozat:17a`).
",
https://github.com/hankcs/HanLP,biaffine_model.py,0,0.0,90,75.0,8,6.67,7,5.83,15,12.5,0,0.0,6,3,24,0,,"BiaffineDecoder, BiaffineDependencyModel, EncoderWithContextualLayer, __init__, apply_mlps, arc_attn, arc_d, arc_dropout, arc_h, average_subwords, batch_size, biaffine_decoder, config, decode, embed, embed_dropout, encoder, excludes, ext_mask, ext_words, feat, feat_embed, feats, float, forward, ge, get, hanlp, hasattr, hidden_dropout, hidden_size, input_ids, input_size, isinstance, lens, lstm, mask, masked_fill, masked_fill_, max_sequence_length, mlp_arc_d, mlp_arc_h, mlp_dropout, mlp_rel_d, mlp_rel_h, n_embed, n_feats, n_lstm_hidden, n_lstm_layers, n_mlp_arc, n_mlp_rel, n_rels, n_words, ne, num_embeddings, oov, pad_index, pretrained, pretrained_embed, rel_attn, rel_d, rel_dropout, rel_h, run_rnn, run_transformer, s_arc, s_rel, secondary_encoder, self, seq_len, shape, split, staticmethod, sum, super, token_span, tolist, torch, trans_embed, transformer, transformer_lr, transformer_tokenizer, typing, unk_index, unsqueeze, weight, word_dropout, word_embed, words, x","hanlp.components.parsers.biaffine.biaffine.Biaffine, hanlp.components.parsers.biaffine.mlp.MLP, hanlp.components.parsers.biaffine.variationalbilstm.VariationalLSTM, hanlp.layers.dropout.IndependentDropout, hanlp.layers.dropout.SharedDropout, hanlp.layers.dropout.WordDropout, hanlp.layers.transformers.encoder.TransformerEncoder, hanlp.layers.transformers.pt_imports.PreTrainedModel, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.layers.transformers.utils.transformer_encode, nn.Embedding, nn.Embedding.from_pretrained, nn.Module, nn.init.zeros_, torch.Tensor, torch.cat, torch.nn.utils.rnn.pack_padded_sequence, torch.nn.utils.rnn.pad_packed_sequence, torch.nn.utils.rnn.pad_sequence, typing.Any, typing.Tuple","__init__, apply_mlps, decode, forward, run_rnn, run_transformer","BiaffineDecoder, BiaffineDependencyModel, EncoderWithContextualLayer","arc_d, arc_h, batch_size, embed, excludes, ext_mask, ext_words, feat_embed, hidden_size, input_size, lens, mask, oov, pretrained_embed, rel_d, rel_h, s_arc, s_rel, seq_len, trans_embed, transformer, word_embed, words, x",,"-inf, bert, char, feat_embed, lstm, pos, pretrained, secondary_encoder","0, 1, 2, 3, False, None, True","# -*- coding: utf-8 -*-, # [batch_size, seq_len, seq_len, n_rels], # [batch_size, seq_len, seq_len], # apply MLPs to the hidden states, # concatenate the word and feat representations, # get arc and rel scores from the bilinear attention, # get outputs from embedding layers, # get the mask and lengths of given batch, # set the indices larger than num_embeddings to unk_index, # set the scores that exceed the length of each sentence to -inf, # the Biaffine layers, # the MLP layers, # the embedding layer, # the word-lstm layer, # trans_embed = self.run_transformer(input_ids, token_span=token_span)",
https://github.com/hankcs/HanLP,biaffine_sdp.py,0,0.0,96,70.07,15,10.95,23,16.79,3,2.19,0,0.0,14,1,34,1,,"BCEWithLogitsLoss, BiaffineSemanticDependencyParser, CrossEntropyLoss, __init__, all, append, apply_constraint, arc, arc_loss, arc_loss_interpolation, arc_preds, arc_scores, arc_scores_T, arc_scores_fix, arcs, argmax, batch, bce, build_criterion, build_dataset, build_metric, cache_dataset, cell, collect_outputs_extend, collections, compute_loss, config, convert_to_3d_mask, convert_to_3d_puncts, criterion, decode, deprels, device, each, enumerate, extend, eye, feed_batch, fit, functools, get, get_pad_dict, hanlp, hanlp_common, head_is_root, heads, idx, inf, info, int, len, list, log, logger, loss, mask, masks, most_common, no_zero_head, num_roots, outputs, pad_rel, predictions, predictions_to_human, puncts, range, rel_loss, rel_preds, rel_scores, rels, root_mask, root_rel, root_rel_id, root_rels, scatter_, self, sent, single_root, size, staticmethod, sum, super, timer, to, token, tolist, torch, training, transforms, transpose, typing, unsqueeze, use_pos, vocabs, x, zip","collections.Counter, functools.partial, hanlp.common.transform.TransformList, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.datasets.parsing.semeval15.append_bos_to_form_pos, hanlp.datasets.parsing.semeval15.unpack_deps_to_head_deprel, hanlp.metrics.parsing.labeled_f1.LabeledF1, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLUWord, hanlp_common.constant.UNK, hanlp_common.util.merge_locals_kwargs, torch.BoolTensor, torch.arange, torch.float, torch.nn, typing.List, typing.Union","__init__, build_criterion, build_dataset, build_metric, cache_dataset, collect_outputs_extend, compute_loss, convert_to_3d_mask, convert_to_3d_puncts, decode, feed_batch, fit, get_pad_dict, predictions_to_human",BiaffineSemanticDependencyParser,"arc, arc_loss, arc_loss_interpolation, arc_scores, arc_scores_T, arc_scores_fix, arcs, bce, cell, deprels, each, eye, head_is_root, heads, idx, inf, loss, mask, masks, no_zero_head, num_roots, puncts, rel, rel_loss, rel_scores, rels, root_mask, root_rel, root_rel_id, root_rels, sent, token, transforms, x","Implementation of ""Stanford's graph-based neural dependency parser at
the conll 2017 shared task"" (:cite:`dozat2017stanford`) and ""Establishing Strong Baselines for the New Decade""
(:cite:`he-choi-2019`).","DEPS, Implementation of ""Stanford's graph-based neural dependency parser at
        the conll 2017 shared task"" (:cite:`dozat2017stanford`) and ""Establishing Strong Baselines for the New Decade""
        (:cite:`he-choi-2019`).
        , Preprocessing and caching samples [blink][yellow]...[/yellow][/blink], Training set properties: [blue]single_root = , UPOS, [/blue], [/blue], [blue]no_zero_head = , [/blue], [blue]root_rel = , _, arc, arc_loss_interpolation, inf, no_zero_head, rel, single_root","0, 0.002, 0.1, 0.2, 0.33, 0.4, 0.75, 0.9, 1, 100, 1e-12, 2, 3, 400, 5.0, 500, 5000, 50000, 512, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-28 15:12",
https://github.com/hankcs/HanLP,mlp.py,0,0.0,20,40.82,7,14.29,3,6.12,19,38.78,0,0.0,4,1,2,2,,"MLP, __class__, __init__, __name__, __repr__, activation, bias, dropout, forward, hanlp, linear, n_in, n_out, p, reset_parameters, self, super, torch, weight, x","hanlp.layers.dropout.SharedDropout, nn.Identity, nn.LeakyReLU, nn.Linear, nn.Module, nn.init.orthogonal_, nn.init.zeros_","__init__, __repr__, forward, reset_parameters",MLP,"s, x","Applies a linear transformation together with a non-linear activation to the incoming tensor:
:math:`y = \mathrm{Activation}(x A^T + b)`

Args:
    n_in (~torch.Tensor):
        The size of each input feature.
    n_out (~torch.Tensor):
        The size of each output feature.
    dropout (float):
        If non-zero, introduce a :class:`SharedDropout` layer on the output with this dropout ratio. Default: 0.
    activation (bool):
        Whether to use activations. Default: True., Args:
    x (~torch.Tensor):
        The size of each input feature is `n_in`.

Returns:
    A tensor with the size of each output feature `n_out`.","
        Args:
            x (~torch.Tensor):
                The size of each input feature is `n_in`.

        Returns:
            A tensor with the size of each output feature `n_out`.
        , 
    Applies a linear transformation together with a non-linear activation to the incoming tensor:
    :math:`y = \mathrm{Activation}(x A^T + b)`

    Args:
        n_in (~torch.Tensor):
            The size of each input feature.
        n_out (~torch.Tensor):
            The size of each output feature.
        dropout (float):
            If non-zero, introduce a :class:`SharedDropout` layer on the output with this dropout ratio. Default: 0.
        activation (bool):
            Whether to use activations. Default: True.
    , (, ), , dropout=, , n_out=, n_in=","0, 0.1, True","

def __init__(self, n_in, n_out, dropout=0, activation=True):
super().__init__()

self.n_in = n_in
self.n_out = n_out
self.linear = nn.Linear(n_in, n_out)
self.activation = nn.LeakyReLU(negative_slope=0.1) if activation else nn.Identity()
self.dropout = SharedDropout(p=dropout)

self.reset_parameters()

def __repr__(self):
s = f""n_in={self.n_in}, n_out={self.n_out}""
if self.dropout.p > 0:
s += f"", dropout={self.dropout.p}""

return f""{self.__class__.__name__}({s})""

def reset_parameters(self):
nn.init.orthogonal_(self.linear.weight)
nn.init.zeros_(self.linear.bias)

def forward(self, x):
r""""""
Args:
x (~torch.Tensor):
The size of each input feature is `n_in`.

Returns:
A tensor with the size of each output feature `n_out`.
, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,structual_attention.py,0,0.0,110,70.97,17,10.97,20,12.9,8,5.16,0,0.0,12,4,36,0,,"Ax, AxW, GELU, Linear, MaskedTokenGenerator, Module, Parameter, StructuralAttentionLayer, StructuralAttentionModel, StructuralAttentionParser, __call__, __init__, _report, activation, arc_scores, arcs, attention_mask, average_subwords, batch, biaffine, build_metric, build_model, build_tokenizer_transform, build_transformer, clone, cls, cls_token_id, compute_loss, config, criterion, dense, dropout_mask, encoder, excludes, feats, feed_batch, fit, flatten, forward, gather, get, gold_input_ids, hanlp, hanlp_common, head_WV, hidden_size, input_ids, input_ids_mask, int, isinstance, layer_dropout, lens, loss, mask_prob, mask_token_id, max_sequence_length, mlm, mlm_generator, mlm_loss, mlm_metric, mlp_dropout, model, n_mlp_arc, n_mlp_rel, n_rels, new_empty, new_ones, object, on_config_ready, oov, oov_fill, oov_mask, p_arc, p_rel, pad, pad_token_id, padding_mask, parse_loss, parse_metric, pred_input_ids, prefix_mask, projeciton, projection, puncts, rel_scores, rels, result, s_arc, s_rel, sa, scalar_mix, self, sep, sep_token_id, size, super, token_span, token_type_ids, tokens, torch, training, transformer_hidden_dropout, transformer_tokenizer, tuple, typing, unsqueeze, update_metric, vocab_size, words, x","F.cross_entropy, F.softmax, hanlp.common.torch_component.TorchComponent, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDecoder, hanlp.layers.transformers.encoder.TransformerEncoder, hanlp.layers.transformers.pt_imports.PreTrainedModel, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.metrics.accuracy.CategoricalAccuracy, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.torch_util.lengths_to_mask, hanlp_common.util.merge_locals_kwargs, torch.LongTensor, torch.bool, torch.einsum, torch.float, torch.long, torch.nn, torch.nn.Module, torch.randn, torch.where, typing.List, typing.Union","__call__, __init__, _report, build_metric, build_model, build_tokenizer_transform, compute_loss, feed_batch, fit, forward, on_config_ready, update_metric","MaskedTokenGenerator, StructuralAttentionLayer, StructuralAttentionModel, StructuralAttentionParser","A, Ax, AxW, arc_scores, dropout_mask, feats, gold_input_ids, h, hidden_size, input_ids, input_ids_mask, lens, loss, mask, mlm_loss, mlm_metric, model, oov_fill, oov_mask, p_arc, p_rel, pad, padding_mask, parse_loss, parse_metric, pred_input_ids, prefix_mask, puncts, rel_scores, result, s_arc, s_rel, token_span, transformer, words, x",,",  , bihk,khm->bihk, bijk,bih->bihk, gold_input_ids, input_ids, input_ids_mask, kmeans, max_sequence_length, pos_id, pred_input_ids, prefix_mask, punct_mask, sent_length, token, token_id, token_span","0, 0.002, 0.15, 0.33, 0.75, 0.9, 1, 100, 1e-12, 2, 32, 5.0, 500, 5000, 50000, 512, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Create a uniformly random mask selecting either the original words or OOV tokens, # Date: 2020-06-26 10:40, # No word_dropout since SA is predicting masked tokens, # Only mask prefixes since the others won't be attended, # RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation, # ignore the first token of each sentence",
https://github.com/hankcs/HanLP,variationalbilstm.py,0,0.0,69,67.65,8,7.84,5,4.9,20,19.61,0,0.0,7,2,27,2,,"VariationalLSTM, VariationalLSTMEncoder, __class__, __init__, __name__, __repr__, append, b_cells, batch_size, batch_sizes, bidirectional, c_b, c_i, c_n, cell, data, dropout, embed, f_cells, forward, get_mask, get_output_dim, h_b, h_i, h_n, hanlp, hid_mask, hidden_size, hx, hx_0, hx_i, hx_n, ih, input_size, last_batch_size, layer_forward, len, locals, lstm_dropout, mask, new_zeros, num_directions, num_layers, output, param, parameters, permute_hidden, range, reset_parameters, reverse, reversed, self, seq_len, shape, sorted_indices, steps, sum, super, tolist, torch, training, unsorted_indices, variational_dropout, view, word_dropout, x, x_b, x_i, zip","hanlp.common.structure.ConfigTracker, hanlp.layers.dropout.SharedDropout, nn.LSTMCell, nn.Module, nn.ModuleList, nn.init.orthogonal_, nn.init.zeros_, torch.cat, torch.nn.modules.rnn.apply_permutation, torch.nn.utils.rnn.PackedSequence, torch.nn.utils.rnn.pack_padded_sequence, torch.nn.utils.rnn.pad_packed_sequence, torch.split, torch.stack","__init__, __repr__, forward, get_output_dim, layer_forward, permute_hidden, reset_parameters","VariationalLSTM, VariationalLSTMEncoder","batch_size, batch_sizes, c, c_b, c_i, c_n, h, h_b, h_i, h_n, hid_mask, hx, hx_0, hx_i, hx_n, ih, input_size, last_batch_size, mask, output, param, s, seq_len, steps, x, x_b, x_i","Args:
    sequence (~torch.nn.utils.rnn.PackedSequence):
        A packed variable length sequence.
    hx (~torch.Tensor, ~torch.Tensor):
        A tuple composed of two tensors `h` and `c`.
        `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial hidden state
        for each element in the batch.
        `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial cell state
        for each element in the batch.
        If `hx` is not provided, both `h` and `c` default to zero.
        Default: ``None``.

Returns:
    ~torch.nn.utils.rnn.PackedSequence, (~torch.Tensor, ~torch.Tensor):
        The first is a packed variable length sequence.
        The second is a tuple of tensors `h` and `c`.
        `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the hidden state for `t=seq_len`.
        Like output, the layers can be separated using ``h.view(num_layers, num_directions, batch_size, hidden_size)``
        and similarly for c.
        `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the cell state for `t=seq_len`., LSTM is an variant of the vanilla bidirectional LSTM adopted by Biaffine Parser
with the only difference of the dropout strategy.
It drops nodes in the LSTM layers (input and recurrent connections)
and applies the same dropout mask at every recurrent timesteps.

APIs are roughly the same as :class:`~torch.nn.LSTM` except that we only allows
:class:`~torch.nn.utils.rnn.PackedSequence` as input.

References:
    - Timothy Dozat and Christopher D. Manning. 2017.
      `Deep Biaffine Attention for Neural Dependency Parsing`_.

Args:
    input_size (int):
        The number of expected features in the input.
    hidden_size (int):
        The number of features in the hidden state `h`.
    num_layers (int):
        The number of recurrent layers. Default: 1.
    bidirectional (bool):
        If ``True``, becomes a bidirectional LSTM. Default: ``False``
    dropout (float):
        If non-zero, introduces a :class:`SharedDropout` layer on the outputs of each LSTM layer except the last layer.
        Default: 0.

.. _Deep Biaffine Attention for Neural Dependency Parsing:
    https://openreview.net/forum?id=Hk95PK9le","
        Args:
            sequence (~torch.nn.utils.rnn.PackedSequence):
                A packed variable length sequence.
            hx (~torch.Tensor, ~torch.Tensor):
                A tuple composed of two tensors `h` and `c`.
                `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial hidden state
                for each element in the batch.
                `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial cell state
                for each element in the batch.
                If `hx` is not provided, both `h` and `c` default to zero.
                Default: ``None``.

        Returns:
            ~torch.nn.utils.rnn.PackedSequence, (~torch.Tensor, ~torch.Tensor):
                The first is a packed variable length sequence.
                The second is a tuple of tensors `h` and `c`.
                `h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the hidden state for `t=seq_len`.
                Like output, the layers can be separated using ``h.view(num_layers, num_directions, batch_size, hidden_size)``
                and similarly for c.
                `c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the cell state for `t=seq_len`.
        , 
    LSTM is an variant of the vanilla bidirectional LSTM adopted by Biaffine Parser
    with the only difference of the dropout strategy.
    It drops nodes in the LSTM layers (input and recurrent connections)
    and applies the same dropout mask at every recurrent timesteps.

    APIs are roughly the same as :class:`~torch.nn.LSTM` except that we only allows
    :class:`~torch.nn.utils.rnn.PackedSequence` as input.

    References:
        - Timothy Dozat and Christopher D. Manning. 2017.
          `Deep Biaffine Attention for Neural Dependency Parsing`_.

    Args:
        input_size (int):
            The number of expected features in the input.
        hidden_size (int):
            The number of features in the hidden state `h`.
        num_layers (int):
            The number of recurrent layers. Default: 1.
        bidirectional (bool):
            If ``True``, becomes a bidirectional LSTM. Default: ``False``
        dropout (float):
            If non-zero, introduces a :class:`SharedDropout` layer on the outputs of each LSTM layer except the last layer.
            Default: 0.

    .. _Deep Biaffine Attention for Neural Dependency Parsing:
        https://openreview.net/forum?id=Hk95PK9le
    , (, ), , , , bidirectional=, , dropout=, , num_layers=","0, 1, False, None, True","

def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, dropout=0):
super().__init__()

self.input_size = input_size
self.hidden_size = hidden_size
self.num_layers = num_layers
self.bidirectional = bidirectional
self.dropout = dropout
self.num_directions = 1 + self.bidirectional

self.f_cells = nn.ModuleList()
if bidirectional:
self.b_cells = nn.ModuleList()
for _ in range(self.num_layers):
self.f_cells.append(nn.LSTMCell(input_size=input_size, hidden_size=hidden_size))
if bidirectional:
self.b_cells.append(nn.LSTMCell(input_size=input_size, hidden_size=hidden_size))
input_size = hidden_size * self.num_directions

self.reset_parameters()

def __repr__(self):
s = f""{self.input_size}, {self.hidden_size}""
if self.num_layers > 1:
s += f"", num_layers={self.num_layers}""
if self.bidirectional:
s += f"", bidirectional={self.bidirectional}""
if self.dropout > 0:
s += f"", dropout={self.dropout}""

return f""{self.__class__.__name__}({s})""

def reset_parameters(self):
for param in self.parameters():
# apply orthogonal_ to weight
if len(param.shape) > 1:
nn.init.orthogonal_(param)
# apply zeros_ to bias
else:
nn.init.zeros_(param)

def permute_hidden(self, hx, permutation):
if permutation is None:
return hx
h = apply_permutation(hx[0], permutation)
c = apply_permutation(hx[1], permutation)

return h, c

def layer_forward(self, x, hx, cell, batch_sizes, reverse=False):
hx_0 = hx_i = hx
hx_n, output = [], []
steps = reversed(range(len(x))) if reverse else range(len(x))
if self.training:
hid_mask = SharedDropout.get_mask(hx_0[0], self.dropout)

for t in steps:
last_batch_size, batch_size = len(hx_i[0]), batch_sizes[t]
if last_batch_size < batch_size:
hx_i = [torch.cat((h, ih[last_batch_size:batch_size])) for h, ih in zip(hx_i, hx_0)]
else:
hx_n.append([h[batch_size:] for h in hx_i])
hx_i = [h[:batch_size] for h in hx_i]
hx_i = [h for h in cell(x[t], hx_i)]
output.append(hx_i[0])
if self.training:
hx_i[0] = hx_i[0] * hid_mask[:batch_size]
if reverse:
hx_n = hx_i
output.reverse()
else:
hx_n.append(hx_i)
hx_n = [torch.cat(h) for h in zip(*reversed(hx_n))]
output = torch.cat(output)

return output, hx_n

def forward(self, sequence, hx=None):
r""""""
Args:
sequence (~torch.nn.utils.rnn.PackedSequence):
A packed variable length sequence.
hx (~torch.Tensor, ~torch.Tensor):
A tuple composed of two tensors `h` and `c`.
`h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial hidden state
for each element in the batch.
`c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the initial cell state
for each element in the batch.
If `hx` is not provided, both `h` and `c` default to zero.
Default: ``None``.

Returns:
~torch.nn.utils.rnn.PackedSequence, (~torch.Tensor, ~torch.Tensor):
The first is a packed variable length sequence.
The second is a tuple of tensors `h` and `c`.
`h` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the hidden state for `t=seq_len`.
Like output, the layers can be separated using ``h.view(num_layers, num_directions, batch_size, hidden_size)``
and similarly for c.
`c` of shape ``[num_layers*num_directions, batch_size, hidden_size]`` holds the cell state for `t=seq_len`.
, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # noinspection PyMethodOverriding, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-05-08 20:43",
https://github.com/hankcs/HanLP,alg.py,0,0.0,86,69.35,9,7.26,5,4.03,24,19.35,0,0.0,14,1,47,6,,"I, SCC, SCCs, Tarjan, _SCCs, __init__, _edges, _indices, _lowlinks, _onstack, _vertices, add, append, arange, assigned, biggest, centroids, change, changed_cycle, clusters, collections, cycle, dep, dependents, dims, dists, edges, ensure_tree, enumerate, farthest, head, index, indices, int, isinstance, k, kmeans, len, length, lens, list, mask, new_head, new_head_probs, new_heads, new_rel_preds, new_rel_probs, new_root, new_root_probs, node, non_heads, nonzero, numpy, old, old_head, old_head_probs, old_heads, parse_preds, parse_probs, pop, prediction, property, randperm, rel_argmax, rel_preds, rel_probs, root, root_probs, roots, seed, self, set, strongconnect, tarjan, tensorflow, to_visit, tokens, tokens_to_keep, tolist, total, typing, update, v, vertices, view, x","collections.defaultdict, np.arange, np.argmax, np.argmin, np.array, np.eye, np.repeat, np.where, tf.Tensor, tf.abs, tf.argmax, tf.argmin, tf.cast, tf.constant, tf.expand_dims, tf.float32, tf.gather, tf.gather_nd, tf.int32, tf.random.shuffle, tf.range, tf.reduce_all, tf.reduce_any, tf.reduce_sum, tf.reshape, tf.shape, tf.squeeze, tf.stack, tf.tensor_scatter_nd_update, tf.transpose, tf.unique, tf.unique_with_counts, tf.where, typing.List","SCCs, __init__, arange, edges, indices, kmeans, nonzero, randperm, rel_argmax, strongconnect, tarjan, tolist, vertices, view",Tarjan,"I, SCC, assigned, biggest, centroids, change, changed_cycle, clusters, cycle, dep, dependents, dists, f, farthest, head, index, indices, lens, mask, new_head, new_head_probs, new_heads, new_rel_preds, new_rel_probs, new_root, new_root_probs, node, non_heads, old, old_head, old_head_probs, old_heads, parse_preds, parse_probs, rel_preds, root_probs, roots, stack, t, tarjan, to_visit, tokens, total, v, w, x, y","Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

Args:
  parse_probs(NDArray): seq_len x seq_len, the probability of arcs
  length(NDArray): sentence length including ROOT
  tokens_to_keep(NDArray): mask matrix
  ensure_tree:  (Default value = True)

Returns:, Args:
  v: 
  index: 
  stack: 

Returns:, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, Fix the relation prediction by heuristic rules

Args:
  rel_probs(NDArray): seq_len x rel_size
  length: real sentence length
  ensure_tree:  (Default value = True)
  root: 

Returns:, Parameters
----------
prediction : numpy.ndarray
    a predicted dependency tree where prediction[dep_idx] = head_idx
tokens : numpy.ndarray
    the tokens we care about (i.e. exclude _GO, _EOS, and _PAD), See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

Args:
  x(list): Lengths of sentences
  k(int): 
  seed:  (Default value = None)

Returns:","

        Args:
          v: 
          index: 
          stack: 

        Returns:

        , 

        Parameters
        ----------
        prediction : numpy.ndarray
            a predicted dependency tree where prediction[dep_idx] = head_idx
        tokens : numpy.ndarray
            the tokens we care about (i.e. exclude _GO, _EOS, and _PAD)
        ,  clusters,  datapoints to , Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py

    Args:
      parse_probs(NDArray): seq_len x seq_len, the probability of arcs
      length(NDArray): sentence length including ROOT
      tokens_to_keep(NDArray): mask matrix
      ensure_tree:  (Default value = True)

    Returns:

    
    , Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph, Fix the relation prediction by heuristic rules

    Args:
      rel_probs(NDArray): seq_len x rel_size
      length: real sentence length
      ensure_tree:  (Default value = True)
      root: 

    Returns:

    
    , See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

    Args:
      x(list): Lengths of sentences
      k(int): 
      seed:  (Default value = None)

    Returns:

    
    , unable to assign ","0, 1, False, None, True","

self._indices[v] = index
self._lowlinks[v] = index
index += 1
stack.append(v)
self._onstack[v] = True
for w in self.edges[v]:
if w not in self.indices:
self.strongconnect(w, index, stack)
self._lowlinks[v] = min(self._lowlinks[v], self._lowlinks[w])
elif self._onstack[w]:
self._lowlinks[v] = min(self._lowlinks[v], self._indices[w])

if self._lowlinks[v] == self._indices[v]:
self._SCCs.append(set())
while stack[-1] != v:
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
w = stack.pop()
self._onstack[w] = False
self._SCCs[-1].add(w)
return

# ======================
@property
def edges(self):
return self._edges

@property
def vertices(self):
return self._vertices

@property
def indices(self):
return self._indices

@property
def SCCs(self):
return self._SCCs


def tarjan(parse_probs, length, tokens_to_keep, ensure_tree=True):
Adopted from Timothy Dozat https://github.com/tdozat/Parser/blob/master/lib/models/nn.py, 
if ensure_tree:
I = np.eye(len(tokens_to_keep))
# block loops and pad heads
parse_probs = parse_probs * tokens_to_keep * (1 - I)
parse_preds = np.argmax(parse_probs, axis=1)
tokens = np.arange(1, length)
roots = np.where(parse_preds[tokens] == 0)[0] + 1
# ensure at least one root
if len(roots) < 1:
# The current root probabilities
root_probs = parse_probs[tokens, 0]
# The current head probabilities
old_head_probs = parse_probs[tokens, parse_preds[tokens]]
# Get new potential root probabilities
new_root_probs = root_probs / old_head_probs
# Select the most probable root
new_root = tokens[np.argmax(new_root_probs)]
# Make the change
parse_preds[new_root] = 0
# ensure at most one root
elif len(roots) > 1:
# The probabilities of the current heads
root_probs = parse_probs[roots, 0]
# Set the probability of depending on the root zero
parse_probs[roots, 0] = 0
# Get new potential heads and their probabilities
new_heads = np.argmax(parse_probs[roots][:, tokens], axis=1) + 1
new_head_probs = parse_probs[roots, new_heads] / root_probs
# Select the most probable root
new_root = roots[np.argmin(new_head_probs)]
# Make the change
parse_preds[roots] = new_heads
parse_preds[new_root] = 0
# remove cycles
tarjan = Tarjan(parse_preds, tokens)
for SCC in tarjan.SCCs:
if len(SCC) > 1:
dependents = set()
to_visit = set(SCC)
while len(to_visit) > 0:
node = to_visit.pop()
if not node in dependents:
dependents.add(node)
to_visit.update(tarjan.edges[node])
# The indices of the nodes that participate in the cycle
cycle = np.array(list(SCC))
# The probabilities of the current heads
old_heads = parse_preds[cycle]
old_head_probs = parse_probs[cycle, old_heads]
# Set the probability of depending on a non-head to zero
non_heads = np.array(list(dependents))
parse_probs[np.repeat(cycle, len(non_heads)), np.repeat([non_heads], len(cycle), axis=0).flatten()] = 0
# Get new potential heads and their probabilities
new_heads = np.argmax(parse_probs[cycle][:, tokens], axis=1) + 1
new_head_probs = parse_probs[cycle, new_heads] / old_head_probs
# Select the most probable change
change = np.argmax(new_head_probs)
changed_cycle = cycle[change]
old_head = old_heads[change]
new_head = new_heads[change]
# Make the change
parse_preds[changed_cycle] = new_head
tarjan.edges[new_head].add(changed_cycle)
tarjan.edges[old_head].remove(changed_cycle)
return parse_preds
else:
# block and pad heads
parse_probs = parse_probs * tokens_to_keep
parse_preds = np.argmax(parse_probs, axis=1)
return parse_preds


def rel_argmax(rel_probs, length, root, ensure_tree=True):
Fix the relation prediction by heuristic rules, 
self._edges = defaultdict(set)
self._vertices = set((0,))
for dep, head in enumerate(prediction[tokens]):
self._vertices.add(dep + 1)
self._edges[head].add(dep + 1)
self._indices = {}
self._lowlinks = {}
self._onstack = defaultdict(lambda: False)
self._SCCs = []

index = 0
stack = []
for v in self.vertices:
if v not in self.indices:
self.strongconnect(v, index, stack)

# =============================================================
def strongconnect(self, v, index, stack):
, # ***************************************************************, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 19:49, # Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser, # and move that the empty one, # assign all datapoints to the new-generated clusters, # assign labels to each datapoint based on centroids, # calculate the sum of the values of the same datapoints, # choose the farthest datapoint from the biggest cluster, # count the frequency of each datapoint, # get the centroids of the assigned clusters, # if an empty cluster is encountered,, # initialize k centroids randomly, # make sure number of datapoints is greater than that of clusters, # map all values of datapoints to buckets, # re-assign all datapoints to clusters, # update the centroids, # without considering the empty ones, Computes Tarjan's algorithm for finding strongly connected components (cycles) of a graph

def __init__(self, prediction, tokens):
, See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

Args:
x(list): Lengths of sentences
k(int):
seed:  (Default value = None)

Returns:


",
https://github.com/hankcs/HanLP,layers.py,0,0.0,41,59.42,12,17.39,8,11.59,8,11.59,0,0.0,5,4,9,2,,"Biaffine, IndependentDropout, MLP, SharedDropout, __init__, activation, add_weight, batch_first, bias_x, bias_y, build, call, dropout, dtype, dynamic, extra_repr, get_mask, hanlp, inputs, item, kwargs, len, linear, mask, masks, n_hidden, n_in, n_out, name, p, scale, self, staticmethod, super, tensorflow, total, trainable, training, weight, x, zip","hanlp.utils.tf_util.tf_bernoulli, tf.concat, tf.einsum, tf.expand_dims, tf.keras.layers.Dense, tf.keras.layers.Layer, tf.keras.layers.LeakyReLU, tf.ones_like, tf.reduce_max, tf.shape, tf.squeeze","__init__, build, call, extra_repr, get_mask","Biaffine, IndependentDropout, MLP, SharedDropout","inputs, item, mask, masks, s, scale, total, x, y","Dropout on the first two dimensions, Dropout on timesteps with bernoulli distribution",", batch_first=, , bias_x=, , bias_y=, , n_out=, Dropout on the first two dimensions, Dropout on timesteps with bernoulli distribution, bxi,oij,byj->boxy, kernel, n_in=, orthogonal, p=, zero","0, 0.1, 0.5, 1, 2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 23:05, # Ported from the PyTorch implementation https://github.com/zysite/biaffine-parser, # [batch_size, n_out, seq_len, seq_len], # noinspection PyMethodOverriding, # remove dim 1 if n_out == 1, Dropout on timesteps with bernoulli distribution
super().__init__(trainable, name, dtype, dynamic, **kwargs)
self.p = p
self.batch_first = batch_first

def extra_repr(self):
s = f""p={self.p}""
if self.batch_first:
s += f"", batch_first={self.batch_first}""

return s

def call(self, x, training=None, **kwargs):
if training and self.p > 0:
if self.batch_first:
mask = self.get_mask(x[:, 0], self.p)
else:
mask = self.get_mask(x[0], self.p)
x *= tf.expand_dims(mask, axis=1) if self.batch_first else mask

return x

@staticmethod
def get_mask(x, p):
mask = tf_bernoulli(tf.shape(x), 1 - p, x.dtype)
mask = mask / (1 - p)

return mask


class IndependentDropout(tf.keras.layers.Layer):

def __init__(self, p=0.5, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):
Dropout on the first two dimensions",
https://github.com/hankcs/HanLP,model.py,0,0.0,70,55.12,23,18.11,9,7.09,22,17.32,3,2.36,4,1,26,1,,"BiaffineModelTF, __init__, add, add_weight, arc_attn, arc_d, arc_h, batch_size, call, config, d_positional, embed, embed_dropout, ext_mask, ext_words, feat_embed, feats, float, get, hanlp, hasattr, input_dim, input_ids, input_mask, inputs, isinstance, lstm, lstm_dropout, mask, mask_inf, max_seq_length, mlp_arc_d, mlp_arc_h, mlp_dropout, mlp_rel_d, mlp_rel_h, n_embed, n_feats, n_lstm_hidden, n_lstm_layers, n_mlp_arc, n_mlp_rel, n_rels, n_words, normal, pad_index, position_table, prefix_offset, pretrained, range, rel_attn, rel_d, rel_h, run_transformer, s_arc, s_rel, self, seq_length, sequence_output, shape, super, tensorflow, timing_signal, to_functional, transformer, transformer_dropout, unk_index, word_embed, words, x","hanlp.components.parsers.biaffine_tf.layers.Biaffine, hanlp.components.parsers.biaffine_tf.layers.IndependentDropout, hanlp.components.parsers.biaffine_tf.layers.MLP, hanlp.components.parsers.biaffine_tf.layers.SharedDropout, hanlp.layers.transformers.tf_imports.TFPreTrainedModel, tf.broadcast_to, tf.concat, tf.constant, tf.expand_dims, tf.gather, tf.greater_equal, tf.int64, tf.keras.Input, tf.keras.Model, tf.keras.initializers.RandomNormal, tf.keras.initializers.zeros, tf.keras.layers.Bidirectional, tf.keras.layers.Embedding, tf.keras.layers.LSTM, tf.keras.models.Sequential, tf.not_equal, tf.shape, tf.transpose, tf.where, tf.zeros_like","__init__, call, run_transformer, to_functional",BiaffineModelTF,"arc_d, arc_h, batch_size, d_positional, embed, ext_mask, ext_words, feat_embed, feats, input_ids, input_mask, mask, max_seq_length, normal, prefix_offset, rel_d, rel_h, s_arc, s_rel, seq_length, sequence_output, timing_signal, transformer_dropout, word_embed, words, x","An implementation of T. Dozat and C. D. Manning, “Deep Biaffine Attention for Neural Dependency Parsing.,” ICLR, 2017.
    Although I have my MXNet implementation, I found zysite's PyTorch implementation is cleaner so I port it to TensorFlow

Args:
  config: param embed:

Returns:","-inf, An implementation of T. Dozat and C. D. Manning, “Deep Biaffine Attention for Neural Dependency Parsing.,” ICLR, 2017.
            Although I have my MXNet implementation, I found zysite's PyTorch implementation is cleaner so I port it to TensorFlow

        Args:
          config: param embed:

        Returns:

        , Either pre-trained word embed and transformer is supported, but not both, arc_attn, d_positional, embed_dropout, feat_embed, feats, lstm, lstm_dropout, max_seq_length, mlp_arc_d, mlp_arc_h, mlp_rel_d, mlp_rel_h, orthogonal, position_table, pretrained, random_normal, rel_attn, transformer_dropout, word_embed, words","0, 1, 1.0, 2, 256, 3, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 23:04, # [batch_size, seq_len, seq_len, n_rels], # [batch_size, seq_len, seq_len], # apply MLPs to the BiLSTM output states, # batch_size, seq_len = words.shape, # concatenate the word and feat representations, # ext_mask = words.ge(self.word_embed.num_embeddings), # get arc and rel scores from the bilinear attention, # get outputs from embedding layers, # get the mask and lengths of given batch, # mask = words.ne(self.pad_index), # noinspection PyMethodOverriding, # set the indices larger than num_embeddings to unk_index, # set the scores that exceed the length of each sentence to -inf, # the Biaffine layers, # the MLP layers, # the embedding layer, # the word-lstm layer, # turns out to hinder performance, An implementation of T. Dozat and C. D. Manning, “Deep Biaffine Attention for Neural Dependency Parsing.,” ICLR, 2017.
Although I have my MXNet implementation, I found zysite's PyTorch implementation is cleaner so I port it to TensorFlow

Args:
config: param embed:

Returns:

","An implementation of T. Dozat and C. D. Manning, “Deep Biaffine Attention for Neural Dependency Parsing.,” ICLR, 2017.
Although I have my MXNet implementation, I found zysite's PyTorch implementation is cleaner so I port it to TensorFlow

Args:
config: param embed:

Returns:

, “Deep, ”"
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2019-12-26 23:03",
https://github.com/hankcs/HanLP,crf_constituency_model.py,0,0.0,53,56.38,5,5.32,11,11.7,25,26.6,0,0.0,4,2,19,5,,"CRFConstituencyDecoder, CRFConstituencyModel, CrossEntropyLoss, Module, __init__, argmax, batch, charts, chunk, crf, criterion, decode, decoder, encoder, forward, ge, hanlp, j, label_attn, label_l, label_loss, label_preds, label_r, labels, loss, mask, mbr, mlp_dropout, mlp_label_l, mlp_label_r, mlp_span_l, mlp_span_r, n_hidden, n_labels, n_mlp_label, n_mlp_span, s_label, s_span, self, span_attn, span_l, span_loss, span_mask, span_preds, span_probs, span_r, spans, super, torch, x, x_b, x_f, zip","hanlp.components.parsers.alg.cky, hanlp.components.parsers.biaffine.biaffine.Biaffine, hanlp.components.parsers.biaffine.mlp.MLP, hanlp.components.parsers.constituency.treecrf.CRFConstituency, torch.cat, torch.nn","__init__, decode, forward, loss","CRFConstituencyDecoder, CRFConstituencyModel","j, label_l, label_loss, label_preds, label_r, labels, loss, s_label, s_span, span_l, span_loss, span_mask, span_preds, span_probs, span_r, spans, x, x_b, x_f","Args:
    batch (~dict):
        Batch of input data.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.
        The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds
        scores of all possible labels on each span., Args:
    s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all spans
    s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
        Scores of all labels on each span.
    charts (~torch.LongTensor): ``[batch_size, seq_len, seq_len]``.
        The tensor of gold-standard labels, in which positions without labels are filled with -1.
    mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
        The mask for covering the unpadded tokens in each chart.
    mbr (bool):
        If ``True``, returns marginals for MBR decoding. Default: ``True``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The training loss and
        original span scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise., Args:
    s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all spans.
    s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
        Scores of all labels on each span.
    mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
        The mask for covering the unpadded tokens in each chart.

Returns:
    list[list[tuple]]:
        Sequences of factorized labeled trees traversed in pre-order., Args:
    x (~torch.FloatTensor): ``[batch_size, seq_len, hidden_dim]``.
        Hidden states from encoder.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.
        The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds
        scores of all possible labels on each span., The implementation of CRF Constituency Parser,
also called FANCY (abbr. of Fast and Accurate Neural Crf constituencY) Parser.

References:
    - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.
      `Fast and Accurate Neural CRF Constituency Parsing`_.

Args:
    n_words (int):
        The size of the word vocabulary.
    n_feats (int):
        The size of the feat vocabulary.
    n_labels (int):
        The number of labels.
    feat (str):
        Specifies which type of additional feature to use: ``'char'`` | ``'bert'`` | ``'tag'``.
        ``'char'``: Character-level representations extracted by CharLSTM.
        ``'bert'``: BERT representations, other pretrained langugae models like XLNet are also feasible.
        ``'tag'``: POS tag embeddings.
        Default: 'char'.
    n_embed (int):
        The size of word embeddings. Default: 100.
    n_feat_embed (int):
        The size of feature representations. Default: 100.
    n_char_embed (int):
        The size of character embeddings serving as inputs of CharLSTM, required if ``feat='char'``. Default: 50.
    bert (str):
        Specifies which kind of language model to use, e.g., ``'bert-base-cased'`` and ``'xlnet-base-cased'``.
        This is required if ``feat='bert'``. The full list can be found in `transformers`.
        Default: ``None``.
    n_bert_layers (int):
        Specifies how many last layers to use. Required if ``feat='bert'``.
        The final outputs would be the weight sum of the hidden states of these layers.
        Default: 4.
    mix_dropout (float):
        The dropout ratio of BERT layers. Required if ``feat='bert'``. Default: .0.
    embed_dropout (float):
        The dropout ratio of input embeddings. Default: .33.
    n_hidden (int):
        The size of LSTM hidden states. Default: 400.
    n_lstm_layers (int):
        The number of LSTM layers. Default: 3.
    lstm_dropout (float):
        The dropout ratio of LSTM. Default: .33.
    n_mlp_span (int):
        Span MLP size. Default: 500.
    n_mlp_label  (int):
        Label MLP size. Default: 100.
    mlp_dropout (float):
        The dropout ratio of MLP layers. Default: .33.
    feat_pad_index (int):
        The index of the padding token in the feat vocabulary. Default: 0.
    pad_index (int):
        The index of the padding token in the word vocabulary. Default: 0.
    unk_index (int):
        The index of the unknown token in the word vocabulary. Default: 1.

.. _Fast and Accurate Neural CRF Constituency Parsing:
    https://www.ijcai.org/Proceedings/2020/560/
.. _transformers:
    https://github.com/huggingface/transformers","
        Args:
            batch (~dict):
                Batch of input data.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.
                The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds
                scores of all possible labels on each span.
        , 
        Args:
            s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
                Scores of all spans
            s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
                Scores of all labels on each span.
            charts (~torch.LongTensor): ``[batch_size, seq_len, seq_len]``.
                The tensor of gold-standard labels, in which positions without labels are filled with -1.
            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
                The mask for covering the unpadded tokens in each chart.
            mbr (bool):
                If ``True``, returns marginals for MBR decoding. Default: ``True``.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The training loss and
                original span scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise.
        , 
        Args:
            s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
                Scores of all spans.
            s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
                Scores of all labels on each span.
            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
                The mask for covering the unpadded tokens in each chart.

        Returns:
            list[list[tuple]]:
                Sequences of factorized labeled trees traversed in pre-order.
        , 
        Args:
            x (~torch.FloatTensor): ``[batch_size, seq_len, hidden_dim]``.
                Hidden states from encoder.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.
                The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds
                scores of all possible labels on each span.
        , 
    The implementation of CRF Constituency Parser,
    also called FANCY (abbr. of Fast and Accurate Neural Crf constituencY) Parser.

    References:
        - Yu Zhang, Houquan Zhou and Zhenghua Li. 2020.
          `Fast and Accurate Neural CRF Constituency Parsing`_.

    Args:
        n_words (int):
            The size of the word vocabulary.
        n_feats (int):
            The size of the feat vocabulary.
        n_labels (int):
            The number of labels.
        feat (str):
            Specifies which type of additional feature to use: ``'char'`` | ``'bert'`` | ``'tag'``.
            ``'char'``: Character-level representations extracted by CharLSTM.
            ``'bert'``: BERT representations, other pretrained langugae models like XLNet are also feasible.
            ``'tag'``: POS tag embeddings.
            Default: 'char'.
        n_embed (int):
            The size of word embeddings. Default: 100.
        n_feat_embed (int):
            The size of feature representations. Default: 100.
        n_char_embed (int):
            The size of character embeddings serving as inputs of CharLSTM, required if ``feat='char'``. Default: 50.
        bert (str):
            Specifies which kind of language model to use, e.g., ``'bert-base-cased'`` and ``'xlnet-base-cased'``.
            This is required if ``feat='bert'``. The full list can be found in `transformers`.
            Default: ``None``.
        n_bert_layers (int):
            Specifies how many last layers to use. Required if ``feat='bert'``.
            The final outputs would be the weight sum of the hidden states of these layers.
            Default: 4.
        mix_dropout (float):
            The dropout ratio of BERT layers. Required if ``feat='bert'``. Default: .0.
        embed_dropout (float):
            The dropout ratio of input embeddings. Default: .33.
        n_hidden (int):
            The size of LSTM hidden states. Default: 400.
        n_lstm_layers (int):
            The number of LSTM layers. Default: 3.
        lstm_dropout (float):
            The dropout ratio of LSTM. Default: .33.
        n_mlp_span (int):
            Span MLP size. Default: 500.
        n_mlp_label  (int):
            Label MLP size. Default: 100.
        mlp_dropout (float):
            The dropout ratio of MLP layers. Default: .33.
        feat_pad_index (int):
            The index of the padding token in the feat vocabulary. Default: 0.
        pad_index (int):
            The index of the padding token in the word vocabulary. Default: 0.
        unk_index (int):
            The index of the unknown token in the word vocabulary. Default: 1.

    .. _Fast and Accurate Neural CRF Constituency Parsing:
        https://www.ijcai.org/Proceedings/2020/560/
    .. _transformers:
        https://github.com/huggingface/transformers
    ","0, 0.33, 1, 100, 2, 3, 400, 500, False, None, True","

def __init__(self,
n_labels,
n_hidden=400,
n_mlp_span=500,
n_mlp_label=100,
mlp_dropout=.33,
**kwargs
):
super().__init__()

# the MLP layers
self.mlp_span_l = MLP(n_in=n_hidden, n_out=n_mlp_span, dropout=mlp_dropout)
self.mlp_span_r = MLP(n_in=n_hidden, n_out=n_mlp_span, dropout=mlp_dropout)
self.mlp_label_l = MLP(n_in=n_hidden, n_out=n_mlp_label, dropout=mlp_dropout)
self.mlp_label_r = MLP(n_in=n_hidden, n_out=n_mlp_label, dropout=mlp_dropout)

# the Biaffine layers
self.span_attn = Biaffine(n_in=n_mlp_span, bias_x=True, bias_y=False)
self.label_attn = Biaffine(n_in=n_mlp_label, n_out=n_labels, bias_x=True, bias_y=True)
self.crf = CRFConstituency()
self.criterion = nn.CrossEntropyLoss()

def forward(self, x, **kwargs):
r""""""
Args:
x (~torch.FloatTensor): ``[batch_size, seq_len, hidden_dim]``.
Hidden states from encoder.

Returns:
~torch.Tensor, ~torch.Tensor:
The first tensor of shape ``[batch_size, seq_len, seq_len]`` holds scores of all possible spans.
The second of shape ``[batch_size, seq_len, seq_len, n_labels]`` holds
scores of all possible labels on each span.
, 

span_mask = charts.ge(0) & mask
span_loss, span_probs = self.crf(s_span, mask, span_mask, mbr)
label_loss = self.criterion(s_label[span_mask], charts[span_mask])
loss = span_loss + label_loss

return loss, span_probs

def decode(self, s_span, s_label, mask):
r""""""
Args:
s_span (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
Scores of all spans.
s_label (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
Scores of all labels on each span.
mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
The mask for covering the unpadded tokens in each chart.

Returns:
list[list[tuple]]:
Sequences of factorized labeled trees traversed in pre-order.
, #, # -*- coding:utf-8 -*-, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Adopted from https://github.com/yzhangcs/parser, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # [batch_size, seq_len, seq_len, n_labels], # [batch_size, seq_len, seq_len], # apply MLPs to the BiLSTM output states, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,crf_constituency_parser.py,0,0.0,145,68.4,39,18.4,16,7.55,12,5.66,0,0.0,21,1,52,1,,"CRFConstituencyParser, __init__, _step, _transform, any, backward, batch, batch_size, best_epoch, best_metric, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, chart, chart_preds, close, compute_loss, compute_mask, config, crf, crf_decoder, criterion, dataloader, decode, decode_output, decoder, delete, dev, dev_metric, device, each, elapsed_average_human, elapsed_human, encoder, enumerate, epoch, epochs, equal, eta_human, eval, eval_trn, evaluate_dataloader, execute_training_loop, extend, feed_batch, fit, fit_dataloader, flat, float, get, get_output_dim, gold, gradient_accumulation, hanlp, hanlp_common, height, history, idx, idx_to_token, info, input_is_flat, insert, int, isinstance, item, j, label, len, lens, load_weights, lock, log, logger, logging, loss, mask, max, max_seq_len, mbr, model, module, mutable, new_ones, new_tensor, num_training_steps, offset, optimizer, orders, out, output, outputs, patience, phrasetree, pos, pred, predict, prediction, range, ratio_width, report, reset, s_label, s_span, sampler, sampler_builder, samples, save_dir, save_weights, scheduler, self, seq_len, set_label, set_unk_as_safe_unk, shuffle, span_probs, step, subtrees, summary, super, timer, token, tokens, torch, total_loss, total_time_human, train, training, transform, trees, trn, tuple, typing, update, update_metrics, view, vocabs, x, zero_grad, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.dataset.TransformableDataset, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.transform.TransformList, hanlp.common.vocab.VocabWithNone, hanlp.components.classifiers.transformer_classifier.TransformerComponent, hanlp.components.parsers.constituency.crf_constituency_model.CRFConstituencyDecoder, hanlp.components.parsers.constituency.crf_constituency_model.CRFConstituencyModel, hanlp.datasets.parsing.loaders.constituency_dataset.ConstituencyDataset, hanlp.datasets.parsing.loaders.constituency_dataset.build_tree, hanlp.datasets.parsing.loaders.constituency_dataset.factorize, hanlp.datasets.parsing.loaders.constituency_dataset.remove_subcategory, hanlp.datasets.parsing.loaders.constituency_dataset.unpack_tree_to_features, hanlp.metrics.parsing.span.SpanMetric, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.clip_grad_norm, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.constant.IDX, hanlp_common.util.merge_dict, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, phrasetree.tree.Tree, torch.nn.Module, torch.no_grad, torch.utils.data.DataLoader, typing.List, typing.Union","__init__, _step, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, compute_loss, compute_mask, decode_output, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, input_is_flat, predict, update_metrics",CRFConstituencyParser,"_transform, batch, best_epoch, best_metric, chart, chart_preds, crf_decoder, data, dataloader, dataset, decoder, dev_metric, each, encoder, epoch, equal, flat, gold, history, idx, idx_to_token, j, label, lens, loss, mask, max_seq_len, metric, optimizer, orders, out, outputs, patience, pos, pred, prediction, report, s_label, s_span, sampler, samples, scheduler, seq_len, span_probs, timer, token, tokens, total_loss, transform, trees, x, y","Two-stage CRF Parsing (:cite:`ijcai2020-560`).

Args:
    **kwargs: Predefined config.",",  ,  (,  / ,  ETA: ,  [red](saved)[/red],  at epoch ,  early stop,  elapsed, !, '', ), ,, -NONE-, ., .4f, :, :[/yellow], ?, ADVP, Average time of each epoch is , Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: , Max score of dev is , PRT, S1, TOP, Two-stage CRF Parsing (:cite:`ijcai2020-560`).

        Args:
            **kwargs: Predefined config.
        , [yellow]Epoch , _, ``, chart_id, constituency, loss: , no_subcategory, token, token_, token_input_ids, token_length, transform","0, 0.1, 0.33, 0.5, 1, 1.0, 100, 1e-08, 2, 30, 500, 5000, 5e-05, False, None, True","# -*- coding:utf-8 -*-, # Add pre-terminals (pos tags) back to prediction for safe factorization (deletion based on pos), # Author: hankcs, # Date: 2020-11-28 21:24, # Use the original tokens if any, # noinspection PyCallByClass,PyTypeChecker, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, # prediction = [x[0] for x in prediction], # probs = [prob[:i - 1, 1:i].cpu() for i, prob in zip(lens, s_span.unbind())], # wired negative loss, Two-stage CRF Parsing (:cite:`ijcai2020-560`).

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,treecrf.py,0,0.0,82,57.34,7,4.9,7,4.9,47,32.87,0,0.0,5,2,43,6,,"CRF2oDependency, CRFConstituency, __init__, any, arc_loss, arc_mask, arc_preds, arc_probs, arc_seq, arcs, argmax, bad, batch_size, cands, cl, cr, criterion, decode, diagonal, eq, float, forward, ge, gt, hanlp, il, il0, index_fill, index_fill_, inside, ir, ir0, len, lens, logZ, logsumexp, loss, lt, mask, masked_fill, masked_fill_, mbr, ne, new_tensor, partial, permute, probs, proj, range, register_hook, rel_loss, rel_preds, rels, requires_grad, requires_grad_, s_arc, s_c, s_i, s_rel, s_s, s_sib, score, scores, self, seq, seq_len, shape, sib_mask, sib_seq, sibs, slr, squeeze, sum, super, target, tolist, torch, training, unsqueeze, w, x, zip","autograd.grad, hanlp.components.parsers.alg.eisner, hanlp.components.parsers.alg.eisner2o, hanlp.components.parsers.alg.istree, hanlp.components.parsers.alg.mst, hanlp.components.parsers.alg.stripe, nn.CrossEntropyLoss, nn.Module, torch.arange, torch.enable_grad, torch.full_like, torch.isnan","__init__, decode, forward, inside, loss","CRF2oDependency, CRFConstituency","alg, arc_loss, arc_mask, arc_preds, arc_probs, arc_seq, arcs, bad, batch_size, cands, cl, cr, il, il0, ir, ir0, lens, logZ, loss, mask, n, probs, rel_loss, rel_preds, rels, s, s_arc, s_c, s_i, s_rel, s_s, s_sib, score, scores, seq, seq_len, sib_mask, sib_seq, sibs, slr, target, training, w","Args:
    s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all possible arcs.
    s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.
        Scores of all possible dependent-head-sibling triples.
    s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
        Scores of all possible labels on each arc.
    arcs (~torch.LongTensor): ``[batch_size, seq_len]``.
        The tensor of gold-standard arcs.
    sibs (~torch.LongTensor): ``[batch_size, seq_len]``.
        The tensor of gold-standard siblings.
    rels (~torch.LongTensor): ``[batch_size, seq_len]``.
        The tensor of gold-standard labels.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask for covering the unpadded tokens.
    mbr (bool):
        If ``True``, returns marginals for MBR decoding. Default: ``True``.
    partial (bool):
        ``True`` denotes the trees are partially annotated. Default: ``False``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The training loss and
        original arc scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise., Args:
    s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all possible arcs.
    s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.
        Scores of all possible dependent-head-sibling triples.
    s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
        Scores of all possible labels on each arc.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask for covering the unpadded tokens.
    tree (bool):
        If ``True``, ensures to output well-formed trees. Default: ``False``.
    mbr (bool):
        If ``True``, performs MBR decoding. Default: ``True``.
    proj (bool):
        If ``True``, ensures to output projective trees. Default: ``False``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        Predicted arcs and labels of shape ``[batch_size, seq_len]``., Args:
    scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
        Scores of all possible constituents.
    mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
        The mask to avoid parsing over padding tokens.
        For each square matrix in a batch, the positions except upper triangular part should be masked out.
    target (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
        The tensor of gold-standard constituents. ``True`` if a constituent exists. Default: ``None``.
    mbr (bool):
        If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
        The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
        or original scores otherwise., Args:
    scores (~torch.Tensor, ~torch.Tensor):
        Tuple of two tensors `s_arc` and `s_sib`.
        `s_arc` (``[batch_size, seq_len, seq_len]``) holds Scores of all possible dependent-head pairs.
        `s_sib` (``[batch_size, seq_len, seq_len, seq_len]``) holds the scores of dependent-head-sibling triples.
    mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
        The mask to avoid aggregation on padding tokens.
        The first column serving as pseudo words for roots should be ``False``.
    target (~torch.LongTensor): ``[batch_size, seq_len]``.
        Tensors of gold-standard dependent-head pairs and dependent-head-sibling triples.
        If partially annotated, the unannotated positions should be filled with -1.
        Default: ``None``.
    mbr (bool):
        If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.
    partial (bool):
        ``True`` indicates that the trees are partially annotated. Default: ``False``.

Returns:
    ~torch.Tensor, ~torch.Tensor:
        The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
        The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
        or original scores otherwise., Second-order TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for projective dependency trees.

References:
    - Yu Zhang, Zhenghua Li and Min Zhang. 2020.
      `Efficient Second-Order TreeCRF for Neural Dependency Parsing`_.

.. _Efficient Second-Order TreeCRF for Neural Dependency Parsing:
    https://www.aclweb.org/anthology/2020.acl-main.302/, TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for constituency trees.

References:
    - Yu Zhang, houquan Zhou and Zhenghua Li. 2020.
      `Fast and Accurate Neural CRF Constituency Parsing`_.

.. _Fast and Accurate Neural CRF Constituency Parsing:
    https://www.ijcai.org/Proceedings/2020/560/","
        Args:
            s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
                Scores of all possible arcs.
            s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.
                Scores of all possible dependent-head-sibling triples.
            s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
                Scores of all possible labels on each arc.
            arcs (~torch.LongTensor): ``[batch_size, seq_len]``.
                The tensor of gold-standard arcs.
            sibs (~torch.LongTensor): ``[batch_size, seq_len]``.
                The tensor of gold-standard siblings.
            rels (~torch.LongTensor): ``[batch_size, seq_len]``.
                The tensor of gold-standard labels.
            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
                The mask for covering the unpadded tokens.
            mbr (bool):
                If ``True``, returns marginals for MBR decoding. Default: ``True``.
            partial (bool):
                ``True`` denotes the trees are partially annotated. Default: ``False``.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The training loss and
                original arc scores of shape ``[batch_size, seq_len, seq_len]`` if ``mbr=False``, or marginals otherwise.
        , 
        Args:
            s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
                Scores of all possible arcs.
            s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.
                Scores of all possible dependent-head-sibling triples.
            s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
                Scores of all possible labels on each arc.
            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
                The mask for covering the unpadded tokens.
            tree (bool):
                If ``True``, ensures to output well-formed trees. Default: ``False``.
            mbr (bool):
                If ``True``, performs MBR decoding. Default: ``True``.
            proj (bool):
                If ``True``, ensures to output projective trees. Default: ``False``.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                Predicted arcs and labels of shape ``[batch_size, seq_len]``.
        , 
        Args:
            scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
                Scores of all possible constituents.
            mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
                The mask to avoid parsing over padding tokens.
                For each square matrix in a batch, the positions except upper triangular part should be masked out.
            target (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
                The tensor of gold-standard constituents. ``True`` if a constituent exists. Default: ``None``.
            mbr (bool):
                If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
                The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
                or original scores otherwise.
        , 
        Args:
            scores (~torch.Tensor, ~torch.Tensor):
                Tuple of two tensors `s_arc` and `s_sib`.
                `s_arc` (``[batch_size, seq_len, seq_len]``) holds Scores of all possible dependent-head pairs.
                `s_sib` (``[batch_size, seq_len, seq_len, seq_len]``) holds the scores of dependent-head-sibling triples.
            mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
                The mask to avoid aggregation on padding tokens.
                The first column serving as pseudo words for roots should be ``False``.
            target (~torch.LongTensor): ``[batch_size, seq_len]``.
                Tensors of gold-standard dependent-head pairs and dependent-head-sibling triples.
                If partially annotated, the unannotated positions should be filled with -1.
                Default: ``None``.
            mbr (bool):
                If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.
            partial (bool):
                ``True`` indicates that the trees are partially annotated. Default: ``False``.

        Returns:
            ~torch.Tensor, ~torch.Tensor:
                The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
                The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
                or original scores otherwise.
        , 
    Second-order TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for projective dependency trees.

    References:
        - Yu Zhang, Zhenghua Li and Min Zhang. 2020.
          `Efficient Second-Order TreeCRF for Neural Dependency Parsing`_.

    .. _Efficient Second-Order TreeCRF for Neural Dependency Parsing:
        https://www.aclweb.org/anthology/2020.acl-main.302/
    , 
    TreeCRF for calculating partition functions and marginals in :math:`O(n^3)` for constituency trees.

    References:
        - Yu Zhang, houquan Zhou and Zhenghua Li. 2020.
          `Fast and Accurate Neural CRF Constituency Parsing`_.

    .. _Fast and Accurate Neural CRF Constituency Parsing:
        https://www.ijcai.org/Proceedings/2020/560/
    , -inf","0, 1, 2, 3, False, None, True","

@torch.enable_grad()
def forward(self, scores, mask, target=None, mbr=False):
r""""""
Args:
scores (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
Scores of all possible constituents.
mask (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
The mask to avoid parsing over padding tokens.
For each square matrix in a batch, the positions except upper triangular part should be masked out.
target (~torch.BoolTensor): ``[batch_size, seq_len, seq_len]``.
The tensor of gold-standard constituents. ``True`` if a constituent exists. Default: ``None``.
mbr (bool):
If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.

Returns:
~torch.Tensor, ~torch.Tensor:
The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
or original scores otherwise.
, 

def __init__(self):
super().__init__()
self.criterion = nn.CrossEntropyLoss()

@torch.enable_grad()
def forward(self, scores, mask, target=None, mbr=True, partial=False):
r""""""
Args:
scores (~torch.Tensor, ~torch.Tensor):
Tuple of two tensors `s_arc` and `s_sib`.
`s_arc` (``[batch_size, seq_len, seq_len]``) holds Scores of all possible dependent-head pairs.
`s_sib` (``[batch_size, seq_len, seq_len, seq_len]``) holds the scores of dependent-head-sibling triples.
mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
The mask to avoid aggregation on padding tokens.
The first column serving as pseudo words for roots should be ``False``.
target (~torch.LongTensor): ``[batch_size, seq_len]``.
Tensors of gold-standard dependent-head pairs and dependent-head-sibling triples.
If partially annotated, the unannotated positions should be filled with -1.
Default: ``None``.
mbr (bool):
If ``True``, marginals will be returned to perform minimum Bayes-risk (MBR) decoding. Default: ``False``.
partial (bool):
``True`` indicates that the trees are partially annotated. Default: ``False``.

Returns:
~torch.Tensor, ~torch.Tensor:
The first is the training loss averaged by the number of tokens, which won't be returned if ``target=None``.
The second is a tensor of shape ``[batch_size, seq_len, seq_len]``, in which are marginals if ``mbr=True``,
or original scores otherwise.
, 

scores, target = (s_arc, s_sib), (arcs, sibs)
arc_loss, arc_probs = self.forward(scores, mask, target, mbr, partial)
# -1 denotes un-annotated arcs
if partial:
mask = mask & arcs.ge(0)
s_rel, rels = s_rel[mask], rels[mask]
s_rel = s_rel[torch.arange(len(rels)), arcs[mask]]
rel_loss = self.criterion(s_rel, rels)
loss = arc_loss + rel_loss
return loss, arc_probs

# def decode(self, s_arc, s_rel, mask, tree=False, proj=False, alg=None):
#     r""""""
#     Args:
#         s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
#             Scores of all possible arcs.
#         s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
#             Scores of all possible labels on each arc.
#         mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
#             The mask for covering the unpadded tokens.
#         tree (bool):
#             If ``True``, ensures to output well-formed trees. Default: ``False``.
#         proj (bool):
#             If ``True``, ensures to output projective trees. Default: ``False``.
#
#     Returns:
#         ~torch.Tensor, ~torch.Tensor:
#             Predicted arcs and labels of shape ``[batch_size, seq_len]``.
#     """"""
#
#     lens = mask.sum(1)
#     arc_preds = s_arc.argmax(-1)
#     if tree and not alg:
#         bad = [not istree(seq[1:i + 1], proj)
#                for i, seq in zip(lens.tolist(), arc_preds.tolist())]
#         if any(bad):
#             alg = eisner if proj else mst
#             arc_preds[bad] = alg(s_arc[bad], mask[bad])
#     rel_preds = s_rel.argmax(-1).gather(-1, arc_preds.unsqueeze(-1)).squeeze(-1)
#
#     return arc_preds, rel_preds
def decode(self, s_arc, s_sib, s_rel, mask, tree=False, mbr=True, proj=False):
r""""""
Args:
s_arc (~torch.Tensor): ``[batch_size, seq_len, seq_len]``.
Scores of all possible arcs.
s_sib (~torch.Tensor): ``[batch_size, seq_len, seq_len, seq_len]``.
Scores of all possible dependent-head-sibling triples.
s_rel (~torch.Tensor): ``[batch_size, seq_len, seq_len, n_labels]``.
Scores of all possible labels on each arc.
mask (~torch.BoolTensor): ``[batch_size, seq_len]``.
The mask for covering the unpadded tokens.
tree (bool):
If ``True``, ensures to output well-formed trees. Default: ``False``.
mbr (bool):
If ``True``, performs MBR decoding. Default: ``True``.
proj (bool):
If ``True``, ensures to output projective trees. Default: ``False``.

Returns:
~torch.Tensor, ~torch.Tensor:
Predicted arcs and labels of shape ``[batch_size, seq_len]``.
, #, #                  exp(C(i->i) + C(j->i+1))), #                  exp(C(j->j) + C(i->j-1))), #           + s(i->j), #           + s(j->i), # -*- coding:utf-8 -*-, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Adopted from https://github.com/yzhangcs/parser, # C(i->j) = logsumexp(I(i->r) + C(r->j)), i < r <= j, # C(j->i) = logsumexp(C(r->i) + I(j->r)), i <= r < j, # Copyright (c) 2020 Yu Zhang, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # I(i->j) = logsum(exp(I(i->r) + S(i->r, j)) +, i < r < j, # I(j->i) = logsum(exp(I(j->r) + S(j->r, i)) +, i < r < j, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # S(i, j) = logsumexp(C(i->r) + C(j->r+1)), i <= r < j, # S(j, i) = logsumexp(C(i->r) + C(j->r+1)), i <= r < j, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # [batch_size, n, w], # [n, 1, batch_size], # [n, w, batch_size], # [seq_len, seq_len, batch_size], # [seq_len, seq_len, seq_len, batch_size], # always enable the gradient computation of scores in order for the computation of marginals, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # disable multi words to modify the root, # from span (0, w) to span (n, n+w) given width w, # furnished to do so, subject to the following conditions:, # il0[0] are set to zeros since the scores of the complete spans starting from 0 are always -inf, # in the Software without restriction, including without limitation the rights, # marginals are used for decoding, and can be computed by combining the inside pass and autograd mechanism, # n denotes the number of spans to iterate,, # of this software and associated documentation files (the ""Software""), to deal, # set the scores of arcs excluded by cands to -inf, # the end position of each sentence in a batch, # the second inside process is needed if use partial annotation, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-28 19:26",
https://github.com/hankcs/HanLP,lemma_edit.py,0,0.0,38,57.58,16,24.24,5,7.58,6,9.09,1,1.52,3,0,24,4,,"_, a, allow_copy, append, apply_lemma_rule, best, best_form, best_lemma, c, case, casing, cells, cpl, enumerate, f, form, form_offset, gen_lemma_rule, i, int, j, l, lemma, lemma_casing, lemma_rule, len, lower, min_edit_script, offset, previous_case, range, rule, rule_sources, rules, source, split, startswith, target",,"apply_lemma_rule, gen_lemma_rule, min_edit_script",,"_, a, best, best_form, best_lemma, c, case, casing, cells, cpl, f, form, form_offset, i, j, l, lemma, lemma_casing, offset, previous_case, rule, rule_sources, rules, source","Applies the lemma rule to the form to generate the lemma

Args:
  form: 
  lemma_rule: 

Returns:, Finds the minimum edit script to transform the source to the target

Args:
  source: 
  target: 
  allow_copy:  (Default value = False)

Returns:, Generates a lemma rule to transform the source to the target

Args:
  form: 
  lemma: 
  allow_copy:  (Default value = False)

Returns:, Utilities for processing lemmas

Adopted from UDPipe Future
https://github.com/CoNLL-UD-2018/UDPipe-Future",", 
Utilities for processing lemmas

Adopted from UDPipe Future
https://github.com/CoNLL-UD-2018/UDPipe-Future
, +, -, ;, Applies the lemma rule to the form to generate the lemma

    Args:
      form: 
      lemma_rule: 

    Returns:

    , Finds the minimum edit script to transform the source to the target

    Args:
      source: 
      target: 
      allow_copy:  (Default value = False)

    Returns:

    , Generates a lemma rule to transform the source to the target

    Args:
      form: 
      lemma: 
      allow_copy:  (Default value = False)

    Returns:

    , a, d{}¦{}, {}{}{}, ¦, ↑, →, ↓, ↓0","0, 1, 2, False, None","
Utilities for processing lemmas

Adopted from UDPipe Future
https://github.com/CoNLL-UD-2018/UDPipe-Future
, # Some predicted lemma rules are _, which might be due to partial annotation, # The lemma is lowercased initially, Applies the lemma rule to the form to generate the lemma

Args:
form:
lemma_rule:

Returns:

, Finds the minimum edit script to transform the source to the target

Args:
source:
target:
allow_copy:  (Default value = False)

Returns:

, Generates a lemma rule to transform the source to the target

Args:
form:
lemma:
allow_copy:  (Default value = False)

Returns:

",¦
https://github.com/hankcs/HanLP,tag_decoder.py,0,0.0,55,59.14,9,9.68,9,9.68,20,21.51,0,0.0,6,1,19,2,,"TagDecoder, __init__, _adaptive_loss, _loss, adaptive, adaptive_cutoffs, all_predictions, all_tags, all_words, append, argmax_indices, batch_size, bool, class_probabilities, decode, decode_lemma, encoded_text, float, forward, get_token_from_index, gold_tags, hanlp, hidden, input_dim, label_smoothing, log_prob, loss_fn, mask, ndim, num_classes, numpy, output_dict, output_dim, predictions, predictions_list, range, reshape, reshaped_log_probs, round, self, sequence_length, shape, size, str, super, tags, task, task_output, torch, typing, view, vocab, word, words, zip","F.softmax, hanlp.components.parsers.ud.lemma_edit.apply_lemma_rule, hanlp.components.parsers.ud.udify_util.sequence_cross_entropy, hanlp.components.parsers.ud.udify_util.sequence_cross_entropy_with_logits, numpy.argmax, torch.FloatTensor, torch.LongTensor, torch.Tensor, torch.nn.Module, torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss, torch.nn.modules.linear.Linear, typing.Dict","__init__, _adaptive_loss, _loss, decode, decode_lemma, forward",TagDecoder,"adaptive_cutoffs, all_predictions, all_tags, all_words, argmax_indices, batch_size, class_probabilities, hidden, logits, loss_fn, output_dict, output_dim, predictions, predictions_list, reshaped_log_probs, sequence_length, tags, word, words","A basic sequence tagger that decodes from inputs of word embeddings, Decodes sequences of tags, e.g., POS tags, given a list of contextualized word embeddings","
Decodes sequences of tags, e.g., POS tags, given a list of contextualized word embeddings
, @@UNKNOWN@@, A basic sequence tagger that decodes from inputs of word embeddings, _, class_probabilities, lemmas, logits, loss, words","0, 0.03, 1, 15, 2, 3, 4.0, False, None","
Decodes sequences of tags, e.g., POS tags, given a list of contextualized word embeddings
, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2019 Dan Kondratyuk, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # This file is modified from udify, which is licensed under the MIT license:, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,udify_util.py,0,0.0,72,50.0,17,11.81,9,6.25,44,30.56,2,1.39,8,0,27,7,,"TypeError, ValueError, alpha, alpha_factor, average, combine_initial_dims_to_1d_or_2d, conllu_files, dataset_dir, datasets, dev_file, device, dim, dtype, endswith, exp, file, float, focal_factor, gamma, get_device, get_device_of, get_range_vector, get_ud_treebank_files, int, is_cuda, is_floating_point, isinstance, label_smoothing, len, log_probs, log_probs_flat, logits, logits_flat, negative_log_likelihood, negative_log_likelihood_flat, non_batch_dims, num_classes, num_non_empty_sequences, numpy, one_hot_targets, original_size, os, per_batch_loss, probs_flat, range, sequence_cross_entropy, sequence_cross_entropy_with_logits, shape, size, smoothed_targets, smoothing_value, sorted, str, sum, targets, targets_flat, test_file, tiny_value_of_dtype, to, torch, train_file, treebank, treebank_path, treebanks, tuple, type, typing, uncombine_initial_dims, view, view_args, weights, weights_batch_sum","numpy.ndarray, os.listdir, os.path.join, torch.BoolTensor, torch.FloatTensor, torch.LongTensor, torch.Size, torch.Tensor, torch.arange, torch.cat, torch.cuda.LongTensor, torch.double, torch.dtype, torch.float, torch.gather, torch.half, torch.long, torch.nn.functional.log_softmax, torch.tensor, torch.zeros_like, typing.Dict, typing.List, typing.Tuple, typing.Union","combine_initial_dims_to_1d_or_2d, get_device_of, get_range_vector, get_ud_treebank_files, sequence_cross_entropy, sequence_cross_entropy_with_logits, tiny_value_of_dtype, uncombine_initial_dims",,"alpha_factor, conllu_files, datasets, dev_file, file, focal_factor, log_probs_flat, logits_flat, negative_log_likelihood, negative_log_likelihood_flat, non_batch_dims, num_classes, num_non_empty_sequences, one_hot_targets, per_batch_loss, probs_flat, smoothed_targets, smoothing_value, targets_flat, test_file, train_file, treebank, treebank_path, treebanks, view_args, weights, weights_batch_sum","Computes the cross entropy loss of a sequence, weighted with respect to
some user provided weights. Note that the weighting here is not the same as
in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting
classes; here we are weighting the loss contribution from particular elements
in the sequence. This allows loss computations for models which use padding.

# Parameters

logits : `torch.FloatTensor`, required.
    A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)
    which contains the unnormalized probability for each class.
targets : `torch.LongTensor`, required.
    A `torch.LongTensor` of size (batch, sequence_length) which contains the
    index of the true class for each corresponding step.
weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.
    A `torch.FloatTensor` of size (batch, sequence_length)
average: `str`, optional (default = `""batch""`)
    If ""batch"", average the loss across the batches. If ""token"", average
    the loss across each item in the input. If `None`, return a vector
    of losses per batch element.
label_smoothing : `float`, optional (default = `None`)
    Whether or not to apply label smoothing to the cross-entropy loss.
    For example, with a label smoothing value of 0.2, a 4 class classification
    target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was
    the correct label.
gamma : `float`, optional (default = `None`)
    Focal loss[*] focusing parameter `gamma` to reduces the relative loss for
    well-classified examples and put more focus on hard. The greater value
    `gamma` is, the more focus on hard examples.
alpha : `Union[float, List[float]]`, optional (default = `None`)
    Focal loss[*] weighting factor `alpha` to balance between classes. Can be
    used independently with `gamma`. If a single `float` is provided, it
    is assumed binary case using `alpha` and `1 - alpha` for positive and
    negative respectively. If a list of `float` is provided, with the same
    length as the number of classes, the weights will match the classes.
    [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, ""Focal Loss for
    Dense Object Detection,"" 2017 IEEE International Conference on Computer
    Vision (ICCV), Venice, 2017, pp. 2999-3007.

# Returns

`torch.FloatTensor`
    A torch.FloatTensor representing the cross entropy loss.
    If `average==""batch""` or `average==""token""`, the returned loss is a scalar.
    If `average is None`, the returned loss is a vector of shape (batch_size,).

Args:
  logits: torch.FloatTensor: 
  targets: torch.LongTensor: 
  weights: Union[torch.FloatTensor: 
  torch.BoolTensor]: 
  average: str:  (Default value = ""batch"")
  label_smoothing: float:  (Default value = None)
  gamma: float:  (Default value = None)
  alpha: Union[float: 
  List[float]: 
  torch.FloatTensor]:  (Default value = None)

Returns:, Given a (possibly higher order) tensor of ids with shape
(d1, ..., dn, sequence_length)

Args:
  tensor: torch.Tensor: 

Returns:
  If original tensor is 1-d or 2-d, return it as is., Given a tensor of embeddings with shape
(d1 * ... * dn, sequence_length, embedding_dim)
and the original shape
(d1, ..., dn, sequence_length),

Args:
  tensor: torch.Tensor: 
  original_size: torch.Size: 

Returns:
  (d1, ..., dn, sequence_length, embedding_dim).
  If original size is 1-d or 2-d, return it as is., Retrieves all treebank data paths in the given directory.
Adopted from https://github.com/Hyperparticle/udify
MIT Licence

Args:
  dataset_dir: 
  treebanks: 
  dataset_dir: str: 
  treebanks: List[str]:  (Default value = None)

Returns:, Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical
issues such as division by zero.
This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.
Only supports floating point dtypes.

Args:
  dtype: torch.dtype: 

Returns:, Returns a range vector with the desired size, starting at 0. The CUDA implementation
is meant to avoid copy data from CPU to GPU.

Args:
  size: int: 
  device: int: 

Returns:, Returns the device of the tensor.

Args:
  tensor: torch.Tensor: 

Returns:",".conllu, Computes the cross entropy loss of a sequence, weighted with respect to
    some user provided weights. Note that the weighting here is not the same as
    in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting
    classes; here we are weighting the loss contribution from particular elements
    in the sequence. This allows loss computations for models which use padding.
    
    # Parameters
    
    logits : `torch.FloatTensor`, required.
        A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)
        which contains the unnormalized probability for each class.
    targets : `torch.LongTensor`, required.
        A `torch.LongTensor` of size (batch, sequence_length) which contains the
        index of the true class for each corresponding step.
    weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.
        A `torch.FloatTensor` of size (batch, sequence_length)
    average: `str`, optional (default = `""batch""`)
        If ""batch"", average the loss across the batches. If ""token"", average
        the loss across each item in the input. If `None`, return a vector
        of losses per batch element.
    label_smoothing : `float`, optional (default = `None`)
        Whether or not to apply label smoothing to the cross-entropy loss.
        For example, with a label smoothing value of 0.2, a 4 class classification
        target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was
        the correct label.
    gamma : `float`, optional (default = `None`)
        Focal loss[*] focusing parameter `gamma` to reduces the relative loss for
        well-classified examples and put more focus on hard. The greater value
        `gamma` is, the more focus on hard examples.
    alpha : `Union[float, List[float]]`, optional (default = `None`)
        Focal loss[*] weighting factor `alpha` to balance between classes. Can be
        used independently with `gamma`. If a single `float` is provided, it
        is assumed binary case using `alpha` and `1 - alpha` for positive and
        negative respectively. If a list of `float` is provided, with the same
        length as the number of classes, the weights will match the classes.
        [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, ""Focal Loss for
        Dense Object Detection,"" 2017 IEEE International Conference on Computer
        Vision (ICCV), Venice, 2017, pp. 2999-3007.
    
    # Returns
    
    `torch.FloatTensor`
        A torch.FloatTensor representing the cross entropy loss.
        If `average==""batch""` or `average==""token""`, the returned loss is a scalar.
        If `average is None`, the returned loss is a vector of shape (batch_size,).

    Args:
      logits: torch.FloatTensor: 
      targets: torch.LongTensor: 
      weights: Union[torch.FloatTensor: 
      torch.BoolTensor]: 
      average: str:  (Default value = ""batch"")
      label_smoothing: float:  (Default value = None)
      gamma: float:  (Default value = None)
      alpha: Union[float: 
      List[float]: 
      torch.FloatTensor]:  (Default value = None)

    Returns:

    , Does not support dtype , Given a (possibly higher order) tensor of ids with shape
    (d1, ..., dn, sequence_length)

    Args:
      tensor: torch.Tensor: 

    Returns:
      If original tensor is 1-d or 2-d, return it as is.

    , Given a tensor of embeddings with shape
    (d1 * ... * dn, sequence_length, embedding_dim)
    and the original shape
    (d1, ..., dn, sequence_length),

    Args:
      tensor: torch.Tensor: 
      original_size: torch.Size: 

    Returns:
      (d1, ..., dn, sequence_length, embedding_dim).
      If original size is 1-d or 2-d, return it as is.

    , Got average f{average}, expected one of None, 'token', or 'batch', Only supports floating point dtypes., Retrieves all treebank data paths in the given directory.
    Adopted from https://github.com/Hyperparticle/udify
    MIT Licence

    Args:
      dataset_dir: 
      treebanks: 
      dataset_dir: str: 
      treebanks: List[str]:  (Default value = None)

    Returns:

    
    , Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical
    issues such as division by zero.
    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.
    Only supports floating point dtypes.

    Args:
      dtype: torch.dtype: 

    Returns:

    , Returns a range vector with the desired size, starting at 0. The CUDA implementation
    is meant to avoid copy data from CPU to GPU.

    Args:
      size: int: 
      device: int: 

    Returns:

    , Returns the device of the tensor.

    Args:
      tensor: torch.Tensor: 

    Returns:

    , alpha must be float, list of float, or torch.FloatTensor, {} provided., batch, dev.conllu, test.conllu, token, train.conllu","0, 0.0, 0.0001, 1, 1.0, 1e-13, 2, None, True","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Contribution to the negative log likelihood only comes from the exact indices, # Copyright (c) 2019 Dan Kondratyuk and allennlp, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # Fill all the correct indices with 1 - smoothing value., # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # This file is modified from udify and allennlp, which are licensed under the MIT license:, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # focal loss coefficient, # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # make sure weights are float, # of the targets, as the target distributions are one-hot. Here we use torch.gather, # of this software and associated documentation files (the ""Software""), to deal, # shape : () / (num_classes,), # shape : (1,), # shape : (2,), # shape : (batch * max_len, 1), # shape : (batch * sequence_length, 1), # shape : (batch * sequence_length, num_classes), # shape : (batch * sequence_length,), # shape : (batch, max_len), # shape : (batch, sequence_length), # shape : (batch_size,), # shape : (c,), # sum all dim except batch, # to extract the indices of the num_classes dimension which contribute to the loss., # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell, Computes the cross entropy loss of a sequence, weighted with respect to
some user provided weights. Note that the weighting here is not the same as
in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting
classes; here we are weighting the loss contribution from particular elements
in the sequence. This allows loss computations for models which use padding.

# Parameters

logits : `torch.FloatTensor`, required.
A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)
which contains the unnormalized probability for each class.
targets : `torch.LongTensor`, required.
A `torch.LongTensor` of size (batch, sequence_length) which contains the
index of the true class for each corresponding step.
weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.
A `torch.FloatTensor` of size (batch, sequence_length)
average: `str`, optional (default = `""batch""`)
If ""batch"", average the loss across the batches. If ""token"", average
the loss across each item in the input. If `None`, return a vector
of losses per batch element.
label_smoothing : `float`, optional (default = `None`)
Whether or not to apply label smoothing to the cross-entropy loss.
For example, with a label smoothing value of 0.2, a 4 class classification
target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was
the correct label.
gamma : `float`, optional (default = `None`)
Focal loss[*] focusing parameter `gamma` to reduces the relative loss for
well-classified examples and put more focus on hard. The greater value
`gamma` is, the more focus on hard examples.
alpha : `Union[float, List[float]]`, optional (default = `None`)
Focal loss[*] weighting factor `alpha` to balance between classes. Can be
used independently with `gamma`. If a single `float` is provided, it
is assumed binary case using `alpha` and `1 - alpha` for positive and
negative respectively. If a list of `float` is provided, with the same
length as the number of classes, the weights will match the classes.
[*] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, ""Focal Loss for
Dense Object Detection,"" 2017 IEEE International Conference on Computer
Vision (ICCV), Venice, 2017, pp. 2999-3007.

# Returns

`torch.FloatTensor`
A torch.FloatTensor representing the cross entropy loss.
If `average==""batch""` or `average==""token""`, the returned loss is a scalar.
If `average is None`, the returned loss is a vector of shape (batch_size,).

Args:
logits: torch.FloatTensor:
targets: torch.LongTensor:
weights: Union[torch.FloatTensor:
torch.BoolTensor]:
average: str:  (Default value = ""batch"")
label_smoothing: float:  (Default value = None)
gamma: float:  (Default value = None)
alpha: Union[float:
List[float]:
torch.FloatTensor]:  (Default value = None)

Returns:

, Given a (possibly higher order) tensor of ids with shape
(d1, ..., dn, sequence_length)

Args:
tensor: torch.Tensor:

Returns:
If original tensor is 1-d or 2-d, return it as is.

, Given a tensor of embeddings with shape
(d1 * ... * dn, sequence_length, embedding_dim)
and the original shape
(d1, ..., dn, sequence_length),

Args:
tensor: torch.Tensor:
original_size: torch.Size:

Returns:
(d1, ..., dn, sequence_length, embedding_dim).
If original size is 1-d or 2-d, return it as is.

, Retrieves all treebank data paths in the given directory.
Adopted from https://github.com/Hyperparticle/udify
MIT Licence

Args:
dataset_dir:
treebanks:
dataset_dir: str:
treebanks: List[str]:  (Default value = None)

Returns:


, Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical
issues such as division by zero.
This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.
Only supports floating point dtypes.

Args:
dtype: torch.dtype:

Returns:

, Returns a range vector with the desired size, starting at 0. The CUDA implementation
is meant to avoid copy data from CPU to GPU.

Args:
size: int:
device: int:

Returns:

, Returns the device of the tensor.

Args:
tensor: torch.Tensor:

Returns:

","Computes the cross entropy loss of a sequence, weighted with respect to
some user provided weights. Note that the weighting here is not the same as
in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting
classes; here we are weighting the loss contribution from particular elements
in the sequence. This allows loss computations for models which use padding.

# Parameters

logits : `torch.FloatTensor`, required.
A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)
which contains the unnormalized probability for each class.
targets : `torch.LongTensor`, required.
A `torch.LongTensor` of size (batch, sequence_length) which contains the
index of the true class for each corresponding step.
weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.
A `torch.FloatTensor` of size (batch, sequence_length)
average: `str`, optional (default = `""batch""`)
If ""batch"", average the loss across the batches. If ""token"", average
the loss across each item in the input. If `None`, return a vector
of losses per batch element.
label_smoothing : `float`, optional (default = `None`)
Whether or not to apply label smoothing to the cross-entropy loss.
For example, with a label smoothing value of 0.2, a 4 class classification
target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was
the correct label.
gamma : `float`, optional (default = `None`)
Focal loss[*] focusing parameter `gamma` to reduces the relative loss for
well-classified examples and put more focus on hard. The greater value
`gamma` is, the more focus on hard examples.
alpha : `Union[float, List[float]]`, optional (default = `None`)
Focal loss[*] weighting factor `alpha` to balance between classes. Can be
used independently with `gamma`. If a single `float` is provided, it
is assumed binary case using `alpha` and `1 - alpha` for positive and
negative respectively. If a list of `float` is provided, with the same
length as the number of classes, the weights will match the classes.
[*] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, ""Focal Loss for
Dense Object Detection,"" 2017 IEEE International Conference on Computer
Vision (ICCV), Venice, 2017, pp. 2999-3007.

# Returns

`torch.FloatTensor`
A torch.FloatTensor representing the cross entropy loss.
If `average==""batch""` or `average==""token""`, the returned loss is a scalar.
If `average is None`, the returned loss is a vector of shape (batch_size,).

Args:
logits: torch.FloatTensor:
targets: torch.LongTensor:
weights: Union[torch.FloatTensor:
torch.BoolTensor]:
average: str:  (Default value = ""batch"")
label_smoothing: float:  (Default value = None)
gamma: float:  (Default value = None)
alpha: Union[float:
List[float]:
torch.FloatTensor]:  (Default value = None)

Returns:

, Dollár"
https://github.com/hankcs/HanLP,ud_model.py,0,0.0,46,61.33,14,18.67,8,10.67,7,9.33,0,0.0,3,2,12,0,,"UniversalDependenciesDecoder, UniversalDependenciesModel, __init__, arc, batch, class_probabilities, clone, compute_loss, decode, decoder, decoder_input, decoders, encoder, forward, get, get_output_dim, gold_keys, hanlp, hidden, hidden_size, int, layer_dropout, logits, loss, mask, mask_without_root, mix_embedding, mlp_dropout, n_mlp_arc, n_mlp_rel, num_feats, num_lemmas, num_rels, num_upos, output_dict, pred_output, s_arc, s_rel, scalar_mix, self, str, super, task, tasks, torch, typing","hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDecoder, hanlp.components.parsers.ud.tag_decoder.TagDecoder, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbeddingModule, hanlp.layers.scalar_mix.ScalarMixWithDropout, torch.Tensor, torch.nn.Module, torch.nn.ModuleDict, torch.nn.functional.cross_entropy, typing.Any, typing.Dict","__init__, decode, forward","UniversalDependenciesDecoder, UniversalDependenciesModel","arc, class_probabilities, decoder_input, hidden, logits, loss, mask_without_root, output_dict, pred_output, s_arc, s_rel, task",,"arc, class_probabilities, deps, feat_id, feats, lemma_id, lemmas, logits, loss, pos_id, rel_id, s_arc, s_rel, upos","0, 0.0, 0.03, 1, 13, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-15 14:21, # Keep track of the loss if we have the gold tags available, # Run through each of the tasks on the shared encoder and save predictions, # decoders, # noinspection PyTypeChecker",
https://github.com/hankcs/HanLP,ud_parser.py,0,0.0,145,69.05,41,19.52,17,8.1,7,3.33,0,0.0,22,1,53,1,,"UniversalDependenciesParser, __call__, __init__, _step, append, arc_preds, arc_scores, arcs, backward, batch, batch_size, best_epoch, best_metric, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, clone, compute_mask, config, copy, criterion, cstr, dataloader, decode, decode_output, detach, dev, dev_metric, device, devices, each, elapsed_average_human, elapsed_human, enumerate, epoch, epochs, eta_human, eval, eval_trn, evaluate_dataloader, execute_training_loop, extend, fe, feat, feat_vocab, feats, feed_batch, fit, fit_dataloader, flat, float, get, gold, gradient_accumulation, hanlp, hanlp_common, history, idx, info, input_is_flat, int, isinstance, item, key, layer_dropout, lem_vocab, lemma, lemmas, lens, load_weights, lock, log, logger, logging, loss, max, max_seq_len, mix_embedding, mlp_dropout, model, module, mutable, n_mlp_arc, n_mlp_rel, num_training_steps, optimizer, out, output_dict, outputs, patience, pos, pos_vocab, pred, predict, prediction_to_head_rel, prediction_to_human, punct, puncts, range, ratio_width, rel, rel_preds, rel_scores, rels, report, reset, sampler, sampler_builder, samples, save_dir, save_weights, scalar_mix, scheduler, self, set_unk_as_safe_unk, shuffle, step, summary, super, task, timer, torch, total_loss, total_time_human, train, training, transform, tree, trn, typing, update, update_metric, update_metrics, upos, values, vocabs, zero_grad, zip","copy.deepcopy, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.transform.PunctuationMask, hanlp.common.vocab.Vocab, hanlp.components.classifiers.transformer_classifier.TransformerComponent, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.ud.lemma_edit.apply_lemma_rule, hanlp.components.parsers.ud.ud_model.UniversalDependenciesModel, hanlp.components.parsers.ud.util.append_bos, hanlp.components.parsers.ud.util.generate_lemma_rule, hanlp.components.parsers.ud.util.sample_form_missing, hanlp.datasets.parsing.loaders.conll_dataset.CoNLLParsingDataset, hanlp.layers.embeddings.contextual_word_embedding.ContextualWordEmbedding, hanlp.metrics.accuracy.CategoricalAccuracy, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.metrics.parsing.attachmentscore.AttachmentScore, hanlp.utils.time_util.CountdownTimer, hanlp.utils.torch_util.clip_grad_norm, hanlp.utils.torch_util.lengths_to_mask, hanlp_common.conll.CoNLLSentence, hanlp_common.conll.CoNLLUWord, hanlp_common.constant.IDX, hanlp_common.util.merge_dict, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, torch.nn.Module, torch.utils.data.DataLoader, typing.Callable, typing.List, typing.Union","__call__, __init__, _step, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, compute_mask, decode_output, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, input_is_flat, predict, prediction_to_human, update_metrics",UniversalDependenciesParser,"arc_preds, arc_scores, arcs, batch, batch_size, best_epoch, best_metric, data, dataloader, dataset, dev_metric, each, epoch, fe, feat, feat_vocab, feats, flat, form, gold, history, idx, key, lem_vocab, lemma, lemmas, lens, loss, mask, max_seq_len, optimizer, order, out, output_dict, outputs, patience, pos, pos_vocab, pred, puncts, rel_preds, rel_scores, rels, report, sampler, samples, scheduler, task, timer, total_loss, transform, tree, upos","Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation
of ""75 Languages, 1 Model: Parsing Universal Dependencies Universally"" (:cite:`kondratyuk-straka-2019-75`).

Args:
    **kwargs: Predefined config."," ,  (,  / ,  ETA: ,  [red](saved)[/red],  at epoch ,  early stop,  elapsed, ), .4f, :[/yellow], Average time of each epoch is , Building vocab [blink][yellow]...[/yellow][/blink] (longest sequence: , FORM, Max score of dev is , Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation
        of ""75 Languages, 1 Model: Parsing Universal Dependencies Universally"" (:cite:`kondratyuk-straka-2019-75`).

        Args:
            **kwargs: Predefined config.
        , [yellow]Epoch , arc, arc_preds, class_probabilities, deps, feat, feat_id, feats, lemma, lemma_id, lemmas, loss, loss: , pos, pos_id, punct_mask, rel_id, rel_preds, s_arc, s_rel, token, token_input_ids, token_length, transformer.scalar_mix has to be 1 when mix_embedding is non-zero., upos","0, 0.001, 0.1, 0.33, 0.5, 1, 1.0, 13, 1e-08, 2.5e-05, 256, 30, 32, 768, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-14 20:34, # noinspection PyCallByClass,PyTypeChecker, # noinspection PyMethodOverriding, # noinspection PyTypeChecker, Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation
of ""75 Languages, 1 Model: Parsing Universal Dependencies Universally"" (:cite:`kondratyuk-straka-2019-75`).

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,util.py,0,0.0,10,35.71,13,46.43,2,7.14,3,10.71,0,0.0,3,0,1,0,,"all, append_bos, dict, generate_lemma_rule, hanlp, hanlp_common, sample, sample_form_missing, word, zip","hanlp.components.parsers.ud.lemma_edit.gen_lemma_rule, hanlp_common.constant.ROOT","append_bos, generate_lemma_rule, sample_form_missing",,word,,"DEPREL, FEATS, FORM, HEAD, LEMMA, UPOS, _, arc, feat, lemma, pos, rel, token","0, 1","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-14 20:44",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-14 20:34",
https://github.com/hankcs/HanLP,biaffine_ner.py,0,0.0,148,69.81,34,16.04,17,8.02,13,6.13,0,0.0,22,1,50,2,,"BiaffineNamedEntityRecognizer, __init__, _get_transformer, adam_epsilon, add, append, append_transform, backward, batch, batch_size, best_epoch, best_metric, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_vocabs, candidates, close, config, context_layer, criterion, current, dataloader, dev, dev_score, device, dict, each, elapsed_human, embed, entity, enumerate, epoch, epochs, eval, evaluate_dataloader, execute_training_loop, extend, feed_batch, fit, fit_dataloader, flat, fp, get, get_pred_ner, getattr, gold, grad_norm, gradient_accumulation, hanlp, hanlp_common, history, idx_to_label, info, input_is_flat, int, is_flat_ner, isinstance, item, join, label, label_vocab, label_vocab_name, len, lens, linear_scheduler, lock, log, logger, logging, loss, lr, model, model_, module, mutable, num_training_steps, open, orders, output, output_dict, parameters, pred, pred_mentions, predict, prediction, prediction_per_sent, prediction_to_result, predictions, property, purge_cache, range, ratio_width, report, report_metrics, reset, reset_metrics, ret_tokens, sampler, sampler_builder, save_dir, save_weights, score, self, sent, sent_pred_mentions, sentences, set_unk_as_safe_unk, shuffle, sid, sorted, span_scores, span_scores_cpu, spr, staticmethod, step, summary, super, timer, token, tokens, tolist, top_span, top_spans, torch, total, total_loss, total_time_human, train, transform, transformer_lr, trn, type, typing, update_metrics, use_transformer, vocabs, warmup_steps, weight_decay, write, x, zero_grad, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.transform.FieldLength, hanlp.common.transform.TransformList, hanlp.common.vocab.Vocab, hanlp.components.ner.biaffine_ner.biaffine_ner_model.BiaffineNamedEntityRecognitionModel, hanlp.datasets.ner.loaders.json_ner.JsonNERDataset, hanlp.datasets.ner.loaders.json_ner.unpack_ner, hanlp.layers.embeddings.embedding.Embedding, hanlp.layers.transformers.utils.build_optimizer_scheduler_with_transformer, hanlp.metrics.f1.F1, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, torch.argmax, torch.nn.Module, torch.nn.utils.clip_grad_norm_, torch.no_grad, torch.optim.Adam, torch.optim.lr_scheduler.ReduceLROnPlateau, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.List, typing.Union","__init__, _get_transformer, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_vocabs, evaluate_dataloader, execute_training_loop, feed_batch, fit, fit_dataloader, get_pred_ner, input_is_flat, predict, prediction_to_result, report_metrics, reset_metrics, update_metrics, use_transformer",BiaffineNamedEntityRecognizer,"batch, best_epoch, best_metric, candidates, data, dataloader, dataset, dev_score, each, entity, epoch, flat, fp, gold, history, idx_to_label, is_flat_ner, label, label_vocab, lens, loss, model, num_training_steps, optimizer, orders, output_dict, pred, pred_mentions, prediction, prediction_per_sent, predictions, ratio_width, report, sampler, scheduler, sent, sent_pred_mentions, sid, span_scores_cpu, spr, timer, token, tokens, top_span, top_spans, total_loss, transform, type, vocabs, x","An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
assumption about the spans, it naturally supports flat NER and nested NER.

Args:
    **kwargs: Predefined config., Args:
    trn_data: Path to training set.
    dev_data: Path to dev set.
    save_dir: The directory to save trained component.
    embed: Embeddings to use.
    context_layer: A contextualization layer (transformer or RNN).
    sampler: Sampler to use.
    n_buckets: Number of buckets to use in KMeans sampler.
    batch_size: The number of samples in a batch.
    lexical_dropout: Dropout applied to hidden states of context layer.
    ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
    is_flat_ner: ``True`` for flat NER, otherwise nested NER.
    doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
    lr: Learning rate for decoder.
    transformer_lr: Learning rate for encoder.
    adam_epsilon: The epsilon to use in Adam.
    weight_decay: The weight decay to use.
    warmup_steps: The number of warmup steps.
    grad_norm: Gradient norm for clipping.
    epochs: The number of epochs to train.
    loss_reduction: The loss reduction used in aggregating losses.
    gradient_accumulation: Number of mini-batches per update step.
    ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
    tagset: Optional tagset to prune entities outside of this tagset from datasets.
    sampler_builder: The builder to build sampler, which will override batch_size.
    devices: Devices this component will live on.
    logger: Any :class:`logging.Logger` instance.
    seed: Random seed to reproduce this training.
    **kwargs: Not used.

Returns:
    The best metrics on training set.","	, 
, 

        Args:
            trn_data: Path to training set.
            dev_data: Path to dev set.
            save_dir: The directory to save trained component.
            embed: Embeddings to use.
            context_layer: A contextualization layer (transformer or RNN).
            sampler: Sampler to use.
            n_buckets: Number of buckets to use in KMeans sampler.
            batch_size: The number of samples in a batch.
            lexical_dropout: Dropout applied to hidden states of context layer.
            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
            is_flat_ner: ``True`` for flat NER, otherwise nested NER.
            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
            lr: Learning rate for decoder.
            transformer_lr: Learning rate for encoder.
            adam_epsilon: The epsilon to use in Adam.
            weight_decay: The weight decay to use.
            warmup_steps: The number of warmup steps.
            grad_norm: Gradient norm for clipping.
            epochs: The number of epochs to train.
            loss_reduction: The loss reduction used in aggregating losses.
            gradient_accumulation: Number of mini-batches per update step.
            ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
            tagset: Optional tagset to prune entities outside of this tagset from datasets.
            sampler_builder: The builder to build sampler, which will override batch_size.
            devices: Devices this component will live on.
            logger: Any :class:`logging.Logger` instance.
            seed: Random seed to reproduce this training.
            **kwargs: Not used.

        Returns:
            The best metrics on training set.
        ,  ,  / ,  [red]saved[/red], .4f, /, :[/yellow], <null>, An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
        every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
        label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
        assumption about the spans, it naturally supports flat NER and nested NER.

        Args:
            **kwargs: Predefined config.
        , Building NER vocab [blink][yellow]...[/yellow][/blink], Gold	, Pred	, Tokens	, [, [yellow]Epoch , ]/, candidate_ner_scores, doc_level_offset, gradient_accumulation, label, loss, loss: , max, ner, prediction, sorting, sum, tagset, token, token_input_ids, transformer, w","0, 0.001, 0.01, 0.1, 0.5, 1, 150, 1e-05, 1e-06, 2, 3, 32, 5.0, 50, False, None, True","

Args:
trn_data: Path to training set.
dev_data: Path to dev set.
save_dir: The directory to save trained component.
embed: Embeddings to use.
context_layer: A contextualization layer (transformer or RNN).
sampler: Sampler to use.
n_buckets: Number of buckets to use in KMeans sampler.
batch_size: The number of samples in a batch.
lexical_dropout: Dropout applied to hidden states of context layer.
ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
is_flat_ner: ``True`` for flat NER, otherwise nested NER.
doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
lr: Learning rate for decoder.
transformer_lr: Learning rate for encoder.
adam_epsilon: The epsilon to use in Adam.
weight_decay: The weight decay to use.
warmup_steps: The number of warmup steps.
grad_norm: Gradient norm for clipping.
epochs: The number of epochs to train.
loss_reduction: The loss reduction used in aggregating losses.
gradient_accumulation: Number of mini-batches per update step.
ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
tagset: Optional tagset to prune entities outside of this tagset from datasets.
sampler_builder: The builder to build sampler, which will override batch_size.
devices: Devices this component will live on.
logger: Any :class:`logging.Logger` instance.
seed: Random seed to reproduce this training.
**kwargs: Not used.

Returns:
The best metrics on training set.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-09 18:13, # Enable cache, # Use null to indicate no relationship, # embed: torch.nn.Embedding = self.config.embed.module(vocabs=self.vocabs)[0].embed, # for both nested and flat ner no clash is allowed, # for flat ner nested mentions are not allowed, # noinspection PyMethodOverriding, # noinspection PyProtectedMember, # noinspection PyTypeChecker, An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
assumption about the spans, it naturally supports flat NER and nested NER.

Args:
**kwargs: Predefined config.
",
https://github.com/hankcs/HanLP,biaffine_ner_model.py,0,0.0,74,78.72,9,9.57,5,5.32,6,6.38,0,0.0,6,2,28,1,,"BiaffineNamedEntityRecognitionDecoder, BiaffineNamedEntityRecognitionModel, Linear, Module, __init__, batch, biaffine, candidate_ends_emb, candidate_ner_scores, candidate_scores_mask, candidate_starts_emb, config, context_layer, context_layer_output_dim, contextualized_embeddings, decode, decoder, dense_labels, device, embed, end_mlp, ffnn_size, float, forward, get, get_dense_span_labels, get_output_dim, gold_ends, gold_labels, gold_ner_labels, gold_starts, hanlp, hidden_size, initializer, initializer_1d, input_tensor, k, keys, label_space_size, len, lengths_to_mask, lexical_dropout, loss, loss_reduction, mask, masks, max_sent_length, max_sentence_length, max_spans_num, new_mlp, num_sentences, parsers, predict_dict, rank, raw_embeddings, self, sent_lengths, sentence_ends_leq_starts, sentence_indices, size, span_ends, span_labels, span_starts, sparse_indices, start_mlp, super, torch, torch_util, training, typing, unsqueeze, utils, view, word_embedding_dim","F.dropout, hanlp.layers.time_distributed.TimeDistributed, parsers.biaffine.biaffine.Biaffine, torch.FloatTensor, torch.Size, torch.Tensor, torch.arange, torch.cat, torch.nn, torch.nn.Module, torch.nn.functional.cross_entropy, torch.sparse.LongTensor, typing.Dict","__init__, decode, forward, get_dense_span_labels, initializer_1d, new_mlp","BiaffineNamedEntityRecognitionDecoder, BiaffineNamedEntityRecognitionModel","candidate_ends_emb, candidate_ner_scores, candidate_scores_mask, candidate_starts_emb, context_layer_output_dim, contextualized_embeddings, dense_labels, device, gold_ends, gold_labels, gold_ner_labels, gold_starts, input_tensor, k, keys, loss, mask, masks, max_sent_length, max_spans_num, num_sentences, predict_dict, rank, raw_embeddings, sent_lengths, sentence_ends_leq_starts, sentence_indices, sparse_indices","An implementation of the biaffine decoder in ""Named Entity Recognition as Dependency Parsing""
(:cite:`yu-etal-2020-named`).

Args:
    hidden_size: Size of hidden states.
    ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
    label_space_size: Size of tag set.
    loss_reduction: The loss reduction used in aggregating losses.","An implementation of the biaffine decoder in ""Named Entity Recognition as Dependency Parsing""
        (:cite:`yu-etal-2020-named`).

        Args:
            hidden_size: Size of hidden states.
            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
            label_space_size: Size of tag set.
            loss_reduction: The loss reduction used in aggregating losses.
        , begin_offset, candidate_ner_scores, end_offset, generate candidate spans with argument pruning, label_id, loss, sum, token_length","0, 1, 2, 3, None","# Apply MLPs to starts and ends, [num_sentences, max_sentences_length,emb], # Initialize context layer, # Initialize layers and parameters, # MLPs, # get the embedding dim, An implementation of the biaffine decoder in ""Named Entity Recognition as Dependency Parsing
(:cite:`yu-etal-2020-named`).

Args:
hidden_size: Size of hidden states.
ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
label_space_size: Size of tag set.
loss_reduction: The loss reduction used in aggregating losses.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-07-21 18:41",
https://github.com/hankcs/HanLP,amr.py,0,0.0,75,65.79,13,11.4,22,19.3,4,3.51,0,0.0,12,1,25,0,,"GraphAbstractMeaningRepresentationParsing, __init__, alpha, amr_graph, amr_version, arc_loss, beam_size, bool, build, build_batchify, build_dataloader, build_dataset, build_metric, build_model, build_samples, build_vocabs, cls_is_bos, compute_loss, concept_correct, concept_loss, concept_total, config, copy_seq, dataloader, decode_output, device, encoder_size, evaluate_dataloader, feed_batch, from_json, get, gradient_accumulation, graph_arc_loss, hanlp, hanlp_common, input, input_is_flat, isinstance, lens, logger, logging, loss, mask, max_time_step, mutable, output, pop, pp, predictable_concept, prediction, prediction_to_result, rel_correct, rel_loss, rel_total, relation, restore_graph, results, ret, sampler_builder, score, self, sense_restore, sep_is_eos, stog, super, to_amr, torch, training, transform_batch, typing, update, update_metrics, utils_dir, vocabs, zip","hanlp.common.dataset.PrefetchDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.amr.amr_parser.graph_amr_decoder.GraphAbstractMeaningRepresentationDecoder, hanlp.components.amr.amr_parser.graph_parser.GraphAbstractMeaningRepresentationParser, hanlp.components.amr.amr_parser.postprocess.PostProcessor, hanlp.components.amr.amr_parser.work.parse_batch, hanlp.components.mtl.tasks.Task, hanlp.datasets.parsing.amr.batchify, hanlp.datasets.parsing.amr.get_concepts, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.amr.smatch_eval.SmatchScores, hanlp.metrics.amr.smatch_eval.get_amr_utils, hanlp.metrics.f1.F1_, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.utils.io_util.get_resource, hanlp_common.constant.CLS, hanlp_common.util.merge_list_of_dict, hanlp_common.util.merge_locals_kwargs, logging.Logger, stog.data.dataset_readers.amr_parsing.amr.AMRGraph, stog.data.dataset_readers.amr_parsing.node_utils.NodeUtilities, stog.data.dataset_readers.amr_parsing.postprocess.node_restore.NodeRestore, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, evaluate_dataloader, feed_batch, input_is_flat, prediction_to_result, transform_batch, update_metrics",GraphAbstractMeaningRepresentationParsing,"alpha, amr, amr_graph, arc_loss, batch, beam_size, concept_correct, concept_loss, concept_total, copy_seq, data, dataloader, graph_arc_loss, lens, loss, max_time_step, pp, rel_correct, rel_loss, rel_total, relation, res, ret, score, utils_dir",,"2.0, Smatch, alpha, beam_size, concept, lem, lemma, max_time_step, rel, relation, score, token, token_input_ids","0, 0.001, 0.2, 0.33, 0.6, 1, 100, 1024, 128, 2, 20, 256, 3, 300, 32, 4, 5, 512, 8, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-12 16:05, # noinspection PyTypeChecker",
https://github.com/hankcs/HanLP,constituency.py,0,0.0,70,67.31,22,21.15,9,8.65,3,2.88,0,0.0,12,1,14,1,,"CRFConstituencyParsing, __init__, append, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_samples, build_vocabs, cache_dataset, chart, compute_lens, compute_loss, compute_mask, criterion, decode_output, decoder, device, dict, encoder_size, equal, feed_batch, finalize_document, get, gradient_accumulation, hanlp, hanlp_common, height, input_is_flat, inputs, isinstance, label, len, logger, logging, loss, mask, mutable, offset, output, phrasetree, pos, pos_key, pos_per_sent, prediction, prediction_to_result, purge_cache, sampler_builder, self, sent, set_label, span_probs, str, subtree, subtrees, super, tag, task_name, timer, tokens, torch, training, tuple, typing, update_metrics, vocabs, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.constituency.crf_constituency_model.CRFConstituencyDecoder, hanlp.components.parsers.constituency.crf_constituency_parser.CRFConstituencyParser, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.document.Document, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.prefix_match, logging.Logger, phrasetree.tree.Tree, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, build_samples, compute_loss, decode_output, feed_batch, finalize_document, input_is_flat, prediction_to_result, update_metrics",CRFConstituencyParsing,"dataset, equal, loss, mask, offset, pos, pos_key, pos_per_sent, sent, span_probs, subtree, tag, timer, tokens","Two-stage CRF Parsing (:cite:`ijcai2020-560`).

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    delete: Constituencies to be deleted from training and evaluation.
    equal: Constituencies that are regarded as equal during evaluation.
    mbr: ``True`` to enable Minimum Bayes Risk (MBR) decoding (:cite:`smith-smith-2007-probabilistic`).
    n_mlp_span: Number of features for span decoder.
    n_mlp_label: Number of features for label decoder.
    mlp_dropout: Dropout applied to MLPs.
    no_subcategory: Strip out subcategories.
    **kwargs: Not used.",", !, '', ,, -NONE-, ., :, ?, ADVP, PRT, S1, TOP, Two-stage CRF Parsing (:cite:`ijcai2020-560`).

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            delete: Constituencies to be deleted from training and evaluation.
            equal: Constituencies that are regarded as equal during evaluation.
            mbr: ``True`` to enable Minimum Bayes Risk (MBR) decoding (:cite:`smith-smith-2007-probabilistic`).
            n_mlp_span: Number of features for span decoder.
            n_mlp_label: Number of features for label decoder.
            mlp_dropout: Dropout applied to MLPs.
            no_subcategory: Strip out subcategories.
            **kwargs: Not used.
        , _, ``, chart_id, constituency, mask, output, pos, span_probs, token","0, 0.33, 1, 100, 2, 500, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-11-29 16:52",
https://github.com/hankcs/HanLP,dep.py,0,0.0,84,75.68,9,8.11,13,11.71,5,4.5,0,0.0,12,1,19,1,,"BiaffineDependencyParsing, __init__, arc_scores, arcs, arcs_per_sent, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, cache_dataset, clone, compute_lens, compute_loss, config, criterion, decay, decay_steps, decode, decode_output, decoder, device, encoder_size, epsilon, feed_batch, get, get_pad_dict, gradient_accumulation, hanlp, hanlp_common, input_is_flat, inputs, insert, isinstance, len, list, logger, logging, logits, lr, mask, max_seq_len, mlp_dropout, mu, mutable, n_mlp_arc, n_mlp_rel, nu, optimizer, output, parameters, prediction, prediction_to_result, prune, purge_cache, rel, rel_scores, rels, rels_per_sent, result, sampler_builder, self, sent_len, sep_is_eos, str, super, timer, token, tokens, tolist, torch, training, typing, update_metric, update_metrics, use_pos, vocab, vocabs, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.TransformList, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.parsers.biaffine.biaffine_dep.BiaffineDependencyParser, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDecoder, hanlp.datasets.parsing.loaders.conll_dataset.append_bos, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.EOS, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.optim.Adam, torch.optim.lr_scheduler.ExponentialLR, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, build_optimizer, build_samples, compute_loss, decode_output, feed_batch, input_is_flat, prediction_to_result, update_metrics",BiaffineDependencyParsing,"arc_scores, arcs, arcs_per_sent, config, dataset, logits, mask, max_seq_len, optimizer, rel_scores, rels, rels_per_sent, result, scheduler, sent_len, timer, token, tokens, vocab","Biaffine dependency parsing (:cite:`dozat:17a`).

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    punct: ``True`` to include punctuations in evaluation.
    tree: ``True`` to enforce tree constraint.
    proj: ``True`` for projective parsing.
    n_mlp_arc: Number of features for arc representation.
    n_mlp_rel: Number of features for rel representation.
    mlp_dropout: Dropout applied to MLPs.
    mu: First coefficient used for computing running averages of gradient and its square in Adam.
    nu: Second coefficient used for computing running averages of gradient and its square in Adam.
    epsilon: Term added to the denominator to improve numerical stability
    decay: Decay rate for exceptional lr scheduler.
    decay_steps: Decay every ``decay_steps`` steps.
    use_pos: Use pos feature.
    max_seq_len: Prune samples longer than this length.
    **kwargs: Not used.","Biaffine dependency parsing (:cite:`dozat:17a`).

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            punct: ``True`` to include punctuations in evaluation.
            tree: ``True`` to enforce tree constraint.
            proj: ``True`` for projective parsing.
            n_mlp_arc: Number of features for arc representation.
            n_mlp_rel: Number of features for rel representation.
            mlp_dropout: Dropout applied to MLPs.
            mu: First coefficient used for computing running averages of gradient and its square in Adam.
            nu: Second coefficient used for computing running averages of gradient and its square in Adam.
            epsilon: Term added to the denominator to improve numerical stability
            decay: Decay rate for exceptional lr scheduler.
            decay_steps: Decay every ``decay_steps`` steps.
            use_pos: Use pos feature.
            max_seq_len: Prune samples longer than this length.
            **kwargs: Not used.
        , FORM, arc, max_seq_len, punct_mask, rel, rel_id, token, token_input_ids","0, 0.002, 0.33, 0.75, 0.9, 1, 100, 1e-12, 500, 5000, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-13 21:39, # Skip the ROOT, Biaffine dependency parsing (:cite:`dozat:17a`).

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
punct: ``True`` to include punctuations in evaluation.
tree: ``True`` to enforce tree constraint.
proj: ``True`` for projective parsing.
n_mlp_arc: Number of features for arc representation.
n_mlp_rel: Number of features for rel representation.
mlp_dropout: Dropout applied to MLPs.
mu: First coefficient used for computing running averages of gradient and its square in Adam.
nu: Second coefficient used for computing running averages of gradient and its square in Adam.
epsilon: Term added to the denominator to improve numerical stability
decay: Decay rate for exceptional lr scheduler.
decay_steps: Decay every ``decay_steps`` steps.
use_pos: Use pos feature.
max_seq_len: Prune samples longer than this length.
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,dep_2nd.py,0,0.0,61,75.31,6,7.41,11,13.58,3,3.7,0,0.0,12,2,6,0,,"BiaffineSecondaryDependencyDecoder, BiaffineSecondaryDependencyParsing, __init__, batch, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_vocabs, clone, compute_lens, compute_loss, config, contextualized_embeddings, criterion, decode, decode_output, decoder, device, encoder_size, epsilon, forward, gradient_accumulation, hanlp, hanlp_common, hidden_size, input_is_flat, joint, lengths_to_mask, logger, logging, lr, mask, mu, mutable, nu, optimizer, output, outputs, parameters, prediction, prediction_to_result, predictions_to_human, purge_cache, sampler_builder, scores, self, str, super, torch, torch_util, training, typing, update_metric, update_metrics, utils, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.parsers.biaffine.biaffine_2nd_dep.BiaffineJointDecoder, hanlp.components.parsers.biaffine.biaffine_2nd_dep.BiaffineSecondaryParser, hanlp.components.parsers.biaffine.biaffine_2nd_dep.BiaffineSeparateDecoder, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.optim.Adam, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, compute_loss, decode_output, forward, input_is_flat, prediction_to_result, update_metrics","BiaffineSecondaryDependencyDecoder, BiaffineSecondaryDependencyParsing","config, dataset, mask, optimizer, outputs, scores",,"arc, arc_2nd, punct_mask, rel_id, token, token_length","0, 0.002, 0.33, 0.9, 1, 100, 1e-12, 500, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-07 14:14",
https://github.com/hankcs/HanLP,lem.py,0,0.0,48,71.64,10,14.93,5,7.46,4,5.97,0,0.0,10,2,2,1,,"LinearDecoder, TransformerLemmatization, __init__, append_transform, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, classifier, compute_lens, compute_loss, config, contextualized_embeddings, criterion, decode_output, decoder, device, dict, encoder_size, forward, gradient_accumulation, hanlp, hanlp_common, hidden_size, input_is_flat, len, logger, logging, mask, mutable, num_labels, output, prediction, prediction_to_human, prediction_to_result, sampler_builder, self, str, super, torch, training, typing, update_metrics, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.lemmatizer.TransformerLemmatizer, hanlp.components.mtl.tasks.Task, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Linear, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, forward, input_is_flat, prediction_to_result, update_metrics","LinearDecoder, TransformerLemmatization","args, dataset","Transition based lemmatization (:cite:`kondratyuk-straka-2019-75`).

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level, which is never the case for
        lemmatization.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
    **kwargs: Not used."," Transition based lemmatization (:cite:`kondratyuk-straka-2019-75`).

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level, which is never the case for
                lemmatization.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
            **kwargs: Not used.
        , char_level, delimiter, hard_constraint, mask, max_seq_len, sent_delimiter, tag, tag_id, token","0.001, 1, False, None, True"," Transition based lemmatization (:cite:`kondratyuk-straka-2019-75`).

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
be split here.
char_level: Whether the sequence length is measured at char level, which is never the case for
lemmatization.
hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
in a sentence, it will be split at a token anyway.
token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
**kwargs: Not used.
, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-09 16:37",
https://github.com/hankcs/HanLP,pos.py,0,0.0,51,68.92,12,16.22,5,6.76,6,8.11,0,0.0,10,2,2,3,,"LinearCRFDecoder, TransformerTagging, __init__, append_transform, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, classifier, compute_lens, compute_loss, config, contextualized_embeddings, crf, criterion, decode_output, decoder, device, dict, dict_tags, encoder_size, forward, gradient_accumulation, hanlp, hanlp_common, hanlp_trie, hidden_size, input_is_flat, len, logger, logging, mask, mutable, num_labels, output, prediction, prediction_to_human, prediction_to_result, sampler_builder, self, str, super, torch, training, typing, update_metrics, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.taggers.transformers.transformer_tagger.TransformerTagger, hanlp.layers.crf.crf.CRF, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, hanlp_trie.DictInterface, hanlp_trie.TrieDict, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Linear, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Sequence, typing.Tuple, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, forward, input_is_flat, prediction_to_result, update_metrics","LinearCRFDecoder, TransformerTagging","args, dataset","A linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer on top of it.

Args:
    hidden_size: Size of hidden states.
    num_labels: Size of tag set.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`)., A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
any tagging tasks including PoS tagging and many others. It also features with a custom dictionary ``dict_tags``
to perform ``longest-prefix-matching`` which replaces matched tokens with given tags.


.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
    do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level, which is never the case for
        lemmatization.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
    dict_tags: A custom dictionary to override predicted tags by performing longest-prefix-matching.
    **kwargs: Not used., Args:
    contextualized_embeddings: Hidden states for contextual layer.
    batch: A dict of a batch.
    mask: Mask for tokens.

Returns:
    Logits. Users are expected to call ``CRF.decode`` on these emissions during decoding and ``CRF.forward``
    during training.","

        Args:
            contextualized_embeddings: Hidden states for contextual layer.
            batch: A dict of a batch.
            mask: Mask for tokens.

        Returns:
            Logits. Users are expected to call ``CRF.decode`` on these emissions during decoding and ``CRF.forward``
            during training.

        , A linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer on top of it.

        Args:
            hidden_size: Size of hidden states.
            num_labels: Size of tag set.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
        , A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
        any tagging tasks including PoS tagging and many others. It also features with a custom dictionary ``dict_tags``
        to perform ``longest-prefix-matching`` which replaces matched tokens with given tags.


        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level, which is never the case for
                lemmatization.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
            dict_tags: A custom dictionary to override predicted tags by performing longest-prefix-matching.
            **kwargs: Not used.
        , char_level, delimiter, hard_constraint, mask, max_seq_len, sent_delimiter, tag, tag_id, token","0.001, 1, False, None, True","

Args:
contextualized_embeddings: Hidden states for contextual layer.
batch: A dict of a batch.
mask: Mask for tokens.

Returns:
Logits. Users are expected to call ``CRF.decode`` on these emissions during decoding and ``CRF.forward``
during training.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-10-19 18:56, A linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer on top of it.

Args:
hidden_size: Size of hidden states.
num_labels: Size of tag set.
crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
, A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
any tagging tasks including PoS tagging and many others. It also features with a custom dictionary ``dict_tags``
to perform ``longest-prefix-matching`` which replaces matched tokens with given tags.


.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
be split here.
char_level: Whether the sequence length is measured at char level, which is never the case for
lemmatization.
hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
in a sentence, it will be split at a token anyway.
crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
dict_tags: A custom dictionary to override predicted tags by performing longest-prefix-matching.
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,sdp.py,0,0.0,86,79.63,6,5.56,13,12.04,3,2.78,0,0.0,12,1,20,1,,"BiaffineSemanticDependencyParsing, __init__, append, arc_scores, arcs, arcs_per_sent, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_samples, build_vocabs, cache_dataset, clone, compute_lens, compute_loss, config, convert_to_3d_mask, convert_to_3d_puncts, criterion, decay, decay_steps, decode, decode_output, decoder, deprels, device, encoder_size, epsilon, feed_batch, get, get_pad_dict, gradient_accumulation, hanlp, hanlp_common, heads, input_is_flat, inputs, isinstance, len, list, logger, logging, logits, lr, mask, mlp_dropout, mu, mutable, n_mlp_arc, n_mlp_rel, nu, optimizer, output, parameters, prediction, prediction_to_result, punct_mask, purge_cache, range, rel, rel_scores, rels, rels_per_sent, result, sampler_builder, self, sent_len, str, super, timer, tokens, tolist, torch, training, typing, update_metric, update_metrics, use_pos, vocab, vocabs, zip","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.TransformList, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.parsers.biaffine.biaffine_model.BiaffineDecoder, hanlp.components.parsers.biaffine.biaffine_sdp.BiaffineSemanticDependencyParser, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.utils.time_util.CountdownTimer, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.optim.Adam, torch.optim.lr_scheduler.ExponentialLR, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, build_optimizer, build_samples, compute_loss, decode_output, feed_batch, input_is_flat, prediction_to_result, update_metrics",BiaffineSemanticDependencyParsing,"arc_scores, arcs, arcs_per_sent, config, dataset, deprels, heads, logits, mask, optimizer, punct_mask, rel_scores, rels, rels_per_sent, result, scheduler, sent_len, timer, tokens, vocab","Implementation of ""Stanford's graph-based neural dependency parser at
the conll 2017 shared task"" (:cite:`dozat2017stanford`) and ""Establishing Strong Baselines for the New Decade""
(:cite:`he-choi-2019`).

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    punct: ``True`` to include punctuations in evaluation.
    pad_rel: Padding token for relations.
    apply_constraint: Enforce constraints (see following parameters).
    single_root: Force single root.
    no_zero_head: Every token has at least one head.
    n_mlp_arc: Number of features for arc representation.
    n_mlp_rel: Number of features for rel representation.
    mlp_dropout: Dropout applied to MLPs.
    mu: First coefficient used for computing running averages of gradient and its square in Adam.
    nu: Second coefficient used for computing running averages of gradient and its square in Adam.
    epsilon: Term added to the denominator to improve numerical stability
    decay: Decay rate for exceptional lr scheduler.
    decay_steps: Decay every ``decay_steps`` steps.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    use_pos: Use pos feature.
    **kwargs: Not used.","Implementation of ""Stanford's graph-based neural dependency parser at
        the conll 2017 shared task"" (:cite:`dozat2017stanford`) and ""Establishing Strong Baselines for the New Decade""
        (:cite:`he-choi-2019`).

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            punct: ``True`` to include punctuations in evaluation.
            pad_rel: Padding token for relations.
            apply_constraint: Enforce constraints (see following parameters).
            single_root: Force single root.
            no_zero_head: Every token has at least one head.
            n_mlp_arc: Number of features for arc representation.
            n_mlp_rel: Number of features for rel representation.
            mlp_dropout: Dropout applied to MLPs.
            mu: First coefficient used for computing running averages of gradient and its square in Adam.
            nu: Second coefficient used for computing running averages of gradient and its square in Adam.
            epsilon: Term added to the denominator to improve numerical stability
            decay: Decay rate for exceptional lr scheduler.
            decay_steps: Decay every ``decay_steps`` steps.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            use_pos: Use pos feature.
            **kwargs: Not used.
        , arc, punct_mask, rel, rel_id, token","0, 0.002, 0.33, 0.75, 0.9, 1, 100, 1e-12, 500, 5000, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-13 21:39",
https://github.com/hankcs/HanLP,ud.py,0,0.0,68,73.91,12,13.04,9,9.78,3,3.26,0,0.0,11,1,11,1,,"UniversalDependenciesParsing, __init__, _transform, append, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, clone, compute_lens, compute_loss, compute_mask, config, decode_output, decoder, dep, deprel, device, encoder_size, feat, feats, feed_batch, finalize_document, get, gradient_accumulation, hanlp, hanlp_common, head, input_is_flat, isinstance, lemma, len, logger, logging, mask, max_seq_len, mlp_dropout, mutable, n_mlp_arc, n_mlp_rel, output, output_dict, pop, pos, prediction, prediction_to_human, prediction_to_result, promoted, prune, punct, purge_cache, rel, sampler_builder, self, sent, str, super, task_name, torch, training, typing, update_metrics, upos, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.PunctuationMask, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.parsers.ud.ud_model.UniversalDependenciesDecoder, hanlp.components.parsers.ud.ud_parser.UniversalDependenciesParser, hanlp.components.parsers.ud.util.append_bos, hanlp.components.parsers.ud.util.generate_lemma_rule, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.conll.CoNLLUWord, hanlp_common.document.Document, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, feed_batch, finalize_document, input_is_flat, prediction_to_result, update_metrics",UniversalDependenciesParsing,"_transform, dataset, dep, feat, lem, mask, max_seq_len, output_dict, pos, promoted, sent","Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation
of ""75 Languages, 1 Model: Parsing Universal Dependencies Universally"" (:cite:`kondratyuk-straka-2019-75`).

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    n_mlp_arc: Number of features for arc representation.
    n_mlp_rel: Number of features for rel representation.
    mlp_dropout: Dropout applied to MLPs.
    tree: ``True`` to enforce tree constraint.
    proj: ``True`` for projective parsing.
    punct: ``True`` to include punctuations in evaluation.
    max_seq_len: Prune samples longer than this length. Useful for reducing GPU consumption.
    **kwargs: Not used.","Universal Dependencies Parsing (lemmatization, features, PoS tagging and dependency parsing) implementation
        of ""75 Languages, 1 Model: Parsing Universal Dependencies Universally"" (:cite:`kondratyuk-straka-2019-75`).

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            n_mlp_arc: Number of features for arc representation.
            n_mlp_rel: Number of features for rel representation.
            mlp_dropout: Dropout applied to MLPs.
            tree: ``True`` to enforce tree constraint.
            proj: ``True`` for projective parsing.
            punct: ``True`` to include punctuations in evaluation.
            max_seq_len: Prune samples longer than this length. Useful for reducing GPU consumption.
            **kwargs: Not used.
        , arc, dep, fea, feat, lem, loss, max_seq_len, pos, punct_mask, token, token_input_ids","0, 0.33, 1, 256, 4, 768, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-17 21:54",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,66,62.86,18,17.14,6,5.71,15,14.29,0,0.0,21,1,9,7,,"NotImplementedError, Task, __init__, _adjust_token, abc, append, batch, bool, build_batch_wise_scheduler, build_dataloader, build_metric, build_model, build_optimizer, build_samples, build_tokenizer, cache, cls_is_bos, compute_lens, compute_loss, config, copy, decode_output, decoder, delta, dependencies, dev, each, erase, evaluate_dataloader, execute_training_loop, feed_batch, finalize_document, fit_dataloader, hanlp, hanlp_common, input_ids, input_is_flat, inputs, isinstance, len, list, log, logging, lr, mask, os, pop, prediction_to_result, sampler_builder, scalar_mix, self, sent, sep_is_eos, separate_optimizer, timer, token_key, tokens, torch, transform_batch, trn, tst, typing, update_metrics, use_raw_hidden_states, warnings, zip","abc.ABC, abc.abstractmethod, copy.copy, hanlp.common.dataset.KMeansSamplerBuilder, hanlp.common.dataset.SamplerBuilder, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.dataset.TransformableDataset, hanlp.common.structure.ConfigTracker, hanlp.common.torch_component.TorchComponent, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.document.Document, hanlp_common.util.merge_locals_kwargs, logging.Logger, os.path.isfile, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union, warnings.warn","__init__, _adjust_token, build_batch_wise_scheduler, build_dataloader, build_metric, build_model, build_optimizer, build_samples, build_tokenizer, compute_lens, compute_loss, decode_output, evaluate_dataloader, execute_training_loop, feed_batch, finalize_document, fit_dataloader, input_is_flat, prediction_to_result, transform_batch, update_metrics",Task,"batch, delta, each, inputs, sampler_builder, sent, timer, tokenizer, tokens","A task in the multi-task learning framework

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    **kwargs: Additional config., Args:
    data: Samples to be measured or path to dataset during training time.
    dataset: During training time, use this dataset to measure the length of each sample inside.
    input_ids: Field name corresponds to input ids.

Returns:

    Length list of this samples, Build a dataloader for training or evaluation.

Args:
    data: Either a path or a list of samples.
    transform: The transform from MTL, which is usually [TransformerSequenceTokenizer, FieldLength('token')]
    training: Whether this method is called on training set.
    device: The device dataloader is intended to work with.
    logger: Logger for printing message indicating progress.
    cache: Whether the dataloader should be cached.
    gradient_accumulation: Gradient accumulation to be passed to sampler builder.
    **kwargs: Additional experimental arguments., Build a transformer tokenizer for this task.

Args:
    tokenizer: A tokenizer which is shared but can be adjusted to provide per-task settings.

Returns:
    A TransformerSequenceTokenizer., Build samples for this task. Called when this task is the first task. Default behaviour is to take inputs as
list of tokens and put these tokens into a dict per sample.

Args:
    inputs: Inputs from users, usually a list of lists of tokens.
    cls_is_bos: Insert BOS to the head of each sentence.
    sep_is_eos: Append EOS to the tail of each sentence.

Returns:
    List of samples., Check whether the data is flat (meaning that it's only a single sample, not even batched).

Returns:
    bool: ``True`` to indicate the input data is flat., Let the task transform the batch before feeding the batch into its decoder. The default behavior is to
adjust the head and tail of tokens, according to ``cls_is_bos``, ``sep_is_eos`` passed in and the two
settings of the task itself.

Args:
    batch: A batch of samples.
    results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
        uses both token and pos as features, then it will need both tok and pos results to make a batch.
    cls_is_bos: First token in this batch is BOS.
    sep_is_eos: Last token in this batch is EOS.

Returns:
    A batch.","

        Args:
            data: Samples to be measured or path to dataset during training time.
            dataset: During training time, use this dataset to measure the length of each sample inside.
            input_ids: Field name corresponds to input ids.

        Returns:

            Length list of this samples

        , 
        A task in the multi-task learning framework

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            **kwargs: Additional config.
        , 
        Build a dataloader for training or evaluation.

        Args:
            data: Either a path or a list of samples.
            transform: The transform from MTL, which is usually [TransformerSequenceTokenizer, FieldLength('token')]
            training: Whether this method is called on training set.
            device: The device dataloader is intended to work with.
            logger: Logger for printing message indicating progress.
            cache: Whether the dataloader should be cached.
            gradient_accumulation: Gradient accumulation to be passed to sampler builder.
            **kwargs: Additional experimental arguments.
        , 
        Build samples for this task. Called when this task is the first task. Default behaviour is to take inputs as
        list of tokens and put these tokens into a dict per sample.

        Args:
            inputs: Inputs from users, usually a list of lists of tokens.
            cls_is_bos: Insert BOS to the head of each sentence.
            sep_is_eos: Append EOS to the tail of each sentence.

        Returns:
            List of samples.

        , 
        Check whether the data is flat (meaning that it's only a single sample, not even batched).

        Returns:
            bool: ``True`` to indicate the input data is flat.
        , 
        Let the task transform the batch before feeding the batch into its decoder. The default behavior is to
        adjust the head and tail of tokens, according to ``cls_is_bos``, ``sep_is_eos`` passed in and the two
        settings of the task itself.

        Args:
            batch: A batch of samples.
            results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
                uses both token and pos as features, then it will need both tok and pos results to make a batch.
            cls_is_bos: First token in this batch is BOS.
            sep_is_eos: Last token in this batch is EOS.

        Returns:
            A batch.

        , ., Build a transformer tokenizer for this task.

        Args:
            tokenizer: A tokenizer which is shared but can be adjusted to provide per-task settings.

        Returns:
            A TransformerSequenceTokenizer.

        , Caching for the dataset is not enabled, try `dataset.purge_cache()` if possible. The dataset is , Preprocessing and caching samples [blink][yellow]...[/yellow][/blink], `input_is_flat()` needs to be implemented for the task component to accept raw input from user., dev, token, token_, token_input_ids, token_length, trn, tst","0, 1, 32, False, None, True","

Args:
data: Samples to be measured or path to dataset during training time.
dataset: During training time, use this dataset to measure the length of each sample inside.
input_ids: Field name corresponds to input ids.

Returns:

Length list of this samples

, 
A task in the multi-task learning framework

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
**kwargs: Additional config.
, 
Build a dataloader for training or evaluation.

Args:
data: Either a path or a list of samples.
transform: The transform from MTL, which is usually [TransformerSequenceTokenizer, FieldLength('token')]
training: Whether this method is called on training set.
device: The device dataloader is intended to work with.
logger: Logger for printing message indicating progress.
cache: Whether the dataloader should be cached.
gradient_accumulation: Gradient accumulation to be passed to sampler builder.
**kwargs: Additional experimental arguments.
, 
Build samples for this task. Called when this task is the first task. Default behaviour is to take inputs as
list of tokens and put these tokens into a dict per sample.

Args:
inputs: Inputs from users, usually a list of lists of tokens.
cls_is_bos: Insert BOS to the head of each sentence.
sep_is_eos: Append EOS to the tail of each sentence.

Returns:
List of samples.

, 
Check whether the data is flat (meaning that it's only a single sample, not even batched).

Returns:
bool: ``True`` to indicate the input data is flat.
, 
Let the task transform the batch before feeding the batch into its decoder. The default behavior is to
adjust the head and tail of tokens, according to ``cls_is_bos``, ``sep_is_eos`` passed in and the two
settings of the task itself.

Args:
batch: A batch of samples.
results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
uses both token and pos as features, then it will need both tok and pos results to make a batch.
cls_is_bos: First token in this batch is BOS.
sep_is_eos: Last token in this batch is EOS.

Returns:
A batch.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-02 16:51, # anonymize local file names, # inputs: List[List[str]],, # noinspection PyMethodMayBeStatic, # noinspection PyMethodOverriding, # noinspection PyMissingConstructor, Build a transformer tokenizer for this task.

Args:
tokenizer: A tokenizer which is shared but can be adjusted to provide per-task settings.

Returns:
A TransformerSequenceTokenizer.

",
https://github.com/hankcs/HanLP,biaffine_ner.py,0,0.0,45,73.77,7,11.48,5,8.2,4,6.56,0,0.0,9,1,3,1,,"BiaffineNamedEntityRecognition, __init__, append, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, compute_lens, compute_loss, config, copy, decode_output, device, encoder_size, ffnn_size, get, get_pred_ner, gradient_accumulation, hanlp, hanlp_common, input_is_flat, label, len, logger, logging, loss_reduction, mutable, output, prediction, prediction_to_result, purge_cache, results, sampler_builder, self, str, super, torch, training, typing, update_metrics, vocabs","copy.copy, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.TransformList, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.ner.biaffine_ner.biaffine_ner.BiaffineNamedEntityRecognizer, hanlp.components.ner.biaffine_ner.biaffine_ner_model.BiaffineNamedEntityRecognitionDecoder, hanlp.datasets.ner.loaders.json_ner.unpack_ner, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, input_is_flat, prediction_to_result, update_metrics",BiaffineNamedEntityRecognition,"dataset, results, transform","An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
assumption about the spans, it naturally supports flat NER and nested NER.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
    is_flat_ner: ``True`` for flat NER, otherwise nested NER.
    tagset: Optional tagset to prune entities outside of this tagset from datasets.
    ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
    ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
    loss_reduction: The loss reduction used in aggregating losses.
    **kwargs: Not used."," , An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
        every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
        label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
        assumption about the spans, it naturally supports flat NER and nested NER.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
            is_flat_ner: ``True`` for flat NER, otherwise nested NER.
            tagset: Optional tagset to prune entities outside of this tagset from datasets.
            ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
            ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
            loss_reduction: The loss reduction used in aggregating losses.
            **kwargs: Not used.
        , candidate_ner_scores, loss, mean, ret_tokens, token","1, 150, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-05 01:49, An implementation of Named Entity Recognition as Dependency Parsing (:cite:`yu-etal-2020-named`). It treats
every possible span as a candidate of entity and predicts its entity label. Non-entity spans are assigned NULL
label to be excluded. The label prediction is done with a biaffine layer (:cite:`dozat:17a`). As it makes no
assumption about the spans, it naturally supports flat NER and nested NER.

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
is_flat_ner: ``True`` for flat NER, otherwise nested NER.
tagset: Optional tagset to prune entities outside of this tagset from datasets.
ret_tokens: A delimiter between tokens in entities so that the surface form of an entity can be rebuilt.
ffnn_size: Feedforward size for MLPs extracting the head/tail representations.
loss_reduction: The loss reduction used in aggregating losses.
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,tag_ner.py,0,0.0,56,75.68,10,13.51,5,6.76,3,4.05,0,0.0,10,2,3,1,,"LinearCRFDecoder, TaggingNamedEntityRecognition, __init__, append_transform, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, cache, classifier, compute_lens, compute_loss, config, contextualized_embeddings, crf, criterion, decode_output, decoder, device, dict, dict_blacklist, dict_tags, dict_whitelist, encoder_size, forward, gradient_accumulation, hanlp, hanlp_common, hanlp_trie, hidden_size, input_is_flat, len, logger, logging, mask, mutable, num_labels, output, prediction, prediction_to_human, prediction_to_result, purge_cache, sampler_builder, secondary_encoder, self, str, super, torch, training, typing, update_metrics, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.ner.transformer_ner.TransformerNamedEntityRecognizer, hanlp.layers.crf.crf.CRF, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, hanlp_trie.DictInterface, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Linear, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Sequence, typing.Set, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, forward, input_is_flat, prediction_to_result, update_metrics","LinearCRFDecoder, TaggingNamedEntityRecognition","args, contextualized_embeddings, dataset","A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.
During decoding, it performs longest-prefix-matching of these words to override the prediction from
underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.

.. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
    do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level, which is never the case for
        lemmatization.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining
        tokens during decoding.
    merge_types: The types of consecutive entities to be merged.
    secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden
        states from the main encoder as input.
    token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
    dict_whitelist: A :class:`dict` or a :class:`~hanlp_trie.dictionary.DictInterface` of gazetteers to be
        included into the final results.
    dict_blacklist: A :class:`set` or a :class:`~hanlp_trie.dictionary.DictInterface` of badcases to be
        excluded from the final results.
    **kwargs:","A simple tagger using a linear layer with an optional CRF (:cite:`lafferty2001conditional`) layer for
        NER task. It can utilize whitelist gazetteers which is dict mapping from entity name to entity type.
        During decoding, it performs longest-prefix-matching of these words to override the prediction from
        underlying statistical model. It also uses a blacklist to mask out mis-predicted  entities.

        .. Note:: For algorithm beginners, longest-prefix-matching is the prerequisite to understand what dictionary can
            do and what it can't do. The tutorial in `this book <http://nlp.hankcs.com/book.php>`_ can be very helpful.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level, which is never the case for
                lemmatization.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            delimiter_in_entity: The delimiter between tokens in entity, which is used to rebuild entity by joining
                tokens during decoding.
            merge_types: The types of consecutive entities to be merged.
            secondary_encoder: An optional secondary encoder to provide enhanced representation by taking the hidden
                states from the main encoder as input.
            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
            dict_whitelist: A :class:`dict` or a :class:`~hanlp_trie.dictionary.DictInterface` of gazetteers to be
                included into the final results.
            dict_blacklist: A :class:`set` or a :class:`~hanlp_trie.dictionary.DictInterface` of badcases to be
                excluded from the final results.
            **kwargs:
        , char_level, delimiter, hard_constraint, mask, max_seq_len, sent_delimiter, tag, tag_id, token","0.001, 1, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-03 14:35",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-03 14:34",
https://github.com/hankcs/HanLP,bio_srl.py,0,0.0,50,75.76,4,6.06,7,10.61,5,7.58,0,0.0,10,1,5,1,,"SpanBIOSemanticRoleLabeling, __init__, batch, bool, build, build_dataloader, build_dataset, build_metric, build_model, build_vocabs, compute_lens, compute_loss, compute_mask, config, crf, criterion, decode_output, decoder, device, encoder_size, feed_batch, flatten, gradient_accumulation, hanlp, hanlp_common, input_is_flat, len, logger, logging, mask, mask3d, mlp_dropout, mutable, n_mlp_rel, numel, output, pred, prediction, prediction_to_result, purge_cache, sampler_builder, self, str, super, token_index, torch, training, typing, update_metrics, vocabs","F.log_softmax, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.srl.span_bio.baffine_tagging.BiaffineTaggingDecoder, hanlp.components.srl.span_bio.span_bio.SpanBIOSemanticRoleLabeler, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_dataloader, build_metric, build_model, compute_loss, decode_output, feed_batch, input_is_flat, prediction_to_result, update_metrics",SpanBIOSemanticRoleLabeling,"dataset, mask, mask3d, pred, token_index","A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    n_mlp_rel: Output size of MLPs for representing predicate and tokens.
    mlp_dropout: Dropout applied to MLPs.
    loss_reduction: Loss reduction for aggregating losses.
    doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
    **kwargs: Not used.","A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
        predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            n_mlp_rel: Output size of MLPs for representing predicate and tokens.
            mlp_dropout: Dropout applied to MLPs.
            loss_reduction: Loss reduction for aggregating losses.
            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
            **kwargs: Not used.
        , mean, srl, srl_id","0, 0.2, 1, 300, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-04 16:50, # No tokens, don't bother to run the decoder, A span based Semantic Role Labeling task using BIO scheme for tagging the role of each token. Given a
predicate and a token, it uses biaffine (:cite:`dozat:17a`) to predict their relations as one of BIO-ROLE.

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
n_mlp_rel: Output size of MLPs for representing predicate and tokens.
mlp_dropout: Dropout applied to MLPs.
loss_reduction: Loss reduction for aggregating losses.
doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,rank_srl.py,0,0.0,45,63.38,9,12.68,14,19.72,3,4.23,0,0.0,10,1,3,1,,"SpanRankingSemanticRoleLabeling, __init__, batch, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, compute_lens, compute_loss, config, decode_output, device, encoder_size, end_to_end_f1, format_dict_to_results, get, gradient_accumulation, hanlp, hanlp_common, input_is_flat, isinstance, len, list, logger, logging, output, predicate_f1, prediction, prediction_to_result, purge_cache, sampler_builder, self, srl_label, str, super, torch, training, tuple, typing, update_metrics, values, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.srl.span_rank.span_rank.SpanRankingSemanticRoleLabeler, hanlp.components.srl.span_rank.span_ranking_srl_model.SpanRankingSRLDecoder, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Callable, typing.Dict, typing.Iterable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, compute_loss, decode_output, input_is_flat, prediction_to_result, update_metrics",SpanRankingSemanticRoleLabeling,"dataset, end_to_end_f1, predicate_f1","An implementation of ""Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling""
(:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    lexical_dropout: Dropout applied to hidden states of encoder.
    dropout: Dropout used for other layers except the encoder.
    span_width_feature_size: Span width feature size.
    ffnn_size: Feedforward size.
    ffnn_depth: Number of layers of feedforward MLPs.
    argument_ratio: Ratio of candidate arguments over number of tokens.
    predicate_ratio: Ratio of candidate predicates over number of tokens.
    max_arg_width: Maximum argument width.
    mlp_label_size: Feature size for label representation.
    enforce_srl_constraint: Enforce SRL constraints (number of core ARGs etc.).
    use_gold_predicates: Use gold predicates instead of predicting them.
    doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
    use_biaffine: ``True`` to use biaffine (:cite:`dozat:17a`) instead of lineary layer for label prediction.
    loss_reduction: The loss reduction used in aggregating losses.
    with_argument: The delimiter between tokens in arguments to be used for joining tokens for outputs.
    **kwargs: Not used."," ,  An implementation of ""Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling""
        (:cite:`he-etal-2018-jointly`). It generates candidates triples of (predicate, arg_start, arg_end) and rank them.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            lexical_dropout: Dropout applied to hidden states of encoder.
            dropout: Dropout used for other layers except the encoder.
            span_width_feature_size: Span width feature size.
            ffnn_size: Feedforward size.
            ffnn_depth: Number of layers of feedforward MLPs.
            argument_ratio: Ratio of candidate arguments over number of tokens.
            predicate_ratio: Ratio of candidate predicates over number of tokens.
            max_arg_width: Maximum argument width.
            mlp_label_size: Feature size for label representation.
            enforce_srl_constraint: Enforce SRL constraints (number of core ARGs etc.).
            use_gold_predicates: Use gold predicates instead of predicting them.
            doc_level_offset: ``True`` to indicate the offsets in ``jsonlines`` are of document level.
            use_biaffine: ``True`` to use biaffine (:cite:`dozat:17a`) instead of lineary layer for label prediction.
            loss_reduction: The loss reduction used in aggregating losses.
            with_argument: The delimiter between tokens in arguments to be used for joining tokens for outputs.
            **kwargs: Not used.
        , e2e, loss, mean, predicate, prediction, token, with_argument","0.001, 0.2, 0.4, 0.5, 0.8, 1, 100, 150, 2, 20, 30, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-05 15:43",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-12-04 16:49",
https://github.com/hankcs/HanLP,reg_tok.py,0,0.0,58,77.33,6,8.0,7,9.33,4,5.33,0,0.0,11,2,6,0,,"RegressionTokenization, RegressionTokenizingDecoder, __init__, append, batch, bias, bool, build, build_criterion, build_dataloader, build_metric, build_model, compute_lens, compute_loss, config, criterion, decode_output, decode_spans, device, dict, encoder_size, enumerate, forward, generate_token_span_tuple, get, hanlp, hanlp_common, in_features, input, int, isinstance, len, lengths_to_mask, list, logging, mask, max_seq_len, out_features, output, predict, prediction, prefix_mask, prefix_mask_, previous_prefix, sample, sampler_builder, self, sent_delimiter, spans, str, super, torch, torch_util, training, typing, update, update_metrics, utils","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.FieldLength, hanlp.common.transform.TransformList, hanlp.components.mtl.tasks.Task, hanlp.datasets.tokenization.loaders.txt.TextTokenizingDataset, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.layers.transformers.pt_imports.PreTrainedTokenizer, hanlp.metrics.chunking.binary_chunking_f1.BinaryChunkingF1, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp_common.util.merge_locals_kwargs, logging.Logger, torch.Tensor, torch.float, torch.nn.BCEWithLogitsLoss, torch.nn.Linear, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Tuple, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, compute_loss, decode_output, forward, generate_token_span_tuple, predict, update_metrics","RegressionTokenization, RegressionTokenizingDecoder","dataset, mask, prefix_mask, prefix_mask_, previous_prefix, spans",,"mean, span_tuple, text, text_input_ids, text_input_ids_length, text_prefix_mask","0, 0.001, 1, 2, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-02 16:51, # noinspection PyMethodOverriding",
https://github.com/hankcs/HanLP,tag_tok.py,0,0.0,79,70.54,18,16.07,6,5.36,9,8.04,0,0.0,16,2,6,2,,"LinearCRFDecoder, TaggingTokenization, __init__, append, batch, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_samples, build_tokenizer, build_vocabs, cache, classifier, compute_lens, compute_loss, config, contextualized_embeddings, crf, criterion, decode_output, decoder, device, dict, dict_combine, dict_force, dictionary, encoder_size, fget, forward, fset, gradient_accumulation, hanlp, hanlp_common, hanlp_trie, hidden_size, index_by_type, input_is_flat, input_key, inputs, insert, is_fast, isinstance, last_transform, len, logger, logging, mask, max_seq_length, model, mutable, num_labels, output, output_key, prediction, prediction_to_human, prediction_to_result, property, purge_cache, sampler_builder, self, sent, setter, str, super, token_key, tokenizer, torch, training, transform, transform_batch, transformer_index, truncate_long_sequences, typing, update_metrics, vocabs","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.transform.TransformList, hanlp.common.transform.VocabDict, hanlp.components.mtl.tasks.Task, hanlp.components.tokenizers.transformer.TransformerTaggingTokenizer, hanlp.layers.crf.crf.CRF, hanlp.layers.scalar_mix.ScalarMixWithDropoutBuilder, hanlp.metrics.metric.Metric, hanlp.metrics.mtl.MetricDict, hanlp.transform.transformer_tokenizer.TransformerSequenceTokenizer, hanlp_common.util.merge_locals_kwargs, hanlp_trie.DictInterface, hanlp_trie.TrieDict, logging.Logger, torch.BoolTensor, torch.FloatTensor, torch.Tensor, torch.nn.Linear, torch.nn.Module, torch.utils.data.DataLoader, typing.Any, typing.Dict, typing.Iterable, typing.List, typing.Set, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_samples, build_tokenizer, compute_loss, decode_output, dict_combine, dict_force, forward, input_is_flat, prediction_to_result, transform_batch, update_metrics","LinearCRFDecoder, TaggingTokenization","args, dataset, dictionary, sent, transform, transformer_index","This method is overrode to honor the zero indexed token used in custom dict. Although for a tokenizer,
cls_is_bos = sep_is_eos = True, its tokens don't contain [CLS] or [SEP]. This behaviour is adopted from the
early versions and it is better kept to avoid migration efforts.


Args:
    batch: A batch of samples.
    results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
        uses both token and pos as features, then it will need both tok and pos results to make a batch.
    cls_is_bos: First token in this batch is BOS.
    sep_is_eos: Last token in this batch is EOS.

Returns:
    A batch., Tokenization which casts a chunking problem into a tagging problem.
This task has to create batch of tokens containing both [CLS] and [SEP] since it's usually the first task
and later tasks might need them.

Args:
    trn: Path to training set.
    dev: Path to dev set.
    tst: Path to test set.
    sampler_builder: A builder which builds a sampler.
    dependencies: Its dependencies on other tasks.
    scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
    use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
    lr: Learning rate for this task.
    separate_optimizer: Use customized separate optimizer for this task.
    cls_is_bos: ``True`` to treat the first token as ``BOS``.
    sep_is_eos: ``True`` to treat the last token as ``EOS``.
    delimiter: Delimiter used to split a line in the corpus.
    max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
    sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
        be split here.
    char_level: Whether the sequence length is measured at char level.
    hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
        in a sentence, it will be split at a token anyway.
    transform: An optional transform to be applied to samples. Usually a character normalization transform is
        passed in.
    tagging_scheme: Either ``BMES`` or ``BI``.
    crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
    token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
    **kwargs: Not used.","
        This method is overrode to honor the zero indexed token used in custom dict. Although for a tokenizer,
        cls_is_bos = sep_is_eos = True, its tokens don't contain [CLS] or [SEP]. This behaviour is adopted from the
        early versions and it is better kept to avoid migration efforts.


        Args:
            batch: A batch of samples.
            results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
                uses both token and pos as features, then it will need both tok and pos results to make a batch.
            cls_is_bos: First token in this batch is BOS.
            sep_is_eos: Last token in this batch is EOS.

        Returns:
            A batch.

        , BMES, Tokenization which casts a chunking problem into a tagging problem.
        This task has to create batch of tokens containing both [CLS] and [SEP] since it's usually the first task
        and later tasks might need them.

        Args:
            trn: Path to training set.
            dev: Path to dev set.
            tst: Path to test set.
            sampler_builder: A builder which builds a sampler.
            dependencies: Its dependencies on other tasks.
            scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
            use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
            lr: Learning rate for this task.
            separate_optimizer: Use customized separate optimizer for this task.
            cls_is_bos: ``True`` to treat the first token as ``BOS``.
            sep_is_eos: ``True`` to treat the last token as ``EOS``.
            delimiter: Delimiter used to split a line in the corpus.
            max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
            sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
                be split here.
            char_level: Whether the sequence length is measured at char level.
            hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
                in a sentence, it will be split at a token anyway.
            transform: An optional transform to be applied to samples. Usually a character normalization transform is
                passed in.
            tagging_scheme: Either ``BMES`` or ``BI``.
            crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
            token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
            **kwargs: Not used.
        , __class__, char_level, delimiter, dict_combine, dict_force, hard_constraint, kwargs, mask, max_seq_len, mean, self, sent_delimiter, tag, tag_id, token","0, 0.001, 1, False, None, True","
This method is overrode to honor the zero indexed token used in custom dict. Although for a tokenizer,
cls_is_bos = sep_is_eos = True, its tokens don't contain [CLS] or [SEP]. This behaviour is adopted from the
early versions and it is better kept to avoid migration efforts.


Args:
batch: A batch of samples.
results: Predicted results from other tasks which might be useful for this task to utilize. Say a dep task
uses both token and pos as features, then it will need both tok and pos results to make a batch.
cls_is_bos: First token in this batch is BOS.
sep_is_eos: Last token in this batch is EOS.

Returns:
A batch.

, # -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-11 16:35, # The transform for tokenizer needs very special settings, ensure these settings are set properly., # We only need those transforms before TransformerTokenizer, # avoid to config, # noinspection PyArgumentList, Tokenization which casts a chunking problem into a tagging problem.
This task has to create batch of tokens containing both [CLS] and [SEP] since it's usually the first task
and later tasks might need them.

Args:
trn: Path to training set.
dev: Path to dev set.
tst: Path to test set.
sampler_builder: A builder which builds a sampler.
dependencies: Its dependencies on other tasks.
scalar_mix: A builder which builds a `ScalarMixWithDropout` object.
use_raw_hidden_states: Whether to use raw hidden states from transformer without any pooling.
lr: Learning rate for this task.
separate_optimizer: Use customized separate optimizer for this task.
cls_is_bos: ``True`` to treat the first token as ``BOS``.
sep_is_eos: ``True`` to treat the last token as ``EOS``.
delimiter: Delimiter used to split a line in the corpus.
max_seq_len: Sentences longer than ``max_seq_len`` will be split into shorter ones if possible.
sent_delimiter: Delimiter between sentences, like period or comma, which indicates a long sentence can
be split here.
char_level: Whether the sequence length is measured at char level.
hard_constraint: Whether to enforce hard length constraint on sentences. If there is no ``sent_delimiter``
in a sentence, it will be split at a token anyway.
transform: An optional transform to be applied to samples. Usually a character normalization transform is
passed in.
tagging_scheme: Either ``BMES`` or ``BI``.
crf: ``True`` to enable CRF (:cite:`lafferty2001conditional`).
token_key: The key to tokens in dataset. This should always be set to ``token`` in MTL.
**kwargs: Not used.
",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2020-08-11 16:34",
https://github.com/hankcs/HanLP,bart_amr_generation.py,0,0.0,70,74.47,9,9.57,10,10.64,5,5.32,0,0.0,15,1,17,0,,"BART_AMR_Generation, __init__, append_transform, batch, batch_decode, batch_size, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, dataloader, decoded_preds, device, eos_token_id, eval, evaluate_dataloader, execute_training_loop, extend, filename, fit_dataloader, flat, from_pretrained, generate, hanlp, hanlp_common, input_ids, input_is_flat, isdir, isinstance, kwargs, len, load_config, load_vocabs, load_weights, log, logging, max_length, model, num_beams, orders, os, pad_token_id, path, penman, pieces, predict, predict_batch, preds, resize_token_embeddings, results, sampler, sampler_builder, save_dir, self, shuffle, str, strip, super, timer, tokenize, tokenizer, torch, training, transformer, transformer_config, typing, verbose","hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.torch_component.TorchComponent, hanlp.components.amr.amrbart.data_interface.dataset.AMR2TextDataSet, hanlp.components.amr.amrbart.model_interface.modeling_bart.BartForConditionalGeneration, hanlp.components.amr.amrbart.model_interface.tokenization_bart.AMRBartTokenizer, hanlp.components.amr.amrbart.preprocess.read_and_process.dfs_linearize, hanlp.components.amr.seq2seq.dataset.dataset.AMRDataset, hanlp.layers.transformers.pt_imports.AutoConfig_, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.reorder, logging.Logger, penman.Graph, penman.loads, torch.nn.Module, torch.utils.data.DataLoader, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, evaluate_dataloader, execute_training_loop, fit_dataloader, input_is_flat, load_config, load_vocabs, load_weights, predict, predict_batch",BART_AMR_Generation,"batch, data, dataloader, dataset, decoded_preds, flat, input_ids, model, orders, pieces, preds, results, sampler, sampler_builder, timer, tokenizer, transformer",," , amr, config.json, input_ids, labels, lamr, model.pt, text, vocabs.json","0, 1, 1.0, 1024, 32, 5, 500, False, None, True","# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-05 17:56, # noinspection PyUnboundLocalVariable, # tokens = batch['tgt']",
https://github.com/hankcs/HanLP,bart_amr_parser.py,0,0.0,107,75.35,13,9.15,10,7.04,12,8.45,0,0.0,16,1,31,0,,"BART_AMR_Parser, __init__, amr_bos_token_id, amr_eos_token_id, append, append_transform, backr, backreferences, batch, batch_size, bos_token_id, build, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, config, copy, cstr, dataloader, datetime, decode_amr, device, enumerate, eos_token_id, epidata, eval, evaluate, evaluate_dataloader, execute_training_loop, extend, filename, fit_dataloader, flat, from_pretrained, generate, gp, graph, graphs, graphs_per_batch, graphs_same_source, gs, hanlp, hanlp_common, idx, input, input_ids, input_is_flat, isdir, isinstance, ith_pred, itm, j, kwargs, len, load_config, load_vocabs, load_weights, log, logger, logging, max_length, metadata, model, name_or_path, nodes, num_beams, orders, os, output, pad_token_id, path, pieces, predict, predict_batch, preds, range, resize_token_embeddings, results, sampler, sampler_builder, save_dir, self, shuffle, sorted, status, str, super, timer, tokenize, tokenizer, tokens, top, torch, total, training, transformer, transformer_config, triples, tst_data, tuple, typing, use_fast, verbose, x, zip","datetime.datetime.now, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.torch_component.TorchComponent, hanlp.components.amr.amrbart.data_interface.dataset.AMRParsingDataSet, hanlp.components.amr.amrbart.model_interface.modeling_bart.BartForConditionalGeneration, hanlp.components.amr.amrbart.model_interface.tokenization_bart.AMRBartTokenizer, hanlp.components.amr.seq2seq.dataset.dataset.AMRDataset, hanlp.components.amr.seq2seq.dataset.penman.AMRGraph, hanlp.components.amr.seq2seq.evaluation.compute_smatch, hanlp.components.amr.seq2seq.evaluation.write_predictions, hanlp.layers.transformers.pt_imports.AutoConfig_, hanlp.metrics.amr.smatch_eval.smatch_eval, hanlp.metrics.mtl.MetricDict, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.reorder, logging.Logger, torch.nn.Module, torch.no_grad, torch.utils.data.DataLoader, typing.Callable, typing.List, typing.Union","__init__, build_criterion, build_dataloader, build_metric, build_model, build_optimizer, evaluate, evaluate_dataloader, execute_training_loop, fit_dataloader, input_is_flat, load_config, load_vocabs, load_weights, predict, predict_batch",BART_AMR_Parser,"backr, batch, data, dataloader, dataset, flat, gp, graph, graphs, graphs_per_batch, graphs_same_source, gs, idx, input_ids, ith_pred, itm, j, metadata, model, orders, pieces, preds, results, sampler, sampler_builder, smatch, status, timer, tokenizer, transformer, x",,"-amr, .2%, amr, annotator, config.json, date, input_ids, labels, model.pt, save-date, snt, text, vocabs.json","0, 1, 1.0, 1024, 32, 5, 500, False, None, True","#                        ""snt"": snt.replace(""<AMR>"", '').replace(""</AMR>"", '').strip()}, #         gp.metadata = {""id"": str(idx), ""annotator"": ""bart-amr"",, #     for gp in gps:, # -*- coding:utf-8 -*-, # Author: hankcs, # Copy meta data from gold graph, # Date: 2022-12-05 17:56, # assert len(graphs) == len(tokens), f""inconsistent lengths {len(graphs)} vs {len(tokens)}"", # for idx, gps, snt in zip(batch[IDX], graphs, tokens):, # inputs, logits, labels, loss = torch.load('/local/scratch/hhe43/amrbart/batch.pt'), # noinspection PyUnboundLocalVariable, # tokens = batch['tgt']",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-05 17:53",
https://github.com/hankcs/HanLP,evaluation.py,0,0.0,22,88.0,2,8.0,1,4.0,0,0.0,0,0.0,3,0,4,0,,"INIT, compute_bleu, compute_smatch, g, gold, gold_sentences, graphs, next, pathlib, penman, perin_parser, pieces, pred, pred_sentences, predictions_path, replace, sacrebleu, score, score_amr_pairs, text, tokenizer, write_predictions","pathlib.Path, penman.encode, perin_parser.thirdparty.mtool.smatch, sacrebleu.corpus_bleu","compute_bleu, compute_smatch, write_predictions",,"g, pieces, score, text",,", 

",2,,
https://github.com/hankcs/HanLP,optim.py,0,0.0,51,62.96,14,17.28,14,17.28,2,2.47,0,0.0,3,1,18,0,,"N_sma, N_sma_max, RAdam, RuntimeError, ValueError, _, __init__, __setstate__, add_, addcdiv_, beta1, beta2, beta2_t, betas, buffered, closure, copy_, data, defaults, degenerated_to_sgd, denom, dict, eps, exp_avg, exp_avg_sq, float, grad, group, int, is_sparse, isinstance, len, list, loss, lr, math, mul_, p_data_fp32, param, param_groups, params, range, self, sqrt, state, step, step_size, super, torch, tuple, weight_decay","math.sqrt, torch.optim.optimizer.Optimizer, torch.zeros_like","__init__, __setstate__, step",RAdam,"N_sma, N_sma_max, _, beta1, beta2, beta2_t, buffered, defaults, denom, exp_avg, exp_avg_sq, grad, group, loss, p_data_fp32, param, state, step_size",,"Invalid beta parameter at index 0: {}, Invalid beta parameter at index 1: {}, Invalid epsilon value: {}, Invalid learning rate: {}, RAdam does not support sparse gradients, betas, buffer, eps, exp_avg, exp_avg_sq, lr, params, step, weight_decay","0, 0.0, 0.001, 0.9, 0.999, 1, 1.0, 10, 1e-08, 2, 4, 5, None, True","# more conservative since it's an approximated value, # taken from",
https://github.com/hankcs/HanLP,seq2seq_amr_parser.py,0,0.0,210,43.75,238,49.58,19,3.96,13,2.71,0,0.0,28,1,67,1,,"INIT, NotImplemented, Seq2seq_AMR_Parser, __init__, _create_dataloader, _get_model_cls, _get_pad_dict, _init_new_embeddings, _model_generate, _step, _tok_bpe, _tokenizer, _transformer_config, add, additional_tokens, append, append_transform, attention_dropout, attention_mask, backr, backreferences, backward, batch, batch_size, beam_size, beautiful_amr_graph, best_epoch, best_metric, bool, build, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_tokenizer, build_vocabs, cache, cls, collapse_name_ops, collect_additional_tokens, config, copy, criterion, cstr, current, data, dataloader, datetime, decode_amr, decoder_input_ids, dev, dev_data, dev_metric, device, dropout, each, elapsed_average_human, elapsed_human, embed_tokens, encoder, endswith, enumerate, epidata, epoch, epochs, eta_human, eval, eval_after, evaluate, evaluate_dataloader, execute_training_loop, extend, feed_batch, finalize_dataset, fit, fit_dataloader, flat, float, frames, freq, from_pretrained, functools, generate, generate_idx, get, get_frames, get_roles, gp, grad_norm, gradient_accumulation, graph, graphs, graphs_per_batch, graphs_same_source, hanlp, hanlp_common, history, i1, i2, idx, idx_split, info, input, input_ids, input_is_flat, int, isinstance, item, items, labels, len, list, load_weights, lock, log, logger, logging, loss, lr, lstrip, max, max_num_tokens, metadata, model, modified, mutable, ne, no_repeat_ngram_size, nodes, noise, num_training_steps, old_enc_size, on_config_ready, optimizer, orders, os, out, output, output_dict, output_past, pad_token_id, parameters, pred_min, predict, predict_amrs, prefix, purge_cache, range, ratio_percentage, ratio_width, raw_graph, recategorization_tokens, report, report_metrics, resize_token_embeddings, results, ret_speed, sampler, sampler_builder, save_dir, save_weights, scheduler, self, shuffle, size, sorted, split, startswith, staticmethod, status, step, summary, super, timer, tok_split, tok_split_, tokens, tokens_same_source, tokk, top, torch, total, total_loss, total_time_human, train, training, transformer, transformers, triples, trn, tst_data, tt, tuple, typing, uniform_, update, use_fast, use_pointer_tokens, vec, vec_split, vecs, vocabs, warmup_steps, weight, weight_decay, x, zero_grad, zip","datetime.datetime.now, functools.partial, hanlp.common.dataset.PadSequenceDataLoader, hanlp.common.dataset.SamplerBuilder, hanlp.common.dataset.SortingSamplerBuilder, hanlp.common.structure.History, hanlp.common.torch_component.TorchComponent, hanlp.common.vocab.Vocab, hanlp.components.amr.seq2seq.dataset.dataset.AMRDataset, hanlp.components.amr.seq2seq.dataset.dataset.dfs_linearize_tokenize, hanlp.components.amr.seq2seq.dataset.penman.AMRGraph, hanlp.components.amr.seq2seq.dataset.tokenization_bart.PENMANBartTokenizer, hanlp.components.amr.seq2seq.dataset.tokenization_t5.PENMANT5Tokenizer, hanlp.components.amr.seq2seq.evaluation.compute_smatch, hanlp.components.amr.seq2seq.evaluation.write_predictions, hanlp.components.amr.seq2seq.optim.RAdam, hanlp.layers.transformers.pt_imports.AutoConfig_, hanlp.layers.transformers.pt_imports.PretrainedConfig, hanlp.layers.transformers.resource.get_model_mirror, hanlp.layers.transformers.resource.get_tokenizer_mirror, hanlp.metrics.amr.smatch_eval.smatch_eval, hanlp.metrics.mtl.MetricDict, hanlp.utils.time_util.CountdownTimer, hanlp_common.constant.IDX, hanlp_common.util.merge_locals_kwargs, hanlp_common.util.reorder, logging.Logger, os.path.join, torch.empty_like, torch.long, torch.nn.Module, torch.nn.utils.clip_grad_norm_, torch.no_grad, torch.stack, torch.utils.data.DataLoader, torch.utils.data.Dataset, transformers.T5ForConditionalGeneration, transformers.get_constant_schedule_with_warmup, transformers.models.bart.modeling_bart.BartForConditionalGeneration, typing.Callable, typing.List, typing.Union","__init__, _create_dataloader, _get_model_cls, _get_pad_dict, _init_new_embeddings, _model_generate, _step, build_criterion, build_dataloader, build_dataset, build_metric, build_model, build_optimizer, build_tokenizer, build_vocabs, collect_additional_tokens, evaluate, evaluate_dataloader, execute_training_loop, feed_batch, finalize_dataset, fit, fit_dataloader, input_is_flat, on_config_ready, predict, predict_amrs, report_metrics",Seq2seq_AMR_Parser,"additional_tokens, attention_mask, backr, batch, best_epoch, best_metric, cls, config, data, dataloader, dataset, decoder_input_ids, dev_metric, each, encoder, epoch, eval_after, flat, frames, freq, gp, graph, graphs, graphs_per_batch, graphs_same_source, history, i1, i2, idx, idx_split, input_ids, labels, loss, max_num_tokens, metadata, modified, noise, num_training_steps, optimizer, orders, out, output_dict, pred_min, report, results, s_, sampler, sampler_builder, scheduler, smatch, status, timer, tok, tok_split, tok_split_, tokenizer, tokens, tokens_same_source, tokk, total_loss, transformer, tt, vec, vec_split, vecs, warmup_steps, x","Args:
    trn_data:
    dev_data:
    save_dir:
    batch_size:
    epochs:
    transformer:
    lr:
    grad_norm:
    weight_decay:
    warmup_steps:
    dropout:
    attention_dropout:
    pred_min:
    eval_after:
    collapse_name_ops: ``True`` to merge name ops.
    use_pointer_tokens: ``True`` to use pointer tokens to represent variables.
    raw_graph: ``True`` to use the raw graph as input and skip all pre/post-processing steps.
    gradient_accumulation:
    recategorization_tokens: Tokens used in re-categorization. They will be added to tokenizer too but do not
    put them into ``additional_tokens``.
    additional_tokens: Tokens to be added to the tokenizer vocab.
    devices:
    logger:
    seed:
    finetune:
    eval_trn:
    _device_placeholder:
    **kwargs:

Returns:","

        Args:
            trn_data:
            dev_data:
            save_dir:
            batch_size:
            epochs:
            transformer:
            lr:
            grad_norm:
            weight_decay:
            warmup_steps:
            dropout:
            attention_dropout:
            pred_min:
            eval_after:
            collapse_name_ops: ``True`` to merge name ops.
            use_pointer_tokens: ``True`` to use pointer tokens to represent variables.
            raw_graph: ``True`` to use the raw graph as input and skip all pre/post-processing steps.
            gradient_accumulation:
            recategorization_tokens: Tokens used in re-categorization. They will be added to tokenizer too but do not
            put them into ``additional_tokens``.
            additional_tokens: Tokens to be added to the tokenizer vocab.
            devices:
            logger:
            seed:
            finetune:
            eval_trn:
            _device_placeholder:
            **kwargs:

        Returns:

        ,  ,  (,  / ,  ETA: ,  [red](saved)[/red],  at epoch ,  elapsed, ), )[blink][yellow]...[/yellow][/blink], -, -00, -01, -02, -03, -04, -05, -06, -07, -08, -09, -10, -11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -31, -32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47, -48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63, -64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79, -80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95, -96, -97, -98, -amr, -of, .2%, .4f, :, :ARG, :[/yellow], :op, :snt, <, <pointer:, >, Average time of each epoch is , CAUSE_OF_DEATH, CITY, COUNTRY, CRIMINAL_CHARGE, DATE, DATE_ATTRS, DURATION, ENTITY, HANDLE, IDEOLOGY, LOCATION, MISC, MONEY, Max score of dev is , NATIONALITY, NUMBER, O, ORDINAL, ORDINAL_ENTITY, ORGANIZATION, PERSON, Preprocessing and caching samples (longest sequence , QUANTITY, RELIGION, SCORE_ENTITY, SET, STATE_OR_PROVINCE, TITLE, URL, Unsupported transformer , [yellow]Epoch , _1, _10, _11, _12, _13, _14, _15, _2, _3, _4, _5, _6, _7, _8, _9, additional_tokens, amr, amr-unknown, annotator, argument, at-least, bart-, be-compared-to-91, be-destined-for-91, be-from-91, be-located-at-91, be-polite-91, be-temporally-at-91, byline-91, chinese, course-91, date, date-entity, dev.pred.txt, et-cetera, facebook/bart-base, government-organization, graph_token_ids, have-concession-91, have-condition-91, have-extent-91, have-frequency-91, have-instrument-91, have-li-91, have-manner-91, have-mod-91, have-name-91, have-org-role-91, have-part-91, have-polarity-91, have-purpose-91, have-quant-91, have-rel-role-91, have-subevent-91, hyperlink-91, include-91, instead-of-91, loss, loss: , mass-quantity, monetary-quantity, multi-sentence, operator, ordinal-entity, percentage-entity, pointer, political-movement, political-party, publication-91, rate-entity-91, regardless-91, relation, religious-group, request-confirmation-91, save-date, score-on-scale-91, sentence, statistical-test-91, street-address-91, t5-, temporal-quantity, text, text_token_ids, url-entity, world-region","0, 0.0, 0.004, 0.1, 0.25, 0.5, 1, 1024, 2.5, 3, 30, 32, 4, 5, 500, 5e-05, False, None, True","

Args:
trn_data:
dev_data:
save_dir:
batch_size:
epochs:
transformer:
lr:
grad_norm:
weight_decay:
warmup_steps:
dropout:
attention_dropout:
pred_min:
eval_after:
collapse_name_ops: ``True`` to merge name ops.
use_pointer_tokens: ``True`` to use pointer tokens to represent variables.
raw_graph: ``True`` to use the raw graph as input and skip all pre/post-processing steps.
gradient_accumulation:
recategorization_tokens: Tokens used in re-categorization. They will be added to tokenizer too but do not
put them into ``additional_tokens``.
additional_tokens: Tokens to be added to the tokenizer vocab.
devices:
logger:
seed:
finetune:
eval_trn:
_device_placeholder:
**kwargs:

Returns:

, #     break, #     report += ' early stop', # -*- coding:utf-8 -*-, # Author: hankcs, # Copy meta data from gold graph, # Date: 2021-04-28 17:33, # config.output_attentions = True, # if epoch - best_epoch >= patience:, # lc = Counter(), # lc[len(each['text_token_ids'])] += 1, # noinspection PyTypeChecker, # print(lc.most_common())",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-04-27 19:24",
https://github.com/hankcs/HanLP,dataset.py,0,0.0,103,74.64,26,18.84,7,5.07,2,1.45,0,0.0,11,2,45,0,,"AMRDataset, AMRPickleDataset, INIT, __init__, append, arc, arc_scores, batch_encode_plus, bos_token_id, buffer, cache, cells, child, children, clear, cls_token, collections, convert_tokens_to_ids, data, dep, dep_graph, dep_to_levi, dereify, dfs, dfs_linearize_constituency, dfs_linearize_levi, dfs_linearize_rgcn, dfs_linearize_tokenize, dfs_linearize_tokenize_with_linguistic_structures, dict, each, encode, end, enumerate, eos_token_id, extend, filepath, flat_arc, flat_ner, flat_pos, flat_rel, frames, from_list, generate_idx, get, get_frames, get_roles, graphs, hanlp, hanlp_common, height, ids, idx, instances, int, items, json, len, levi, linearize, load_file, metadata, ner, ner_spans, num_subtokens, pairs, penman, pformat, phrasetree, pos, rel, rel_scores, remove_space, remove_wiki, restore_bracket, roles, root, sample, self, shape, split, start, str, strip, subtokens, subtrees, sum, super, tag, target, text, text_key, text_token_ids, token_to_1st_subtoken, tokenizer, tokens, torch, transform, triples, typing, use_recategorization, x, zip","collections.Counter, hanlp.common.dataset.TransformableDataset, hanlp.components.amr.seq2seq.dataset.IO.read_raw_amr_data, hanlp.components.amr.seq2seq.dataset.penman.role_is_reverted, hanlp.components.amr.seq2seq.dataset.tokenization_bart.PENMANBartTokenizer, hanlp_common.constant.BOS, hanlp_common.constant.EOS, hanlp_common.constant.ROOT, hanlp_common.io.load_pickle, json.loads, penman.Graph, penman.decode, phrasetree.tree.Tree, torch.cat, torch.load, torch.zeros, typing.Callable, typing.List, typing.Tuple, typing.Union","__init__, dep_to_levi, dfs, dfs_linearize_constituency, dfs_linearize_levi, dfs_linearize_rgcn, dfs_linearize_tokenize, dfs_linearize_tokenize_with_linguistic_structures, get_frames, get_roles, load_file","AMRDataset, AMRPickleDataset","amr, arc, arc_scores, buffer, cells, child, children, dep, dep_graph, each, end, flat_arc, flat_ner, flat_pos, flat_rel, frames, graphs, ids, idx, items, levi, ner, ner_spans, num_subtokens, pairs, pos, r, rel, rel_scores, restore_bracket, roles, root, sample, seq, start, subtokens, t, tag, text, text_token_ids, tok, token_to_1st_subtoken, tokens, tree, x",,",  , (, ), -, <LBR>, <RBR>, O, amr, arc_scores, con_list, dep, dep_arc, dep_graph, dep_rel, graph_token_ids, graph_tokens, linearized_graphs, ner, pos, rel_scores, scores, snt, text, text_token_ids, tok","0, 1, 100000000.0, 2, 3, False, None","# 1 for BOS, # ids = sum(tokenizer.batch_encode_plus([' ' + x for x in levi], add_special_tokens=False).input_ids, [])",
https://github.com/hankcs/HanLP,IO.py,0,0.0,20,64.52,9,29.03,2,6.45,0,0.0,0,0.0,1,0,6,0,,"dereify, endswith, eval, exists, extend, glob, graphs, isinstance, metadata, path_, pathlib, paths, penman, read_raw_amr_data, remove_wiki, startswith, str, tokens, typing, use_recategorization","glob.glob, pathlib.Path, penman.pm_load, typing.Iterable, typing.List, typing.Union",read_raw_amr_data,,"graphs, metadata, path, path_, paths, tokens",," ,  not exist, -, -L, -R, No graphs loaded, snt, snt_orig, tokens","False, True",,
https://github.com/hankcs/HanLP,linearization.py,0,0.0,152,67.86,36,16.07,7,3.12,29,12.95,0,0.0,16,4,87,2,,"AMRLinearizer, AMRTokens, BACKR_SRC_N, BACKR_TRG_N, BOS_E, BOS_N, BaseLinearizer, END, EOS_E, EOS_N, LIT_END, LIT_START, PNTR_N, START, START_E, START_N, STOP_E, STOP_N, SemanticGraph, _, _FIXED_SPECIAL_TOKENS, _FIXED_SPECIAL_TOKENS_E, _FIXED_SPECIAL_TOKENS_N, _TEMPL, __init__, _add_pointer_tokens, _collapse_name_ops, _interleave, _linearize, _re_BACKR_SRC_N, _re_BACKR_TRG_N, abc, add, add_edge, add_node, added_to_queue, amr, amr_, append, attr, attributes, b, backr, backreferences, bool, classmethod, cls, collapse_name_ops, collections, copy, dataclasses, edge, edge_data, edges, edges_visit, end_i, enumerate, expansion, expansion_candidate, explored, extra, first_index, format, get, get_edge_data, graph, idx, index_default, instance, int, interleave_edges, is_node, isinstance, item, items, itertools, k, len, linearize, linearized, list, list_, lit, lits, m_src, m_trg, metadata, min, name_vars, name_vars_to_ops, networkx, new_backreferences, new_backreferences_map, new_edges, new_graph, new_n_node, new_nodes, next, node, node1, node2, nodes, nodes_var, nodes_visit, not_explored, nv, old_n_node, ops, order, penman, pointer, popleft, property, queue, range, re, read_backr, rel, replace, resolved_nodes, resolved_nodes_var, search, self, set, setdefault, sorted, src_occurrence, start, start_i, startswith, stop, str, string, strip, successors, top, triple, triples, triples2order, typing, use_pointer_tokens, v, v1, v2, var, var1, var2, var2instance, var2pointer, variables, x, zip","abc.ABCMeta, abc.abstractmethod, collections.defaultdict, collections.deque, dataclasses.dataclass, itertools.chain, nx.MultiDiGraph, nx.get_node_attributes, penman.Graph, penman.Triple, re.compile, typing.Any, typing.Dict, typing.List, typing.Optional, typing.Set, typing.TypeVar","__init__, _add_pointer_tokens, _collapse_name_ops, _interleave, _linearize, add_edge, add_node, index_default, is_node, linearize, nodes, read_backr, resolved_nodes, resolved_nodes_var, src_occurrence, variables","AMRLinearizer, AMRTokens, BaseLinearizer, SemanticGraph","BACKR_SRC_N, BACKR_TRG_N, BOS_E, BOS_N, END, EOS_E, EOS_N, LIT_END, LIT_START, PNTR_N, START, START_E, START_N, STOP_E, STOP_N, T, _, _FIXED_SPECIAL_TOKENS, _FIXED_SPECIAL_TOKENS_E, _FIXED_SPECIAL_TOKENS_N, _TEMPL, _re_BACKR_SRC_N, _re_BACKR_TRG_N, added_to_queue, amr, amr_, attr, b, backr, backreferences, edge_data, edges, edges_visit, end_i, expansion, expansion_candidate, explored, extra, first_index, graph, idx, instance, k, linearized, lit, lits, m_src, m_trg, name_vars, name_vars_to_ops, new_backreferences, new_backreferences_map, new_edges, new_graph, new_n_node, new_nodes, node, node1, node2, nodes, nodes_var, nodes_visit, not_explored, nv, old_n_node, ops, order, pointer, queue, rel, start, start_i, stop, successors, triple, triples, triples2order, v, v1, v2, var, var1, var2, var2instance, var2pointer, variables, x","Linearized nodes with varids replaced by instances, Set of variables in this semantic graph","
    Dict from var ids to 'lemmatized' readable strings qualifying the node (collapsing the :instance edge for AMR).
    , 
    Holds extra stuff that might be useful, e.g. alignments, NER, EL.
    , 
    List of backpointers to handle rentrancies and cycles.
    , 
    List of linearized edges, with special tokens.
    , 
    List of linearized nodes, with special tokens.
    , "", ([0-9]+), /lit, /s, :, :instance, :op, :op1, <, <pointer:, >, Linearized nodes with varids replaced by instances, Set of variables in this semantic graph, T, XXX, _, amr, backr:src:XXX, backr:trg:XXX, expansion, graph, lit, name, order, pointer, rel, s, start, stop, var:, {}","0, 1, 2, 3, False, None, True","
Dict from var ids to 'lemmatized' readable strings qualifying the node (collapsing the :instance edge for AMR).
, 
Holds extra stuff that might be useful, e.g. alignments, NER, EL.
, 
List of backpointers to handle rentrancies and cycles.
, 
List of linearized edges, with special tokens.
, 
List of linearized nodes, with special tokens.
, #     -graph.out_degree(x),, #     len(nx.shortest_path(undirected_graph, 'var:' + amr.top, x)),, # (, # ), # .. which is mentioned for the first time, # ... which was mentioned before, # 1) not already in Q, # 2) has children, # 3) the edge right before its expansion has been encountered, # @cached_property, # check if they have ops, # edges and trg nodes, interleaved, # identify name triples, # match and read backreferences, # node2 is a constant, # node2 is a variable, # nodes that are not reachable from the root (e.g. because of reification), # print(amr.variables()), # src node, # stop, # to isolate sublist to the stop token, # undirected_graph = graph.to_undirected(), # will be present in the not_explored queue, Set of variables in this semantic graph
variables = {v for v in self.nodes_var if not v.startswith('<')}
return variables

@property
def resolved_nodes_var(self) -> List[str]:
return [self.nodes_var[b] for b in self.backreferences]

# @cached_property
@property
def nodes(self) -> List[str]:
Linearized nodes with varids replaced by instances",
https://github.com/hankcs/HanLP,penman.py,0,0.0,39,79.59,5,10.2,3,6.12,2,4.08,0,0.0,7,1,13,1,,"AMRGraph, DEFAULT, __str__, _get_model, _remove_wiki, amr_model, append, compact, dereify, encode_, endswith, graph, indent, len, load_, loads, loads_, logging, metadata, model, noop_model, op_model, penman, pm_encode, pm_load, range, rel, remove_wiki, role, role_is_reverted, self, source, str, string, top, triples, typing, v1, v2","logging.CRITICAL, penman.Graph, penman.Triple, penman._parse.logger.setLevel, penman.encode, penman.layout.logger.setLevel, penman.load, penman.loads, penman.model.Model, penman.models.amr, penman.models.noop.NoOpModel, typing.List","__str__, _get_model, _remove_wiki, loads, pm_encode, pm_load, role_is_reverted",AMRGraph,"DEFAULT, amr_model, graph, metadata, model, noop_model, op_model, out, rel, t, triples, v1, v2","Args:
    source:
    dereify: Restore reverted relations
    remove_wiki:

Returns:","

    Args:
        source:
        dereify: Restore reverted relations
        remove_wiki:

    Returns:

    , +, -of, :wiki, consist-of","1, False, None","

Args:
source:
dereify: Restore reverted relations
remove_wiki:

Returns:

, # Mute loggers",
https://github.com/hankcs/HanLP,postprocessing.py,0,0.0,157,67.09,42,17.95,8,3.42,27,11.54,0,0.0,11,1,99,0,,"BACKOFF, FIXED, INIT, OK, ParsedStatus, ValueError, _reconstruct_graph_from_nodes, _split_name_ops, add, add_edge, addition, append, b, backreferences, backreferences_addition, build_graph, callable, check, cnt, collections, conn_set, connect_graph_if_not_connected, convert_ids_to_tokens, convert_tokens_to_string, copy, current_token_i, decode_into_node_and_backreferences, decode_into_node_and_backreferences_without_space, edge, edge_index, edges, edges_nodes_slice, edges_oth, element, encoded, end, endswith, enum, enumerate, eval, graph, graph_, hanlp, idx, index, index2variable, index_of, int, is_arg, is_bracket, is_pointer, is_rel, isdigit, isinstance, item, items, iterable, len, list, lit, lit_end, lit_start, lits, lstrip, match, max, metadata, min, name_vars, name_vars_to_ops, nb, networkx, new_backreferences, new_nodes, new_triples, ni, no_pad, node, nodes, nodes_oth, num, nv, nxgraph, old_backreferences, old_start_index, old_tok, old_tokens, ops, oth, other, penman, pointer2i, pop, prev_is_pointer, prev_is_rel, prev_pointer, range, re, rel, removed, replace, res, res_tok, restore_backreferences_from_pointers, restore_name_ops, ret, rex_arg, rex_spc, separate_edges_nodes, shift, sorted, split, src_backr, src_node, src_var, src_var_i, start, start_index, start_search, startswith, stop_index, str, strip, subtok, subtoken_backreferences, subtoken_ids, subtokens, subw_backr, subw_i, subword_to_token_map, tok, token_addition, token_processing, token_to_token_map, tokenizer, tokens, trg, trg_edges, trg_nodes, trg_nodes_backr, trg_nodes_edges, trg_nodes_edges_backr, trg_nodes_edges_indices, trg_nodes_indices, trg_var, trg_var_i, triple, triples, triples_added, tt, update, v1, v2, variable2index, variables, x, zip","collections.Counter, collections.defaultdict, enum.Enum, hanlp.components.amr.seq2seq.dataset.penman.pm_encode, nx.MultiGraph, nx.connected_components, penman.Graph, penman.Triple, re.compile, re.match","_reconstruct_graph_from_nodes, _split_name_ops, build_graph, check, connect_graph_if_not_connected, decode_into_node_and_backreferences, decode_into_node_and_backreferences_without_space, index_of, restore_backreferences_from_pointers, separate_edges_nodes, token_processing",ParsedStatus,"BACKOFF, FIXED, OK, addition, b, backreferences, backreferences_addition, check, cnt, conn_set, current_token_i, edge, edge_index, edges, edges_oth, encoded, end, graph, graph_, idx, index2variable, is_arg, is_bracket, is_pointer, is_rel, item, l, lit, lit_end, lit_start, lits, metadata, name_vars, name_vars_to_ops, nb, new_backreferences, new_nodes, new_triples, ni, no_pad, node, nodes, nodes_oth, num, nv, nxgraph, old_backreferences, old_start_index, old_tok, old_tokens, ops, oth, pointer2i, prev_is_pointer, prev_is_rel, prev_pointer, rel, removed, res, res_tok, ret, rex_arg, rex_spc, shift, src_backr, src_node, src_var, src_var_i, start, start_index, start_search, stop_index, subtok, subtoken_backreferences, subtokens, subw_backr, subw_i, subword_to_token_map, token_addition, token_to_token_map, tokens, trg, trg_edges, trg_nodes, trg_nodes_backr, trg_nodes_edges, trg_nodes_edges_backr, trg_nodes_edges_indices, trg_nodes_indices, trg_var, trg_var_i, triple, triples, triples_added, tt, v1, v2, variable2index, variables",,", "", (, (), (op|snt|conj|prep), ), +, -, -of, /, :, :ARG0, :instance, :li, :mode, :op, :snt, <, <(s|/s|lit|/lit|stop|unk|pad|mask)>, </lit>, </s>, <lit>, <pad>, <pointer:, <s>, <stop>, <unk>, =, >, ^, ^[+-]?\d+\.?\d*$, _, a, abcdefghijklmnopqrstuvwxyz, and, b1, bark-01, d2, dog, name, thing, x","0, 1, 2, 3, 4, False, None, True","#    continue, # <lit> Barack Obama </lit> -> ""Barack Obama"", # Remove possible wrong None, # TODO: this is an ugly patch due to the fact that BART tokenizer splits after ':', # after a special token release, # after a subtoken ':' (which should be followed by the rest of the edge) ignore tokenizer.INIT, # backref can't be splitted, # check if they have ops, # current or prev is a control token, # elif e.startswith(':ARG'):, # fix backreferences, # get strings, # identify name triples, # if empty you cannot do anything but add a new word, # in any other case attach to the previous, # leading tokenizer.INIT, # more resilient logic here, # num = 0, # same edge more than once, # src_var = f'{src_var}_{len(variable2index)}', # strip INIT and fix byte-level, # strip padding, # subword collapse, # tokens = [t.replace(tokenizer.INIT, '') if isinstance(t, str) else t for t in tokens], # trg_var = f'{trg_var}_{len(variable2index)}', # unks are substituted with thing, # very ugly patch for some cases in which tokenizer.INIT is not in the following token to the edge",
https://github.com/hankcs/HanLP,tokenization_bart.py,0,0.0,185,70.34,61,23.19,13,4.94,3,1.14,1,0.38,19,2,76,1,,"ADDITIONAL, AMRBartTokenizer, BACKOFF, BACKR_SRC_N, BACKR_TRG_N, BOS_N, EOS_N, Exception, INIT, LIT_END, LIT_START, PENMANBartTokenizer, PNTR_N, ParsedStatus, STOP_N, __init__, _classify, _fix_and_make_graph, _get_nodes_and_backreferences, _repl1, _repl2, _split_name_ops, _tok_bpe, _tokenize, _tokenize_encoded_graph, add, additional_tokens, additions, all_special_tokens, append, args, backr, backreferences, batch, batch_encode_graphs, batch_encode_graphs_from_linearized, batch_encode_sentences, batch_extra, bb, bos_token, bos_token_id, bpe, bpe_backr, bpe_backreferences, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, build_graph, build_inputs_with_special_tokens, byte_encoder, char, classmethod, closed_cnt, cnt, collapse_name_ops, connect_graph_if_not_connected, copy, counter, decode, decode_amr, decode_into_node_and_backreferences, decoder, encode, encoded, encoder, endswith, enumerate, eos_token, eos_token_id, extend, extra, extras, find, findall, fix_text, fol, from_pretrained, get, graph, graph_, graphs, group, idx, init_amr_vocabulary, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, isinstance, items, kwargs, last, last_remap, len, linearization, linearize, linearized, linearized_nodes, linearized_nodes_, linearizer, list, lst, lstrip, max, maxlen, metadata, modified, newvars, next, node, nodes, nodes_, ntok, nxt, old_enc_size, open_cnt, out, output, pad_token, pad_token_id, pat, patterns, penman, piece, pieces, pieces_, pretrained_model_path, prev, print, pst, quote, range, raw_graph, recategorization_tokens, recategorizations, recats, regex, rel, remap, remove_pars, replace, restore_backreferences_from_pointers, restore_name_ops, rsplit, self, sentences, set, shift, sorted, split, startswith, status, str, strip, super, sys, text, to, tok, tok_span, token, token_ids, token_ids_0, token_ids_1, token_uni_ids, tokenize_amr, tokens, tokk, toks, torch, transformers, triple, triples, typing, unk_token, unk_token_id, use_pointer_tokens, v, var, variables, x, zip","copy.deepcopy, linearization.AMRLinearizer, linearization.AMRTokens, penman.Graph, penman.Triple, penman.decode, penman.encode, penman.pm_encode, postprocessing, re.IGNORECASE, re.MULTILINE, re.compile, re.findall, re.match, re.sub, sys.stderr, torch.device, torch.tensor, transformers.BartTokenizer, typing.Iterable, typing.Set","__init__, _classify, _fix_and_make_graph, _get_nodes_and_backreferences, _repl1, _repl2, _tok_bpe, _tokenize, _tokenize_encoded_graph, batch_encode_graphs, batch_encode_graphs_from_linearized, batch_encode_sentences, build_inputs_with_special_tokens, decode_amr, fix_text, from_pretrained, init_amr_vocabulary, linearize, tokenize_amr","AMRBartTokenizer, PENMANBartTokenizer","ADDITIONAL, backr, backreferences, batch, batch_extra, bb, bpe_backr, bpe_backreferences, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, char, closed_cnt, cnt, counter, e, extra, extras, fol, g, graph, graph_, i, idx, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, last, last_remap, lin, linearized, linearized_nodes, linearized_nodes_, lst, maxlen, n, newvars, next, nodes, nodes_, ntok, nxt, old_enc_size, open_cnt, out, output, piece, pieces, pieces_, prev, pst, quote, recats, rel, remap, sentences, shift, status, tok, tok_span, token, token_ids, token_uni_ids, tokens, tokk, toks, triple, triples, v, var, variables, x",Tokenize a string. Modified in order to handle sentences with recategorization pointers,", 
        line = linearized
        # make sure parentheses match
        # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py
        open_count = 0
        close_count = 0
        for i, c in enumerate(line):
            if c == '(':
                open_count += 1
            elif c == ')':
                close_count += 1
            if open_count == close_count and open_count > 0:
                line = line[:i].strip()
                break
        old_line = line
        while True:
            open_count = len(re.findall(r'\(', line))
            close_count = len(re.findall(r'\)', line))
            if open_count > close_count:
                line += ')' * (open_count - close_count)
            elif close_count > open_count:
                for i in range(close_count - open_count):
                    line = line.rstrip(')')
                    line = line.rstrip(' ')
            if old_line == line:
                break
            old_line = line
        ,  ,  ( ,  ) ,  / ,  :,  ?<[a-z]+:?\d*>| ?:[^\s]+|'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+,  Tokenize a string. Modified in order to handle sentences with recategorization pointers,  \1 , !, "", (, ([^:])(ARG), (\"".+?\""), (\(\s*[a-z][\d+]\s*\/\s*[^\s\)\(:\/]+\s*)((?:/\s*[^\s\)\(:\/]+\s*)+), (\(\s?)([a-z])([^\/:\)]+[:\)]), ), +, ,, -, -of, ., .+-\d\d, /, :, :instance, :mode, <, <pointer:, =, >, ?, Building failure:, CONST, Decoding failure:, EDGE, I, INST, MODE, Reconnction 2 failure:, Reconnection 1 failure:, VAR, \, \1 :\2, \s+, ^[a-z]\d*$, _, cpu, decoder_input_ids, graphs, i, linearized_graphs, lm_labels, pt, sentences, thing, utf-8, x, z, Ġ","0, 1, 1000, 1022, 2, 2000, 3, 3000, 512, 9, False, None, True"," Tokenize a string. Modified in order to handle sentences with recategorization pointers
bpe_tokens = []
for tok_span in text.lstrip().split(' '):
tok_span = tok_span.strip()
recats = tok_span.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
bpe_tokens.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for token in re.findall(self.pat, ' ' + tok_span):
token = """".join(
self.byte_encoder[b] for b in token.encode(""utf-8"")
)  # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)
bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split("" ""))

return bpe_tokens

def _tok_bpe(self, token, add_space=True):
# if add_space:
#     token = ' ' + token.lstrip()
tokk = []
tok = token.strip()
recats = tok.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
tokk.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for tok in self.patterns.findall(' ' + token):
tok = """".join(
self.byte_encoder[b] for b in tok.encode(""utf-8""))
toks = self.bpe(tok).split(' ')
tokk.extend(toks)
return tokk

def _get_nodes_and_backreferences(self, graph):
lin = self.linearizer.linearize(graph)
linearized_nodes, backreferences = lin.nodes, lin.backreferences
return linearized_nodes, backreferences

def tokenize_amr(self, graph):
linearized_nodes, backreferences = self._get_nodes_and_backreferences(graph)

bpe_tokens = []
bpe_backreferences = []
counter = 0

for i, (backr, tokk) in enumerate(zip(backreferences, linearized_nodes)):
is_in_enc = self.INIT + tokk in self.encoder
is_rel = tokk.startswith(':') and len(tokk) > 1
is_spc = tokk.startswith('<') and tokk.endswith('>')
is_of = tokk.startswith(':') and tokk.endswith('-of')
is_frame = re.match(r'.+-\d\d', tokk) is not None

if tokk.startswith('""') and tokk.endswith('""'):
tokk = tokk[1:-1].replace('_', ' ')
bpe_toks = [self.INIT + AMRTokens.LIT_START]
bpe_toks += self._tok_bpe(tokk, add_space=True)
bpe_toks.append(self.INIT + AMRTokens.LIT_END)

elif (is_rel or is_spc or is_frame or is_of):
if is_in_enc:
bpe_toks = [self.INIT + tokk]
elif is_frame:
bpe_toks = self._tok_bpe(tokk[:-3], add_space=True) + [tokk[-3:]]
elif is_of:
rel = tokk[:-3]
if self.INIT + rel in self.encoder:
bpe_toks = [self.INIT + rel, '-of']
else:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(rel[1:], add_space=True) + ['-of']
elif is_rel:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(tokk[1:], add_space=True)
else:
raise

else:
if is_in_enc:
bpe_toks = [self.INIT + tokk]
else:
bpe_toks = self._tok_bpe(tokk, add_space=True)

bpe_tokens.append(bpe_toks)

if i == backr:
bpe_backr = list(range(counter, counter + len(bpe_toks)))
counter += len(bpe_toks)
bpe_backreferences.append(bpe_backr)
else:
bpe_backreferences.append(bpe_backreferences[backr][0:1])
counter += 1
bpe_tokens = [b for bb in bpe_tokens for b in bb]
bpe_token_ids = [self.encoder.get(b, self.unk_token_id) for b in bpe_tokens]
bpe_backreferences = [b for bb in bpe_backreferences for b in bb]
return bpe_tokens, bpe_token_ids, bpe_backreferences

def batch_encode_sentences(self, sentences, device=torch.device('cpu')):
sentences = [s for s in sentences]
extra = {'sentences': sentences}
batch = super().batch_encode_plus(sentences, return_tensors='pt', pad_to_max_length=True)
batch = {k: v.to(device) for k, v in batch.items()}
return batch, extra

def linearize(self, graph):
shift = len(self.encoder)
tokens, token_ids, backreferences = self.tokenize_amr(graph)
extra = {'linearized_graphs': tokens, 'graphs': graph}
token_uni_ids = \
[idx if i == b else b + shift for i, (idx, b) in enumerate(zip(token_ids, backreferences))]
if token_uni_ids[-1] != (self.INIT + AMRTokens.EOS_N):
tokens.append(self.INIT + AMRTokens.EOS_N)
token_ids.append(self.eos_token_id)
token_uni_ids.append(self.eos_token_id)
backreferences.append(len(backreferences))
return token_uni_ids, extra

def batch_encode_graphs(self, graphs, device=torch.device('cpu')):
linearized, extras = zip(*[self.linearize(g) for g in graphs])
return self.batch_encode_graphs_from_linearized(linearized, extras, device=device)

def batch_encode_graphs_from_linearized(self, linearized, extras=None, device=torch.device('cpu')):
if extras is not None:
batch_extra = {'linearized_graphs': [], 'graphs': []}
for extra in extras:
batch_extra['graphs'].append(extra['graphs'])
batch_extra['linearized_graphs'].append(extra['linearized_graphs'])
else:
batch_extra = {}
maxlen = 0
batch = []
for token_uni_ids in linearized:
maxlen = max(len(token_uni_ids), maxlen)
batch.append(token_uni_ids)
batch = [x + [self.pad_token_id] * (maxlen - len(x)) for x in batch]
batch = torch.tensor(batch).to(device)
batch = {'decoder_input_ids': batch[:, :-1], 'lm_labels': batch[:, 1:]}
return batch, batch_extra

def decode_amr(self, tokens, restore_name_ops=False):
try:
nodes, backreferences = postprocessing.decode_into_node_and_backreferences(tokens, self)
except Exception as e:
print('Decoding failure:', file=sys.stderr)
print(e, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
if self.use_pointer_tokens:
nodes, backreferences = postprocessing.restore_backreferences_from_pointers(nodes)
try:
graph_ = graph = postprocessing.build_graph(nodes, backreferences, restore_name_ops=restore_name_ops)
except Exception as e:
print('Building failure:', file=sys.stderr)
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(e, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
try:
graph, status = postprocessing.connect_graph_if_not_connected(graph)
if status == postprocessing.ParsedStatus.BACKOFF:
print('Reconnection 1 failure:')
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(graph_, file=sys.stderr)
return graph, status, (nodes, backreferences)
except Exception as e:
print('Reconnction 2 failure:', file=sys.stderr)
print(e, file=sys.stderr)
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(graph_, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (nodes, backreferences)


class PENMANBartTokenizer(AMRBartTokenizer):

def __init__(self, *args, raw_graph=False, **kwargs):
super().__init__(*args, **kwargs)
self.linearizer = None
self.remove_pars = False
self.raw_graph = raw_graph

def _tokenize_encoded_graph(self, encoded):
linearized = re.sub(r""(\"".+?\"")"", r' \1 ', encoded)
pieces = []
for piece in linearized.split():
if piece.startswith('""') and piece.endswith('""'):
pieces.append(piece)
else:
piece = piece.replace('(', ' ( ')
piece = piece.replace(')', ' ) ')
piece = piece.replace(':', ' :')
piece = piece.replace('/', ' / ')
piece = piece.strip()
pieces.append(piece)
linearized = re.sub(r'\s+', ' ', ' '.join(pieces)).strip()
linearized_nodes = [AMRTokens.BOS_N] + linearized.split(' ')
return linearized_nodes

def tokenize_amr(self, graph):
if self.raw_graph:
graph_ = copy.deepcopy(graph)
graph_.metadata = {}
linearized = penman.encode(graph_)
linearized = re.sub(r""\s+"", ' ', linearized)
bpe_tokens = [self.bos_token] + self._tokenize(linearized)[:1022]
bpe_token_ids = [self.encoder.get(b, self.unk_token_id) for b in bpe_tokens]
bpe_backreferences = list(range(len(bpe_token_ids)))
return bpe_tokens, bpe_token_ids, bpe_backreferences
else:
return super().tokenize_amr(graph)

def _get_nodes_and_backreferences(self, graph):
graph_ = copy.deepcopy(graph)
graph_.metadata = {}
linearized = penman.encode(graph_)
linearized_nodes = self._tokenize_encoded_graph(linearized)

if self.use_pointer_tokens:
remap = {}
for i in range(1, len(linearized_nodes)):
nxt = linearized_nodes[i]
lst = linearized_nodes[i - 1]
if nxt == '/':
remap[lst] = f'<pointer:{len(remap)}>'
i = 1
linearized_nodes_ = [linearized_nodes[0]]
while i < (len(linearized_nodes)):
nxt = linearized_nodes[i]
lst = linearized_nodes_[-1]
if nxt in remap:
if lst == '(' and linearized_nodes[i + 1] == '/':
nxt = remap[nxt]
i += 1
elif lst.startswith(':'):
nxt = remap[nxt]
linearized_nodes_.append(nxt)
i += 1
linearized_nodes = linearized_nodes_
if self.remove_pars:
linearized_nodes = [n for n in linearized_nodes if n != '(']
backreferences = list(range(len(linearized_nodes)))
return linearized_nodes, backreferences

def _classify(self, node):
if not isinstance(node, str):
return ""CONST""
elif node == 'i':
return ""I""
elif re.match(r'^[a-z]\d*$', node) is not None:
return ""VAR""
elif node[0].isdigit():
return ""CONST""
elif node.startswith('""') and node.endswith('""'):
return ""CONST""
elif node in ('+', '-'):
return ""CONST""
elif node == ':mode':
return 'MODE'
elif node.startswith(':'):
return ""EDGE""
elif node in ['/', '(', ')']:
return node
elif node[0].isalpha():
for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\'):
if char in node:
return ""CONST""
return ""INST""
else:
return 'CONST'

def _fix_and_make_graph(self, nodes):

nodes_ = []
for n in nodes:
if isinstance(n, str):
if n.startswith('<') and n.endswith('>') and (not n.startswith('<pointer:')):
pass
else:
nodes_.append(n)
else:
nodes_.append(n)
nodes = nodes_

if self.use_pointer_tokens:

i = 0
nodes_ = []
while i < len(nodes):
nxt = nodes[i]
pst = None
if isinstance(nxt, str) and nxt.startswith('<pointer:'):
e = nxt.find('>')
if e != len(nxt) - 1:
pst = nxt[e + 1:]
nxt = nxt[:e + 1]
nodes_.append(nxt)
if pst is not None:
nodes_.append(pst)
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 1
nodes_ = [nodes[0]]
while i < len(nodes):
nxt = nodes[i]
if isinstance(nxt, str) and nxt.startswith('<pointer:'):
nxt = 'z' + nxt[9:-1]
fol = nodes[i + 1]
# is not expansion
if isinstance(fol, str) and (fol.startswith(':') or (fol == ')')):
nodes_.append(nxt)
else:
if self.remove_pars:
nodes_.append('(')
else:
if nodes_[-1] != '(':
nodes_.append('(')
# pass
nodes_.append(nxt)
nodes_.append('/')
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes) - 1):
if nodes[i] == ':':
nodes_.append(nodes[i] + nodes[i + 1])
i += 2
last = False
else:
nodes_.append(nodes[i])
i += 1
last = True
if last:
nodes_.append(nodes[-1])
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes)):
if i < 2:
nodes_.append(nodes[i])
i += 1
elif nodes_[-2] == '/' and nodes[i] == '/':
i += 2
else:
nodes_.append(nodes[i])
i += 1
nodes = nodes_

i = 0
newvars = 0
variables = set()
remap = {}
nodes_ = []
while i < (len(nodes)):

next = nodes[i]

if next == '/':
last = nodes_[-1]
if last in variables:
last_remap = f""x{newvars + 1000}""
newvars += 1
nodes_[-1] = last_remap
remap[last] = last_remap
variables.add(last)
nodes_.append(next)

elif self._classify(next) == 'VAR' and next in remap and (i < len(nodes) - 1) and nodes[i + 1] != '/':
next = remap[next]
nodes_.append(next)

else:
nodes_.append(next)

i += 1

nodes = nodes_
pieces_ = []
open_cnt = 0
closed_cnt = 0
if nodes[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in nodes:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
nodes = pieces_ + [')'] * (open_cnt - closed_cnt)

pieces = []
for piece in nodes:
if not pieces:
pieces.append('(')
else:
piece = str(piece)
if piece.startswith('""') or piece.startswith('""') or '""' in piece.strip('""'):
piece = '""' + piece.replace('""', '') + '""'

prev = self._classify(pieces[-1])
next = self._classify(piece)

if next == 'CONST':
quote = False
for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\', '_', '='):
if char in piece:
quote = True
break
if quote:
piece = '""' + piece.strip('""') + '""'

if prev == '(':
if next in ('VAR', 'I'):
pieces.append(piece)
elif prev == ')':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'VAR':
if next in ('/', 'EDGE', 'MODE', ')'):
pieces.append(piece)
elif prev == '/':
if next in ('INST', 'I'):
pieces.append(piece)
elif prev == 'INST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'I':
if next in ('/', ')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'EDGE':
if next in ('(', 'VAR', 'CONST', 'I'):
pieces.append(piece)
elif next == ')':
pieces[-1] = piece
elif next in ('EDGE', 'MODE'):
pieces[-1] = piece
elif prev == 'MODE':
if next == 'INST':
pieces.append(piece)
elif prev == 'CONST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)

pieces_ = []
open_cnt = 0
closed_cnt = 0
if pieces[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in pieces:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
pieces = pieces_ + [')'] * (open_cnt - closed_cnt)

linearized = re.sub(r'\s+', ' ', ' '.join(pieces)).strip()

, # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py, # make sure parentheses match",Ġ
https://github.com/hankcs/HanLP,tokenization_t5.py,0,0.0,184,69.17,59,22.18,13,4.89,10,3.76,0,0.0,20,2,73,1,,"ADDITIONAL, AMRT5Tokenizer, BACKOFF, BACKR_SRC_N, BACKR_TRG_N, BOS_N, EOS_N, Exception, INIT, LIT_END, LIT_START, PENMANT5Tokenizer, PNTR_N, ParsedStatus, STOP_N, __init__, _classify, _fix_and_make_graph, _get_nodes_and_backreferences, _repl1, _repl2, _split_name_ops, _tok_bpe, _tokenize, _tokenize_encoded_graph, add, add_tokens, additional_tokens, additions, append, args, backr, backreferences, batch, batch_encode_graphs, batch_encode_graphs_from_linearized, batch_encode_sentences, batch_extra, bb, bos_token, bos_token_id, bpe, bpe_backr, bpe_backreferences, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, build_graph, build_inputs_with_special_tokens, byte_encoder, char, classmethod, closed_cnt, cnt, collapse_name_ops, connect_graph_if_not_connected, convert_tokens_to_ids, copy, counter, decode, decode_amr, decode_into_node_and_backreferences, decode_into_node_and_backreferences_without_space, encode, encoded, encoder, endswith, enumerate, eos_token_id, extend, extra, extras, find, findall, fix_text, fol, from_pretrained, get, get_vocab, graph, graph_, graphs, group, idx, init_amr_vocabulary, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, isinstance, items, kwargs, last, last_remap, len, linearization, linearize, linearized, linearized_nodes, linearized_nodes_, linearizer, list, lst, lstrip, max, maxlen, metadata, modified, newvars, next, node, nodes, nodes_, nxt, old_enc_size, open_cnt, out, output, pad_token, pad_token_id, pat, patterns, penman, piece, pieces, pieces_, pretrained_model_path, prev, print, property, pst, quote, range, raw_graph, recategorization_tokens, recategorizations, recats, regex, rel, remap, remove_pars, replace, restore_backreferences_from_pointers, restore_name_ops, rsplit, self, sentences, set, shift, split, startswith, status, str, strip, super, sys, text, to, tok, tok_span, token, token_ids, token_ids_0, token_ids_1, token_uni_ids, tokenize, tokenize_amr, tokens, tokk, torch, traceback, transformers, triple, triples, typing, unk_token_id, use_pointer_tokens, var, variables, x, zip","copy.deepcopy, linearization.AMRLinearizer, linearization.AMRTokens, penman.Graph, penman.Triple, penman.decode, penman.encode, penman.pm_encode, postprocessing, re.IGNORECASE, re.MULTILINE, re.compile, re.findall, re.match, re.sub, sys.stderr, torch.device, torch.tensor, traceback.print_exc, transformers.T5Tokenizer, transformers.T5TokenizerFast, typing.Dict, typing.Iterable, typing.Set","__init__, _classify, _fix_and_make_graph, _get_nodes_and_backreferences, _repl1, _repl2, _tok_bpe, _tokenize, _tokenize_encoded_graph, batch_encode_graphs, batch_encode_graphs_from_linearized, batch_encode_sentences, build_inputs_with_special_tokens, decode_amr, encoder, fix_text, from_pretrained, init_amr_vocabulary, linearize, tokenize_amr","AMRT5Tokenizer, PENMANT5Tokenizer","ADDITIONAL, backr, backreferences, batch, batch_extra, bb, bpe_backr, bpe_backreferences, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, char, closed_cnt, cnt, counter, e, encoder, extra, extras, fol, g, graph, graph_, i, idx, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, last, last_remap, lin, linearized, linearized_nodes, linearized_nodes_, lst, maxlen, n, newvars, next, nodes, nodes_, nxt, open_cnt, out, output, piece, pieces, pieces_, prev, pst, quote, recats, rel, remap, sentences, shift, status, tok, tok_span, token, token_ids, token_uni_ids, tokens, tokk, triple, triples, var, variables, x",Tokenize a string. Modified in order to handle sentences with recategorization pointers,", 
        line = linearized
        # make sure parentheses match
        # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py
        open_count = 0
        close_count = 0
        for i, c in enumerate(line):
            if c == '(':
                open_count += 1
            elif c == ')':
                close_count += 1
            if open_count == close_count and open_count > 0:
                line = line[:i].strip()
                break
        old_line = line
        while True:
            open_count = len(re.findall(r'\(', line))
            close_count = len(re.findall(r'\)', line))
            if open_count > close_count:
                line += ')' * (open_count - close_count)
            elif close_count > open_count:
                for i in range(close_count - open_count):
                    line = line.rstrip(')')
                    line = line.rstrip(' ')
            if old_line == line:
                break
            old_line = line
        ,  ,  ( ,  ) ,  / ,  :,  ?<[a-z]+:?\d*>| ?:[^\s]+|'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+,  Tokenize a string. Modified in order to handle sentences with recategorization pointers,  \1 , !, "", (, ([^:])(ARG), (\"".+?\""), (\(\s*[a-z][\d+]\s*\/\s*[^\s\)\(:\/]+\s*)((?:/\s*[^\s\)\(:\/]+\s*)+), (\(\s?)([a-z])([^\/:\)]+[:\)]), ), +, ,, -, -of, ., .+-\d\d, /, :, :instance, :mode, <, <pointer:, =, >, ?, Building failure:, CONST, Decoding failure:, EDGE, I, INST, MODE, Reconnction 2 failure:, Reconnection 1 failure:, VAR, \, \1 :\2, \s+, ^[a-z]\d*$, _, cpu, decoder_input_ids, graphs, i, linearized_graphs, lm_labels, pt, sentences, thing, utf-8, z","0, 1, 1000, 1022, 2, 2000, 3, 3000, 512, 9, False, None, True"," Tokenize a string. Modified in order to handle sentences with recategorization pointers
bpe_tokens = []
for tok_span in text.lstrip().split(' '):
tok_span = tok_span.strip()
recats = tok_span.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
bpe_tokens.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for token in re.findall(self.pat, ' ' + tok_span):
token = """".join(
self.byte_encoder[b] for b in token.encode(""utf-8"")
)  # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)
bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split("" ""))

return bpe_tokens

def _tok_bpe(self, token, add_space=True):
# if add_space:
#     token = ' ' + token.lstrip()
tokk = []
tok = token.strip()
recats = tok.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
tokk.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for tok in self.patterns.findall(token):
tokk.extend(self.tokenize(tok))
return tokk

def _get_nodes_and_backreferences(self, graph):
lin = self.linearizer.linearize(graph)
linearized_nodes, backreferences = lin.nodes, lin.backreferences
return linearized_nodes, backreferences

def tokenize_amr(self, graph):
linearized_nodes, backreferences = self._get_nodes_and_backreferences(graph)

bpe_tokens = []
bpe_backreferences = []
counter = 0

encoder = self.encoder
for i, (backr, tokk) in enumerate(zip(backreferences, linearized_nodes)):
is_in_enc = self.INIT + tokk in encoder
is_rel = tokk.startswith(':') and len(tokk) > 1
is_spc = tokk.startswith('<') and tokk.endswith('>')
is_of = tokk.startswith(':') and tokk.endswith('-of')
is_frame = re.match(r'.+-\d\d', tokk) is not None

if tokk.startswith('""') and tokk.endswith('""'):
tokk = tokk[1:-1].replace('_', ' ')
bpe_toks = [self.INIT + AMRTokens.LIT_START]
bpe_toks += self._tok_bpe(tokk, add_space=True)
bpe_toks.append(self.INIT + AMRTokens.LIT_END)

elif (is_rel or is_spc or is_frame or is_of):
if is_in_enc:
bpe_toks = [self.INIT + tokk]
elif is_frame:
bpe_toks = self._tok_bpe(tokk[:-3], add_space=True) + [tokk[-3:]]
elif is_of:
rel = tokk[:-3]
if self.INIT + rel in encoder:
bpe_toks = [self.INIT + rel, '-of']
else:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(rel[1:], add_space=True) + ['-of']
elif is_rel:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(tokk[1:], add_space=True)
else:
raise

else:
if is_in_enc:
bpe_toks = [self.INIT + tokk]
else:
bpe_toks = self._tok_bpe(tokk, add_space=True)

bpe_tokens.append(bpe_toks)

if i == backr:
bpe_backr = list(range(counter, counter + len(bpe_toks)))
counter += len(bpe_toks)
bpe_backreferences.append(bpe_backr)
else:
bpe_backreferences.append(bpe_backreferences[backr][0:1])
counter += 1
bpe_tokens = [b for bb in bpe_tokens for b in bb]
bpe_token_ids = self.convert_tokens_to_ids(bpe_tokens)
bpe_backreferences = [b for bb in bpe_backreferences for b in bb]
return bpe_tokens, bpe_token_ids, bpe_backreferences

def batch_encode_sentences(self, sentences, device=torch.device('cpu')):
sentences = [s for s in sentences]
extra = {'sentences': sentences}
batch = super().batch_encode_plus(sentences, return_tensors='pt', pad_to_max_length=True)
batch = {k: v.to(device) for k, v in batch.items()}
return batch, extra

def linearize(self, graph):
shift = len(self)
tokens, token_ids, backreferences = self.tokenize_amr(graph)
extra = {'linearized_graphs': tokens, 'graphs': graph}
token_uni_ids = \
[idx if i == b else b + shift for i, (idx, b) in enumerate(zip(token_ids, backreferences))]
if token_uni_ids[-1] != (self.INIT + AMRTokens.EOS_N):
tokens.append(self.INIT + AMRTokens.EOS_N)
token_ids.append(self.eos_token_id)
token_uni_ids.append(self.eos_token_id)
backreferences.append(len(backreferences))
return token_uni_ids, extra

def batch_encode_graphs(self, graphs, device=torch.device('cpu')):
linearized, extras = zip(*[self.linearize(g) for g in graphs])
return self.batch_encode_graphs_from_linearized(linearized, extras, device=device)

def batch_encode_graphs_from_linearized(self, linearized, extras=None, device=torch.device('cpu')):
if extras is not None:
batch_extra = {'linearized_graphs': [], 'graphs': []}
for extra in extras:
batch_extra['graphs'].append(extra['graphs'])
batch_extra['linearized_graphs'].append(extra['linearized_graphs'])
else:
batch_extra = {}
maxlen = 0
batch = []
for token_uni_ids in linearized:
maxlen = max(len(token_uni_ids), maxlen)
batch.append(token_uni_ids)
batch = [x + [self.pad_token_id] * (maxlen - len(x)) for x in batch]
batch = torch.tensor(batch).to(device)
batch = {'decoder_input_ids': batch[:, :-1], 'lm_labels': batch[:, 1:]}
return batch, batch_extra

def decode_amr(self, tokens, restore_name_ops=False):
try:
nodes, backreferences = postprocessing.decode_into_node_and_backreferences(tokens, self)
except Exception as e:
print('Decoding failure:', file=sys.stderr)
traceback.print_exc()
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
if self.use_pointer_tokens:
nodes, backreferences = postprocessing.restore_backreferences_from_pointers(nodes)
try:
graph_ = graph = postprocessing.build_graph(nodes, backreferences, restore_name_ops=restore_name_ops)
except Exception as e:
print('Building failure:', file=sys.stderr)
traceback.print_exc()
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(e, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
try:
graph, status = postprocessing.connect_graph_if_not_connected(graph)
if status == postprocessing.ParsedStatus.BACKOFF:
print('Reconnection 1 failure:')
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(graph_, file=sys.stderr)
return graph, status, (nodes, backreferences)
except Exception as e:
print('Reconnction 2 failure:', file=sys.stderr)
traceback.print_exc()
print(nodes, file=sys.stderr)
print(backreferences, file=sys.stderr)
print(graph_, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (nodes, backreferences)


class PENMANT5Tokenizer(AMRT5Tokenizer):

def __init__(self, *args, raw_graph=False, **kwargs):
super().__init__(*args, **kwargs)
self.linearizer = None
self.remove_pars = False
self.raw_graph = raw_graph

def _tokenize_encoded_graph(self, encoded):
linearized = re.sub(r""(\"".+?\"")"", r' \1 ', encoded)
pieces = []
for piece in linearized.split():
if piece.startswith('""') and piece.endswith('""'):
pieces.append(piece)
else:
piece = piece.replace('(', ' ( ')
piece = piece.replace(')', ' ) ')
piece = piece.replace(':', ' :')
piece = piece.replace('/', ' / ')
piece = piece.strip()
pieces.append(piece)
linearized = re.sub(r'\s+', ' ', ' '.join(pieces)).strip()
# T5 uses pad instead of <s>
# linearized_nodes = [AMRTokens.BOS_N] + linearized.split(' ')
linearized_nodes = [self.pad_token] + linearized.split(' ')
return linearized_nodes

def tokenize_amr(self, graph):
if self.raw_graph:
graph_ = copy.deepcopy(graph)
graph_.metadata = {}
linearized = penman.encode(graph_)
linearized = re.sub(r""\s+"", ' ', linearized)
bpe_tokens = [self.bos_token] + self._tokenize(linearized)[:1022]
bpe_token_ids = [self.encoder.get(b, self.unk_token_id) for b in bpe_tokens]
bpe_backreferences = list(range(len(bpe_token_ids)))
return bpe_tokens, bpe_token_ids, bpe_backreferences
else:
return super().tokenize_amr(graph)

def _get_nodes_and_backreferences(self, graph):
graph_ = copy.deepcopy(graph)
graph_.metadata = {}
linearized = penman.encode(graph_)
linearized_nodes = self._tokenize_encoded_graph(linearized)

if self.use_pointer_tokens:
remap = {}
for i in range(1, len(linearized_nodes)):
nxt = linearized_nodes[i]
lst = linearized_nodes[i - 1]
if nxt == '/':
remap[lst] = f'<pointer:{len(remap)}>'
i = 1
linearized_nodes_ = [linearized_nodes[0]]
while i < (len(linearized_nodes)):
nxt = linearized_nodes[i]
lst = linearized_nodes_[-1]
if nxt in remap:
if lst == '(' and linearized_nodes[i + 1] == '/':
nxt = remap[nxt]
i += 1
elif lst.startswith(':'):
nxt = remap[nxt]
linearized_nodes_.append(nxt)
i += 1
linearized_nodes = linearized_nodes_
if self.remove_pars:
linearized_nodes = [n for n in linearized_nodes if n != '(']
backreferences = list(range(len(linearized_nodes)))
return linearized_nodes, backreferences

def _classify(self, node):
if not isinstance(node, str):
return ""CONST""
elif node == 'i':
return ""I""
elif re.match(r'^[a-z]\d*$', node) is not None:
return ""VAR""
elif node[0].isdigit():
return ""CONST""
elif node.startswith('""') and node.endswith('""'):
return ""CONST""
elif node in ('+', '-'):
return ""CONST""
elif node == ':mode':
return 'MODE'
elif node.startswith(':'):
return ""EDGE""
elif node in ['/', '(', ')']:
return node
elif node[0].isalpha():
for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\'):
if char in node:
return ""CONST""
return ""INST""
else:
return 'CONST'

def _fix_and_make_graph(self, nodes):

nodes_ = []
for n in nodes:
if isinstance(n, str):
if n.startswith('<') and n.endswith('>') and (not n.startswith('<pointer:')):
pass
else:
nodes_.append(n)
else:
nodes_.append(n)
nodes = nodes_
if not nodes:
return penman.Graph()

if self.use_pointer_tokens:

i = 0
nodes_ = []
while i < len(nodes):
nxt = nodes[i]
pst = None
if isinstance(nxt, str) and nxt.startswith('<pointer:'):
e = nxt.find('>')
if e != len(nxt) - 1:
pst = nxt[e + 1:]
nxt = nxt[:e + 1]
nodes_.append(nxt)
if pst is not None:
nodes_.append(pst)
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 1
nodes_ = [nodes[0]]
while i < len(nodes):
nxt = nodes[i]
if isinstance(nxt, str) and nxt.startswith('<pointer:') and i + 1 < len(nodes):
nxt = 'z' + nxt[9:-1]
fol = nodes[i + 1]
# is not expansion
if isinstance(fol, str) and (fol.startswith(':') or (fol == ')')):
nodes_.append(nxt)
else:
if self.remove_pars:
nodes_.append('(')
else:
if nodes_[-1] != '(':
nodes_.append('(')
# pass
nodes_.append(nxt)
nodes_.append('/')
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes) - 1):
if nodes[i] == ':':
nodes_.append(nodes[i] + nodes[i + 1])
i += 2
last = False
else:
nodes_.append(nodes[i])
i += 1
last = True
if last:
nodes_.append(nodes[-1])
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes)):
if i < 2:
nodes_.append(nodes[i])
i += 1
elif nodes_[-2] == '/' and nodes[i] == '/':
i += 2
else:
nodes_.append(nodes[i])
i += 1
nodes = nodes_

i = 0
newvars = 0
variables = set()
remap = {}
nodes_ = []
while i < (len(nodes)):

next = nodes[i]

if next == '/':
last = nodes_[-1]
if last in variables:
last_remap = f""z{newvars + 1000}""
newvars += 1
nodes_[-1] = last_remap
remap[last] = last_remap
variables.add(last)
nodes_.append(next)

elif self._classify(next) == 'VAR' and next in remap and (i < len(nodes) - 1) and nodes[i + 1] != '/':
next = remap[next]
nodes_.append(next)

else:
nodes_.append(next)

i += 1

nodes = nodes_
pieces_ = []
open_cnt = 0
closed_cnt = 0
if nodes[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in nodes:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
nodes = pieces_ + [')'] * (open_cnt - closed_cnt)

pieces = []
for piece in nodes:
if not pieces:
pieces.append('(')
else:
piece = str(piece)
if piece.startswith('""') or piece.startswith('""') or '""' in piece.strip('""'):
piece = '""' + piece.replace('""', '') + '""'

prev = self._classify(pieces[-1])
next = self._classify(piece)

if next == 'CONST':
quote = False
for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\', '_', '='):
if char in piece:
quote = True
break
if quote:
piece = '""' + piece.strip('""') + '""'

if prev == '(':
if next in ('VAR', 'I'):
pieces.append(piece)
elif prev == ')':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'VAR':
if next in ('/', 'EDGE', 'MODE', ')'):
pieces.append(piece)
elif prev == '/':
if next in ('INST', 'I'):
pieces.append(piece)
elif prev == 'INST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'I':
if next in ('/', ')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'EDGE':
if next in ('(', 'VAR', 'CONST', 'I'):
pieces.append(piece)
elif next == ')':
pieces[-1] = piece
elif next in ('EDGE', 'MODE'):
pieces[-1] = piece
elif prev == 'MODE':
if next == 'INST':
pieces.append(piece)
elif prev == 'CONST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)

pieces_ = []
open_cnt = 0
closed_cnt = 0
if pieces[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in pieces:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
pieces = pieces_ + [')'] * (open_cnt - closed_cnt)

linearized = re.sub(r'\s+', ' ', ' '.join(pieces)).strip()

, #     del self.encoder[tok], #     i = self.encoder[tok], #     ntok = self.INIT + tok, #     self.decoder[i] = ntok, #     self.encoder[ntok] = i, # T5 has no encoder but it's not a problem for Chinese, # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py, # for tok in self.all_special_tokens:, # make sure parentheses match",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2021-04-27 19:29",
https://github.com/hankcs/HanLP,constant.py,0,0.0,14,0.25,2983,53.14,0,0.0,20,0.36,2596,46.25,0,0,10,0,,"ROUGE_KEYS, arg_to_plm_model, arg_to_scheduler, arg_to_scheduler_choices, arg_to_scheduler_metavar, arg_to_tokenizer, itm, keys, lstrip, raw_special_tokens, recategorizations, sorted, special_tokens, transformers","transformers.AutoModelForSeq2SeqLM, transformers.AutoTokenizer, transformers.BartForConditionalGeneration, transformers.BartTokenizer, transformers.T5ForConditionalGeneration, transformers.T5Model, transformers.T5Tokenizer, transformers.optimization.get_constant_schedule_with_warmup, transformers.optimization.get_cosine_schedule_with_warmup, transformers.optimization.get_cosine_with_hard_restarts_schedule_with_warmup, transformers.optimization.get_linear_schedule_with_warmup, transformers.optimization.get_polynomial_decay_schedule_with_warmup",,,"ROUGE_KEYS, arg_to_plm_model, arg_to_scheduler, arg_to_scheduler_choices, arg_to_scheduler_metavar, arg_to_tokenizer, itm, raw_special_tokens, recategorizations, special_tokens",,", , -00, -01, -02, -03, -04, -05, -06, -07, -08, -09, -10, -11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -31, -32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47, -48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63, -64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79, -80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95, -96, -97, -98, -of, </AMR>, <AMR>, AutoModelForSeq2SeqLM, AutoTokenizer, BartForConditionalGeneration, BartTokenizer, T5ForConditionalGeneration, T5Model, T5Tokenizer, _1, _10, _11, _12, _13, _14, _15, _2, _3, _4, _5, _6, _7, _8, _9, constant, cosine, cosine_w_restarts, linear, polynomial, rouge1, rouge2, rougeL, rougeLsum, {, }, Ġ, Ġ:ARG0, Ġ:ARG1, Ġ:ARG10, Ġ:ARG11, Ġ:ARG12, Ġ:ARG13, Ġ:ARG14, Ġ:ARG15, Ġ:ARG16, Ġ:ARG17, Ġ:ARG18, Ġ:ARG19, Ġ:ARG2, Ġ:ARG20, Ġ:ARG3, Ġ:ARG4, Ġ:ARG5, Ġ:ARG6, Ġ:ARG7, Ġ:ARG8, Ġ:ARG9, Ġ:accompanier, Ġ:age, Ġ:beneficiary, Ġ:calendar, Ġ:cause, Ġ:century, Ġ:compared-to, Ġ:concession, Ġ:condition, Ġ:conj-as-if, Ġ:consist, Ġ:consist-of, Ġ:cost, Ġ:day, Ġ:dayperiod, Ġ:decade, Ġ:degree, Ġ:destination, Ġ:direction, Ġ:domain, Ġ:duration, Ġ:employed-by, Ġ:era, Ġ:example, Ġ:extent, Ġ:frequency, Ġ:instrument, Ġ:li, Ġ:location, Ġ:manner, Ġ:meaning, Ġ:medium, Ġ:mod, Ġ:mode, Ġ:month, Ġ:name, Ġ:op1, Ġ:op2, Ġ:op3, Ġ:op4, Ġ:op5, Ġ:ord, Ġ:part, Ġ:path, Ġ:polarity, Ġ:polite, Ġ:poss, Ġ:purpose, Ġ:quant, Ġ:quarter, Ġ:range, Ġ:relation, Ġ:role, Ġ:scale, Ġ:season, Ġ:snt0, Ġ:snt1, Ġ:snt2, Ġ:snt3, Ġ:snt4, Ġ:snt5, Ġ:source, Ġ:subevent, Ġ:subset, Ġ:superset, Ġ:time, Ġ:timezone, Ġ:topic, Ġ:unit, Ġ:value, Ġ:weekday, Ġ:wiki, Ġ:year, Ġ:year2, Ġ</lit>, Ġ<backr:src:XXX>, Ġ<backr:trg:XXX>, Ġ<lit>, Ġ<pointer:0>, Ġ<pointer:100>, Ġ<pointer:101>, Ġ<pointer:102>, Ġ<pointer:103>, Ġ<pointer:104>, Ġ<pointer:105>, Ġ<pointer:106>, Ġ<pointer:107>, Ġ<pointer:108>, Ġ<pointer:109>, Ġ<pointer:10>, Ġ<pointer:110>, Ġ<pointer:111>, Ġ<pointer:112>, Ġ<pointer:113>, Ġ<pointer:114>, Ġ<pointer:115>, Ġ<pointer:116>, Ġ<pointer:117>, Ġ<pointer:118>, Ġ<pointer:119>, Ġ<pointer:11>, Ġ<pointer:120>, Ġ<pointer:121>, Ġ<pointer:122>, Ġ<pointer:123>, Ġ<pointer:124>, Ġ<pointer:125>, Ġ<pointer:126>, Ġ<pointer:127>, Ġ<pointer:128>, Ġ<pointer:129>, Ġ<pointer:12>, Ġ<pointer:130>, Ġ<pointer:131>, Ġ<pointer:132>, Ġ<pointer:133>, Ġ<pointer:134>, Ġ<pointer:135>, Ġ<pointer:136>, Ġ<pointer:137>, Ġ<pointer:138>, Ġ<pointer:139>, Ġ<pointer:13>, Ġ<pointer:140>, Ġ<pointer:141>, Ġ<pointer:142>, Ġ<pointer:143>, Ġ<pointer:144>, Ġ<pointer:145>, Ġ<pointer:146>, Ġ<pointer:147>, Ġ<pointer:148>, Ġ<pointer:149>, Ġ<pointer:14>, Ġ<pointer:150>, Ġ<pointer:15>, Ġ<pointer:16>, Ġ<pointer:17>, Ġ<pointer:18>, Ġ<pointer:19>, Ġ<pointer:1>, Ġ<pointer:20>, Ġ<pointer:21>, Ġ<pointer:22>, Ġ<pointer:23>, Ġ<pointer:24>, Ġ<pointer:25>, Ġ<pointer:26>, Ġ<pointer:27>, Ġ<pointer:28>, Ġ<pointer:29>, Ġ<pointer:2>, Ġ<pointer:30>, Ġ<pointer:31>, Ġ<pointer:32>, Ġ<pointer:33>, Ġ<pointer:34>, Ġ<pointer:35>, Ġ<pointer:36>, Ġ<pointer:37>, Ġ<pointer:38>, Ġ<pointer:39>, Ġ<pointer:3>, Ġ<pointer:40>, Ġ<pointer:41>, Ġ<pointer:42>, Ġ<pointer:43>, Ġ<pointer:44>, Ġ<pointer:45>, Ġ<pointer:46>, Ġ<pointer:47>, Ġ<pointer:48>, Ġ<pointer:49>, Ġ<pointer:4>, Ġ<pointer:50>, Ġ<pointer:51>, Ġ<pointer:52>, Ġ<pointer:53>, Ġ<pointer:54>, Ġ<pointer:55>, Ġ<pointer:56>, Ġ<pointer:57>, Ġ<pointer:58>, Ġ<pointer:59>, Ġ<pointer:5>, Ġ<pointer:60>, Ġ<pointer:61>, Ġ<pointer:62>, Ġ<pointer:63>, Ġ<pointer:64>, Ġ<pointer:65>, Ġ<pointer:66>, Ġ<pointer:67>, Ġ<pointer:68>, Ġ<pointer:69>, Ġ<pointer:6>, Ġ<pointer:70>, Ġ<pointer:71>, Ġ<pointer:72>, Ġ<pointer:73>, Ġ<pointer:74>, Ġ<pointer:75>, Ġ<pointer:76>, Ġ<pointer:77>, Ġ<pointer:78>, Ġ<pointer:79>, Ġ<pointer:7>, Ġ<pointer:80>, Ġ<pointer:81>, Ġ<pointer:82>, Ġ<pointer:83>, Ġ<pointer:84>, Ġ<pointer:85>, Ġ<pointer:86>, Ġ<pointer:87>, Ġ<pointer:88>, Ġ<pointer:89>, Ġ<pointer:8>, Ġ<pointer:90>, Ġ<pointer:91>, Ġ<pointer:92>, Ġ<pointer:93>, Ġ<pointer:94>, Ġ<pointer:95>, Ġ<pointer:96>, Ġ<pointer:97>, Ġ<pointer:98>, Ġ<pointer:99>, Ġ<pointer:9>, Ġ<pointer>, Ġ<stop>, ĠCAUSE_OF_DEATH, ĠCOUNTRY, ĠCRIMINAL_CHARGE, ĠDATE, ĠDATE_ATTRS, ĠDURATION, ĠENTITY, ĠHANDLE, ĠIDEOLOGY, ĠLOCATION, ĠMISC, ĠMONEY, ĠNATIONALITY, ĠNUMBER, ĠORDINAL, ĠORDINAL_ENTITY, ĠORGANIZATION, ĠQUANTITY, ĠRELIGION, ĠSCORE_ENTITY, ĠSTATE_OR_PROVINCE, ĠTITLE, Ġabandon-01, Ġabet-01, Ġabide-01, Ġabolish-01, Ġabominable-02, Ġabort-01, Ġabsent-01, Ġabsorb-01, Ġabstain-01, Ġabuse-01, Ġabuse-02, Ġaccelerate-01, Ġaccept-01, Ġaccess-01, Ġaccommodate-01, Ġaccompany-01, Ġaccomplish-01, Ġaccord-02, Ġaccord-03, Ġaccount-01, Ġaccountable-02, Ġaccumulate-01, Ġaccuse-01, Ġachieve-01, Ġacknowledge-01, Ġacquaint-01, Ġacquire-01, Ġacquit-01, Ġact-01, Ġact-02, Ġactivate-01, Ġactivity-06, Ġactual-02, Ġadapt-01, Ġadd-01, Ġadd-02, Ġadd-03, Ġaddict-01, Ġaddictive-02, Ġaddress-01, Ġaddress-02, Ġaddress-03, Ġadhere-02, Ġadjust-01, Ġadminister-01, Ġadministrate-01, Ġadmire-01, Ġadmit-01, Ġadmit-02, Ġadopt-01, Ġadvance-01, Ġadvanced-02, Ġadvertise-01, Ġadvise-01, Ġadvocate-01, Ġaffair-01, Ġaffair-02, Ġaffect-01, Ġaffiliate-01, Ġafford-01, Ġafford-02, Ġage-01, Ġaggravate-01, Ġagree-01, Ġaid-01, Ġaim-01, Ġaim-02, Ġair-01, Ġalarm-01, Ġalert-01, Ġalien-01, Ġalienate-01, Ġalign-01, Ġalike-05, Ġallege-01, Ġallocate-01, Ġallow-01, Ġallow-02, Ġally-01, Ġalter-01, Ġalternate-01, Ġamaze-01, Ġambush-01, Ġamend-01, Ġamount-01, Ġamplify-01, Ġamr-unknown, Ġamuse-01, Ġanalyze-01, Ġanger-01, Ġannounce-01, Ġannoy-01, Ġanswer-01, Ġanticipate-01, Ġapologize-01, Ġappall-01, Ġappeal-01, Ġappeal-02, Ġappeal-03, Ġappear-01, Ġappear-02, Ġappease-01, Ġapplaud-01, Ġapply-01, Ġapply-02, Ġappoint-01, Ġappreciate-02, Ġapprehend-01, Ġapprentice-01, Ġapproach-01, Ġapproach-02, Ġappropriate-02, Ġapprove-01, Ġarbitrary-02, Ġargue-01, Ġargue-02, Ġarise-02, Ġarm-01, Ġarmor-01, Ġarrange-01, Ġarrest-01, Ġarrive-01, Ġask-01, Ġask-02, Ġaspire-01, Ġassassinate-01, Ġassault-01, Ġassemble-01, Ġassemble-02, Ġassert-02, Ġassert-03, Ġassess-01, Ġassign-01, Ġassist-01, Ġassociate-01, Ġassume-01, Ġassume-02, Ġassure-01, Ġastonish-01, Ġat-least, Ġattach-01, Ġattack-01, Ġattain-01, Ġattempt-01, Ġattend-01, Ġattend-02, Ġattract-01, Ġattribute-01, Ġaudit-01, Ġauthor-01, Ġauthorize-01, Ġautomate-01, Ġavail-01, Ġavailable-02, Ġaverage-01, Ġaverage-03, Ġaverage-04, Ġavoid-01, Ġawait-01, Ġawake-03, Ġaward-01, Ġawe-01, Ġbabble-01, Ġback-01, Ġback-02, Ġback-up-04, Ġbad-02, Ġbad-04, Ġbad-05, Ġbad-07, Ġbaffle-01, Ġbail-out-02, Ġbait-01, Ġbake-01, Ġbalance-01, Ġban-01, Ġbank-01, Ġbankrupt-01, Ġbar-01, Ġbargain-01, Ġbarter-01, Ġbase-01, Ġbase-02, Ġbash-01, Ġbathe-01, Ġbattle-01, Ġbe-02, Ġbe-compared-to-91, Ġbe-destined-for-91, Ġbe-done-08, Ġbe-from-91, Ġbe-located-at-91, Ġbe-polite-91, Ġbe-temporally-at-91, Ġbear-01, Ġbear-02, Ġbear-06, Ġbeat-01, Ġbeat-03, Ġbeat-up-05, Ġbeautiful-02, Ġbecome-01, Ġbefriend-01, Ġbeg-01, Ġbegin-01, Ġbehave-01, Ġbehead-01, Ġbelieve-01, Ġbelong-01, Ġbend-01, Ġbenefit-01, Ġbestow-01, Ġbet-01, Ġbetray-01, Ġbetter-01, Ġbeware-01, Ġbias-01, Ġbicker-01, Ġbid-01, Ġbid-03, Ġbill-01, Ġbind-01, Ġbind-03, Ġbitch-01, Ġbite-01, Ġblack-04, Ġblack-05, Ġblack-07, Ġblackmail-01, Ġblame-01, Ġblast-01, Ġblast-05, Ġbleed-01, Ġbless-01, Ġblind-02, Ġblock-01, Ġblockade-01, Ġblog-01, Ġblood-02, Ġblow-01, Ġblow-03, Ġblow-14, Ġblow-up-06, Ġblunt-01, Ġblunt-02, Ġboard-01, Ġboast-01, Ġbomb-01, Ġboom-02, Ġboost-01, Ġborder-01, Ġbore-02, Ġborrow-01, Ġbother-01, Ġbother-02, Ġbow-01, Ġboycott-01, Ġbrag-01, Ġbrainwash-01, Ġbrand-01, Ġbrave-02, Ġbreach-01, Ġbreak-01, Ġbreak-02, Ġbreak-13, Ġbreak-18, Ġbreak-19, Ġbreak-down-12, Ġbreak-through-22, Ġbreak-through-26, Ġbreak-up-08, Ġbreathe-01, Ġbreed-01, Ġbribe-01, Ġbridge-01, Ġbrief-01, Ġbright-02, Ġbright-03, Ġbrilliant-01, Ġbring-01, Ġbring-about-05, Ġbring-down-03, Ġbring-on-06, Ġbring-up-02, Ġbring-up-08, Ġbroad-02, Ġbroadcast-01, Ġbroaden-01, Ġbroke-23, Ġbroker-01, Ġbrutal-02, Ġbudget-01, Ġbuild-01, Ġbuild-02, Ġbuild-up-05, Ġbullshit-01, Ġbully-01, Ġbundle-01, Ġburden-01, Ġburn-01, Ġburst-02, Ġbury-01, Ġbust-01, Ġbust-02, Ġbusy-01, Ġbuy-01, Ġbuy-05, Ġbuy-into-04, Ġbyline-91, Ġcalculate-01, Ġcall-01, Ġcall-02, Ġcall-03, Ġcall-13, Ġcall-on-05, Ġcalm-down-02, Ġcamp-02, Ġcampaign-01, Ġcancel-01, Ġcap-01, Ġcap-02, Ġcapable-01, Ġcapture-01, Ġcare-01, Ġcare-02, Ġcare-03, Ġcare-04, Ġcarry-01, Ġcarry-on-02, Ġcarry-out-03, Ġcase-03, Ġcase-04, Ġcast-01, Ġcast-03, Ġcatch-01, Ġcatch-02, Ġcatch-03, Ġcatch-up-04, Ġcater-01, Ġcause-01, Ġcautious-02, Ġcease-01, Ġcelebrate-01, Ġcelebrate-02, Ġcensor-01, Ġcenter-01, Ġcenter-02, Ġcertify-01, Ġchain-01, Ġchair-01, Ġchallenge-01, Ġchampion-01, Ġchance-01, Ġchance-02, Ġchange-01, Ġchange-02, Ġchannel-01, Ġcharacteristic-02, Ġcharacterize-01, Ġcharge-01, Ġcharge-05, Ġcharge-06, Ġcharge-08, Ġcharm-01, Ġchase-01, Ġchat-01, Ġcheap-02, Ġcheat-02, Ġcheat-03, Ġcheck-01, Ġcheck-03, Ġcheck-07, Ġcheck-out-05, Ġchoose-01, Ġcirculate-01, Ġcite-01, Ġcivilize-01, Ġclaim-01, Ġclaim-02, Ġclarify-10, Ġclash-01, Ġclass-01, Ġclassify-01, Ġclean-01, Ġclean-04, Ġclean-up-02, Ġclear-01, Ġclear-06, Ġclever-01, Ġclick-01, Ġclimb-01, Ġclone-01, Ġclose-01, Ġclose-03, Ġclose-06, Ġclose-10, Ġclose-11, Ġclose-13, Ġclose-down-04, Ġclothe-01, Ġcoach-01, Ġcoerce-01, Ġcoexist-01, Ġcohere-01, Ġcoincide-01, Ġcold-01, Ġcold-02, Ġcollaborate-01, Ġcollapse-01, Ġcollect-01, Ġcolonize-01, Ġcolor-01, Ġcombat-01, Ġcombine-01, Ġcome-01, Ġcome-03, Ġcome-04, Ġcome-12, Ġcome-across-21, Ġcome-down-23, Ġcome-in-07, Ġcome-on-25, Ġcome-out-09, Ġcome-up-11, Ġcome-up-13, Ġcomfortable-02, Ġcommand-02, Ġcommence-01, Ġcommend-01, Ġcomment-01, Ġcommit-01, Ġcommit-02, Ġcommunicate-01, Ġcommute-02, Ġcompare-01, Ġcompel-01, Ġcompensate-01, Ġcompete-01, Ġcompete-02, Ġcompetent-01, Ġcompile-01, Ġcomplain-01, Ġcomplete-01, Ġcomplete-02, Ġcomplicate-01, Ġcomply-01, Ġcompose-01, Ġcompound-01, Ġcomprehend-01, Ġcomprise-01, Ġcompromise-01, Ġcompromise-02, Ġconceal-01, Ġconcede-01, Ġconceive-01, Ġconcentrate-01, Ġconcentrate-02, Ġconcern-01, Ġconcern-02, Ġconclude-01, Ġconclude-02, Ġconcrete-02, Ġcondemn-01, Ġcondition-01, Ġcondone-01, Ġconduct-01, Ġconduct-02, Ġconfess-01, Ġconfident-01, Ġconfine-01, Ġconfirm-01, Ġconfiscate-01, Ġconflict-01, Ġconform-01, Ġconfront-01, Ġconfuse-01, Ġcongratulate-01, Ġconnect-01, Ġconquer-01, Ġconsent-01, Ġconsider-01, Ġconsider-02, Ġconsist-01, Ġconsistent-01, Ġconsistent-02, Ġconsolidate-01, Ġconspire-01, Ġconstitute-01, Ġconstrain-01, Ġconstruct-01, Ġconsult-01, Ġconsume-01, Ġcontact-01, Ġcontain-01, Ġcontaminate-01, Ġcontend-01, Ġcontend-02, Ġcontent-01, Ġcontent-02, Ġcontest-02, Ġcontinue-01, Ġcontract-02, Ġcontradict-01, Ġcontrary-01, Ġcontrast-01, Ġcontribute-01, Ġcontrol-01, Ġconvene-01, Ġconverse-01, Ġconvert-01, Ġconvey-01, Ġconvict-01, Ġconviction-02, Ġconvince-01, Ġcook-01, Ġcool-01, Ġcool-04, Ġcooperate-01, Ġcoordinate-01, Ġcope-01, Ġcopy-01, Ġcopyright-01, Ġcorrect-01, Ġcorrect-02, Ġcorrelate-01, Ġcorrespond-02, Ġcorroborate-01, Ġcorrupt-01, Ġcost-01, Ġcounsel-01, Ġcount-01, Ġcount-02, Ġcount-03, Ġcount-04, Ġcounter-01, Ġcounterfeit-01, Ġcouple-01, Ġcourse-91, Ġcover-01, Ġcover-02, Ġcover-03, Ġcover-up-04, Ġcoverage-06, Ġcrack-02, Ġcrack-down-06, Ġcraft-01, Ġcrap-01, Ġcrash-01, Ġcrazy-03, Ġcreate-01, Ġcredit-01, Ġcredit-02, Ġcreepy-04, Ġcrime-02, Ġcriminal-03, Ġcripple-01, Ġcritical-02, Ġcriticism-04, Ġcriticize-01, Ġcross-02, Ġcrowd-01, Ġcruise-01, Ġcrumble-01, Ġcrush-01, Ġcry-01, Ġcry-02, Ġcultivate-01, Ġcurb-01, Ġcure-01, Ġcurious-01, Ġcurious-02, Ġcurse-02, Ġcurtail-01, Ġcut-01, Ġcut-02, Ġcut-03, Ġcut-back-05, Ġcut-down-11, Ġcut-off-04, Ġcut-out-06, Ġcycle-02, Ġdamage-01, Ġdamn-01, Ġdance-01, Ġdare-01, Ġdark-02, Ġdate-01, Ġdate-02, Ġdate-entity, Ġdeal-01, Ġdeal-02, Ġdeal-03, Ġdebate-01, Ġdecapitate-01, Ġdecay-01, Ġdeceive-01, Ġdecide-01, Ġdeclare-01, Ġdeclare-02, Ġdeclassify-01, Ġdecline-01, Ġdecline-02, Ġdecommission-01, Ġdecrease-01, Ġdecry-01, Ġdedicate-01, Ġdeduct-01, Ġdeem-01, Ġdeep-02, Ġdeep-03, Ġdefame-01, Ġdefeat-01, Ġdefend-01, Ġdefine-01, Ġdeflect-01, Ġdefraud-01, Ġdefuse-01, Ġdefy-01, Ġdelay-01, Ġdelegate-01, Ġdeliberate-01, Ġdelight-01, Ġdeliver-01, Ġdelude-01, Ġdemagogue-01, Ġdemand-01, Ġdemolish-01, Ġdemonize-01, Ġdemonstrate-01, Ġdenounce-01, Ġdent-01, Ġdeny-01, Ġdepart-01, Ġdepend-01, Ġdeploy-01, Ġdeport-01, Ġdeposit-01, Ġdepress-01, Ġdeprive-01, Ġderegulate-01, Ġderive-01, Ġdescend-01, Ġdescribe-01, Ġdeserve-01, Ġdesign-01, Ġdesignate-01, Ġdesirable-02, Ġdesire-01, Ġdespair-01, Ġdesperate-02, Ġdespise-01, Ġdestabilize-01, Ġdestroy-01, Ġdetail-01, Ġdetain-01, Ġdetect-01, Ġdeter-01, Ġdeteriorate-01, Ġdetermine-01, Ġdetermined-02, Ġdetonate-01, Ġdevalue-01, Ġdevastate-01, Ġdevelop-01, Ġdevelop-02, Ġdeviate-01, Ġdevote-01, Ġdiagnose-01, Ġdialogue-01, Ġdictate-01, Ġdie-01, Ġdiffer-01, Ġdiffer-02, Ġdifferentiate-01, Ġdig-01, Ġdiminish-01, Ġdine-01, Ġdip-01, Ġdirect-01, Ġdirect-02, Ġdirty-02, Ġdisable-01, Ġdisagree-01, Ġdisappear-01, Ġdisappoint-01, Ġdisapprove-01, Ġdisarm-01, Ġdisclose-01, Ġdiscount-02, Ġdiscover-01, Ġdiscredit-01, Ġdiscriminate-01, Ġdiscriminate-02, Ġdiscuss-01, Ġdisgrace-01, Ġdisgruntle-01, Ġdisguise-01, Ġdisgust-01, Ġdisintegrate-01, Ġdislike-01, Ġdismantle-01, Ġdismiss-01, Ġdismiss-02, Ġdispatch-01, Ġdisperse-01, Ġdisplace-01, Ġdisplay-01, Ġdispose-01, Ġdisprove-01, Ġdispute-01, Ġdisregard-01, Ġdisrespect-01, Ġdisrupt-01, Ġdissent-01, Ġdissolve-01, Ġdistance-01, Ġdistant-02, Ġdistinguish-01, Ġdistort-01, Ġdistract-01, Ġdistress-01, Ġdistribute-01, Ġdisturb-01, Ġdive-01, Ġdiversify-01, Ġdivert-01, Ġdivide-02, Ġdivorce-01, Ġdo-02, Ġdock-01, Ġdocument-01, Ġdodge-01, Ġdominate-01, Ġdonate-01, Ġdose-01, Ġdouble-01, Ġdoubt-01, Ġdown-01, Ġdown-03, Ġdowngrade-02, Ġdraft-01, Ġdraft-02, Ġdrag-01, Ġdrain-01, Ġdraw-01, Ġdraw-02, Ġdraw-up-03, Ġdream-01, Ġdress-01, Ġdrill-01, Ġdrink-01, Ġdrive-01, Ġdrive-02, Ġdrive-04, Ġdrop-01, Ġdrop-05, Ġdrop-out-04, Ġdrug-01, Ġdry-02, Ġdry-08, Ġdubious-02, Ġdump-01, Ġdye-01, Ġearn-01, Ġearnest-01, Ġeasy-05, Ġeat-01, Ġedit-01, Ġeducate-01, Ġeffect-03, Ġeffective-04, Ġefficient-01, Ġeffort-01, Ġelaborate-01, Ġelect-01, Ġelevate-01, Ġeliminate-01, Ġemail-01, Ġembargo-01, Ġembarrass-01, Ġembezzle-01, Ġembrace-01, Ġemerge-01, Ġemerge-02, Ġemit-01, Ġemphasize-01, Ġemploy-01, Ġemploy-02, Ġempower-01, Ġempty-02, Ġenable-01, Ġenact-01, Ġencounter-01, Ġencourage-01, Ġencourage-02, Ġencrypt-01, Ġend-01, Ġend-up-03, Ġendanger-01, Ġendorse-01, Ġendure-01, Ġenforce-01, Ġengage-01, Ġengineer-01, Ġenhance-01, Ġenjoy-01, Ġenlighten-01, Ġenlist-01, Ġenrich-01, Ġenslave-01, Ġensure-01, Ġenter-01, Ġenter-02, Ġentertain-01, Ġenthusiastic-03, Ġentitle-01, Ġentrench-01, Ġenvision-01, Ġenvy-01, Ġequal-01, Ġequate-01, Ġequip-01, Ġeradicate-01, Ġerode-01, Ġerr-01, Ġerupt-01, Ġescalate-01, Ġescape-01, Ġestablish-01, Ġestimate-01, Ġet-cetera, Ġevacuate-01, Ġevade-01, Ġevaluate-01, Ġevidence-01, Ġevolve-01, Ġevolve-02, Ġexacerbate-01, Ġexaggerate-01, Ġexamine-01, Ġexceed-01, Ġexcellent-02, Ġexcept-01, Ġexcessive-02, Ġexchange-01, Ġexcite-01, Ġexclude-01, Ġexclusive-02, Ġexcuse-01, Ġexcuse-02, Ġexecute-01, Ġexecute-02, Ġexemplify-01, Ġexempt-01, Ġexercise-01, Ġexercise-02, Ġexert-01, Ġexhibit-01, Ġexile-01, Ġexist-01, Ġexit-01, Ġexpand-01, Ġexpect-01, Ġexpel-01, Ġexpend-01, Ġexperience-01, Ġexperiment-01, Ġexpert-01, Ġexpire-01, Ġexplain-01, Ġexplicit-03, Ġexplode-01, Ġexploit-01, Ġexplore-01, Ġexport-01, Ġexpose-01, Ġexpress-01, Ġextend-01, Ġextend-02, Ġextensive-03, Ġexterminate-01, Ġextort-01, Ġextract-01, Ġextradite-01, Ġeye-01, Ġfabricate-01, Ġface-01, Ġfacilitate-01, Ġfail-01, Ġfair-01, Ġfair-04, Ġfake-02, Ġfall-01, Ġfall-05, Ġfall-07, Ġfall-10, Ġfall-apart-09, Ġfame-01, Ġfamiliarize-01, Ġfarm-01, Ġfascinate-01, Ġfast-02, Ġfat-03, Ġfathom-01, Ġfault-01, Ġfavor-01, Ġfear-01, Ġfeature-01, Ġfeed-01, Ġfeed-up-03, Ġfeel-01, Ġfeel-02, Ġfeel-05, Ġfeel-06, Ġfight-01, Ġfigure-01, Ġfigure-04, Ġfigure-out-05, Ġfile-01, Ġfill-01, Ġfilm-01, Ġfilter-02, Ġfinalize-01, Ġfinance-01, Ġfind-01, Ġfind-02, Ġfind-out-03, Ġfine-01, Ġfine-03, Ġfine-04, Ġfinish-01, Ġfinish-07, Ġfire-01, Ġfire-02, Ġfire-03, Ġfire-04, Ġfirm-03, Ġfit-05, Ġfit-06, Ġfit-in-02, Ġfix-02, Ġfix-03, Ġflame-01, Ġflat-06, Ġflaw-01, Ġfledge-01, Ġflee-05, Ġflip-01, Ġfloat-01, Ġflood-01, Ġflow-01, Ġfluctuate-01, Ġfly-01, Ġfocus-01, Ġfollow-01, Ġfollow-02, Ġfollow-04, Ġfollow-up-03, Ġfool-01, Ġforbid-01, Ġforce-01, Ġforce-02, Ġforce-04, Ġforecast-01, Ġforeclose-01, Ġforesee-01, Ġforge-02, Ġforget-01, Ġforgive-01, Ġform-01, Ġform-02, Ġformulate-01, Ġfortunate-01, Ġforward-01, Ġfoster-01, Ġfound-01, Ġfree-01, Ġfree-03, Ġfree-04, Ġfreeze-01, Ġfreeze-02, Ġfrequent-02, Ġfresh-04, Ġfriendly-01, Ġfrighten-01, Ġfrustrate-01, Ġfuck-01, Ġfuck-up-02, Ġfuel-01, Ġfulfill-01, Ġfull-09, Ġfun-01, Ġfunction-01, Ġfund-01, Ġfurther-01, Ġfuss-01, Ġgain-01, Ġgain-02, Ġgarner-01, Ġgas-03, Ġgather-01, Ġgather-03, Ġgauge-01, Ġgeneral-02, Ġgenerate-01, Ġgenerous-01, Ġgenocide-01, Ġget-01, Ġget-02, Ġget-03, Ġget-04, Ġget-05, Ġget-06, Ġget-22, Ġget-30, Ġget-along-18, Ġget-away-08, Ġget-back-10, Ġget-by-17, Ġget-off-23, Ġget-on-21, Ġget-through-12, Ġget-through-13, Ġgift-01, Ġgive-01, Ġgive-16, Ġgive-away-02, Ġgive-back-03, Ġgive-in-09, Ġgive-up-07, Ġgive-up-08, Ġglad-02, Ġglance-01, Ġgo-01, Ġgo-02, Ġgo-03, Ġgo-05, Ġgo-06, Ġgo-08, Ġgo-09, Ġgo-10, Ġgo-12, Ġgo-21, Ġgo-22, Ġgo-23, Ġgo-back-19, Ġgo-down-27, Ġgo-off-16, Ġgo-on-15, Ġgo-on-25, Ġgo-out-17, Ġgo-through-20, Ġgolf-01, Ġgood-02, Ġgood-03, Ġgood-04, Ġgoogle-01, Ġgovern-01, Ġgovernment-organization, Ġgrab-01, Ġgrade-01, Ġgraduate-01, Ġgrant-01, Ġgrasp-01, Ġgray-02, Ġgreen-02, Ġgreen-03, Ġgreet-01, Ġgrieve-01, Ġgrind-01, Ġgrip-01, Ġgripe-01, Ġgross-03, Ġgross-06, Ġground-02, Ġgroup-01, Ġgrow-01, Ġgrow-02, Ġgrow-03, Ġgrow-up-04, Ġguarantee-01, Ġguard-01, Ġguess-01, Ġguide-01, Ġguilty-01, Ġgut-01, Ġhack-04, Ġhallucinate-01, Ġhalt-01, Ġhand-01, Ġhand-out-03, Ġhand-over-02, Ġhandle-01, Ġhang-01, Ġhang-out-06, Ġhanging-07, Ġhappen-02, Ġhappy-01, Ġhappy-02, Ġharass-01, Ġhard-02, Ġhard-04, Ġharm-01, Ġharmful-02, Ġharsh-02, Ġharvest-01, Ġhate-01, Ġhaul-01, Ġhave-03, Ġhave-04, Ġhave-05, Ġhave-06, Ġhave-11, Ġhave-concession-91, Ġhave-condition-91, Ġhave-extent-91, Ġhave-frequency-91, Ġhave-instrument-91, Ġhave-li-91, Ġhave-manner-91, Ġhave-mod-91, Ġhave-name-91, Ġhave-org-role-91, Ġhave-part-91, Ġhave-polarity-91, Ġhave-purpose-91, Ġhave-quant-91, Ġhave-rel-role-91, Ġhave-subevent-91, Ġhave-to-do-with-04, Ġhead-01, Ġhead-02, Ġheadline-01, Ġheadquarter-01, Ġheal-01, Ġhear-01, Ġhearing-02, Ġheat-01, Ġheed-01, Ġhelp-01, Ġhelp-02, Ġhelp-out-03, Ġhelpful-04, Ġhesitate-01, Ġhide-01, Ġhigh-02, Ġhighlight-01, Ġhijack-01, Ġhike-02, Ġhinder-01, Ġhint-01, Ġhire-01, Ġhit-01, Ġhit-02, Ġhoard-01, Ġhoax-01, Ġhold-01, Ġhold-02, Ġhold-03, Ġhold-04, Ġhold-back-07, Ġholiday-01, Ġhonest-01, Ġhonor-01, Ġhonorable-03, Ġhook-up-02, Ġhope-01, Ġhopeful-02, Ġhopeful-03, Ġhospitalize-01, Ġhost-01, Ġhot-04, Ġhot-05, Ġhouse-01, Ġhumble-01, Ġhumiliate-01, Ġhunger-01, Ġhunt-01, Ġhurt-01, Ġhurt-02, Ġhype-01, Ġhyperlink-01, Ġhyperlink-91, Ġhypothesize-01, Ġidentical-01, Ġidentify-01, Ġignorant-02, Ġignore-01, Ġill-01, Ġill-02, Ġimagine-01, Ġimitate-01, Ġimmigrate-01, Ġimmune-02, Ġimpact-01, Ġimpair-01, Ġimpeach-01, Ġimpede-01, Ġimplement-01, Ġimplicate-01, Ġimply-01, Ġimport-01, Ġimpose-01, Ġimpoverish-01, Ġimpregnate-01, Ġimpress-01, Ġimpression-03, Ġimprison-01, Ġimprove-01, Ġimprovise-01, Ġincarcerate-01, Ġincentivize-01, Ġincinerate-01, Ġincite-01, Ġincline-01, Ġinclude-01, Ġinclude-91, Ġincorporate-02, Ġincrease-01, Ġincur-01, Ġindicate-01, Ġindict-01, Ġindoctrinate-01, Ġindulge-01, Ġindustrialize-01, Ġinfect-01, Ġinfer-01, Ġinferior-01, Ġinfiltrate-01, Ġinflate-01, Ġinflict-01, Ġinfluence-01, Ġinform-01, Ġinfringe-01, Ġinhabit-01, Ġinherit-01, Ġinhibit-01, Ġinitiate-01, Ġinject-01, Ġinjure-01, Ġinnocent-01, Ġinnovate-01, Ġinquire-01, Ġinsist-01, Ġinspect-01, Ġinspire-01, Ġinstall-01, Ġinstead-of-91, Ġinstitute-01, Ġinstruct-01, Ġinsult-01, Ġinsure-01, Ġinsure-02, Ġintegrate-01, Ġintelligent-01, Ġintend-01, Ġintense-02, Ġintensify-01, Ġintent-02, Ġinteract-01, Ġintercept-01, Ġinterdict-01, Ġinterest-01, Ġinterfere-01, Ġinternal-02, Ġinterpret-01, Ġinterrogate-01, Ġinterrupt-01, Ġintersect-01, Ġintervene-01, Ġinterview-01, Ġintimate-02, Ġintimidate-01, Ġintroduce-01, Ġintroduce-02, Ġinvade-01, Ġinvent-01, Ġinvest-01, Ġinvestigate-01, Ġinvite-01, Ġinvoke-01, Ġinvolve-01, Ġirritate-01, Ġisolate-01, Ġissue-01, Ġissue-02, Ġjail-01, Ġjealous-02, Ġjeopardize-01, Ġjoin-01, Ġjoin-04, Ġjoin-in-05, Ġjoin-up-02, Ġjoke-01, Ġjudge-01, Ġjump-01, Ġjump-03, Ġjust-02, Ġjustify-01, Ġkeep-01, Ġkeep-02, Ġkeep-03, Ġkeep-04, Ġkeep-up-05, Ġkeep-up-10, Ġkey-02, Ġkick-01, Ġkid-01, Ġkidnap-01, Ġkill-01, Ġkill-03, Ġkind-01, Ġkiss-01, Ġkneel-01, Ġknock-01, Ġknow-01, Ġknow-02, Ġknow-03, Ġknow-04, Ġknow-06, Ġlabel-01, Ġlabor-01, Ġlack-01, Ġlag-01, Ġland-01, Ġlast-01, Ġlast-04, Ġlaugh-01, Ġlaughable-03, Ġlaunch-01, Ġlaunder-01, Ġlay-01, Ġlay-04, Ġlay-off-02, Ġlay-off-06, Ġlead-01, Ġlead-02, Ġlead-03, Ġleak-01, Ġlean-01, Ġlearn-01, Ġlease-01, Ġleave-02, Ġleave-10, Ġleave-11, Ġleave-12, Ġleave-13, Ġleave-14, Ġleave-15, Ġleave-17, Ġleave-out-03, Ġlecture-01, Ġleft-19, Ġleft-20, Ġlegal-02, Ġlegalize-01, Ġlegislate-01, Ġlegitimate-02, Ġlend-01, Ġlessen-01, Ġlet-01, Ġlet-down-04, Ġlevel-04, Ġlevy-01, Ġliable-01, Ġlibel-01, Ġliberal-02, Ġliberate-01, Ġlicense-01, Ġlie-07, Ġlie-08, Ġlift-01, Ġlift-02, Ġlight-04, Ġlight-06, Ġlike-01, Ġlike-02, Ġlikely-01, Ġlimit-01, Ġline-01, Ġline-up-02, Ġlink-01, Ġlist-01, Ġlisten-01, Ġlive-01, Ġlive-02, Ġlive-up-04, Ġload-01, Ġloan-01, Ġloathe-01, Ġlobby-01, Ġlocal-02, Ġlocate-01, Ġlock-01, Ġlock-up-03, Ġlong-03, Ġlook-01, Ġlook-02, Ġlook-04, Ġlook-forward-03, Ġlook-up-05, Ġloose-04, Ġloot-01, Ġlose-01, Ġlose-02, Ġlose-03, Ġlose-out-06, Ġlove-01, Ġlove-02, Ġlow-04, Ġlower-05, Ġloyal-01, Ġlump-01, Ġlunch-01, Ġlure-01, Ġlurk-01, Ġlust-01, Ġlynch-01, Ġmad-02, Ġmad-04, Ġmainstream-02, Ġmaintain-01, Ġmajor-02, Ġmake-01, Ġmake-02, Ġmake-05, Ġmake-18, Ġmake-it-14, Ġmake-out-23, Ġmake-up-07, Ġmake-up-08, Ġmake-up-10, Ġman-01, Ġmanage-01, Ġmanage-02, Ġmandate-01, Ġmanifest-01, Ġmanipulate-01, Ġmanipulate-02, Ġmanufacture-01, Ġmarch-01, Ġmark-01, Ġmark-02, Ġmarket-01, Ġmarry-01, Ġmask-01, Ġmass-quantity, Ġmassacre-01, Ġmatch-01, Ġmatch-03, Ġmatter-01, Ġmature-02, Ġmaximize-01, Ġmean-01, Ġmean-02, Ġmean-04, Ġmeaningful-05, Ġmeasure-01, Ġmeasure-02, Ġmeddle-01, Ġmediate-01, Ġmeet-01, Ġmeet-02, Ġmeet-03, Ġmeet-up-04, Ġmelt-01, Ġmenace-01, Ġmention-01, Ġmerchandise-01, Ġmerge-01, Ġmerit-01, Ġmess-up-02, Ġmessage-01, Ġmigrate-01, Ġmind-01, Ġmind-04, Ġmind-05, Ġmine-01, Ġminimal-02, Ġminimize-01, Ġminor-01, Ġminor-02, Ġmislead-01, Ġmisrepresent-01, Ġmiss-01, Ġmiss-02, Ġmistake-01, Ġmistake-02, Ġmisunderstand-01, Ġmitigate-01, Ġmix-01, Ġmoan-01, Ġmobile-02, Ġmobilize-01, Ġmock-01, Ġmodel-01, Ġmoderate-01, Ġmoderate-02, Ġmoderate-03, Ġmodern-02, Ġmodernize-01, Ġmodify-01, Ġmoisturize-01, Ġmolest-01, Ġmonetary-quantity, Ġmoney-01, Ġmonitor-01, Ġmoral-02, Ġmortgage-01, Ġmotivate-01, Ġmotivate-02, Ġmove-01, Ġmove-02, Ġmove-03, Ġmovement-07, Ġmulti-sentence, Ġmultiply-01, Ġmurder-01, Ġname-01, Ġname-02, Ġnarrow-01, Ġnarrow-02, Ġnationalize-01, Ġnatural-02, Ġnatural-03, Ġnear-02, Ġneed-01, Ġnegative-02, Ġnegative-03, Ġnegative-05, Ġneglect-01, Ġnegotiate-01, Ġneighbor-01, Ġnetwork-01, Ġneutral-02, Ġneutralize-01, Ġnew-01, Ġnew-02, Ġnice-01, Ġnominate-01, Ġnormal-02, Ġnormalize-01, Ġnotable-04, Ġnote-01, Ġnote-02, Ġnotice-01, Ġnotice-03, Ġnotify-01, Ġnuke-01, Ġnumber-01, Ġobey-01, Ġobject-01, Ġobligate-01, Ġoblige-02, Ġobserve-01, Ġobsess-01, Ġobstruct-01, Ġobtain-01, Ġobvious-01, Ġoccasion-02, Ġoccupy-01, Ġoffend-01, Ġoffend-03, Ġoffense-02, Ġoffer-01, Ġoffset-01, Ġokay-01, Ġokay-04, Ġopen-01, Ġopen-02, Ġopen-04, Ġopen-05, Ġopen-07, Ġopen-09, Ġopen-up-03, Ġoperate-01, Ġoperate-02, Ġopine-01, Ġoppose-01, Ġopposite-01, Ġoppress-01, Ġopt-01, Ġorbit-01, Ġordain-01, Ġorder-01, Ġorder-02, Ġorder-03, Ġordinal-entity, Ġorganize-01, Ġorient-01, Ġoriginate-01, Ġoust-01, Ġout-01, Ġout-03, Ġout-05, Ġout-06, Ġoutlaw-01, Ġoutrage-01, Ġoutrageous-02, Ġoutweigh-01, Ġovercome-01, Ġoverlook-01, Ġoverpay-01, Ġoverride-01, Ġoversee-01, Ġoverthrow-01, Ġoverturn-01, Ġoverwhelm-01, Ġowe-01, Ġown-01, Ġpace-01, Ġpack-01, Ġpackage-01, Ġpaddle-01, Ġpain-01, Ġpaint-02, Ġpaint-03, Ġpale-03, Ġpanic-01, Ġparade-02, Ġparalyze-01, Ġpardon-01, Ġpark-01, Ġparole-01, Ġparrot-01, Ġpart-01, Ġpartake-01, Ġparticipate-01, Ġpartition-01, Ġpartner-01, Ġparty-01, Ġpass-01, Ġpass-02, Ġpass-03, Ġpass-04, Ġpass-05, Ġpass-07, Ġpass-20, Ġpass-away-16, Ġpass-by-17, Ġpass-on-09, Ġpass-on-14, Ġpaste-01, Ġpatent-01, Ġpatient-01, Ġpatrol-01, Ġpattern-01, Ġpay-01, Ġpay-off-02, Ġpeak-01, Ġpenalize-01, Ġpend-01, Ġpenetrate-01, Ġperceive-01, Ġpercentage-entity, Ġperfect-02, Ġperform-01, Ġperform-02, Ġperjure-01, Ġpermit-01, Ġperpetrate-01, Ġpersecute-01, Ġpersist-01, Ġpersonal-02, Ġpersuade-01, Ġpertain-01, Ġpervert-01, Ġpetition-01, Ġphilander-01, Ġphone-01, Ġphotograph-01, Ġpick-01, Ġpick-up-04, Ġpicture-01, Ġpierce-01, Ġpile-01, Ġpilot-01, Ġpin-01, Ġpiss-01, Ġpiss-03, Ġpiss-off-02, Ġpity-01, Ġplace-01, Ġplague-01, Ġplan-01, Ġplant-01, Ġplay-01, Ġplay-02, Ġplay-08, Ġplay-10, Ġplay-11, Ġplead-02, Ġplease-01, Ġpledge-01, Ġplot-01, Ġpoint-01, Ġpoint-out-02, Ġpolice-01, Ġpolicy-01, Ġpolite-01, Ġpolitical-movement, Ġpolitical-party, Ġpoll-01, Ġpollute-01, Ġpool-01, Ġpopular-02, Ġpopulate-01, Ġportion-01, Ġportray-01, Ġpose-01, Ġpose-02, Ġposition-01, Ġposition-02, Ġpossess-01, Ġpossible-01, Ġpost-01, Ġpostpone-01, Ġpour-01, Ġpower-01, Ġpowerful-02, Ġpractice-01, Ġpraise-01, Ġpray-01, Ġpreach-01, Ġprecede-01, Ġpredict-01, Ġpreexist-01, Ġprefer-01, Ġprejudice-01, Ġpremise-01, Ġprepare-01, Ġprepare-02, Ġprescribe-02, Ġpresent-01, Ġpresent-02, Ġpreserve-01, Ġpreside-01, Ġpress-01, Ġpressure-01, Ġpresume-01, Ġpretend-01, Ġprevail-01, Ġprevail-02, Ġprevent-01, Ġprey-01, Ġprice-01, Ġpride-01, Ġprint-01, Ġprioritize-01, Ġprivate-02, Ġprivate-03, Ġprivatize-01, Ġprivilege-01, Ġprize-01, Ġprobe-01, Ġproceed-01, Ġproceeding-02, Ġprocess-01, Ġprocess-02, Ġproclaim-01, Ġprocure-01, Ġproduce-01, Ġproduce-02, Ġproductive-03, Ġprofess-01, Ġprofit-01, Ġprogram-01, Ġprogress-01, Ġprogressive-02, Ġprohibit-01, Ġproject-01, Ġproject-02, Ġproliferate-01, Ġprolong-01, Ġpromise-01, Ġpromote-01, Ġpromote-02, Ġprompt-01, Ġprop-up-01, Ġpropagate-01, Ġpropel-01, Ġpropose-01, Ġprosecute-01, Ġprospect-02, Ġprosper-01, Ġprostitute-01, Ġprotect-01, Ġprotest-01, Ġprove-01, Ġprovide-01, Ġprovoke-01, Ġpublic-02, Ġpublication-91, Ġpublicize-01, Ġpublish-01, Ġpull-01, Ġpull-06, Ġpull-09, Ġpull-out-02, Ġpump-01, Ġpunch-01, Ġpunish-01, Ġpunishable-02, Ġpurchase-01, Ġpure-02, Ġpurport-01, Ġpursue-01, Ġpush-01, Ġpush-02, Ġpush-04, Ġput-01, Ġput-02, Ġput-03, Ġput-in-05, Ġput-on-08, Ġput-out-10, Ġput-up-11, Ġpuzzle-01, Ġqualify-01, Ġqualify-02, Ġquest-01, Ġquestion-01, Ġquestion-03, Ġquick-02, Ġquiet-04, Ġquit-01, Ġquote-01, Ġrace-02, Ġracket-02, Ġradiate-01, Ġrage-02, Ġraid-01, Ġrail-01, Ġrain-01, Ġraise-01, Ġraise-02, Ġraise-03, Ġrally-01, Ġramble-02, Ġrange-01, Ġrank-01, Ġrant-01, Ġrape-01, Ġrare-02, Ġrate-01, Ġrate-entity-91, Ġratify-01, Ġration-01, Ġreach-01, Ġreach-02, Ġreach-03, Ġreact-01, Ġreactionary-02, Ġreactivate-01, Ġread-01, Ġready-01, Ġready-02, Ġreal-04, Ġrealistic-03, Ġrealize-01, Ġrealize-02, Ġreason-01, Ġreasonable-02, Ġrebel-01, Ġrebuild-01, Ġrecall-02, Ġreceive-01, Ġrecession-02, Ġreckon-01, Ġreclaim-01, Ġrecognize-01, Ġrecognize-02, Ġrecommend-01, Ġreconcile-01, Ġreconstruct-01, Ġrecord-01, Ġrecover-01, Ġrecover-02, Ġrecreation-02, Ġrecruit-01, Ġrectify-01, Ġred-02, Ġredeem-01, Ġredistribute-01, Ġreduce-01, Ġreelect-01, Ġreenter-01, Ġrefer-01, Ġrefer-02, Ġrefer-03, Ġreference-04, Ġrefine-01, Ġreflect-01, Ġreform-01, Ġrefrain-01, Ġrefuse-01, Ġrefute-01, Ġregain-01, Ġregard-01, Ġregardless-91, Ġregister-02, Ġregret-01, Ġregular-02, Ġregular-03, Ġregulate-01, Ġrehabilitate-01, Ġreinforce-01, Ġreiterate-01, Ġreject-01, Ġrelate-01, Ġrelated-04, Ġrelation-03, Ġrelative-05, Ġrelax-01, Ġrelease-01, Ġrelevant-01, Ġrelieve-01, Ġreligious-group, Ġrelinquish-01, Ġrelocate-01, Ġrely-01, Ġremain-01, Ġremand-01, Ġremark-01, Ġremarkable-02, Ġremember-01, Ġremind-01, Ġremit-01, Ġremortgage-01, Ġremove-01, Ġrender-01, Ġrender-02, Ġrenew-01, Ġrenounce-01, Ġrenovate-01, Ġrent-01, Ġreopen-01, Ġreorganize-01, Ġrepair-01, Ġrepay-01, Ġrepeal-01, Ġrepeat-01, Ġreplace-01, Ġreplicate-01, Ġreply-01, Ġreport-01, Ġrepresent-01, Ġrepress-01, Ġreprocess-01, Ġreproduce-01, Ġrepute-01, Ġrequest-01, Ġrequest-confirmation-91, Ġrequire-01, Ġrescue-01, Ġresearch-01, Ġresemble-01, Ġresent-01, Ġreserve-01, Ġreside-01, Ġresign-01, Ġresist-01, Ġresolve-01, Ġresolve-02, Ġresort-01, Ġrespect-01, Ġrespond-01, Ġresponsible-01, Ġresponsible-02, Ġresponsible-03, Ġrest-01, Ġrestart-01, Ġrestore-01, Ġrestore-02, Ġrestrain-01, Ġrestrict-01, Ġrestructure-01, Ġresult-01, Ġresume-01, Ġretail-01, Ġretain-01, Ġretaliate-01, Ġretard-01, Ġretire-01, Ġreturn-01, Ġreturn-02, Ġreturn-05, Ġreunify-01, Ġreveal-01, Ġrevere-01, Ġreverse-01, Ġreview-01, Ġrevise-01, Ġrevisit-01, Ġrevive-01, Ġrevolution-03, Ġrevolutionary-04, Ġreward-01, Ġrewrite-01, Ġrid-01, Ġride-01, Ġridicule-01, Ġridiculous-02, Ġright-02, Ġright-03, Ġright-04, Ġright-05, Ġright-06, Ġright-08, Ġriot-01, Ġrip-01, Ġrise-01, Ġrisk-01, Ġrival-01, Ġrob-01, Ġroll-01, Ġroot-02, Ġrot-01, Ġrotate-01, Ġrotate-02, Ġround-05, Ġruin-01, Ġrule-01, Ġrule-03, Ġrule-out-02, Ġrumor-01, Ġrun-01, Ġrun-02, Ġrun-04, Ġrun-07, Ġrun-08, Ġrun-09, Ġrun-10, Ġrun-13, Ġrun-off-24, Ġrun-out-05, Ġrun-up-19, Ġrush-01, Ġrust-01, Ġsacrifice-01, Ġsad-02, Ġsafe-01, Ġsafeguard-01, Ġsail-01, Ġsame-01, Ġsample-01, Ġsanction-02, Ġsatisfy-01, Ġsave-01, Ġsave-02, Ġsave-03, Ġsay-01, Ġscam-01, Ġscare-01, Ġscary-03, Ġschedule-01, Ġscheme-01, Ġschool-01, Ġscore-01, Ġscore-on-scale-91, Ġscrap-01, Ġscream-01, Ġscrew-02, Ġscrew-up-01, Ġscrutinize-01, Ġseal-01, Ġsearch-01, Ġseat-01, Ġsecure-01, Ġsecure-02, Ġsee-01, Ġsee-03, Ġsee-05, Ġseek-01, Ġseek-out-02, Ġseem-01, Ġsegregate-01, Ġseize-01, Ġselect-01, Ġsell-01, Ġsend-01, Ġsend-02, Ġsend-03, Ġsense-01, Ġsense-02, Ġsensitive-03, Ġsentence-01, Ġseparate-01, Ġseparate-02, Ġserious-01, Ġserious-02, Ġserve-01, Ġserve-02, Ġserve-04, Ġservice-05, Ġset-01, Ġset-02, Ġset-08, Ġset-up-03, Ġsettle-01, Ġsettle-02, Ġsettle-03, Ġsex-01, Ġshake-01, Ġshame-01, Ġshame-02, Ġshape-01, Ġshare-01, Ġsharp-02, Ġshave-01, Ġshelter-01, Ġshield-01, Ġshift-01, Ġshine-01, Ġship-01, Ġshit-01, Ġshock-01, Ġshoot-01, Ġshoot-02, Ġshoot-down-05, Ġshop-01, Ġshort-06, Ġshort-07, Ġshout-01, Ġshove-01, Ġshow-01, Ġshow-04, Ġshow-up-02, Ġshower-01, Ġshred-01, Ġshut-01, Ġshut-down-05, Ġshut-up-06, Ġsick-02, Ġsick-04, Ġsick-05, Ġsicken-01, Ġside-01, Ġsigh-02, Ġsight-01, Ġsign-01, Ġsign-02, Ġsign-up-03, Ġsignal-07, Ġsignificant-02, Ġsignify-01, Ġsimple-02, Ġsimulate-01, Ġsin-01, Ġsing-01, Ġsingle-02, Ġsingle-03, Ġsink-01, Ġsit-01, Ġsit-down-02, Ġsituate-01, Ġsize-01, Ġskip-01, Ġslam-02, Ġslander-01, Ġslant-01, Ġslap-01, Ġslash-02, Ġslaughter-01, Ġslay-01, Ġsleep-01, Ġsleep-02, Ġslide-01, Ġslip-01, Ġslip-02, Ġslow-01, Ġslow-05, Ġslow-down-03, Ġsmart-06, Ġsmear-02, Ġsmell-01, Ġsmell-02, Ġsmile-01, Ġsmoke-01, Ġsmoke-02, Ġsmuggle-01, Ġsneak-01, Ġsneaky-03, Ġsnip-01, Ġsnow-01, Ġsoar-01, Ġsocial-03, Ġsoft-02, Ġsolid-02, Ġsolve-01, Ġsorry-01, Ġsort-out-02, Ġsound-01, Ġsound-02, Ġsource-01, Ġsource-02, Ġspace-01, Ġspan-01, Ġspank-01, Ġspare-01, Ġspark-01, Ġspeak-01, Ġspeak-out-03, Ġspecial-02, Ġspecialize-01, Ġspecific-02, Ġspecify-01, Ġspeculate-01, Ġspeed-01, Ġspell-01, Ġspend-01, Ġspend-02, Ġspend-04, Ġspew-01, Ġspin-01, Ġspin-03, Ġsplit-01, Ġspoil-01, Ġsponsor-01, Ġspot-01, Ġspout-01, Ġspread-01, Ġspread-02, Ġspread-03, Ġspread-out-04, Ġspur-01, Ġspy-01, Ġstab-01, Ġstabilize-01, Ġstable-03, Ġstaff-01, Ġstage-01, Ġstake-01, Ġstall-01, Ġstand-01, Ġstand-02, Ġstand-03, Ġstand-04, Ġstand-08, Ġstand-11, Ġstand-up-07, Ġstandard-02, Ġstandardize-01, Ġstart-01, Ġstart-out-05, Ġstarve-01, Ġstate-01, Ġstation-01, Ġstatistical-test-91, Ġstay-01, Ġstay-on-02, Ġsteady-01, Ġsteal-01, Ġstep-01, Ġstep-in-02, Ġstereotype-01, Ġstick-01, Ġstick-around-03, Ġstimulate-01, Ġstink-01, Ġstipulate-01, Ġstir-up-04, Ġstock-01, Ġstockpile-01, Ġstop-01, Ġstop-03, Ġstore-01, Ġstorm-01, Ġstorm-02, Ġstraight-04, Ġstraight-05, Ġstraight-06, Ġstreet-address-91, Ġstrengthen-01, Ġstress-01, Ġstress-02, Ġstretch-01, Ġstrike-01, Ġstrike-02, Ġstrike-04, Ġstrip-01, Ġstrive-01, Ġstrong-02, Ġstructure-01, Ġstruggle-01, Ġstruggle-02, Ġstudy-01, Ġstuff-01, Ġstun-01, Ġsubject-01, Ġsubmit-01, Ġsubscribe-01, Ġsubsidize-01, Ġsubstitute-01, Ġsucceed-01, Ġsucceed-03, Ġsuck-01, Ġsuck-03, Ġsuck-up-04, Ġsue-02, Ġsuffer-01, Ġsuffice-01, Ġsuggest-01, Ġsuit-01, Ġsuitable-04, Ġsummarize-01, Ġsuperior-01, Ġsupervise-01, Ġsupply-01, Ġsupport-01, Ġsuppose-01, Ġsuppose-02, Ġsuppress-01, Ġsure-02, Ġsurge-01, Ġsurgery-01, Ġsurpass-01, Ġsurprise-01, Ġsurrender-01, Ġsurround-01, Ġsurveil-01, Ġsurvey-01, Ġsurvive-01, Ġsurvive-02, Ġsuspect-01, Ġsuspend-01, Ġsustain-01, Ġswallow-01, Ġswear-01, Ġswear-02, Ġsweep-01, Ġsweep-06, Ġswell-01, Ġswim-01, Ġswitch-01, Ġsymbolize-01, Ġsympathize-01, Ġtable-01, Ġtackle-01, Ġtake-01, Ġtake-02, Ġtake-03, Ġtake-04, Ġtake-10, Ġtake-away-05, Ġtake-down-22, Ġtake-off-07, Ġtake-on-09, Ġtake-out-11, Ġtake-over-12, Ġtalk-01, Ġtally-01, Ġtape-02, Ġtarget-01, Ġtask-01, Ġtattoo-01, Ġtax-01, Ġteach-01, Ġtear-01, Ġtear-down-05, Ġtelephone-01, Ġtell-01, Ġtell-02, Ġtemporal-quantity, Ġtempt-01, Ġtend-02, Ġterm-01, Ġterminate-01, Ġterrible-01, Ġterrify-01, Ġterror-02, Ġtest-01, Ġtestify-01, Ġtext-01, Ġthank-01, Ġthankful-02, Ġthat-is-it-00, Ġthink-01, Ġthreaten-01, Ġthrill-01, Ġthrive-01, Ġthrow-01, Ġthrow-out-06, Ġticket-02, Ġtie-01, Ġtight-05, Ġtighten-01, Ġtimely-03, Ġtip-05, Ġtire-01, Ġtitle-01, Ġtolerate-01, Ġtop-01, Ġtop-02, Ġtopple-01, Ġtorture-01, Ġtoss-01, Ġtotal-01, Ġtouch-01, Ġtough-02, Ġtough-03, Ġtour-01, Ġtrace-02, Ġtrack-01, Ġtrack-down-02, Ġtrade-01, Ġtraffic-01, Ġtrain-01, Ġtrample-01, Ġtransact-01, Ġtransfer-01, Ġtransform-01, Ġtransgress-01, Ġtransit-01, Ġtransition-01, Ġtranslate-01, Ġtransmit-01, Ġtransplant-01, Ġtransport-01, Ġtrap-01, Ġtrash-01, Ġtravel-01, Ġtreat-01, Ġtreat-03, Ġtreat-04, Ġtrend-01, Ġtrick-01, Ġtricky-02, Ġtrigger-01, Ġtrip-03, Ġtriple-01, Ġtroll-01, Ġtrouble-01, Ġtrue-01, Ġtrue-02, Ġtrump-01, Ġtrust-01, Ġtrust-02, Ġtry-01, Ġtry-02, Ġtune-01, Ġturn-01, Ġturn-02, Ġturn-14, Ġturn-18, Ġturn-down-05, Ġturn-on-13, Ġturn-out-11, Ġturn-out-17, Ġturn-over-12, Ġturn-up-15, Ġtwist-01, Ġtype-01, Ġtype-03, Ġtypical-02, Ġuncover-01, Ġunderestimate-01, Ġundergo-28, Ġunderlie-01, Ġundermine-01, Ġunderstand-01, Ġundertake-01, Ġuniform-01, Ġunify-01, Ġunion-02, Ġunite-01, Ġup-01, Ġup-02, Ġup-03, Ġupdate-01, Ġupgrade-01, Ġupgrade-02, Ġuphold-01, Ġupset-01, Ġurge-01, Ġurl-entity, Ġuse-01, Ġuse-02, Ġuseful-05, Ġusher-in-01, Ġutilize-01, Ġvacation-01, Ġvalid-02, Ġvalidate-01, Ġvalue-01, Ġvalue-02, Ġvary-01, Ġvent-01, Ġverify-01, Ġveto-01, Ġvictimize-01, Ġview-01, Ġview-02, Ġvile-02, Ġviolate-01, Ġvisit-01, Ġvoice-01, Ġvoluntary-02, Ġvolunteer-01, Ġvote-01, Ġvote-02, Ġvow-01, Ġwage-01, Ġwait-01, Ġwaive-01, Ġwake-up-02, Ġwalk-01, Ġwander-01, Ġwant-01, Ġwar-01, Ġwarm-01, Ġwarm-06, Ġwarm-07, Ġwarn-01, Ġwarrant-01, Ġwash-01, Ġwaste-01, Ġwatch-01, Ġwave-01, Ġwave-04, Ġweak-02, Ġweaken-01, Ġweaponize-01, Ġwear-01, Ġwed-01, Ġweigh-01, Ġwelcome-01, Ġwell-09, Ġwesternize-01, Ġwhine-01, Ġwhip-up-03, Ġwhite-02, Ġwhite-03, Ġwhore-01, Ġwide-02, Ġwield-01, Ġwill-02, Ġwin-01, Ġwine-01, Ġwipe-01, Ġwipe-out-02, Ġwire-01, Ġwish-01, Ġwithdraw-01, Ġwithhold-01, Ġwitness-01, Ġwonder-01, Ġwonder-02, Ġwonderful-03, Ġword-01, Ġwork-01, Ġwork-06, Ġwork-07, Ġwork-09, Ġwork-12, Ġwork-out-02, Ġworld-region, Ġworry-01, Ġworry-02, Ġworsen-01, Ġworship-01, Ġworth-01, Ġworth-02, Ġwound-01, Ġwow-01, Ġwrap-01, Ġwreck-01, Ġwrite-01, Ġwrong-02, Ġwrong-04, Ġyell-01, Ġyield-01, Ġzap-01",,"#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # special_tokens = [""<AMR>"", ""</AMR>""], # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell","Ġ, ĠCAUSE_OF_DEATH, ĠCOUNTRY, ĠCRIMINAL_CHARGE, ĠDATE, ĠDATE_ATTRS, ĠDURATION, ĠENTITY, ĠHANDLE, ĠIDEOLOGY, ĠLOCATION, ĠMISC, ĠMONEY, ĠNATIONALITY, ĠNUMBER, ĠORDINAL, ĠORDINAL_ENTITY, ĠORGANIZATION, ĠQUANTITY, ĠRELIGION, ĠSCORE_ENTITY, ĠSTATE_OR_PROVINCE, ĠTITLE, Ġabandon-01, Ġabet-01, Ġabide-01, Ġabolish-01, Ġabominable-02, Ġabort-01, Ġabsent-01, Ġabsorb-01, Ġabstain-01, Ġabuse-01, Ġabuse-02, Ġaccelerate-01, Ġaccept-01, Ġaccess-01, Ġaccommodate-01, Ġaccompany-01, Ġaccomplish-01, Ġaccord-02, Ġaccord-03, Ġaccount-01, Ġaccountable-02, Ġaccumulate-01, Ġaccuse-01, Ġachieve-01, Ġacknowledge-01, Ġacquaint-01, Ġacquire-01, Ġacquit-01, Ġact-01, Ġact-02, Ġactivate-01, Ġactivity-06, Ġactual-02, Ġadapt-01, Ġadd-01, Ġadd-02, Ġadd-03, Ġaddict-01, Ġaddictive-02, Ġaddress-01, Ġaddress-02, Ġaddress-03, Ġadhere-02, Ġadjust-01, Ġadminister-01, Ġadministrate-01, Ġadmire-01, Ġadmit-01, Ġadmit-02, Ġadopt-01, Ġadvance-01, Ġadvanced-02, Ġadvertise-01, Ġadvise-01, Ġadvocate-01, Ġaffair-01, Ġaffair-02, Ġaffect-01, Ġaffiliate-01, Ġafford-01, Ġafford-02, Ġage-01, Ġaggravate-01, Ġagree-01, Ġaid-01, Ġaim-01, Ġaim-02, Ġair-01, Ġalarm-01, Ġalert-01, Ġalien-01, Ġalienate-01, Ġalign-01, Ġalike-05, Ġallege-01, Ġallocate-01, Ġallow-01, Ġallow-02, Ġally-01, Ġalter-01, Ġalternate-01, Ġamaze-01, Ġambush-01, Ġamend-01, Ġamount-01, Ġamplify-01, Ġamr-unknown, Ġamuse-01, Ġanalyze-01, Ġanger-01, Ġannounce-01, Ġannoy-01, Ġanswer-01, Ġanticipate-01, Ġapologize-01, Ġappall-01, Ġappeal-01, Ġappeal-02, Ġappeal-03, Ġappear-01, Ġappear-02, Ġappease-01, Ġapplaud-01, Ġapply-01, Ġapply-02, Ġappoint-01, Ġappreciate-02, Ġapprehend-01, Ġapprentice-01, Ġapproach-01, Ġapproach-02, Ġappropriate-02, Ġapprove-01, Ġarbitrary-02, Ġargue-01, Ġargue-02, Ġarise-02, Ġarm-01, Ġarmor-01, Ġarrange-01, Ġarrest-01, Ġarrive-01, Ġask-01, Ġask-02, Ġaspire-01, Ġassassinate-01, Ġassault-01, Ġassemble-01, Ġassemble-02, Ġassert-02, Ġassert-03, Ġassess-01, Ġassign-01, Ġassist-01, Ġassociate-01, Ġassume-01, Ġassume-02, Ġassure-01, Ġastonish-01, Ġat-least, Ġattach-01, Ġattack-01, Ġattain-01, Ġattempt-01, Ġattend-01, Ġattend-02, Ġattract-01, Ġattribute-01, Ġaudit-01, Ġauthor-01, Ġauthorize-01, Ġautomate-01, Ġavail-01, Ġavailable-02, Ġaverage-01, Ġaverage-03, Ġaverage-04, Ġavoid-01, Ġawait-01, Ġawake-03, Ġaward-01, Ġawe-01, Ġbabble-01, Ġback-01, Ġback-02, Ġback-up-04, Ġbad-02, Ġbad-04, Ġbad-05, Ġbad-07, Ġbaffle-01, Ġbail-out-02, Ġbait-01, Ġbake-01, Ġbalance-01, Ġban-01, Ġbank-01, Ġbankrupt-01, Ġbar-01, Ġbargain-01, Ġbarter-01, Ġbase-01, Ġbase-02, Ġbash-01, Ġbathe-01, Ġbattle-01, Ġbe-02, Ġbe-compared-to-91, Ġbe-destined-for-91, Ġbe-done-08, Ġbe-from-91, Ġbe-located-at-91, Ġbe-polite-91, Ġbe-temporally-at-91, Ġbear-01, Ġbear-02, Ġbear-06, Ġbeat-01, Ġbeat-03, Ġbeat-up-05, Ġbeautiful-02, Ġbecome-01, Ġbefriend-01, Ġbeg-01, Ġbegin-01, Ġbehave-01, Ġbehead-01, Ġbelieve-01, Ġbelong-01, Ġbend-01, Ġbenefit-01, Ġbestow-01, Ġbet-01, Ġbetray-01, Ġbetter-01, Ġbeware-01, Ġbias-01, Ġbicker-01, Ġbid-01, Ġbid-03, Ġbill-01, Ġbind-01, Ġbind-03, Ġbitch-01, Ġbite-01, Ġblack-04, Ġblack-05, Ġblack-07, Ġblackmail-01, Ġblame-01, Ġblast-01, Ġblast-05, Ġbleed-01, Ġbless-01, Ġblind-02, Ġblock-01, Ġblockade-01, Ġblog-01, Ġblood-02, Ġblow-01, Ġblow-03, Ġblow-14, Ġblow-up-06, Ġblunt-01, Ġblunt-02, Ġboard-01, Ġboast-01, Ġbomb-01, Ġboom-02, Ġboost-01, Ġborder-01, Ġbore-02, Ġborrow-01, Ġbother-01, Ġbother-02, Ġbow-01, Ġboycott-01, Ġbrag-01, Ġbrainwash-01, Ġbrand-01, Ġbrave-02, Ġbreach-01, Ġbreak-01, Ġbreak-02, Ġbreak-13, Ġbreak-18, Ġbreak-19, Ġbreak-down-12, Ġbreak-through-22, Ġbreak-through-26, Ġbreak-up-08, Ġbreathe-01, Ġbreed-01, Ġbribe-01, Ġbridge-01, Ġbrief-01, Ġbright-02, Ġbright-03, Ġbrilliant-01, Ġbring-01, Ġbring-about-05, Ġbring-down-03, Ġbring-on-06, Ġbring-up-02, Ġbring-up-08, Ġbroad-02, Ġbroadcast-01, Ġbroaden-01, Ġbroke-23, Ġbroker-01, Ġbrutal-02, Ġbudget-01, Ġbuild-01, Ġbuild-02, Ġbuild-up-05, Ġbullshit-01, Ġbully-01, Ġbundle-01, Ġburden-01, Ġburn-01, Ġburst-02, Ġbury-01, Ġbust-01, Ġbust-02, Ġbusy-01, Ġbuy-01, Ġbuy-05, Ġbuy-into-04, Ġbyline-91, Ġcalculate-01, Ġcall-01, Ġcall-02, Ġcall-03, Ġcall-13, Ġcall-on-05, Ġcalm-down-02, Ġcamp-02, Ġcampaign-01, Ġcancel-01, Ġcap-01, Ġcap-02, Ġcapable-01, Ġcapture-01, Ġcare-01, Ġcare-02, Ġcare-03, Ġcare-04, Ġcarry-01, Ġcarry-on-02, Ġcarry-out-03, Ġcase-03, Ġcase-04, Ġcast-01, Ġcast-03, Ġcatch-01, Ġcatch-02, Ġcatch-03, Ġcatch-up-04, Ġcater-01, Ġcause-01, Ġcautious-02, Ġcease-01, Ġcelebrate-01, Ġcelebrate-02, Ġcensor-01, Ġcenter-01, Ġcenter-02, Ġcertify-01, Ġchain-01, Ġchair-01, Ġchallenge-01, Ġchampion-01, Ġchance-01, Ġchance-02, Ġchange-01, Ġchange-02, Ġchannel-01, Ġcharacteristic-02, Ġcharacterize-01, Ġcharge-01, Ġcharge-05, Ġcharge-06, Ġcharge-08, Ġcharm-01, Ġchase-01, Ġchat-01, Ġcheap-02, Ġcheat-02, Ġcheat-03, Ġcheck-01, Ġcheck-03, Ġcheck-07, Ġcheck-out-05, Ġchoose-01, Ġcirculate-01, Ġcite-01, Ġcivilize-01, Ġclaim-01, Ġclaim-02, Ġclarify-10, Ġclash-01, Ġclass-01, Ġclassify-01, Ġclean-01, Ġclean-04, Ġclean-up-02, Ġclear-01, Ġclear-06, Ġclever-01, Ġclick-01, Ġclimb-01, Ġclone-01, Ġclose-01, Ġclose-03, Ġclose-06, Ġclose-10, Ġclose-11, Ġclose-13, Ġclose-down-04, Ġclothe-01, Ġcoach-01, Ġcoerce-01, Ġcoexist-01, Ġcohere-01, Ġcoincide-01, Ġcold-01, Ġcold-02, Ġcollaborate-01, Ġcollapse-01, Ġcollect-01, Ġcolonize-01, Ġcolor-01, Ġcombat-01, Ġcombine-01, Ġcome-01, Ġcome-03, Ġcome-04, Ġcome-12, Ġcome-across-21, Ġcome-down-23, Ġcome-in-07, Ġcome-on-25, Ġcome-out-09, Ġcome-up-11, Ġcome-up-13, Ġcomfortable-02, Ġcommand-02, Ġcommence-01, Ġcommend-01, Ġcomment-01, Ġcommit-01, Ġcommit-02, Ġcommunicate-01, Ġcommute-02, Ġcompare-01, Ġcompel-01, Ġcompensate-01, Ġcompete-01, Ġcompete-02, Ġcompetent-01, Ġcompile-01, Ġcomplain-01, Ġcomplete-01, Ġcomplete-02, Ġcomplicate-01, Ġcomply-01, Ġcompose-01, Ġcompound-01, Ġcomprehend-01, Ġcomprise-01, Ġcompromise-01, Ġcompromise-02, Ġconceal-01, Ġconcede-01, Ġconceive-01, Ġconcentrate-01, Ġconcentrate-02, Ġconcern-01, Ġconcern-02, Ġconclude-01, Ġconclude-02, Ġconcrete-02, Ġcondemn-01, Ġcondition-01, Ġcondone-01, Ġconduct-01, Ġconduct-02, Ġconfess-01, Ġconfident-01, Ġconfine-01, Ġconfirm-01, Ġconfiscate-01, Ġconflict-01, Ġconform-01, Ġconfront-01, Ġconfuse-01, Ġcongratulate-01, Ġconnect-01, Ġconquer-01, Ġconsent-01, Ġconsider-01, Ġconsider-02, Ġconsist-01, Ġconsistent-01, Ġconsistent-02, Ġconsolidate-01, Ġconspire-01, Ġconstitute-01, Ġconstrain-01, Ġconstruct-01, Ġconsult-01, Ġconsume-01, Ġcontact-01, Ġcontain-01, Ġcontaminate-01, Ġcontend-01, Ġcontend-02, Ġcontent-01, Ġcontent-02, Ġcontest-02, Ġcontinue-01, Ġcontract-02, Ġcontradict-01, Ġcontrary-01, Ġcontrast-01, Ġcontribute-01, Ġcontrol-01, Ġconvene-01, Ġconverse-01, Ġconvert-01, Ġconvey-01, Ġconvict-01, Ġconviction-02, Ġconvince-01, Ġcook-01, Ġcool-01, Ġcool-04, Ġcooperate-01, Ġcoordinate-01, Ġcope-01, Ġcopy-01, Ġcopyright-01, Ġcorrect-01, Ġcorrect-02, Ġcorrelate-01, Ġcorrespond-02, Ġcorroborate-01, Ġcorrupt-01, Ġcost-01, Ġcounsel-01, Ġcount-01, Ġcount-02, Ġcount-03, Ġcount-04, Ġcounter-01, Ġcounterfeit-01, Ġcouple-01, Ġcourse-91, Ġcover-01, Ġcover-02, Ġcover-03, Ġcover-up-04, Ġcoverage-06, Ġcrack-02, Ġcrack-down-06, Ġcraft-01, Ġcrap-01, Ġcrash-01, Ġcrazy-03, Ġcreate-01, Ġcredit-01, Ġcredit-02, Ġcreepy-04, Ġcrime-02, Ġcriminal-03, Ġcripple-01, Ġcritical-02, Ġcriticism-04, Ġcriticize-01, Ġcross-02, Ġcrowd-01, Ġcruise-01, Ġcrumble-01, Ġcrush-01, Ġcry-01, Ġcry-02, Ġcultivate-01, Ġcurb-01, Ġcure-01, Ġcurious-01, Ġcurious-02, Ġcurse-02, Ġcurtail-01, Ġcut-01, Ġcut-02, Ġcut-03, Ġcut-back-05, Ġcut-down-11, Ġcut-off-04, Ġcut-out-06, Ġcycle-02, Ġdamage-01, Ġdamn-01, Ġdance-01, Ġdare-01, Ġdark-02, Ġdate-01, Ġdate-02, Ġdate-entity, Ġdeal-01, Ġdeal-02, Ġdeal-03, Ġdebate-01, Ġdecapitate-01, Ġdecay-01, Ġdeceive-01, Ġdecide-01, Ġdeclare-01, Ġdeclare-02, Ġdeclassify-01, Ġdecline-01, Ġdecline-02, Ġdecommission-01, Ġdecrease-01, Ġdecry-01, Ġdedicate-01, Ġdeduct-01, Ġdeem-01, Ġdeep-02, Ġdeep-03, Ġdefame-01, Ġdefeat-01, Ġdefend-01, Ġdefine-01, Ġdeflect-01, Ġdefraud-01, Ġdefuse-01, Ġdefy-01, Ġdelay-01, Ġdelegate-01, Ġdeliberate-01, Ġdelight-01, Ġdeliver-01, Ġdelude-01, Ġdemagogue-01, Ġdemand-01, Ġdemolish-01, Ġdemonize-01, Ġdemonstrate-01, Ġdenounce-01, Ġdent-01, Ġdeny-01, Ġdepart-01, Ġdepend-01, Ġdeploy-01, Ġdeport-01, Ġdeposit-01, Ġdepress-01, Ġdeprive-01, Ġderegulate-01, Ġderive-01, Ġdescend-01, Ġdescribe-01, Ġdeserve-01, Ġdesign-01, Ġdesignate-01, Ġdesirable-02, Ġdesire-01, Ġdespair-01, Ġdesperate-02, Ġdespise-01, Ġdestabilize-01, Ġdestroy-01, Ġdetail-01, Ġdetain-01, Ġdetect-01, Ġdeter-01, Ġdeteriorate-01, Ġdetermine-01, Ġdetermined-02, Ġdetonate-01, Ġdevalue-01, Ġdevastate-01, Ġdevelop-01, Ġdevelop-02, Ġdeviate-01, Ġdevote-01, Ġdiagnose-01, Ġdialogue-01, Ġdictate-01, Ġdie-01, Ġdiffer-01, Ġdiffer-02, Ġdifferentiate-01, Ġdig-01, Ġdiminish-01, Ġdine-01, Ġdip-01, Ġdirect-01, Ġdirect-02, Ġdirty-02, Ġdisable-01, Ġdisagree-01, Ġdisappear-01, Ġdisappoint-01, Ġdisapprove-01, Ġdisarm-01, Ġdisclose-01, Ġdiscount-02, Ġdiscover-01, Ġdiscredit-01, Ġdiscriminate-01, Ġdiscriminate-02, Ġdiscuss-01, Ġdisgrace-01, Ġdisgruntle-01, Ġdisguise-01, Ġdisgust-01, Ġdisintegrate-01, Ġdislike-01, Ġdismantle-01, Ġdismiss-01, Ġdismiss-02, Ġdispatch-01, Ġdisperse-01, Ġdisplace-01, Ġdisplay-01, Ġdispose-01, Ġdisprove-01, Ġdispute-01, Ġdisregard-01, Ġdisrespect-01, Ġdisrupt-01, Ġdissent-01, Ġdissolve-01, Ġdistance-01, Ġdistant-02, Ġdistinguish-01, Ġdistort-01, Ġdistract-01, Ġdistress-01, Ġdistribute-01, Ġdisturb-01, Ġdive-01, Ġdiversify-01, Ġdivert-01, Ġdivide-02, Ġdivorce-01, Ġdo-02, Ġdock-01, Ġdocument-01, Ġdodge-01, Ġdominate-01, Ġdonate-01, Ġdose-01, Ġdouble-01, Ġdoubt-01, Ġdown-01, Ġdown-03, Ġdowngrade-02, Ġdraft-01, Ġdraft-02, Ġdrag-01, Ġdrain-01, Ġdraw-01, Ġdraw-02, Ġdraw-up-03, Ġdream-01, Ġdress-01, Ġdrill-01, Ġdrink-01, Ġdrive-01, Ġdrive-02, Ġdrive-04, Ġdrop-01, Ġdrop-05, Ġdrop-out-04, Ġdrug-01, Ġdry-02, Ġdry-08, Ġdubious-02, Ġdump-01, Ġdye-01, Ġearn-01, Ġearnest-01, Ġeasy-05, Ġeat-01, Ġedit-01, Ġeducate-01, Ġeffect-03, Ġeffective-04, Ġefficient-01, Ġeffort-01, Ġelaborate-01, Ġelect-01, Ġelevate-01, Ġeliminate-01, Ġemail-01, Ġembargo-01, Ġembarrass-01, Ġembezzle-01, Ġembrace-01, Ġemerge-01, Ġemerge-02, Ġemit-01, Ġemphasize-01, Ġemploy-01, Ġemploy-02, Ġempower-01, Ġempty-02, Ġenable-01, Ġenact-01, Ġencounter-01, Ġencourage-01, Ġencourage-02, Ġencrypt-01, Ġend-01, Ġend-up-03, Ġendanger-01, Ġendorse-01, Ġendure-01, Ġenforce-01, Ġengage-01, Ġengineer-01, Ġenhance-01, Ġenjoy-01, Ġenlighten-01, Ġenlist-01, Ġenrich-01, Ġenslave-01, Ġensure-01, Ġenter-01, Ġenter-02, Ġentertain-01, Ġenthusiastic-03, Ġentitle-01, Ġentrench-01, Ġenvision-01, Ġenvy-01, Ġequal-01, Ġequate-01, Ġequip-01, Ġeradicate-01, Ġerode-01, Ġerr-01, Ġerupt-01, Ġescalate-01, Ġescape-01, Ġestablish-01, Ġestimate-01, Ġet-cetera, Ġevacuate-01, Ġevade-01, Ġevaluate-01, Ġevidence-01, Ġevolve-01, Ġevolve-02, Ġexacerbate-01, Ġexaggerate-01, Ġexamine-01, Ġexceed-01, Ġexcellent-02, Ġexcept-01, Ġexcessive-02, Ġexchange-01, Ġexcite-01, Ġexclude-01, Ġexclusive-02, Ġexcuse-01, Ġexcuse-02, Ġexecute-01, Ġexecute-02, Ġexemplify-01, Ġexempt-01, Ġexercise-01, Ġexercise-02, Ġexert-01, Ġexhibit-01, Ġexile-01, Ġexist-01, Ġexit-01, Ġexpand-01, Ġexpect-01, Ġexpel-01, Ġexpend-01, Ġexperience-01, Ġexperiment-01, Ġexpert-01, Ġexpire-01, Ġexplain-01, Ġexplicit-03, Ġexplode-01, Ġexploit-01, Ġexplore-01, Ġexport-01, Ġexpose-01, Ġexpress-01, Ġextend-01, Ġextend-02, Ġextensive-03, Ġexterminate-01, Ġextort-01, Ġextract-01, Ġextradite-01, Ġeye-01, Ġfabricate-01, Ġface-01, Ġfacilitate-01, Ġfail-01, Ġfair-01, Ġfair-04, Ġfake-02, Ġfall-01, Ġfall-05, Ġfall-07, Ġfall-10, Ġfall-apart-09, Ġfame-01, Ġfamiliarize-01, Ġfarm-01, Ġfascinate-01, Ġfast-02, Ġfat-03, Ġfathom-01, Ġfault-01, Ġfavor-01, Ġfear-01, Ġfeature-01, Ġfeed-01, Ġfeed-up-03, Ġfeel-01, Ġfeel-02, Ġfeel-05, Ġfeel-06, Ġfight-01, Ġfigure-01, Ġfigure-04, Ġfigure-out-05, Ġfile-01, Ġfill-01, Ġfilm-01, Ġfilter-02, Ġfinalize-01, Ġfinance-01, Ġfind-01, Ġfind-02, Ġfind-out-03, Ġfine-01, Ġfine-03, Ġfine-04, Ġfinish-01, Ġfinish-07, Ġfire-01, Ġfire-02, Ġfire-03, Ġfire-04, Ġfirm-03, Ġfit-05, Ġfit-06, Ġfit-in-02, Ġfix-02, Ġfix-03, Ġflame-01, Ġflat-06, Ġflaw-01, Ġfledge-01, Ġflee-05, Ġflip-01, Ġfloat-01, Ġflood-01, Ġflow-01, Ġfluctuate-01, Ġfly-01, Ġfocus-01, Ġfollow-01, Ġfollow-02, Ġfollow-04, Ġfollow-up-03, Ġfool-01, Ġforbid-01, Ġforce-01, Ġforce-02, Ġforce-04, Ġforecast-01, Ġforeclose-01, Ġforesee-01, Ġforge-02, Ġforget-01, Ġforgive-01, Ġform-01, Ġform-02, Ġformulate-01, Ġfortunate-01, Ġforward-01, Ġfoster-01, Ġfound-01, Ġfree-01, Ġfree-03, Ġfree-04, Ġfreeze-01, Ġfreeze-02, Ġfrequent-02, Ġfresh-04, Ġfriendly-01, Ġfrighten-01, Ġfrustrate-01, Ġfuck-01, Ġfuck-up-02, Ġfuel-01, Ġfulfill-01, Ġfull-09, Ġfun-01, Ġfunction-01, Ġfund-01, Ġfurther-01, Ġfuss-01, Ġgain-01, Ġgain-02, Ġgarner-01, Ġgas-03, Ġgather-01, Ġgather-03, Ġgauge-01, Ġgeneral-02, Ġgenerate-01, Ġgenerous-01, Ġgenocide-01, Ġget-01, Ġget-02, Ġget-03, Ġget-04, Ġget-05, Ġget-06, Ġget-22, Ġget-30, Ġget-along-18, Ġget-away-08, Ġget-back-10, Ġget-by-17, Ġget-off-23, Ġget-on-21, Ġget-through-12, Ġget-through-13, Ġgift-01, Ġgive-01, Ġgive-16, Ġgive-away-02, Ġgive-back-03, Ġgive-in-09, Ġgive-up-07, Ġgive-up-08, Ġglad-02, Ġglance-01, Ġgo-01, Ġgo-02, Ġgo-03, Ġgo-05, Ġgo-06, Ġgo-08, Ġgo-09, Ġgo-10, Ġgo-12, Ġgo-21, Ġgo-22, Ġgo-23, Ġgo-back-19, Ġgo-down-27, Ġgo-off-16, Ġgo-on-15, Ġgo-on-25, Ġgo-out-17, Ġgo-through-20, Ġgolf-01, Ġgood-02, Ġgood-03, Ġgood-04, Ġgoogle-01, Ġgovern-01, Ġgovernment-organization, Ġgrab-01, Ġgrade-01, Ġgraduate-01, Ġgrant-01, Ġgrasp-01, Ġgray-02, Ġgreen-02, Ġgreen-03, Ġgreet-01, Ġgrieve-01, Ġgrind-01, Ġgrip-01, Ġgripe-01, Ġgross-03, Ġgross-06, Ġground-02, Ġgroup-01, Ġgrow-01, Ġgrow-02, Ġgrow-03, Ġgrow-up-04, Ġguarantee-01, Ġguard-01, Ġguess-01, Ġguide-01, Ġguilty-01, Ġgut-01, Ġhack-04, Ġhallucinate-01, Ġhalt-01, Ġhand-01, Ġhand-out-03, Ġhand-over-02, Ġhandle-01, Ġhang-01, Ġhang-out-06, Ġhanging-07, Ġhappen-02, Ġhappy-01, Ġhappy-02, Ġharass-01, Ġhard-02, Ġhard-04, Ġharm-01, Ġharmful-02, Ġharsh-02, Ġharvest-01, Ġhate-01, Ġhaul-01, Ġhave-03, Ġhave-04, Ġhave-05, Ġhave-06, Ġhave-11, Ġhave-concession-91, Ġhave-condition-91, Ġhave-extent-91, Ġhave-frequency-91, Ġhave-instrument-91, Ġhave-li-91, Ġhave-manner-91, Ġhave-mod-91, Ġhave-name-91, Ġhave-org-role-91, Ġhave-part-91, Ġhave-polarity-91, Ġhave-purpose-91, Ġhave-quant-91, Ġhave-rel-role-91, Ġhave-subevent-91, Ġhave-to-do-with-04, Ġhead-01, Ġhead-02, Ġheadline-01, Ġheadquarter-01, Ġheal-01, Ġhear-01, Ġhearing-02, Ġheat-01, Ġheed-01, Ġhelp-01, Ġhelp-02, Ġhelp-out-03, Ġhelpful-04, Ġhesitate-01, Ġhide-01, Ġhigh-02, Ġhighlight-01, Ġhijack-01, Ġhike-02, Ġhinder-01, Ġhint-01, Ġhire-01, Ġhit-01, Ġhit-02, Ġhoard-01, Ġhoax-01, Ġhold-01, Ġhold-02, Ġhold-03, Ġhold-04, Ġhold-back-07, Ġholiday-01, Ġhonest-01, Ġhonor-01, Ġhonorable-03, Ġhook-up-02, Ġhope-01, Ġhopeful-02, Ġhopeful-03, Ġhospitalize-01, Ġhost-01, Ġhot-04, Ġhot-05, Ġhouse-01, Ġhumble-01, Ġhumiliate-01, Ġhunger-01, Ġhunt-01, Ġhurt-01, Ġhurt-02, Ġhype-01, Ġhyperlink-01, Ġhyperlink-91, Ġhypothesize-01, Ġidentical-01, Ġidentify-01, Ġignorant-02, Ġignore-01, Ġill-01, Ġill-02, Ġimagine-01, Ġimitate-01, Ġimmigrate-01, Ġimmune-02, Ġimpact-01, Ġimpair-01, Ġimpeach-01, Ġimpede-01, Ġimplement-01, Ġimplicate-01, Ġimply-01, Ġimport-01, Ġimpose-01, Ġimpoverish-01, Ġimpregnate-01, Ġimpress-01, Ġimpression-03, Ġimprison-01, Ġimprove-01, Ġimprovise-01, Ġincarcerate-01, Ġincentivize-01, Ġincinerate-01, Ġincite-01, Ġincline-01, Ġinclude-01, Ġinclude-91, Ġincorporate-02, Ġincrease-01, Ġincur-01, Ġindicate-01, Ġindict-01, Ġindoctrinate-01, Ġindulge-01, Ġindustrialize-01, Ġinfect-01, Ġinfer-01, Ġinferior-01, Ġinfiltrate-01, Ġinflate-01, Ġinflict-01, Ġinfluence-01, Ġinform-01, Ġinfringe-01, Ġinhabit-01, Ġinherit-01, Ġinhibit-01, Ġinitiate-01, Ġinject-01, Ġinjure-01, Ġinnocent-01, Ġinnovate-01, Ġinquire-01, Ġinsist-01, Ġinspect-01, Ġinspire-01, Ġinstall-01, Ġinstead-of-91, Ġinstitute-01, Ġinstruct-01, Ġinsult-01, Ġinsure-01, Ġinsure-02, Ġintegrate-01, Ġintelligent-01, Ġintend-01, Ġintense-02, Ġintensify-01, Ġintent-02, Ġinteract-01, Ġintercept-01, Ġinterdict-01, Ġinterest-01, Ġinterfere-01, Ġinternal-02, Ġinterpret-01, Ġinterrogate-01, Ġinterrupt-01, Ġintersect-01, Ġintervene-01, Ġinterview-01, Ġintimate-02, Ġintimidate-01, Ġintroduce-01, Ġintroduce-02, Ġinvade-01, Ġinvent-01, Ġinvest-01, Ġinvestigate-01, Ġinvite-01, Ġinvoke-01, Ġinvolve-01, Ġirritate-01, Ġisolate-01, Ġissue-01, Ġissue-02, Ġjail-01, Ġjealous-02, Ġjeopardize-01, Ġjoin-01, Ġjoin-04, Ġjoin-in-05, Ġjoin-up-02, Ġjoke-01, Ġjudge-01, Ġjump-01, Ġjump-03, Ġjust-02, Ġjustify-01, Ġkeep-01, Ġkeep-02, Ġkeep-03, Ġkeep-04, Ġkeep-up-05, Ġkeep-up-10, Ġkey-02, Ġkick-01, Ġkid-01, Ġkidnap-01, Ġkill-01, Ġkill-03, Ġkind-01, Ġkiss-01, Ġkneel-01, Ġknock-01, Ġknow-01, Ġknow-02, Ġknow-03, Ġknow-04, Ġknow-06, Ġlabel-01, Ġlabor-01, Ġlack-01, Ġlag-01, Ġland-01, Ġlast-01, Ġlast-04, Ġlaugh-01, Ġlaughable-03, Ġlaunch-01, Ġlaunder-01, Ġlay-01, Ġlay-04, Ġlay-off-02, Ġlay-off-06, Ġlead-01, Ġlead-02, Ġlead-03, Ġleak-01, Ġlean-01, Ġlearn-01, Ġlease-01, Ġleave-02, Ġleave-10, Ġleave-11, Ġleave-12, Ġleave-13, Ġleave-14, Ġleave-15, Ġleave-17, Ġleave-out-03, Ġlecture-01, Ġleft-19, Ġleft-20, Ġlegal-02, Ġlegalize-01, Ġlegislate-01, Ġlegitimate-02, Ġlend-01, Ġlessen-01, Ġlet-01, Ġlet-down-04, Ġlevel-04, Ġlevy-01, Ġliable-01, Ġlibel-01, Ġliberal-02, Ġliberate-01, Ġlicense-01, Ġlie-07, Ġlie-08, Ġlift-01, Ġlift-02, Ġlight-04, Ġlight-06, Ġlike-01, Ġlike-02, Ġlikely-01, Ġlimit-01, Ġline-01, Ġline-up-02, Ġlink-01, Ġlist-01, Ġlisten-01, Ġlive-01, Ġlive-02, Ġlive-up-04, Ġload-01, Ġloan-01, Ġloathe-01, Ġlobby-01, Ġlocal-02, Ġlocate-01, Ġlock-01, Ġlock-up-03, Ġlong-03, Ġlook-01, Ġlook-02, Ġlook-04, Ġlook-forward-03, Ġlook-up-05, Ġloose-04, Ġloot-01, Ġlose-01, Ġlose-02, Ġlose-03, Ġlose-out-06, Ġlove-01, Ġlove-02, Ġlow-04, Ġlower-05, Ġloyal-01, Ġlump-01, Ġlunch-01, Ġlure-01, Ġlurk-01, Ġlust-01, Ġlynch-01, Ġmad-02, Ġmad-04, Ġmainstream-02, Ġmaintain-01, Ġmajor-02, Ġmake-01, Ġmake-02, Ġmake-05, Ġmake-18, Ġmake-it-14, Ġmake-out-23, Ġmake-up-07, Ġmake-up-08, Ġmake-up-10, Ġman-01, Ġmanage-01, Ġmanage-02, Ġmandate-01, Ġmanifest-01, Ġmanipulate-01, Ġmanipulate-02, Ġmanufacture-01, Ġmarch-01, Ġmark-01, Ġmark-02, Ġmarket-01, Ġmarry-01, Ġmask-01, Ġmass-quantity, Ġmassacre-01, Ġmatch-01, Ġmatch-03, Ġmatter-01, Ġmature-02, Ġmaximize-01, Ġmean-01, Ġmean-02, Ġmean-04, Ġmeaningful-05, Ġmeasure-01, Ġmeasure-02, Ġmeddle-01, Ġmediate-01, Ġmeet-01, Ġmeet-02, Ġmeet-03, Ġmeet-up-04, Ġmelt-01, Ġmenace-01, Ġmention-01, Ġmerchandise-01, Ġmerge-01, Ġmerit-01, Ġmess-up-02, Ġmessage-01, Ġmigrate-01, Ġmind-01, Ġmind-04, Ġmind-05, Ġmine-01, Ġminimal-02, Ġminimize-01, Ġminor-01, Ġminor-02, Ġmislead-01, Ġmisrepresent-01, Ġmiss-01, Ġmiss-02, Ġmistake-01, Ġmistake-02, Ġmisunderstand-01, Ġmitigate-01, Ġmix-01, Ġmoan-01, Ġmobile-02, Ġmobilize-01, Ġmock-01, Ġmodel-01, Ġmoderate-01, Ġmoderate-02, Ġmoderate-03, Ġmodern-02, Ġmodernize-01, Ġmodify-01, Ġmoisturize-01, Ġmolest-01, Ġmonetary-quantity, Ġmoney-01, Ġmonitor-01, Ġmoral-02, Ġmortgage-01, Ġmotivate-01, Ġmotivate-02, Ġmove-01, Ġmove-02, Ġmove-03, Ġmovement-07, Ġmulti-sentence, Ġmultiply-01, Ġmurder-01, Ġname-01, Ġname-02, Ġnarrow-01, Ġnarrow-02, Ġnationalize-01, Ġnatural-02, Ġnatural-03, Ġnear-02, Ġneed-01, Ġnegative-02, Ġnegative-03, Ġnegative-05, Ġneglect-01, Ġnegotiate-01, Ġneighbor-01, Ġnetwork-01, Ġneutral-02, Ġneutralize-01, Ġnew-01, Ġnew-02, Ġnice-01, Ġnominate-01, Ġnormal-02, Ġnormalize-01, Ġnotable-04, Ġnote-01, Ġnote-02, Ġnotice-01, Ġnotice-03, Ġnotify-01, Ġnuke-01, Ġnumber-01, Ġobey-01, Ġobject-01, Ġobligate-01, Ġoblige-02, Ġobserve-01, Ġobsess-01, Ġobstruct-01, Ġobtain-01, Ġobvious-01, Ġoccasion-02, Ġoccupy-01, Ġoffend-01, Ġoffend-03, Ġoffense-02, Ġoffer-01, Ġoffset-01, Ġokay-01, Ġokay-04, Ġopen-01, Ġopen-02, Ġopen-04, Ġopen-05, Ġopen-07, Ġopen-09, Ġopen-up-03, Ġoperate-01, Ġoperate-02, Ġopine-01, Ġoppose-01, Ġopposite-01, Ġoppress-01, Ġopt-01, Ġorbit-01, Ġordain-01, Ġorder-01, Ġorder-02, Ġorder-03, Ġordinal-entity, Ġorganize-01, Ġorient-01, Ġoriginate-01, Ġoust-01, Ġout-01, Ġout-03, Ġout-05, Ġout-06, Ġoutlaw-01, Ġoutrage-01, Ġoutrageous-02, Ġoutweigh-01, Ġovercome-01, Ġoverlook-01, Ġoverpay-01, Ġoverride-01, Ġoversee-01, Ġoverthrow-01, Ġoverturn-01, Ġoverwhelm-01, Ġowe-01, Ġown-01, Ġpace-01, Ġpack-01, Ġpackage-01, Ġpaddle-01, Ġpain-01, Ġpaint-02, Ġpaint-03, Ġpale-03, Ġpanic-01, Ġparade-02, Ġparalyze-01, Ġpardon-01, Ġpark-01, Ġparole-01, Ġparrot-01, Ġpart-01, Ġpartake-01, Ġparticipate-01, Ġpartition-01, Ġpartner-01, Ġparty-01, Ġpass-01, Ġpass-02, Ġpass-03, Ġpass-04, Ġpass-05, Ġpass-07, Ġpass-20, Ġpass-away-16, Ġpass-by-17, Ġpass-on-09, Ġpass-on-14, Ġpaste-01, Ġpatent-01, Ġpatient-01, Ġpatrol-01, Ġpattern-01, Ġpay-01, Ġpay-off-02, Ġpeak-01, Ġpenalize-01, Ġpend-01, Ġpenetrate-01, Ġperceive-01, Ġpercentage-entity, Ġperfect-02, Ġperform-01, Ġperform-02, Ġperjure-01, Ġpermit-01, Ġperpetrate-01, Ġpersecute-01, Ġpersist-01, Ġpersonal-02, Ġpersuade-01, Ġpertain-01, Ġpervert-01, Ġpetition-01, Ġphilander-01, Ġphone-01, Ġphotograph-01, Ġpick-01, Ġpick-up-04, Ġpicture-01, Ġpierce-01, Ġpile-01, Ġpilot-01, Ġpin-01, Ġpiss-01, Ġpiss-03, Ġpiss-off-02, Ġpity-01, Ġplace-01, Ġplague-01, Ġplan-01, Ġplant-01, Ġplay-01, Ġplay-02, Ġplay-08, Ġplay-10, Ġplay-11, Ġplead-02, Ġplease-01, Ġpledge-01, Ġplot-01, Ġpoint-01, Ġpoint-out-02, Ġpolice-01, Ġpolicy-01, Ġpolite-01, Ġpolitical-movement, Ġpolitical-party, Ġpoll-01, Ġpollute-01, Ġpool-01, Ġpopular-02, Ġpopulate-01, Ġportion-01, Ġportray-01, Ġpose-01, Ġpose-02, Ġposition-01, Ġposition-02, Ġpossess-01, Ġpossible-01, Ġpost-01, Ġpostpone-01, Ġpour-01, Ġpower-01, Ġpowerful-02, Ġpractice-01, Ġpraise-01, Ġpray-01, Ġpreach-01, Ġprecede-01, Ġpredict-01, Ġpreexist-01, Ġprefer-01, Ġprejudice-01, Ġpremise-01, Ġprepare-01, Ġprepare-02, Ġprescribe-02, Ġpresent-01, Ġpresent-02, Ġpreserve-01, Ġpreside-01, Ġpress-01, Ġpressure-01, Ġpresume-01, Ġpretend-01, Ġprevail-01, Ġprevail-02, Ġprevent-01, Ġprey-01, Ġprice-01, Ġpride-01, Ġprint-01, Ġprioritize-01, Ġprivate-02, Ġprivate-03, Ġprivatize-01, Ġprivilege-01, Ġprize-01, Ġprobe-01, Ġproceed-01, Ġproceeding-02, Ġprocess-01, Ġprocess-02, Ġproclaim-01, Ġprocure-01, Ġproduce-01, Ġproduce-02, Ġproductive-03, Ġprofess-01, Ġprofit-01, Ġprogram-01, Ġprogress-01, Ġprogressive-02, Ġprohibit-01, Ġproject-01, Ġproject-02, Ġproliferate-01, Ġprolong-01, Ġpromise-01, Ġpromote-01, Ġpromote-02, Ġprompt-01, Ġprop-up-01, Ġpropagate-01, Ġpropel-01, Ġpropose-01, Ġprosecute-01, Ġprospect-02, Ġprosper-01, Ġprostitute-01, Ġprotect-01, Ġprotest-01, Ġprove-01, Ġprovide-01, Ġprovoke-01, Ġpublic-02, Ġpublication-91, Ġpublicize-01, Ġpublish-01, Ġpull-01, Ġpull-06, Ġpull-09, Ġpull-out-02, Ġpump-01, Ġpunch-01, Ġpunish-01, Ġpunishable-02, Ġpurchase-01, Ġpure-02, Ġpurport-01, Ġpursue-01, Ġpush-01, Ġpush-02, Ġpush-04, Ġput-01, Ġput-02, Ġput-03, Ġput-in-05, Ġput-on-08, Ġput-out-10, Ġput-up-11, Ġpuzzle-01, Ġqualify-01, Ġqualify-02, Ġquest-01, Ġquestion-01, Ġquestion-03, Ġquick-02, Ġquiet-04, Ġquit-01, Ġquote-01, Ġrace-02, Ġracket-02, Ġradiate-01, Ġrage-02, Ġraid-01, Ġrail-01, Ġrain-01, Ġraise-01, Ġraise-02, Ġraise-03, Ġrally-01, Ġramble-02, Ġrange-01, Ġrank-01, Ġrant-01, Ġrape-01, Ġrare-02, Ġrate-01, Ġrate-entity-91, Ġratify-01, Ġration-01, Ġreach-01, Ġreach-02, Ġreach-03, Ġreact-01, Ġreactionary-02, Ġreactivate-01, Ġread-01, Ġready-01, Ġready-02, Ġreal-04, Ġrealistic-03, Ġrealize-01, Ġrealize-02, Ġreason-01, Ġreasonable-02, Ġrebel-01, Ġrebuild-01, Ġrecall-02, Ġreceive-01, Ġrecession-02, Ġreckon-01, Ġreclaim-01, Ġrecognize-01, Ġrecognize-02, Ġrecommend-01, Ġreconcile-01, Ġreconstruct-01, Ġrecord-01, Ġrecover-01, Ġrecover-02, Ġrecreation-02, Ġrecruit-01, Ġrectify-01, Ġred-02, Ġredeem-01, Ġredistribute-01, Ġreduce-01, Ġreelect-01, Ġreenter-01, Ġrefer-01, Ġrefer-02, Ġrefer-03, Ġreference-04, Ġrefine-01, Ġreflect-01, Ġreform-01, Ġrefrain-01, Ġrefuse-01, Ġrefute-01, Ġregain-01, Ġregard-01, Ġregardless-91, Ġregister-02, Ġregret-01, Ġregular-02, Ġregular-03, Ġregulate-01, Ġrehabilitate-01, Ġreinforce-01, Ġreiterate-01, Ġreject-01, Ġrelate-01, Ġrelated-04, Ġrelation-03, Ġrelative-05, Ġrelax-01, Ġrelease-01, Ġrelevant-01, Ġrelieve-01, Ġreligious-group, Ġrelinquish-01, Ġrelocate-01, Ġrely-01, Ġremain-01, Ġremand-01, Ġremark-01, Ġremarkable-02, Ġremember-01, Ġremind-01, Ġremit-01, Ġremortgage-01, Ġremove-01, Ġrender-01, Ġrender-02, Ġrenew-01, Ġrenounce-01, Ġrenovate-01, Ġrent-01, Ġreopen-01, Ġreorganize-01, Ġrepair-01, Ġrepay-01, Ġrepeal-01, Ġrepeat-01, Ġreplace-01, Ġreplicate-01, Ġreply-01, Ġreport-01, Ġrepresent-01, Ġrepress-01, Ġreprocess-01, Ġreproduce-01, Ġrepute-01, Ġrequest-01, Ġrequest-confirmation-91, Ġrequire-01, Ġrescue-01, Ġresearch-01, Ġresemble-01, Ġresent-01, Ġreserve-01, Ġreside-01, Ġresign-01, Ġresist-01, Ġresolve-01, Ġresolve-02, Ġresort-01, Ġrespect-01, Ġrespond-01, Ġresponsible-01, Ġresponsible-02, Ġresponsible-03, Ġrest-01, Ġrestart-01, Ġrestore-01, Ġrestore-02, Ġrestrain-01, Ġrestrict-01, Ġrestructure-01, Ġresult-01, Ġresume-01, Ġretail-01, Ġretain-01, Ġretaliate-01, Ġretard-01, Ġretire-01, Ġreturn-01, Ġreturn-02, Ġreturn-05, Ġreunify-01, Ġreveal-01, Ġrevere-01, Ġreverse-01, Ġreview-01, Ġrevise-01, Ġrevisit-01, Ġrevive-01, Ġrevolution-03, Ġrevolutionary-04, Ġreward-01, Ġrewrite-01, Ġrid-01, Ġride-01, Ġridicule-01, Ġridiculous-02, Ġright-02, Ġright-03, Ġright-04, Ġright-05, Ġright-06, Ġright-08, Ġriot-01, Ġrip-01, Ġrise-01, Ġrisk-01, Ġrival-01, Ġrob-01, Ġroll-01, Ġroot-02, Ġrot-01, Ġrotate-01, Ġrotate-02, Ġround-05, Ġruin-01, Ġrule-01, Ġrule-03, Ġrule-out-02, Ġrumor-01, Ġrun-01, Ġrun-02, Ġrun-04, Ġrun-07, Ġrun-08, Ġrun-09, Ġrun-10, Ġrun-13, Ġrun-off-24, Ġrun-out-05, Ġrun-up-19, Ġrush-01, Ġrust-01, Ġsacrifice-01, Ġsad-02, Ġsafe-01, Ġsafeguard-01, Ġsail-01, Ġsame-01, Ġsample-01, Ġsanction-02, Ġsatisfy-01, Ġsave-01, Ġsave-02, Ġsave-03, Ġsay-01, Ġscam-01, Ġscare-01, Ġscary-03, Ġschedule-01, Ġscheme-01, Ġschool-01, Ġscore-01, Ġscore-on-scale-91, Ġscrap-01, Ġscream-01, Ġscrew-02, Ġscrew-up-01, Ġscrutinize-01, Ġseal-01, Ġsearch-01, Ġseat-01, Ġsecure-01, Ġsecure-02, Ġsee-01, Ġsee-03, Ġsee-05, Ġseek-01, Ġseek-out-02, Ġseem-01, Ġsegregate-01, Ġseize-01, Ġselect-01, Ġsell-01, Ġsend-01, Ġsend-02, Ġsend-03, Ġsense-01, Ġsense-02, Ġsensitive-03, Ġsentence-01, Ġseparate-01, Ġseparate-02, Ġserious-01, Ġserious-02, Ġserve-01, Ġserve-02, Ġserve-04, Ġservice-05, Ġset-01, Ġset-02, Ġset-08, Ġset-up-03, Ġsettle-01, Ġsettle-02, Ġsettle-03, Ġsex-01, Ġshake-01, Ġshame-01, Ġshame-02, Ġshape-01, Ġshare-01, Ġsharp-02, Ġshave-01, Ġshelter-01, Ġshield-01, Ġshift-01, Ġshine-01, Ġship-01, Ġshit-01, Ġshock-01, Ġshoot-01, Ġshoot-02, Ġshoot-down-05, Ġshop-01, Ġshort-06, Ġshort-07, Ġshout-01, Ġshove-01, Ġshow-01, Ġshow-04, Ġshow-up-02, Ġshower-01, Ġshred-01, Ġshut-01, Ġshut-down-05, Ġshut-up-06, Ġsick-02, Ġsick-04, Ġsick-05, Ġsicken-01, Ġside-01, Ġsigh-02, Ġsight-01, Ġsign-01, Ġsign-02, Ġsign-up-03, Ġsignal-07, Ġsignificant-02, Ġsignify-01, Ġsimple-02, Ġsimulate-01, Ġsin-01, Ġsing-01, Ġsingle-02, Ġsingle-03, Ġsink-01, Ġsit-01, Ġsit-down-02, Ġsituate-01, Ġsize-01, Ġskip-01, Ġslam-02, Ġslander-01, Ġslant-01, Ġslap-01, Ġslash-02, Ġslaughter-01, Ġslay-01, Ġsleep-01, Ġsleep-02, Ġslide-01, Ġslip-01, Ġslip-02, Ġslow-01, Ġslow-05, Ġslow-down-03, Ġsmart-06, Ġsmear-02, Ġsmell-01, Ġsmell-02, Ġsmile-01, Ġsmoke-01, Ġsmoke-02, Ġsmuggle-01, Ġsneak-01, Ġsneaky-03, Ġsnip-01, Ġsnow-01, Ġsoar-01, Ġsocial-03, Ġsoft-02, Ġsolid-02, Ġsolve-01, Ġsorry-01, Ġsort-out-02, Ġsound-01, Ġsound-02, Ġsource-01, Ġsource-02, Ġspace-01, Ġspan-01, Ġspank-01, Ġspare-01, Ġspark-01, Ġspeak-01, Ġspeak-out-03, Ġspecial-02, Ġspecialize-01, Ġspecific-02, Ġspecify-01, Ġspeculate-01, Ġspeed-01, Ġspell-01, Ġspend-01, Ġspend-02, Ġspend-04, Ġspew-01, Ġspin-01, Ġspin-03, Ġsplit-01, Ġspoil-01, Ġsponsor-01, Ġspot-01, Ġspout-01, Ġspread-01, Ġspread-02, Ġspread-03, Ġspread-out-04, Ġspur-01, Ġspy-01, Ġstab-01, Ġstabilize-01, Ġstable-03, Ġstaff-01, Ġstage-01, Ġstake-01, Ġstall-01, Ġstand-01, Ġstand-02, Ġstand-03, Ġstand-04, Ġstand-08, Ġstand-11, Ġstand-up-07, Ġstandard-02, Ġstandardize-01, Ġstart-01, Ġstart-out-05, Ġstarve-01, Ġstate-01, Ġstation-01, Ġstatistical-test-91, Ġstay-01, Ġstay-on-02, Ġsteady-01, Ġsteal-01, Ġstep-01, Ġstep-in-02, Ġstereotype-01, Ġstick-01, Ġstick-around-03, Ġstimulate-01, Ġstink-01, Ġstipulate-01, Ġstir-up-04, Ġstock-01, Ġstockpile-01, Ġstop-01, Ġstop-03, Ġstore-01, Ġstorm-01, Ġstorm-02, Ġstraight-04, Ġstraight-05, Ġstraight-06, Ġstreet-address-91, Ġstrengthen-01, Ġstress-01, Ġstress-02, Ġstretch-01, Ġstrike-01, Ġstrike-02, Ġstrike-04, Ġstrip-01, Ġstrive-01, Ġstrong-02, Ġstructure-01, Ġstruggle-01, Ġstruggle-02, Ġstudy-01, Ġstuff-01, Ġstun-01, Ġsubject-01, Ġsubmit-01, Ġsubscribe-01, Ġsubsidize-01, Ġsubstitute-01, Ġsucceed-01, Ġsucceed-03, Ġsuck-01, Ġsuck-03, Ġsuck-up-04, Ġsue-02, Ġsuffer-01, Ġsuffice-01, Ġsuggest-01, Ġsuit-01, Ġsuitable-04, Ġsummarize-01, Ġsuperior-01, Ġsupervise-01, Ġsupply-01, Ġsupport-01, Ġsuppose-01, Ġsuppose-02, Ġsuppress-01, Ġsure-02, Ġsurge-01, Ġsurgery-01, Ġsurpass-01, Ġsurprise-01, Ġsurrender-01, Ġsurround-01, Ġsurveil-01, Ġsurvey-01, Ġsurvive-01, Ġsurvive-02, Ġsuspect-01, Ġsuspend-01, Ġsustain-01, Ġswallow-01, Ġswear-01, Ġswear-02, Ġsweep-01, Ġsweep-06, Ġswell-01, Ġswim-01, Ġswitch-01, Ġsymbolize-01, Ġsympathize-01, Ġtable-01, Ġtackle-01, Ġtake-01, Ġtake-02, Ġtake-03, Ġtake-04, Ġtake-10, Ġtake-away-05, Ġtake-down-22, Ġtake-off-07, Ġtake-on-09, Ġtake-out-11, Ġtake-over-12, Ġtalk-01, Ġtally-01, Ġtape-02, Ġtarget-01, Ġtask-01, Ġtattoo-01, Ġtax-01, Ġteach-01, Ġtear-01, Ġtear-down-05, Ġtelephone-01, Ġtell-01, Ġtell-02, Ġtemporal-quantity, Ġtempt-01, Ġtend-02, Ġterm-01, Ġterminate-01, Ġterrible-01, Ġterrify-01, Ġterror-02, Ġtest-01, Ġtestify-01, Ġtext-01, Ġthank-01, Ġthankful-02, Ġthat-is-it-00, Ġthink-01, Ġthreaten-01, Ġthrill-01, Ġthrive-01, Ġthrow-01, Ġthrow-out-06, Ġticket-02, Ġtie-01, Ġtight-05, Ġtighten-01, Ġtimely-03, Ġtip-05, Ġtire-01, Ġtitle-01, Ġtolerate-01, Ġtop-01, Ġtop-02, Ġtopple-01, Ġtorture-01, Ġtoss-01, Ġtotal-01, Ġtouch-01, Ġtough-02, Ġtough-03, Ġtour-01, Ġtrace-02, Ġtrack-01, Ġtrack-down-02, Ġtrade-01, Ġtraffic-01, Ġtrain-01, Ġtrample-01, Ġtransact-01, Ġtransfer-01, Ġtransform-01, Ġtransgress-01, Ġtransit-01, Ġtransition-01, Ġtranslate-01, Ġtransmit-01, Ġtransplant-01, Ġtransport-01, Ġtrap-01, Ġtrash-01, Ġtravel-01, Ġtreat-01, Ġtreat-03, Ġtreat-04, Ġtrend-01, Ġtrick-01, Ġtricky-02, Ġtrigger-01, Ġtrip-03, Ġtriple-01, Ġtroll-01, Ġtrouble-01, Ġtrue-01, Ġtrue-02, Ġtrump-01, Ġtrust-01, Ġtrust-02, Ġtry-01, Ġtry-02, Ġtune-01, Ġturn-01, Ġturn-02, Ġturn-14, Ġturn-18, Ġturn-down-05, Ġturn-on-13, Ġturn-out-11, Ġturn-out-17, Ġturn-over-12, Ġturn-up-15, Ġtwist-01, Ġtype-01, Ġtype-03, Ġtypical-02, Ġuncover-01, Ġunderestimate-01, Ġundergo-28, Ġunderlie-01, Ġundermine-01, Ġunderstand-01, Ġundertake-01, Ġuniform-01, Ġunify-01, Ġunion-02, Ġunite-01, Ġup-01, Ġup-02, Ġup-03, Ġupdate-01, Ġupgrade-01, Ġupgrade-02, Ġuphold-01, Ġupset-01, Ġurge-01, Ġurl-entity, Ġuse-01, Ġuse-02, Ġuseful-05, Ġusher-in-01, Ġutilize-01, Ġvacation-01, Ġvalid-02, Ġvalidate-01, Ġvalue-01, Ġvalue-02, Ġvary-01, Ġvent-01, Ġverify-01, Ġveto-01, Ġvictimize-01, Ġview-01, Ġview-02, Ġvile-02, Ġviolate-01, Ġvisit-01, Ġvoice-01, Ġvoluntary-02, Ġvolunteer-01, Ġvote-01, Ġvote-02, Ġvow-01, Ġwage-01, Ġwait-01, Ġwaive-01, Ġwake-up-02, Ġwalk-01, Ġwander-01, Ġwant-01, Ġwar-01, Ġwarm-01, Ġwarm-06, Ġwarm-07, Ġwarn-01, Ġwarrant-01, Ġwash-01, Ġwaste-01, Ġwatch-01, Ġwave-01, Ġwave-04, Ġweak-02, Ġweaken-01, Ġweaponize-01, Ġwear-01, Ġwed-01, Ġweigh-01, Ġwelcome-01, Ġwell-09, Ġwesternize-01, Ġwhine-01, Ġwhip-up-03, Ġwhite-02, Ġwhite-03, Ġwhore-01, Ġwide-02, Ġwield-01, Ġwill-02, Ġwin-01, Ġwine-01, Ġwipe-01, Ġwipe-out-02, Ġwire-01, Ġwish-01, Ġwithdraw-01, Ġwithhold-01, Ġwitness-01, Ġwonder-01, Ġwonder-02, Ġwonderful-03, Ġword-01, Ġwork-01, Ġwork-06, Ġwork-07, Ġwork-09, Ġwork-12, Ġwork-out-02, Ġworld-region, Ġworry-01, Ġworry-02, Ġworsen-01, Ġworship-01, Ġworth-01, Ġworth-02, Ġwound-01, Ġwow-01, Ġwrap-01, Ġwreck-01, Ġwrite-01, Ġwrong-02, Ġwrong-04, Ġyell-01, Ġyield-01, Ġzap-01"
https://github.com/hankcs/HanLP,penman_interface.py,0,0.0,33,57.89,2,3.51,3,5.26,19,33.33,0,0.0,5,0,13,0,,"DEFAULT, _get_model, _remove_wiki, amr_model, append, compact, dereify, encode, encode_, g, graph, indent, len, load, load_, loads, loads_, metadata, model, noop_model, op_model, out, penman, range, rel, remove_wiki, source, string, t, top, triples, v1, v2","penman.Graph, penman.Triple, penman.encode, penman.load, penman.loads, penman.model.Model, penman.models.amr, penman.models.noop.NoOpModel","_get_model, _remove_wiki, encode, load, loads",,"DEFAULT, amr_model, graph, metadata, model, noop_model, op_model, out, rel, t, triples, v1, v2",,"+, :wiki","1, False, None","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,postprocessing.py,0,0.0,152,61.04,41,16.47,8,3.21,48,19.28,0,0.0,10,1,93,0,,"BACKOFF, FIXED, INIT, OK, ParsedStatus, ValueError, _reconstruct_graph_from_nodes, _split_name_ops, add, add_edge, addition, append, backreferences, backreferences_addition, build_graph, callable, check, cnt, collections, conn_set, connect_graph_if_not_connected, convert_tokens_to_string, copy, current_token_i, decode_into_node_and_backreferences, decoder, edge, edge_index, edges, edges_nodes_slice, edges_oth, element, encoded, encoder, end, endswith, enum, enumerate, eval, get, graph, graph_, hanlp, idx, index, index2variable, index_of, is_arg, is_pointer, isdigit, isinstance, item, items, iterable, len, list, lit, lit_end, lit_start, lits, lstrip, match, max, metadata, min, name_vars, name_vars_to_ops, nb, networkx, new_backreferences, new_nodes, new_triples, ni, node, nodes, nodes_oth, num, nv, nxgraph, old_backreferences, old_start_index, old_tok, old_tokens, ops, oth, other, penman, pointer2i, pop, prev_pointer, range, re, rel, removed, replace, res, res_tok, restore_backreferences_from_pointers, restore_name_ops, ret, rex_arg, rex_spc, separate_edges_nodes, set, shift, sorted, split, src_backr, src_node, src_var, src_var_i, start, start_index, start_search, startswith, stop_index, str, strip, subtok, subtoken_backreferences, subtoken_ids, subtokens, subw_backr, subw_i, subword_to_token_map, tok, token_addition, token_processing, token_to_token_map, tokenizer, tokens, trg, trg_edges, trg_nodes, trg_nodes_backr, trg_nodes_edges, trg_nodes_edges_backr, trg_nodes_edges_indices, trg_nodes_indices, trg_var, trg_var_i, triple, triples, triples_added, tt, update, v1, v2, variable2index, variables, x, zip","collections.Counter, collections.defaultdict, enum.Enum, hanlp.components.amr.amrbart.common.penman_interface.encode, nx.MultiGraph, nx.connected_components, penman.Graph, penman.Triple, re.compile, re.match","_reconstruct_graph_from_nodes, _split_name_ops, build_graph, check, connect_graph_if_not_connected, decode_into_node_and_backreferences, index_of, restore_backreferences_from_pointers, separate_edges_nodes, token_processing",ParsedStatus,"BACKOFF, FIXED, OK, addition, backreferences, backreferences_addition, check, cnt, conn_set, current_token_i, edge, edge_index, edges, edges_oth, encoded, end, graph, graph_, idx, index2variable, is_arg, is_pointer, item, l, lit, lit_end, lit_start, lits, metadata, name_vars, name_vars_to_ops, nb, new_backreferences, new_nodes, new_triples, ni, node, nodes, nodes_oth, num, nv, nxgraph, old_backreferences, old_start_index, old_tok, old_tokens, ops, oth, pointer2i, prev_pointer, rel, removed, res, res_tok, ret, rex_arg, rex_spc, shift, src_backr, src_node, src_var, src_var_i, start, start_index, start_search, stop_index, subtok, subtoken_backreferences, subtokens, subw_backr, subw_i, subword_to_token_map, token_addition, token_to_token_map, tokens, trg, trg_edges, trg_nodes, trg_nodes_backr, trg_nodes_edges, trg_nodes_edges_backr, trg_nodes_edges_indices, trg_nodes_indices, trg_var, trg_var_i, triple, triples, triples_added, tt, v1, v2, variable2index, variables",,", "", (, (op|snt|conj|prep), ), +, -, -of, /, :, :ARG0, :instance, :li, :mode, :op, :snt, <, <(s|/s|lit|/lit|stop|unk|pad|mask)>, </lit>, </s>, <lit>, <pad>, <pointer:, <s>, <stop>, <unk>, =, >, ^, ^[+-]?\d+\.?\d*$, _, a, abcdefghijklmnopqrstuvwxyz, and, b1, bark-01, d2, dog, name, thing, x","0, 1, 2, 3, 4, False, None, True","#, #    continue, # <lit> Barack Obama </lit> -> ""Barack Obama"", # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # Remove possible wrong None, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # TODO: this is an ugly patch due to the fact that BART tokenizer splits after ':', # The above copyright notice and this permission notice shall be included in all, # after a special token release, # after a subtoken ':' (which should be followed by the rest of the edge) ignore tokenizer.INIT, # backref can't be splitted, # check if they have ops, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # elif e.startswith(':ARG'):, # fix backreferences, # furnished to do so, subject to the following conditions:, # get strings, # identify name triples, # if empty you cannot do anything but add a new word, # in any other case attach to the previous, # in the Software without restriction, including without limitation the rights, # leading tokenizer.INIT, # more resilient logic here, # num = 0, # of this software and associated documentation files (the ""Software""), to deal, # print(""subtokens:"", subtokens), # same edge more than once, # src_var = f'{src_var}_{len(variable2index)}', # strip INIT and fix byte-level, # strip padding, # subtoken_ids.insert(-1, 4839)       # add "")"" id, # subtoken_ids.insert(1,36)           # add ""("" id, # subword collapse, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell, # tokens = [t.replace(tokenizer.INIT, '') if isinstance(t, str) else t for t in tokens], # trg_var = f'{trg_var}_{len(variable2index)}', # unks are substituted with thing, # very ugly patch for some cases in which tokenizer.INIT is not in the following token to the edge",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-05 17:53",
https://github.com/hankcs/HanLP,dataset.py,0,0.0,30,46.15,4,6.15,9,13.85,22,33.85,0,0.0,1,2,9,0,,"AMR2TextDataSet, AMRParsingDataSet, amr, amr_bos_token_id, amr_eos_token_id, as_target_tokenizer, bos_token_id, dict, eos_token_id, get, label, mask_token_id, max_src_length, max_tgt_length, object, raw_txt_ids, sample, split, src, src_ids, staticmethod, text, tgt, tgt_ids, tokenize, tokenize_amr, tokenizer, txt, txt_ids, unified_input",,tokenize,"AMR2TextDataSet, AMRParsingDataSet","amr, label, raw_txt_ids, src, src_ids, tgt, tgt_ids, txt, txt_ids",,"input_ids, labels, src, tgt","1, 1024, 2, 3, 400, 5, False, None, True","#, # AMR tokens, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # Text tokens, # The above copyright notice and this permission notice shall be included in all, # [<s>[mask]</s><AMR>xxx</AMR>], # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-07 14:36",
https://github.com/hankcs/HanLP,modeling_bart.py,0,0.0,301,63.77,76,16.1,19,4.03,76,16.1,0,0.0,25,15,111,20,,"BART_GENERATION_EXAMPLE, BART_INPUTS_DOCSTRING, BART_PRETRAINED_MODEL_ARCHIVE_LIST, BART_START_DOCSTRING, BartAttention, BartClassificationHead, BartDecoder, BartDecoderLayer, BartDecoderWrapper, BartEncoder, BartEncoderLayer, BartForCausalLM, BartForConditionalGeneration, BartForQuestionAnswering, BartForSequenceClassification, BartLearnedPositionalEmbedding, BartModel, BartPretrainedModel, Dropout, Embedding, FutureWarning, LayerNorm, Linear, Module, ModuleList, NotImplementedError, PretrainedBartModel, ValueError, _CHECKPOINT_FOR_DOC, _CHECKPOINT_FOR_QA, _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION, _CONFIG_FOR_DOC, _EXPECTED_OUTPUT_SHAPE, _QA_EXPECTED_LOSS, _QA_EXPECTED_OUTPUT, _SEQ_CLASS_EXPECTED_LOSS, _SEQ_CLASS_EXPECTED_OUTPUT, _TOKENIZER_FOR_DOC, __class__, __init__, __init_subclass__, __name__, _expand_mask, _init_weights, _keys_to_ignore_on_load_missing, _keys_to_ignore_on_load_unexpected, _make_causal_mask, _prepare_decoder_attention_mask, _reorder_cache, _resize_final_logits_bias, _set_gradient_checkpointing, _shape, activation_dropout, activation_fn, activation_function, all_attentions, all_cross_attentions, all_hidden_states, all_self_attns, args, attention_dropout, attention_mask, attentions, attn_mask, attn_output, attn_probs, attn_weights, attn_weights_reshaped, base_model_prefix, beam_idx, bias, bsz, clamp, clamp_value, classification_head, classifier_dropout, combined_attention_mask, config, config_class, copy, create_custom_forward, cross_attentions, cross_attn_head_mask, cross_attn_layer_head_mask, cross_attn_past_key_value, cross_attn_present_key_value, cross_attn_weights, custom_forward, d_model, data, decoder, decoder_attention_heads, decoder_attention_mask, decoder_attentions, decoder_ffn_dim, decoder_head_mask, decoder_hidden_states, decoder_input_ids, decoder_inputs_embeds, decoder_layer, decoder_layerdrop, decoder_layers, decoder_outputs, decoder_start_token_id, dense, device, dropout, dropout_probability, dtype, dummy_inputs, embed_dim, embed_pos, embed_positions, embed_scale, embed_tokens, embedding_dim, encoder, encoder_attention_heads, encoder_attention_mask, encoder_attentions, encoder_attn, encoder_attn_layer_norm, encoder_ffn_dim, encoder_hidden_states, encoder_last_hidden_state, encoder_layer, encoder_layerdrop, encoder_layers, encoder_outputs, encoder_states, end_logits, end_loss, end_positions, enumerate, eos_mask, eos_token_id, eq, expanded_attn_mask, expanded_mask, extra_bias, fc1, fc2, final_layer_norm, final_logits_bias, float, forward, functional, get_decoder, get_encoder, get_input_embeddings, get_logger, get_output_embeddings, gradient_checkpointing, head_dim, head_mask, hidden_size, hidden_states, idx, ignored_index, index_select, init_std, inner_dim, input_dim, input_ids, input_ids_shape, input_shape, inputs, inputs_embeds, int, inverted_mask, is_cross_attention, is_decoder, is_encoder_decoder, isinstance, k_proj, key_states, key_value_states, kwargs, labels, last_hidden_state, layer_head_mask, layer_outputs, layer_past, layerdrop, layernorm_embedding, layers, len, lm_head, lm_logits, logger, logits, loss, loss_fct, mask, mask_cond, mask_name, masked_fill, masked_fill_, masked_lm_loss, math, max_position_embeddings, max_source_positions, max_target_positions, model, module, ne, new_bias, new_embeddings, new_num_tokens, new_ones, new_zeros, next_cache, next_decoder_cache, normal_, num_classes, num_embeddings, num_heads, num_labels, offset, old_num_tokens, out_proj, output_attentions, output_hidden_states, pad_token, pad_token_id, padding_idx, past, past_key_value, past_key_values, past_key_values_length, past_state, pooler_dropout, positions, post_init, prepare_decoder_input_ids_from_labels, prepare_inputs_for_generation, present_key_value, problem_type, proj_shape, property, q_proj, qa_outputs, query_states, random, register_buffer, reordered_past, reshape, residual, resize_token_embeddings, return_dict, scale_embedding, scaling, self, self_attn, self_attn_layer_norm, self_attn_past_key_value, self_attn_weights, sentence_representation, seq_len, sequence_output, set_decoder, set_input_embeddings, set_output_embeddings, shape, shared, shift_tokens_right, shifted_input_ids, size, softmax, split, squeeze, src_len, start_logits, start_loss, start_positions, staticmethod, std, sum, super, supports_gradient_checkpointing, tgt_len, to, torch, total_loss, training, transformers, transpose, tuple, typing, use_cache, use_return_dict, v_proj, value, value_states, view, vocab_size, warning, warnings, weight, zero_, zip","copy.deepcopy, math.sqrt, random.uniform, torch.FloatTensor, torch.LongTensor, torch.Size, torch.Tensor, torch.arange, torch.bmm, torch.bool, torch.cat, torch.clamp, torch.dtype, torch.finfo, torch.float16, torch.full, torch.int, torch.isinf, torch.isnan, torch.long, torch.nn, torch.nn.BCEWithLogitsLoss, torch.nn.CrossEntropyLoss, torch.nn.MSELoss, torch.tanh, torch.tensor, torch.unique_consecutive, torch.utils.checkpoint.checkpoint, torch.zeros, transformers.activations.ACT2FN, transformers.modeling_outputs.BaseModelOutput, transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions, transformers.modeling_outputs.CausalLMOutputWithCrossAttentions, transformers.modeling_outputs.Seq2SeqLMOutput, transformers.modeling_outputs.Seq2SeqModelOutput, transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput, transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput, transformers.modeling_utils.PreTrainedModel, transformers.models.bart.configuration_bart.BartConfig, transformers.utils.add_code_sample_docstrings, transformers.utils.add_end_docstrings, transformers.utils.add_start_docstrings, transformers.utils.add_start_docstrings_to_model_forward, transformers.utils.logging, transformers.utils.replace_return_docstrings, typing.List, typing.Optional, typing.Tuple, typing.Union, warnings.warn","__init__, __init_subclass__, _expand_mask, _init_weights, _make_causal_mask, _prepare_decoder_attention_mask, _reorder_cache, _resize_final_logits_bias, _set_gradient_checkpointing, _shape, create_custom_forward, custom_forward, dummy_inputs, forward, get_decoder, get_encoder, get_input_embeddings, get_output_embeddings, prepare_decoder_input_ids_from_labels, prepare_inputs_for_generation, resize_token_embeddings, set_decoder, set_input_embeddings, set_output_embeddings, shift_tokens_right","BartAttention, BartClassificationHead, BartDecoder, BartDecoderLayer, BartDecoderWrapper, BartEncoder, BartEncoderLayer, BartForCausalLM, BartForConditionalGeneration, BartForQuestionAnswering, BartForSequenceClassification, BartLearnedPositionalEmbedding, BartModel, BartPretrainedModel, PretrainedBartModel","BART_GENERATION_EXAMPLE, BART_INPUTS_DOCSTRING, BART_PRETRAINED_MODEL_ARCHIVE_LIST, BART_START_DOCSTRING, _CHECKPOINT_FOR_DOC, _CHECKPOINT_FOR_QA, _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION, _CONFIG_FOR_DOC, _EXPECTED_OUTPUT_SHAPE, _QA_EXPECTED_LOSS, _QA_EXPECTED_OUTPUT, _SEQ_CLASS_EXPECTED_LOSS, _SEQ_CLASS_EXPECTED_OUTPUT, _TOKENIZER_FOR_DOC, _keys_to_ignore_on_load_missing, _keys_to_ignore_on_load_unexpected, all_attentions, all_cross_attentions, all_hidden_states, all_self_attns, attention_mask, attn_mask, attn_output, attn_probs, attn_weights, attn_weights_reshaped, base_model_prefix, bsz, clamp_value, combined_attention_mask, config, config_class, cross_attn_past_key_value, cross_attn_present_key_value, cross_attn_weights, decoder_input_ids, decoder_layer, decoder_outputs, dropout_probability, dummy_inputs, embed_dim, embed_pos, encoder_attention_mask, encoder_layer, encoder_outputs, encoder_states, end_logits, end_loss, end_positions, eos_mask, expanded_attn_mask, expanded_mask, extra_bias, hidden_states, idx, ignored_index, input_ids, input_shape, inputs_embeds, inverted_mask, is_cross_attention, key_states, layer_outputs, layer_past, lm_logits, logger, logits, loss, loss_fct, mask, mask_cond, mask_name, masked_lm_loss, new_bias, new_embeddings, next_cache, next_decoder_cache, old_num_tokens, output, output_attentions, output_hidden_states, outputs, pad_token, padding_idx, past_key_value, past_key_values_length, past_state, positions, present_key_value, proj_shape, query_states, reordered_past, residual, return_dict, self_attn_past_key_value, self_attn_weights, sentence_representation, seq_len, sequence_output, shifted_input_ids, src_len, start_logits, start_loss, start_positions, std, supports_gradient_checkpointing, tgt_len, total_loss, use_cache, value_states, vocab_size","Args:
    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
    attention_mask (`torch.FloatTensor`): attention mask of size
        `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
    encoder_hidden_states (`torch.FloatTensor`):
        cross attention input to the layer of shape `(batch, seq_len, embed_dim)`
    encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
        `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
    layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
        `(encoder_attention_heads,)`.
    cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of
        size `(decoder_attention_heads,)`.
    past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under
        returned tensors for more detail., Args:
    hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
    attention_mask (`torch.FloatTensor`): attention mask of size
        `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
    layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
        `(encoder_attention_heads,)`.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under
        returned tensors for more detail., Args:
    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
        Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
        provide it.

        Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
        [`PreTrainedTokenizer.__call__`] for details.

        [What are input IDs?](../glossary#input-ids)
    attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.

        [What are attention masks?](../glossary#attention-mask)
    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
        if the model is configured as a decoder.
    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used
        in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:
    head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
        shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
        shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional
        tensors are only required when the model is used as a decoder in a Sequence to Sequence model.

        Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
        cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
        that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
        all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
        config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
        (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
    use_cache (`bool`, *optional*):
        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
        (see `past_key_values`).

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under
        returned tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
        for more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.

Returns:

Example:

```python
>>> from transformers import BartTokenizer, BartForCausalLM

>>> tokenizer = BartTokenizer.from_pretrained(""facebook/bart-base"")
>>> model = BartForCausalLM.from_pretrained(""facebook/bart-base"", add_cross_attention=False)
>>> assert model.config.is_decoder, f""{model.__class__} has to be configured as a decoder.""
>>> inputs = tokenizer(""Hello, my dog is cute"", return_tensors=""pt"")
>>> outputs = model(**inputs)

>>> logits = outputs.logits
>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]
>>> list(logits.shape) == expected_shape
True
```, Args:
    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
        Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
        provide it.

        Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
        [`PreTrainedTokenizer.__call__`] for details.

        [What are input IDs?](../glossary#input-ids)
    attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.

        [What are attention masks?](../glossary#attention-mask)
    encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):
        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
        of the decoder.
    encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):
        Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
        selected in `[0, 1]`:

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.

        [What are attention masks?](../glossary#attention-mask)
    head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing
        cross-attention on hidden heads. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
        shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
        shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

        Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
        cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
        that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
        all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of
        shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing
        `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more
        control over how to convert `input_ids` indices into associated vectors than the model's internal
        embedding lookup matrix.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under
        returned tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
        for more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple., Args:
    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
        Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
        provide it.

        Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
        [`PreTrainedTokenizer.__call__`] for details.

        [What are input IDs?](../glossary#input-ids)
    attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

        - 1 for tokens that are **not masked**,
        - 0 for tokens that are **masked**.

        [What are attention masks?](../glossary#attention-mask)
    head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
        Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

        - 1 indicates the head is **not masked**,
        - 0 indicates the head is **masked**.

    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
        This is useful if you want more control over how to convert `input_ids` indices into associated vectors
        than the model's internal embedding lookup matrix.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under
        returned tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
        for more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple., Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`., Head for sentence-level classification tasks., Input shape: Batch x Time x Channel, Make causal mask used for bi-directional self-attention., Multi-headed attention from 'Attention Is All You Need' paper, PyTorch BART model., Shift input ids one token to the right., This module learns positional embeddings up to a fixed maximum size., This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is
used in combination with the [`EncoderDecoderModel`] framework., Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]

Args:
    config: BartConfig
    embed_tokens (nn.Embedding): output embedding, Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a
[`BartEncoderLayer`].

Args:
    config: BartConfig
    embed_tokens (nn.Embedding): output embedding, `input_ids_shape` is expected to be [bsz x seqlen]., labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
    Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
    config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
    (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

Returns:, labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
    config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy)., start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
    Labels for position (index) of the start of the labelled span for computing the token classification loss.
    Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
    are not taken into account for computing the loss.
end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
    Labels for position (index) of the end of the labelled span for computing the token classification loss.
    Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
    are not taken into account for computing the loss.","
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            encoder_hidden_states (`torch.FloatTensor`):
                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`
            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of
                size `(decoder_attention_heads,)`.
            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        , 
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        , 
        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
                provide it.

                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
                [`PreTrainedTokenizer.__call__`] for details.

                [What are input IDs?](../glossary#input-ids)
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

                [What are attention masks?](../glossary#attention-mask)
            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
                if the model is configured as a decoder.
            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used
                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:
            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.

            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.

            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional
                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.

                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.

        Returns:

        Example:

        ```python
        >>> from transformers import BartTokenizer, BartForCausalLM

        >>> tokenizer = BartTokenizer.from_pretrained(""facebook/bart-base"")
        >>> model = BartForCausalLM.from_pretrained(""facebook/bart-base"", add_cross_attention=False)
        >>> assert model.config.is_decoder, f""{model.__class__} has to be configured as a decoder.""
        >>> inputs = tokenizer(""Hello, my dog is cute"", return_tensors=""pt"")
        >>> outputs = model(**inputs)

        >>> logits = outputs.logits
        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]
        >>> list(logits.shape) == expected_shape
        True
        ```, 
        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
                provide it.

                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
                [`PreTrainedTokenizer.__call__`] for details.

                [What are input IDs?](../glossary#input-ids)
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

                [What are attention masks?](../glossary#attention-mask)
            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):
                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
                of the decoder.
            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):
                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
                selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

                [What are attention masks?](../glossary#attention-mask)
            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.

            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing
                cross-attention on hidden heads. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.

            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of
                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing
                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more
                control over how to convert `input_ids` indices into associated vectors than the model's internal
                embedding lookup matrix.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        , 
        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
                provide it.

                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
                [`PreTrainedTokenizer.__call__`] for details.

                [What are input IDs?](../glossary#input-ids)
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

                [What are attention masks?](../glossary#attention-mask)
            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked**.

            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
                This is useful if you want more control over how to convert `input_ids` indices into associated vectors
                than the model's internal embedding lookup matrix.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        , 
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:
        , 
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        , 
        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the start of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
            are not taken into account for computing the loss.
        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the end of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence
            are not taken into account for computing the loss.
        , 
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)
        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Indices of decoder input sequence tokens in the vocabulary.

            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are decoder input IDs?](../glossary#decoder-input-ids)

            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).

            For translation and summarization training, `decoder_input_ids` should be provided. If no
            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
            for denoising pre-training following the paper.
        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
            be used by default.

            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.
        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
            1]`:

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.

        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape
            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you
            can choose to directly pass an embedded representation. This is useful if you want more control over how to
            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
            input (see `past_key_values`). This is useful if you want more control over how to convert
            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.

            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
            of `inputs_embeds`.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
, 
    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).
    , 
    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE
    tasks.
    , 
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    , 
    Make causal mask used for bi-directional self-attention.
    , 
    Shift input ids one token to the right.
    , 
    Summarization example:

    ```python
    >>> from transformers import BartTokenizer, BartForConditionalGeneration

    >>> model = BartForConditionalGeneration.from_pretrained(""facebook/bart-large-cnn"")
    >>> tokenizer = BartTokenizer.from_pretrained(""facebook/bart-large-cnn"")

    >>> ARTICLE_TO_SUMMARIZE = (
    ...     ""PG&E stated it scheduled the blackouts in response to forecasts for high winds ""
    ...     ""amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were ""
    ...     ""scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.""
    ... )
    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=""pt"")

    >>> # Generate Summary
    >>> summary_ids = model.generate(inputs[""input_ids""], num_beams=2, min_length=0, max_length=20)
    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'
    ```

    Mask filling example:

    ```python
    >>> from transformers import BartTokenizer, BartForConditionalGeneration

    >>> tokenizer = BartTokenizer.from_pretrained(""facebook/bart-base"")
    >>> model = BartForConditionalGeneration.from_pretrained(""facebook/bart-base"")

    >>> TXT = ""My friends are <mask> but they eat too many carbs.""
    >>> input_ids = tokenizer([TXT], return_tensors=""pt"")[""input_ids""]
    >>> logits = model(input_ids).logits

    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
    >>> probs = logits[0, masked_index].softmax(dim=0)
    >>> values, predictions = probs.topk(5)

    >>> tokenizer.decode(predictions).split()
    ['not', 'good', 'healthy', 'great', 'very']
    ```
, 
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`BartConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
, 
    This module learns positional embeddings up to a fixed maximum size.
    , 
    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is
    used in combination with the [`EncoderDecoderModel`] framework.
    , 
    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]

    Args:
        config: BartConfig
        embed_tokens (nn.Embedding): output embedding
    , 
    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a
    [`BartEncoderLayer`].

    Args:
        config: BartConfig
        embed_tokens (nn.Embedding): output embedding
    ,  PyTorch BART model.,  and `num_heads`: ,  layers, but it is for , ' nice puppet', 'POSITIVE', )., , but is , ., All examples must have the same number of <eos> tokens., Attention mask should be of size , Attention weights should be of size , BartConfig, BartTokenizer, Head for sentence-level classification tasks., Head mask for a single layer should be of size , If no `decoder_input_ids` or `decoder_inputs_embeds` are passed, `input_ids` cannot be `None`. Please pass either `input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`., Input shape: Batch x Time x Channel, Multi-headed attention from 'Attention Is All You Need' paper, Passing input embeddings is currently not supported for , The BART Model with a language modeling head. Can be used for summarization., The `, The `use_cache` argument is changed to `False` since `labels` is provided., The bare BART Model outputting raw hidden-states without any specific head on top., The class `PretrainedBartModel` has been depreciated, please use `BartPretrainedModel` instead., The head_mask should be specified for , You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time, You cannot specify both input_ids and inputs_embeds at the same time, You have to specify either decoder_input_ids or decoder_inputs_embeds, You have to specify either input_ids or inputs_embeds, ` should be specified for , `attn_output` should be of size , `input_ids_shape` is expected to be [bsz x seqlen]., `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..., attention_mask, cross_attn_head_mask, decoder.version, decoder_head_mask, decoder_input_ids, embed_dim must be divisible by num_heads (got `embed_dim`: , encoder.version, encoder_outputs, facebook/bart-base, facebook/bart-large, final_logits_bias, head_mask, input_ids, lm_head.weight, model, multi_label_classification, past_key_values, regression, self.model.config.pad_token_id has to be defined., single_label_classification, use_cache, valhalla/bart-large-finetuned-squadv1, valhalla/bart-large-sst2","0, 0.0, 0.5, 0.59, 1, 1.0, 10, 100, 1000, 12, 2, 3, 4, 6, 768, 8, False, None, True","

BART_INPUTS_DOCSTRING = r""""""
Args:
input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.

Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
[`PreTrainedTokenizer.__call__`] for details.

[What are input IDs?](../glossary#input-ids)
attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

- 1 for tokens that are **not masked**,
- 0 for tokens that are **masked**.

[What are attention masks?](../glossary#attention-mask)
decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
Indices of decoder input sequence tokens in the vocabulary.

Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
[`PreTrainedTokenizer.__call__`] for details.

[What are decoder input IDs?](../glossary#decoder-input-ids)

Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`
is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).

For translation and summarization training, `decoder_input_ids` should be provided. If no
`decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right
for denoising pre-training following the paper.
decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):
Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also
be used by default.

If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]
and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
information on the default strategy.
head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:

- 1 indicates the head is **not masked**,
- 0 indicates the head is **masked**.

decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:

- 1 indicates the head is **not masked**,
- 0 indicates the head is **masked**.

cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,
1]`:

- 1 indicates the head is **not masked**,
- 0 indicates the head is **masked**.

encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):
Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
`last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
`decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape
`(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you
can choose to directly pass an embedded representation. This is useful if you want more control over how to
convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.
decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):
Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded
representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be
input (see `past_key_values`). This is useful if you want more control over how to convert
`decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.

If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value
of `inputs_embeds`.
use_cache (`bool`, *optional*):
If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
`past_key_values`).
output_attentions (`bool`, *optional*):
Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
tensors for more detail.
output_hidden_states (`bool`, *optional*):
Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
more detail.
return_dict (`bool`, *optional*):
Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
, 

def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):
super().__init__(config)
self.dropout = config.dropout
self.layerdrop = config.decoder_layerdrop
self.padding_idx = config.pad_token_id
self.max_target_positions = config.max_position_embeddings
self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0

if embed_tokens is not None:
self.embed_tokens = embed_tokens
else:
self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)

self.embed_positions = BartLearnedPositionalEmbedding(
config.max_position_embeddings,
config.d_model,
)
self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])
self.layernorm_embedding = nn.LayerNorm(config.d_model)

self.gradient_checkpointing = False
# Initialize weights and apply final processing
self.post_init()

def get_input_embeddings(self):
return self.embed_tokens

def set_input_embeddings(self, value):
self.embed_tokens = value

def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):
# create causal mask
# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
combined_attention_mask = None
if input_shape[-1] > 1:
combined_attention_mask = _make_causal_mask(
input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length
).to(inputs_embeds.device)

if attention_mask is not None:
# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])
combined_attention_mask = (
expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask
)

return combined_attention_mask

def forward(
self,
input_ids: torch.LongTensor = None,
attention_mask: Optional[torch.Tensor] = None,
encoder_hidden_states: Optional[torch.FloatTensor] = None,
encoder_attention_mask: Optional[torch.LongTensor] = None,
head_mask: Optional[torch.Tensor] = None,
cross_attn_head_mask: Optional[torch.Tensor] = None,
past_key_values: Optional[List[torch.FloatTensor]] = None,
inputs_embeds: Optional[torch.FloatTensor] = None,
use_cache: Optional[bool] = None,
output_attentions: Optional[bool] = None,
output_hidden_states: Optional[bool] = None,
return_dict: Optional[bool] = None,
) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:
r""""""
Args:
input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.

Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and
[`PreTrainedTokenizer.__call__`] for details.

[What are input IDs?](../glossary#input-ids)
attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

- 1 for tokens that are **not masked**,
- 0 for tokens that are **masked**.

[What are attention masks?](../glossary#attention-mask)
encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
of the decoder.
encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):
Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values
selected in `[0, 1]`:

- 1 for tokens that are **not masked**,
- 0 for tokens that are **masked**.

[What are attention masks?](../glossary#attention-mask)
head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

- 1 indicates the head is **not masked**,
- 0 indicates the head is **masked**.

cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):
Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing
cross-attention on hidden heads. Mask values selected in `[0, 1]`:

- 1 indicates the head is **not masked**,
- 0 indicates the head is **masked**.

past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those
that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of
all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of
shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing
`input_ids` you can choose to directly pass an embedded representation. This is useful if you want more
control over how to convert `input_ids` indices into associated vectors than the model's internal
embedding lookup matrix.
output_attentions (`bool`, *optional*):
Whether or not to return the attentions tensors of all attention layers. See `attentions` under
returned tensors for more detail.
output_hidden_states (`bool`, *optional*):
Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
for more detail.
return_dict (`bool`, *optional*):
Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
, 

def __init__(self, num_embeddings: int, embedding_dim: int):
# Bart is set up so that if padding_idx is specified then offset the embedding ids by 2
# and adjust num_embeddings appropriately. Other models don't have this hack
self.offset = 2
super().__init__(num_embeddings + self.offset, embedding_dim)

def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):
`input_ids_shape` is expected to be [bsz x seqlen]., 
Args:
hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
attention_mask (`torch.FloatTensor`): attention mask of size
`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
encoder_hidden_states (`torch.FloatTensor`):
cross attention input to the layer of shape `(batch, seq_len, embed_dim)`
encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
`(encoder_attention_heads,)`.
cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of
size `(decoder_attention_heads,)`.
past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states
output_attentions (`bool`, *optional*):
Whether or not to return the attentions tensors of all attention layers. See `attentions` under
returned tensors for more detail.
, 
Args:
hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
attention_mask (`torch.FloatTensor`): attention mask of size
`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
`(encoder_attention_heads,)`.
output_attentions (`bool`, *optional*):
Whether or not to return the attentions tensors of all attention layers. See `attentions` under
returned tensors for more detail.
, 
BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layer on top of the hidden-states output to compute `span start logits` and `span end logits`).
,, 
Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a
[`BartEncoderLayer`].

Args:
config: BartConfig
embed_tokens (nn.Embedding): output embedding
, 
bsz, src_len = mask.size()
tgt_len = tgt_len if tgt_len is not None else src_len

expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)

inverted_mask = 1.0 - expanded_mask

return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)


class BartLearnedPositionalEmbedding(nn.Embedding):
, 
bsz, tgt_len = input_ids_shape
mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))
mask_cond = torch.arange(mask.size(-1))
mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
mask = mask.to(dtype)

if past_key_values_length > 0:
mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)
return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)


def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
, 
output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
output_hidden_states = (
output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
)
return_dict = return_dict if return_dict is not None else self.config.use_return_dict

# retrieve input_ids and inputs_embeds
if input_ids is not None and inputs_embeds is not None:
raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
elif input_ids is not None:
input_shape = input_ids.size()
input_ids = input_ids.view(-1, input_shape[-1])
elif inputs_embeds is not None:
input_shape = inputs_embeds.size()[:-1]
else:
raise ValueError(""You have to specify either input_ids or inputs_embeds"")

if inputs_embeds is None:
inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale

embed_pos = self.embed_positions(input_shape)

hidden_states = inputs_embeds + embed_pos
hidden_states = self.layernorm_embedding(hidden_states)
hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

# expand attention_mask
if attention_mask is not None:
# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)

encoder_states = () if output_hidden_states else None
all_attentions = () if output_attentions else None

# check if head_mask has a correct number of layers specified if desired
if head_mask is not None:
if head_mask.size()[0] != (len(self.layers)):
raise ValueError(
f""The head_mask should be specified for {len(self.layers)} layers, but it is for""
f"" {head_mask.size()[0]}.""
)

for idx, encoder_layer in enumerate(self.layers):
if output_hidden_states:
encoder_states = encoder_states + (hidden_states,)
# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
dropout_probability = random.uniform(0, 1)
if self.training and (dropout_probability < self.layerdrop):  # skip the layer
layer_outputs = (None, None)
else:
if self.gradient_checkpointing and self.training:

def create_custom_forward(module):
def custom_forward(*inputs):
return module(*inputs, output_attentions)

return custom_forward

layer_outputs = torch.utils.checkpoint.checkpoint(
create_custom_forward(encoder_layer),
hidden_states,
attention_mask,
(head_mask[idx] if head_mask is not None else None),
)
else:
layer_outputs = encoder_layer(
hidden_states,
attention_mask,
layer_head_mask=(head_mask[idx] if head_mask is not None else None),
output_attentions=output_attentions,
)

hidden_states = layer_outputs[0]

if output_attentions:
all_attentions = all_attentions + (layer_outputs[1],)

if output_hidden_states:
encoder_states = encoder_states + (hidden_states,)

if not return_dict:
return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)
return BaseModelOutput(
last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions
)


class BartDecoder(BartPretrainedModel):
, 
return_dict = return_dict if return_dict is not None else self.config.use_return_dict

if labels is not None:
if use_cache:
logger.warning(""The `use_cache` argument is changed to `False` since `labels` is provided."")
use_cache = False
if decoder_input_ids is None and decoder_inputs_embeds is None:
decoder_input_ids = shift_tokens_right(
labels, self.config.pad_token_id, self.config.decoder_start_token_id
)

outputs = self.model(
input_ids,
attention_mask=attention_mask,
decoder_input_ids=decoder_input_ids,
encoder_outputs=encoder_outputs,
decoder_attention_mask=decoder_attention_mask,
head_mask=head_mask,
decoder_head_mask=decoder_head_mask,
cross_attn_head_mask=cross_attn_head_mask,
past_key_values=past_key_values,
inputs_embeds=inputs_embeds,
decoder_inputs_embeds=decoder_inputs_embeds,
use_cache=use_cache,
output_attentions=output_attentions,
output_hidden_states=output_hidden_states,
return_dict=return_dict,
)
lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias

masked_lm_loss = None
if labels is not None:
loss_fct = CrossEntropyLoss()
masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))

if not return_dict:
output = (lm_logits,) + outputs[1:]
return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output

return Seq2SeqLMOutput(
loss=masked_lm_loss,
logits=lm_logits,
past_key_values=outputs.past_key_values,
decoder_hidden_states=outputs.decoder_hidden_states,
decoder_attentions=outputs.decoder_attentions,
cross_attentions=outputs.cross_attentions,
encoder_last_hidden_state=outputs.encoder_last_hidden_state,
encoder_hidden_states=outputs.encoder_hidden_states,
encoder_attentions=outputs.encoder_attentions,
)

def prepare_inputs_for_generation(
self,
decoder_input_ids,
past=None,
attention_mask=None,
head_mask=None,
decoder_head_mask=None,
cross_attn_head_mask=None,
use_cache=None,
encoder_outputs=None,
**kwargs
):
# cut decoder_input_ids if past is used
if past is not None:
decoder_input_ids = decoder_input_ids[:, -1:]

return {
""input_ids"": None,  # encoder_outputs is defined. input_ids not needed
""encoder_outputs"": encoder_outputs,
""past_key_values"": past,
""decoder_input_ids"": decoder_input_ids,
""attention_mask"": attention_mask,
""head_mask"": head_mask,
""decoder_head_mask"": decoder_head_mask,
""cross_attn_head_mask"": cross_attn_head_mask,
""use_cache"": use_cache,  # change this to avoid caching (presumably for debugging)
}

def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):
return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)

@staticmethod
def _reorder_cache(past, beam_idx):
reordered_past = ()
for layer_past in past:
# cached cross_attention states don't have to be reordered -> they are always the same
reordered_past += (
tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],
)
return reordered_past


@add_start_docstrings(
, 
return_dict = return_dict if return_dict is not None else self.config.use_return_dict
if start_positions is not None and end_positions is not None:
use_cache = False

outputs = self.model(
input_ids,
attention_mask=attention_mask,
decoder_input_ids=decoder_input_ids,
decoder_attention_mask=decoder_attention_mask,
head_mask=head_mask,
decoder_head_mask=decoder_head_mask,
cross_attn_head_mask=cross_attn_head_mask,
encoder_outputs=encoder_outputs,
inputs_embeds=inputs_embeds,
decoder_inputs_embeds=decoder_inputs_embeds,
use_cache=use_cache,
output_attentions=output_attentions,
output_hidden_states=output_hidden_states,
return_dict=return_dict,
)

sequence_output = outputs[0]

logits = self.qa_outputs(sequence_output)
start_logits, end_logits = logits.split(1, dim=-1)
start_logits = start_logits.squeeze(-1).contiguous()
end_logits = end_logits.squeeze(-1).contiguous()

total_loss = None
if start_positions is not None and end_positions is not None:
# If we are on multi-GPU, split add a dimension
if len(start_positions.size()) > 1:
start_positions = start_positions.squeeze(-1)
if len(end_positions.size()) > 1:
end_positions = end_positions.squeeze(-1)
# sometimes the start/end positions are outside our model inputs, we ignore these terms
ignored_index = start_logits.size(1)
start_positions = start_positions.clamp(0, ignored_index)
end_positions = end_positions.clamp(0, ignored_index)

loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
start_loss = loss_fct(start_logits, start_positions)
end_loss = loss_fct(end_logits, end_positions)
total_loss = (start_loss + end_loss) / 2

if not return_dict:
output = (
start_logits,
end_logits,
) + outputs[1:]
return ((total_loss,) + output) if total_loss is not None else output

return Seq2SeqQuestionAnsweringModelOutput(
loss=total_loss,
start_logits=start_logits,
end_logits=end_logits,
past_key_values=outputs.past_key_values,
decoder_hidden_states=outputs.decoder_hidden_states,
decoder_attentions=outputs.decoder_attentions,
cross_attentions=outputs.cross_attentions,
encoder_last_hidden_state=outputs.encoder_last_hidden_state,
encoder_hidden_states=outputs.encoder_hidden_states,
encoder_attentions=outputs.encoder_attentions,
)


class BartDecoderWrapper(BartPretrainedModel):
, 
shifted_input_ids = input_ids.new_zeros(input_ids.shape)
shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
shifted_input_ids[:, 0] = decoder_start_token_id

if pad_token_id is None:
raise ValueError(""self.model.config.pad_token_id has to be defined."")
# replace possible -100 values in labels by `pad_token_id`
shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)

return shifted_input_ids


def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):
,  PyTorch BART model.
import copy
import math
import random
import warnings
from typing import List, Optional, Tuple, Union

import torch
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.modeling_outputs import (
BaseModelOutput,
BaseModelOutputWithPastAndCrossAttentions,
CausalLMOutputWithCrossAttentions,
Seq2SeqLMOutput,
Seq2SeqModelOutput,
Seq2SeqQuestionAnsweringModelOutput,
Seq2SeqSequenceClassifierOutput,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
add_code_sample_docstrings,
add_end_docstrings,
add_start_docstrings,
add_start_docstrings_to_model_forward,
logging,
replace_return_docstrings,
)
from transformers.models.bart.configuration_bart import BartConfig


logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = ""facebook/bart-base""
_CONFIG_FOR_DOC = ""BartConfig""
_TOKENIZER_FOR_DOC = ""BartTokenizer""

# Base model docstring
_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]

# SequenceClassification docstring
_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = ""valhalla/bart-large-sst2""
_SEQ_CLASS_EXPECTED_LOSS = 0.0
_SEQ_CLASS_EXPECTED_OUTPUT = ""'POSITIVE'""

# QuestionAsnwering docstring
_CHECKPOINT_FOR_QA = ""valhalla/bart-large-finetuned-squadv1""
_QA_EXPECTED_LOSS = 0.59
_QA_EXPECTED_OUTPUT = ""' nice puppet'""


BART_PRETRAINED_MODEL_ARCHIVE_LIST = [
""facebook/bart-large"",
# see all BART models at https://huggingface.co/models?filter=bart
]


def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):
, #, #     http://www.apache.org/licenses/LICENSE-2.0, # Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved., # Cross-Attention Block, # Fully Connected, # Further calls to cross_attention layer can then reuse all cross-attention, # Generate Summary, # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True, # In order to do so, attn_weights have to be reshaped, # Initialize weights and apply final processing, # Licensed under the Apache License, Version 2.0 (the ""License"");, # None for past_key_value, # See the License for the specific language governing permissions and, # Self Attention, # Unless required by applicable law or agreed to in writing, software, # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied., # You may obtain a copy of the License at, # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len], # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description), # add cross-attn to positions 3,4 of present_key_value tuple, # add hidden states from the last decoder layer, # add present self-attn cache to positions 1,2 of present_key_value tuple, # all previous decoder key/value_states. Further calls to uni-directional self-attention, # can concat previous decoder key/value_states to current projected key/value_states (third ""elif"" case), # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired, # coding=utf-8, # cross_attentions, # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple, # decoder layers, # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn), # decoder uni-directional self-attention cached key/values tuple is at positions 1,2, # different to other models, Bart automatically creates decoder_input_ids from, # distributed under the License is distributed on an ""AS IS"" BASIS,, # embed positions, # expand encoder attention mask, # for the decoder, # get key, value proj, # get query proj, # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states., # if encoder bi-directional self-attention `past_key_value` is always `None`, # if key_value_states are provided this layer is used as a cross-attention layer, # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of, # input_ids if no decoder_input_ids are provided, # key/value_states (first ""if"" case), # last hidden state, # limitations under the License., # make sure that attn_weights keeps its gradient., # partitioned aross GPUs when using tensor-parallelism., # past_key_values_length, # retrieve input_ids and inputs_embeds, # reuse k, v, self_attention, # reuse k,v, cross_attentions, # self_attention, # this operation is a bit awkward, but it's required to, # twice and have to be reused in the following, # you may not use this file except in compliance with the License., #attention-mask), #input-ids), ,
BART_START_DOCSTRING,
)
class BartForSequenceClassification(BartPretrainedModel):
def __init__(self, config: BartConfig, **kwargs):
super().__init__(config, **kwargs)
self.model = BartModel(config)
self.classification_head = BartClassificationHead(
config.d_model,
config.d_model,
config.num_labels,
config.classifier_dropout,
)
self.model._init_weights(self.classification_head.dense)
self.model._init_weights(self.classification_head.out_proj)

@add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)
@add_code_sample_docstrings(
processor_class=_TOKENIZER_FOR_DOC,
checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,
output_type=Seq2SeqSequenceClassifierOutput,
config_class=_CONFIG_FOR_DOC,
expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,
expected_loss=_SEQ_CLASS_EXPECTED_LOSS,
)
def forward(
self,
input_ids: torch.LongTensor = None,
attention_mask: Optional[torch.Tensor] = None,
decoder_input_ids: Optional[torch.LongTensor] = None,
decoder_attention_mask: Optional[torch.LongTensor] = None,
head_mask: Optional[torch.Tensor] = None,
decoder_head_mask: Optional[torch.Tensor] = None,
cross_attn_head_mask: Optional[torch.Tensor] = None,
encoder_outputs: Optional[List[torch.FloatTensor]] = None,
inputs_embeds: Optional[torch.FloatTensor] = None,
decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
labels: Optional[torch.LongTensor] = None,
use_cache: Optional[bool] = None,
output_attentions: Optional[bool] = None,
output_hidden_states: Optional[bool] = None,
return_dict: Optional[bool] = None,
) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:
r""""""
labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
, Head for sentence-level classification tasks.

def __init__(
self,
input_dim: int,
inner_dim: int,
num_classes: int,
pooler_dropout: float,
):
super().__init__()
self.dense = nn.Linear(input_dim, inner_dim)
self.dropout = nn.Dropout(p=pooler_dropout)
self.out_proj = nn.Linear(inner_dim, num_classes)

def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
hidden_states = self.dropout(hidden_states)
hidden_states = self.dense(hidden_states)
hidden_states = torch.tanh(hidden_states)
hidden_states = self.dropout(hidden_states)
hidden_states = self.out_proj(hidden_states)
return hidden_states


class BartPretrainedModel(PreTrainedModel):
config_class = BartConfig
base_model_prefix = ""model""
supports_gradient_checkpointing = True
_keys_to_ignore_on_load_unexpected = [r""encoder.version"", r""decoder.version""]

def _init_weights(self, module):
std = self.config.init_std
if isinstance(module, nn.Linear):
module.weight.data.normal_(mean=0.0, std=std)
if module.bias is not None:
module.bias.data.zero_()
elif isinstance(module, nn.Embedding):
module.weight.data.normal_(mean=0.0, std=std)
if module.padding_idx is not None:
module.weight.data[module.padding_idx].zero_()

def _set_gradient_checkpointing(self, module, value=False):
if isinstance(module, (BartDecoder, BartEncoder)):
module.gradient_checkpointing = value

@property
def dummy_inputs(self):
pad_token = self.config.pad_token_id
input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)
dummy_inputs = {
""attention_mask"": input_ids.ne(pad_token),
""input_ids"": input_ids,
}
return dummy_inputs


class PretrainedBartModel(BartPretrainedModel):
def __init_subclass__(self):
warnings.warn(
""The class `PretrainedBartModel` has been depreciated, please use `BartPretrainedModel` instead."",
FutureWarning,
)


BART_START_DOCSTRING = r""""""
This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)

This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.

Parameters:
config ([`BartConfig`]):
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
[`~PreTrainedModel.from_pretrained`] method to load the model weights.
, Multi-headed attention from 'Attention Is All You Need' paper

def __init__(
self,
embed_dim: int,
num_heads: int,
dropout: float = 0.0,
is_decoder: bool = False,
bias: bool = True,
):
super().__init__()
self.embed_dim = embed_dim
self.num_heads = num_heads
self.dropout = dropout
self.head_dim = embed_dim // num_heads

if (self.head_dim * num_heads) != self.embed_dim:
raise ValueError(
f""embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}""
f"" and `num_heads`: {num_heads}).""
)
self.scaling = self.head_dim**-0.5
self.is_decoder = is_decoder

self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

def forward(
self,
hidden_states: torch.Tensor,
key_value_states: Optional[torch.Tensor] = None,
past_key_value: Optional[Tuple[torch.Tensor]] = None,
attention_mask: Optional[torch.Tensor] = None,
layer_head_mask: Optional[torch.Tensor] = None,
output_attentions: bool = False,
) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
Input shape: Batch x Time x Channel",
https://github.com/hankcs/HanLP,tokenization_bart.py,0,0.0,128,57.4,59,26.46,11,4.93,24,10.76,1,0.45,12,1,56,1,,"AMRBartTokenizer, BACKOFF, Exception, INIT, ParsedStatus, __init__, _classify, _fix_and_make_graph, _repl1, _repl2, _tok_bpe, _tokenize, add, add_prefix_space, amr_bos_token, amr_bos_token_id, amr_eos_token, amr_eos_token_id, amr_tokens, append, args, backreferences, bb, bos_token, bpe, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, byte_encoder, char, classmethod, closed_cnt, cls_token, connect_graph_if_not_connected, decode_amr, decode_into_node_and_backreferences, decoder, encode, encoder, endswith, enumerate, eos_token, errors, exit, extend, find, findall, fix_text, fol, from_pretrained, get, graph, graph_, group, hanlp, init_amr_vocabulary, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, isinstance, items, kwargs, last, last_remap, len, linearized, lstrip, mask_token, merges_file, modified, newvars, next, node, nodes, nodes_, nxt, old_enc_size, open_cnt, out, pad_token, pat, patterns, penman, piece, pieces, pieces_, pretrained_model_path, prev, print, pst, quote, recategorizations, recats, regex, rel, remap, remove_pars, replace, rsplit, self, sep_token, set, sorted, startswith, status, str, strip, super, text, tok_span, tokenize_amr, tokk, toks, transformers, triple, triples, unk_token, unk_token_id, v, var, variables, vocab_file, x, y","hanlp.components.amr.amrbart.common.constant.raw_special_tokens, hanlp.components.amr.amrbart.common.constant.recategorizations, hanlp.components.amr.amrbart.common.penman_interface.encode, hanlp.components.amr.amrbart.common.postprocessing, penman.Graph, penman.Triple, penman.decode, re.IGNORECASE, re.MULTILINE, re.compile, re.findall, re.match, re.sub, transformers.BartTokenizer","__init__, _classify, _fix_and_make_graph, _repl1, _repl2, _tok_bpe, _tokenize, decode_amr, fix_text, from_pretrained, init_amr_vocabulary, tokenize_amr",AMRBartTokenizer,"INIT, backreferences, bb, bpe_token, bpe_token_ids, bpe_tokens, bpe_toks, char, closed_cnt, e, fol, g, graph, graph_, i, inst, is_frame, is_in_enc, is_of, is_rel, is_spc, last, last_remap, linearized, n, newvars, next, nodes, nodes_, nxt, old_enc_size, open_cnt, out, piece, pieces, pieces_, prev, pst, quote, recats, rel, remap, status, tok, tok_span, token, tokens, tokk, toks, triple, triples, v, var, variables, x, y",Tokenize a string. Modified in order to handle sentences with recategorization pointers,", 
        line = linearized
        # make sure parentheses match
        # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py
        open_count = 0
        close_count = 0
        for i, c in enumerate(line):
            if c == '(':
                open_count += 1
            elif c == ')':
                close_count += 1
            if open_count == close_count and open_count > 0:
                line = line[:i].strip()
                break
        old_line = line
        while True:
            open_count = len(re.findall(r'\(', line))
            close_count = len(re.findall(r'\)', line))
            if open_count > close_count:
                line += ')' * (open_count - close_count)
            elif close_count > open_count:
                for i in range(close_count - open_count):
                    line = line.rstrip(')')
                    line = line.rstrip(' ')
            if old_line == line:
                break
            old_line = line
        ,  ,  / ,  ?<[a-z]+:?\d*>| ?:[^\s]+|'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+,  Tokenize a string. Modified in order to handle sentences with recategorization pointers, !, "", (, ([^:])(ARG), (\(\s*[a-z][\d+]\s*\/\s*[^\s\)\(:\/]+\s*)((?:/\s*[^\s\)\(:\/]+\s*)+), (\(\s?)([a-z])([^\/:\)]+[:\)]), ), +, ,, , is_frame:, , is_of:, , is_spc:, -, -of, ., .+-\d\d, /, :, :instance, :mode, <, </AMR>, </lit>, </s>, <AMR>, <lit>, <mask>, <pad>, <pointer:, <s>, <unk>, =, >, ?, CONST, EDGE, I, INST, MODE, VAR, \, \1 :\2, \s+, ^[a-z]\d*$, _, i, is_rel:, replace, thing, tok:, utf-8, z, Ġ","0, 1, 1000, 2, 2000, 3, 3000, 9, False, None, True"," Tokenize a string. Modified in order to handle sentences with recategorization pointers
bpe_tokens = []
for tok_span in text.lstrip().split(' '):
tok_span = tok_span.strip()
recats = tok_span.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
bpe_tokens.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for token in re.findall(self.pat, ' ' + tok_span):
token = """".join(
self.byte_encoder[b] for b in token.encode(""utf-8"")
)   # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)
bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split("" ""))

return bpe_tokens

def _tok_bpe(self, token):
tokk = []
tok = token.strip()
recats = tok.rsplit('_', 1)
if len(recats) == 2 and recats[0] in self.recategorizations and ('_' + recats[1]) in self.encoder:
tokk.extend([self.INIT + recats[0], '_' + recats[1]])
else:
for tok in self.patterns.findall(' ' + token):
tok = """".join(
self.byte_encoder[b] for b in tok.encode(""utf-8""))
toks = self.bpe(tok).split(' ')
tokk.extend(toks)
return tokk

def tokenize_amr(self, amr_tokens):
bpe_tokens = []
for i, tokk in enumerate(amr_tokens):
is_in_enc = self.INIT + tokk in self.encoder
is_rel = tokk.startswith(':') and len(tokk) > 1
is_spc = tokk.startswith('<') and tokk.endswith('>')
is_of = tokk.startswith(':') and tokk.endswith('-of')
is_frame = re.match(r'.+-\d\d', tokk) is not None

if tokk.startswith('""') and tokk.endswith('""'):                 # dealing with examples like ""The_United_Kingdom_of_xxx""
tokk = tokk[1:-1].replace('_', ' ')
bpe_toks = [self.INIT + ""<lit>""]
bpe_toks += self._tok_bpe(tokk)
bpe_toks.append(self.INIT + ""</lit>"")

elif (is_rel or is_spc or is_frame or is_of):
if is_in_enc:
bpe_toks = [self.INIT + tokk]
elif is_frame:
bpe_toks = self._tok_bpe(tokk[:-3]) + [tokk[-3:]]
elif is_of:
rel = tokk[:-3]
if self.INIT + rel in self.encoder:
bpe_toks = [self.INIT + rel, '-of']
else:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(rel[1:]) + ['-of']
elif is_rel:
bpe_toks = [self.INIT + ':'] + self._tok_bpe(tokk[1:])
else:
print(""tok:"", tokk)
print(f""is_rel:{is_rel}, is_spc:{is_spc}, is_frame:{is_frame}, is_of:{is_of}"")
exit()
raise
else:
if is_in_enc:
bpe_toks = [self.INIT + tokk]
else:
bpe_toks = self._tok_bpe(tokk)

bpe_tokens.append(bpe_toks)
bpe_tokens = [b for bb in bpe_tokens for b in bb]
bpe_token_ids = [self.encoder.get(b, self.unk_token_id) for b in bpe_tokens]
return bpe_token_ids

def decode_amr(self, tokens, restore_name_ops=None):
try:
nodes, backreferences = postprocessing.decode_into_node_and_backreferences(tokens, self)
except Exception as e:
# print('Decoding failure:', file=sys.stderr)
# print(e, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
try:
graph_ = graph = self._fix_and_make_graph(nodes)
# if collapse_name_ops:
#     graph_ = graph = postprocessing._split_name_ops(graph)
except Exception as e:
# print('Building failure:', file=sys.stderr)
# print(nodes, file=sys.stderr)
# print(backreferences, file=sys.stderr)
# print(e, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (None, None)
try:
graph, status = postprocessing.connect_graph_if_not_connected(graph)
# if status == postprocessing.ParsedStatus.BACKOFF:
#     print('Reconnection 1 failure:')
#     print(nodes, file=sys.stderr)
#     print(backreferences, file=sys.stderr)
#     print(graph_, file=sys.stderr)
return graph, status, (nodes, backreferences)
except Exception as e:
# print('Reconnction 2 failure:', file=sys.stderr)
# print(e, file=sys.stderr)
# print(nodes, file=sys.stderr)
# print(backreferences, file=sys.stderr)
# print(graph_, file=sys.stderr)
return postprocessing.BACKOFF, postprocessing.ParsedStatus.BACKOFF, (nodes, backreferences)

def _fix_and_make_graph(self, nodes):

nodes_ = []
for n in nodes:
if isinstance(n, str):
if n.startswith('<') and n.endswith('>') and (not n.startswith('<pointer:')):
pass
else:
nodes_.append(n)
else:
nodes_.append(n)
nodes = nodes_

if True:
i = 0
nodes_ = []
while i < len(nodes):
nxt = nodes[i]
pst = None
if isinstance(nxt, str) and nxt.startswith('<pointer:'):
e = nxt.find('>')
if e != len(nxt) -1:
pst = nxt[e+1:]
nxt = nxt[:e+1]
nodes_.append(nxt)
if pst is not None:
nodes_.append(pst)
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 1
nodes_ = [nodes[0]]
while i < len(nodes):
nxt = nodes[i]
if isinstance(nxt, str) and nxt.startswith('<pointer:'):
nxt = 'z' + nxt[9:-1]
fol = nodes[i+1]
# is not expansion
if isinstance(fol, str) and (fol.startswith(':') or (fol == ')')):
nodes_.append(nxt)
else:
if self.remove_pars:
nodes_.append('(')
else:
if nodes_[-1] != '(':
nodes_.append('(')
#pass
nodes_.append(nxt)
nodes_.append('/')
else:
nodes_.append(nxt)
i += 1
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes) - 1):
if nodes[i] == ':':
nodes_.append(nodes[i] + nodes[i+1])
i += 2
last = False
else:
nodes_.append(nodes[i])
i += 1
last = True
if last:
nodes_.append(nodes[-1])
nodes = nodes_

i = 0
nodes_ = []
while i < (len(nodes)):
if i < 2:
nodes_.append(nodes[i])
i += 1
elif nodes_[-2] == '/' and nodes[i] == '/':
i += 2
else:
nodes_.append(nodes[i])
i += 1
nodes = nodes_

i = 0
newvars = 0
variables = set()
remap = {}
nodes_ = []
while i < (len(nodes)):

next = nodes[i]

if next == '/':
last = nodes_[-1]
if last in variables:
last_remap = f""z{newvars+1000}""
newvars += 1
nodes_[-1] = last_remap
remap[last] = last_remap
variables.add(last)
nodes_.append(next)

elif self._classify(next) == 'VAR' and next in remap and (i < len(nodes) - 1) and nodes[i+1] != '/':
next = remap[next]
nodes_.append(next)

else:
nodes_.append(next)

i += 1

nodes = nodes_
pieces_ = []
open_cnt = 0
closed_cnt = 0
if nodes[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in nodes:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
nodes = pieces_ + [')'] * (open_cnt - closed_cnt)

pieces = []
for piece in nodes:
if not pieces:
pieces.append('(')
else:
piece = str(piece)
if piece.startswith('""') or piece.startswith('""') or '""' in piece.strip('""'):
piece = '""' + piece.replace('""', '') + '""'

prev = self._classify(pieces[-1])
next = self._classify(piece)

if next == 'CONST':
quote = False
for char in (',', ':', '/', '(', ')', '.', '!', '?', '\\', '_', '='):
if char in piece:
quote = True
break
if quote:
piece = '""' + piece.strip('""') + '""'

if  prev == '(':
if next in ('VAR', 'I'):
pieces.append(piece)
elif prev == ')':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'VAR':
if next in ('/', 'EDGE', 'MODE', ')'):
pieces.append(piece)
elif prev == '/':
if next in ('INST', 'I'):
pieces.append(piece)
elif prev == 'INST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'I':
if next in ('/', ')', 'EDGE', 'MODE'):
pieces.append(piece)
elif prev == 'EDGE':
if next in ('(', 'VAR', 'CONST', 'I'):
pieces.append(piece)
elif next == ')':
pieces[-1] = piece
elif next in ('EDGE', 'MODE'):
pieces[-1] = piece
elif prev == 'MODE':
if next == 'INST':
pieces.append(piece)
elif prev == 'CONST':
if next in (')', 'EDGE', 'MODE'):
pieces.append(piece)

pieces_ = []
open_cnt = 0
closed_cnt = 0
if pieces[0] != '(':
pieces_.append('(')
open_cnt += 1
for p in pieces:
if p == '(':
open_cnt += 1
elif p == ')':
closed_cnt += 1
pieces_.append(p)
if open_cnt == closed_cnt:
break
pieces = pieces_ + [')'] * (open_cnt - closed_cnt)

linearized = re.sub(r'\s+', ' ', ' '.join(pieces)).strip()

, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copied from https://github.com/RikVN/AMR/blob/master/restoreAMR/restore_amr.py, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # make sure parentheses match, # of this software and associated documentation files (the ""Software""), to deal, # print(f""Added {self.modified} AMR tokens""), # this is a simplified version of ""https://github.com/SapienzaNLP/spring/blob/main/spring_amr/tokenization_bart.py"", # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",Ġ
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-03 20:33",
https://github.com/hankcs/HanLP,amr_io.py,0,0.0,20,39.22,8,15.69,2,3.92,21,41.18,0,0.0,1,0,6,1,,"dereify, endswith, eval, extend, glob, graphs, hanlp, isinstance, metadata, path_, pathlib, paths, pm_load, read_raw_amr_data, remove_wiki, startswith, str, tokens, typing, use_recategorization","glob.glob, hanlp.components.amr.amrbart.preprocess.penman_interface.load, pathlib.Path, typing.Iterable, typing.List, typing.Union",read_raw_amr_data,,"graphs, metadata, path, path_, paths, tokens","code for loading AMR from a set of files
- use_recategorization: use graph recategorization trick
- dereify: Dereify edges in g that have reifications in model.
- remove_wiki: remove wiki links"," ,  code for loading AMR from a set of files
        - use_recategorization: use graph recategorization trick
        - dereify: Dereify edges in g that have reifications in model.
        - remove_wiki: remove wiki links
    , -, -L, -R, snt, snt_orig, tokens","False, True"," code for loading AMR from a set of files
- use_recategorization: use graph recategorization trick
- dereify: Dereify edges in g that have reifications in model.
- remove_wiki: remove wiki links
, #, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # the code is migrated from https://github.com/SapienzaNLP/spring, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,penman_interface.py,0,0.0,33,57.89,2,3.51,3,5.26,19,33.33,0,0.0,5,0,13,0,,"DEFAULT, _get_model, _remove_wiki, amr_model, append, compact, dereify, encode, encode_, g, graph, indent, len, load, load_, loads, loads_, metadata, model, noop_model, op_model, out, penman, range, rel, remove_wiki, source, string, t, top, triples, v1, v2","penman.Graph, penman.Triple, penman.encode, penman.load, penman.loads, penman.model.Model, penman.models.amr, penman.models.noop.NoOpModel","_get_model, _remove_wiki, encode, load, loads",,"DEFAULT, amr_model, graph, metadata, model, noop_model, op_model, out, rel, t, triples, v1, v2",,"+, :wiki","1, False, None","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,read_and_process.py,0,0.0,56,47.46,38,32.2,4,3.39,20,16.95,0,0.0,3,0,24,0,,"__name__, _tokenize_encoded_graph, add_argument, append, argparse, args, config, copy, dfs_linearize, encoded, endswith, fout, graph, graph_, graphs, hanlp, input_file, json, lamr, len, lin_tokens, line_amr, linearized, linearized_nodes, linearized_nodes_, lst, main, metadata, nxt, open, output_prefix, parse_known_args, parser, pathlib, penman, piece, pieces, print, range, re, remap, remove_pars, replace, res_out, sent, sentences, split, startswith, str, strip, tqdm, unknown, use_pointer_tokens, write, yaml, zip","argparse.ArgumentDefaultsHelpFormatter, argparse.ArgumentParser, copy.deepcopy, hanlp.components.amr.amrbart.preprocess.amr_io.read_raw_amr_data, json.dumps, pathlib.Path, penman.encode, re.sub, tqdm.tqdm, yaml.FullLoader, yaml.load","_tokenize_encoded_graph, dfs_linearize, main",,"args, config, fout, graph_, graphs, i, lamr, lin_tokens, line_amr, linearized, linearized_nodes, linearized_nodes_, lst, nxt, parser, piece, pieces, remap, remove_pars, res_out, sent, sentences, unknown, use_pointer_tokens",,"
,  ,  ( ,  ) ,  / ,  :,  AMRs processed,  \1 , "", (, (\"".+?\""), ), --config, --input_file, --output_prefix, .amr, .jsonl, .txt, /, :, <pointer:, >, AMR processing script, The input AMR file., The output_prefix., Use the following config for hparams., \s+, __main__, all , amr, default.yaml, dereify, remove_wiki, sent, snt, use_recategorization, utf-8, w","0, 1, False, True","#, # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER, # Copyright (c) 2022 xfbai, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE, # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,, # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,, # MIT License, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE, # Permission is hereby granted, free of charge, to any person obtaining a copy, # SOFTWARE., # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR, # The above copyright notice and this permission notice shall be included in all, # coding:utf-8, # copies of the Software, and to permit persons to whom the Software is, # copies or substantial portions of the Software., # furnished to do so, subject to the following conditions:, # in the Software without restriction, including without limitation the rights, # line_amr.append("" "".join(lin_tokens[1:-1])), # of this software and associated documentation files (the ""Software""), to deal, # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",
https://github.com/hankcs/HanLP,__init__.py,0,0.0,0,0.0,0,0.0,0,0.0,3,100.0,0,0.0,0,0,0,0,,,,,,,,,,"# -*- coding:utf-8 -*-, # Author: hankcs, # Date: 2022-12-03 20:33",
